Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 39?47,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsThe First QALB Shared Task on Automatic Text Correction for ArabicBehrang Mohit1?, Alla Rozovskaya2?, Nizar Habash3, Wajdi Zaghouani1, Ossama Obeid11Carnegie Mellon University in Qatar2Center for Computational Learning Systems, Columbia University3New York University Abu Dhabibehrang@cmu.edu, alla@ccls.columbia.edu, nizar.habash@nyu.eduwajdiz@qatar.cmu.edu,owo@qatar.cmu.eduAbstractWe present a summary of the first sharedtask on automatic text correction for Ara-bic text.
The shared task received 18 sys-tems submissions from nine teams in sixcountries and represented a diversity of ap-proaches.
Our report includes an overviewof the QALB corpus which was the sourceof the datasets used for training and eval-uation, an overview of participating sys-tems, results of the competition and ananalysis of the results and systems.1 IntroductionThe task of text correction has recently gained alot of attention in the Natural Language Process-ing (NLP) community.
Most of the effort in thisarea concentrated on English, especially on errorsmade by learners of English as a Second Lan-guage.
Four competitions devoted to error cor-rection for non-native English writers took placerecently: HOO (Dale and Kilgarriff, 2011; Daleet al., 2012) and CoNLL (Ng et al., 2013; Ng etal., 2014).
Shared tasks of this kind are extremelyimportant, as they bring together researchers whofocus on this problem and promote developmentand dissemination of key resources, such as bench-mark datasets.Recently, there have been several efforts aimedat creating data resources related to the correc-tion of Arabic text.
Those include human anno-tated corpora (Zaghouani et al., 2014; Alfaifi andAtwell, 2012), spell-checking lexicon (Attia et al.,2012) and unannotated language learner corpora(Farwaneh and Tamimi, 2012).
A natural exten-sion to these resource production efforts is the cre-ation of robust automatic systems for error correc-tion.
* These authors contributed equally to this work.In this paper, we present a summary of theQALB shared task on automatic text correctionfor Arabic.
The Qatar Arabic Language Bank(QALB) project1is one of the first large scale dataand system development efforts for automatic cor-rection of Arabic which has resulted in annota-tion of the QALB corpus.
In conjunction with theEMNLP Arabic NLP workshop, the QALB sharedtask is the first community effort for constructionand evaluation of automatic correction systems forArabic.The results of the competition indicate that theshared task attracted a lot of interest and generateda diverse set of approaches from the participatingteams.In the next section, we present the shared taskframework.
This is followed by an overview ofthe QALB corpus (Section 3).
Section 4 describesthe shared task data, and Section 5 presents the ap-proaches adopted by the participating teams.
Sec-tion 6 discusses the results of the competition.
Fi-nally, in Section 7, we offer a brief analysis andpresent preliminary experiments on system com-bination.2 Task DescriptionThe QALB shared task was created as a forum forcompetition and collaboration on automatic errorcorrection in Modern Standard Arabic.
The sharedtask makes use of the QALB corpus (Zaghouani etal., 2014), which is a manually-corrected collec-tion of Arabic texts.
The shared task participantswere provided with training and development datato build their systems, but were also free to makeuse of additional resources, including corpora, lin-guistic resources, and software, as long as thesewere publicly available.For evaluation, a standard framework devel-1http://nlp.qatar.cmu.edu/qalb/39Original Corrected?
?K @Q?
@HCJ?jJ?
@?Y??K @Q?
YJ?
?GXA??
?Y?
@?P?
?JK B?X?@?
@ ?<?
@??
???JK.IJ?
?
H.A?
?G @B??Qj??
@ ?YJ?K.@Y??
@ @?YJ.K?A?
?
??
??B@ Yj.???AK.@P?Q??Q???@????
?K @ ???JK.
?A??JJ?B@ ??
?Yg ??
A?
???
?AJ??@.??Jj??
?JJ?@?BA?
???m'?XA?k@ XA?k@?
@ ???JK?
?K @Q?
@HCJ?jJ?
@ ?Y??Z @Q?
YJ?
?GXA??
?Y?
@?P?
?JK B?Q???
@ ?X?@?
@ ?<?
@??
????@IJ??
H.A?
?
?KB .??Qj??
@?, ?AJ??
@ YJ?K.@Y??
@ ?YJ.K?A??
, ??
??B@ Yj.???AK.@P?Q??
@ ???JK?@????
?K@ ?
??K?A??JJ?B@ ???Yg@?
???.??Jj??
?JJ?
@?B A?
???m'?XA?k@ XA?k@lA ttSwrwA mdy1s?Adty ?nd qrA?yh?2h?h?3AltHlylAt AlrA?y?h?
w AlmHtrmh?4l?Any6?Abw knt7btmny8mn Allh An9?
?wdy Al?mrh?
mr-wrA bAlmsjd AlAqSy10w kAn12ybdwA13An14h?A b?yd AlmnAl fkl mA16fy17Hd18ysm?AlAmnyh?19kAn byqwl20Ank21mmkn ttmny23An24?HfAd ?HfAdk yHqqwhAl?n25Amnytk26mstHylh?.lA ttSwrwA md?1s?Adty ?nd qrA?h?2h?h3AltHlylAt AlrA?y?h?
wAlmHtrmh?4.5l?nny6?Abwknt7?tmn?8mn Allh ?n9?
?wdy Al?mrh?mrwrA bAlmsjd Al?qS?10,11wkAn12ybdw13?n14h?A b?yd AlmnAl,15fkl wAHd18ysm?Al?mnyh?19kAn yqwl20?nk21mmkn ?n22ttmn?23?n24?HfAd ?HfAdk yHqqwhA l?n25?mnytk26mstHylh?.TranslationYou cannot imagine the extent of my happiness when I read these wonderful and respectful analysesbecause I am a young man and I wish from God to perform Umrah passing through the Al-AqsaMosque; and it seemed that this was elusive that when anyone heard the wish, he would say that youcan wish that your great grandchildren may achieve it because your wish is impossible.Table 1: A sample of an original (erroneous) text along with its manual correction and English translation.The indices in the table are linked with those in Table 2 and the Appendix.# Error Correction Error Type Correction Action#1 mdy ?Y?
md?
?Y?
Ya/Alif-Maqsura Spelling Edit#9 An?
@ ?n?
@ Alif-Hamza Spelling Edit#11 Missing Comma , Punctuation Add_before#12 w kAn?A?
?
wkAn?A??
Extra Space Merge#13 ybdwA @?YJ.Kybdw ?YJ.KMorphology Edit#20 byqwl ?
??JK.yqwl ?
??KDialectal Edit#25 yHqqwhAl?n?BA?
???m'yHqqwhA l?n?B A?
???m'Missing Space SplitTable 2: Error type and correction action for a few examples extracted from the sentence pair in Table 1.The indices are linked to those in Table 1 and the Appendix.oped for similar error correction competitions isadopted: system outputs are compared againstgold annotations using Precision, Recall and F1.Systems are ranked based on the F1scores ob-tained on the test set.After the initial registration, the participantswere provided with training and development setsand evaluation scripts.
During the test period, theteams were given test data on which they neededto run their systems.
Following the announcementof system results, the answer key to the test set wasreleased.
Participants authored description paperswhich will be presented in the Arabic NLP work-shop.3 The QALB CorpusOne of the goals of the QALB project is to createa large manually corrected corpus of errors for avariety of Arabic texts, including user commentson news web sites, native and non-native speakeressays, and machine translation output.
Within theframework of this project, comprehensive annota-tion guidelines and a specialized web-based anno-tation interface have been developed (Zaghouaniet al., 2014; Obeid et al., 2013).The annotation process includes an initial au-tomatic pre-processing step followed by an auto-matic correction of common spelling errors by the40morphological analysis and disambiguation sys-tem MADA (v3.2) (Habash and Rambow, 2005;Habash et al., 2009).
The pre-processed files arethen assigned to a team of expert (human) annota-tors.For a given sentence, the annotators wereinstructed to correct all errors; these includespelling, punctuation, word choice, morphology,syntax, and dialectal usage.
It should be noted thatthe error classification was only used for guidingthe annotation process; the annotators were not in-structed to mark the type of error but only neededto specify an appropriate correction.Once the annotation was complete, the correc-tions were automatically grouped into the follow-ing seven action categories based on the actionrequired to correct the error: {Edit, Add, Merge,Split, Delete, Move, Other}.2Table 1 presentsa sample erroneous Arabic news comment alongwith its manually corrected form, its romanizedtransliteration,3and the English translation.
Theerrors in the original and the corrected forms areunderlined and co-indexed.
Table 2 presents a sub-set of the errors from the example shown in Table 1along with the error types and annotation actions.The Appendix at the end of the paper lists all an-notation actions for that example.To ensure high annotation quality, the annota-tors went through multiple phases of training; theinter-annotator agreement was reviewed routinely.Zaghouani et al.
(2014) report an average WordError Rate (WER) of 3.8% for all words (exclud-ing punctuation), which is quite high.
When punc-tuation was included, the WER rose to 11.3%.
Thehigh level of agreement indicates that the anno-tations are reliable and the guidelines are usefulin producing homogeneous and consistent data.Punctuation, however, remains a challenge.4 Shared Task DataThe shared task data comes from the QALB cor-pus and consists of user comments from the Al-jazeera News webpage written in Modern Stan-dard Arabic.Comments belonging to the same article were2In the shared task, we specified two Add categories: ad-dBefore and addAfter.
Most of the add errors fall into thefirst category, and we combine these here into a single Addcategory.3Arabic transliteration is presented in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007): (in alphabeticalorder) Abt?jHxd?rzs?SDT?D?
?fqklmnhwy and the additionalsymbols: ?
Z, ?
@,?A @,?A@, ?w?
', ?y Z?
', h??, ?
?.Statistics Train.
Dev.
TestNumber of docs.
19,411 1,017 968Number of words 1M 54K 51KNumber of errors 306K 16K 16KTable 3: Statistics on the shared task data.included only in one of the shared task subsets(i.e., training, development or test).
Furthermore,we split the data by the annotation time.
Conse-quently, the training data is comprised of com-ments annotated between June and December,2013; the development data includes texts anno-tated in December 2013; the test data includesdocuments annotated in the Spring of 2014.We refer to each comment in the shared taskdata as document and assign it a special ID thatindicates the ID of the article to which the com-ment refers and the comment?s number.The data was made available to the participantsin three versions: (1) plain text, one document perline; (2) text with annotations specifying errorsand the corresponding corrections; (3) feature filesspecifying morphological information obtained byrunning MADAMIRA, a tool for morphologicalanalysis and disambiguation of Modern StandardArabic (Pasha et al., 2014).
MADAMIRA per-forms morphological analysis and contextual dis-ambiguation.
Using the output of MADAMIRA,we generated for each word thirty-three features.The features specify various properties: the part-of-speech (POS), lemma, aspect, person, gender,number, and so on.Among its features, MADAMIRA producesforms that correct a large subset of a special classof spelling mistakes in words containing the let-ters Alif and final Ya.
These letters are a source ofthe most common spelling types of spelling errorsin Arabic and involve Hamzated Alifs and Alif-Maqsura/Ya confusion (Habash, 2010; El Kholyand Habash, 2012).
We refer to these errors asAlif/Ya errors (see also Section 6).Table 3 presents statistics on the shared taskdata.
The training data contains over one mil-lion words of text; the development and test datacontain slightly over 50,000 words each.
Table 4shows the distribution of annotations by the actiontype.
The majority of corrections (over 50%) be-long to the type Edit.
This is followed by mistakesthat require several words to be merged together(about a third of all errors).41DataError type (%)Edit Add Merge Split Delete Move OtherTrain.
55.34 32.36 5.95 3.48 2.21 0.14 0.50Dev.
53.51 34.24 5.97 3.67 2.03 0.08 0.49Test 51.94 34.73 5.89 3.48 3.32 0.15 0.49Table 4: Distribution of annotations by type in the shared task data.
Error types denotes the actionrequired in order to correct the error.Team Name AffiliationCLMB (Rozovskaya et al., 2014) Columbia University (USA)CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar)CP13 (Tomeh et al., 2014) Universit?
Paris 13 (France)CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University(Egypt)GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Sci-ence Engineering School (Algeria), and Tabuk University(KSA)GWU (Attia et al., 2014) George Washington University (USA)QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar)TECH (Mostefa et al., 2014) Techlimed.com (France)YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt)Table 5: List of the nine teams that participated in the shared task.Team Approach External ResourcesCLMBCorrections proposed by MADAMIRA; a Maximum Likeli-hood model trained on the training data; regular expressions;a decision-tree classifier for punctuation errors trained on thetraining data; an SVM character-level error correction model; aNa?ve Bayes classifier trained on the training data and the Ara-bic Gigaword corpusArabic Gigaword Fourth Edition (Parker et al.,2009)CMUQA pipeline consisting of rules, corrections proposed byMADAMIRA, a language model for spelling mistakes, and astatistical machine-translation systemAraComLex dictionary (Attia et al., 2012)CP13A pipeline that consists of an error detection SVM clas-sifier that uses MADAMIRA features and language modelscores; a character-level back-off correction model imple-mented as a weighted finite-state transducer; a statisticalmachine-translation system; a discriminative re-ranker; a de-cision tree classifier for inserting missing punctuationNoneCUFERules extracted from the Buckwalter morphological analyser;their probabilities are learned using the training dataBuckwalter morphological analyzer Version 2.0(Buckwalter, 2004)GLTW Regular expressions and word listsAraComLex dictionary (Attia et al., 2012); in-house resources; Ayaspell dictionaryGWUA CRF model for punctuation errors; a dictionary and a lan-guage model for spelling errors; normalization rulesAraComLex Extended dictionary (Attia et al.,2012); Arabic Gigaword Fourth Edition (Parkeret al., 2009)QCRIWord errors: a language model trained on Arabic Wikipediaand Aljazeera data; punctuation mistakes: a CRF model and afrequency-based model trained on the shared task dataArabic Wikipedia; Aljazeera articlesTECHOff-the-shelf spell checkers and a statistical machine-translation modelNewspaper articles from Open Source ArabicCorpora; other corpora collected online; Hun-spellYAMEdit errors: a Na?ve Bayes classifier that uses the following fea-tures: a character confusion matrix based on the training data; acollocation model that uses the target lemma and the surround-ing POS tags; a co-occurrence model that uses lemmata of thesurrounding words; Split and Merge errors: a language modeltrained on the training data; Add errors: a classifierAraComLex dictionary (Attia et al., 2012);Buckwalter Analyzer Version 1.0 (Buckwalter,2002); Arabic stoplistsTable 6: Approaches adopted by the participating teams.425 Participants and ApproachesNine teams from six countries participated in theshared task.
Table 5 presents the list of partici-pating institutions and their names in the sharedtask.
Each team was allowed to submit up tothree outputs.
Overall, we received eighteen out-puts.
The submitted systems included a diverseset of approaches that incorporated rule-basedframeworks, statistical machine translation andmachine learning models, as well as hybrid sys-tems.
The teams that scored at the top employeda variety of techniques and attempted to classifythe errors in some way using that classificationin developing their systems: the CLMB systemcombined machine-learning modules with rulesand MADAMIRA corrections; the CUFE systemextracted rules from the morphological analyzerand learned their probabilities using the trainingdata; and the CMUQ system combined statisticalmachine-translation with a language model, rules,and MADAMIRA corrections.
Table 6 summa-rizes the approaches adopted by each team.6 ResultsIn this section, we present the results of the com-petition.
For evaluation, we adopted the stan-dard Precision (P), Recall (R), and F1metric thatwas used in recent shared tasks on grammaticalerror correction in English: HOO competitions(Dale and Kilgarriff, 2011; Dale et al., 2012) andCoNLL (Ng et al., 2013).
The results are com-puted using the M2 scorer (Dahlmeier and Ng,2012) that was also used in the CoNLL sharedtasks.Table 7 presents the official results of the evalu-ation on the test set.
The results are sorted accord-ing to the F1scores obtained by the systems.
Therange of the scores is quite wide ?
from 20 to 67F1?
but the the majority of the systems stay in the50-60 range.It is interesting to note that these results are con-siderably higher than those shown on the similarshared tasks on English non-native data.
For in-stance, the highest performance in the CoNLL-2013 shared task that also used the same evalua-tion metric was 31.20 (Rozovskaya et al., 2013).4The highest score in the HOO-2011 shared task(Dale and Kilgarriff, 2011) that addressed all er-4This year CoNLL was an extension of the CoNLL-2013competition for all errors but in its evaluation favored preci-sion twice as much as recall, so we are not comparing to thissetting.rors was 21.1 (Rozovskaya et al., 2011).
Ofcourse, the setting was different,as we are deal-ing with texts written by native speakers.
But, inaddition to that, we hypothesize that our data con-tains a set of language-specific errors that may be?easier?, e.g Alif/Ya errors.We also asked the participants for the outputs oftheir systems on the development set.
We show theresults in Table 8.
While these results are not usedfor ranking, since the development set was usedfor tuning the parameters of the systems, it is in-teresting to see how much the performance differsfrom the results obtained on the test.
In general,we do not observe substantial differences in theperformance and the rankings, with a few excep-tions.
In particular, CP13 submissions did muchbetter on the development set, as well as the CUFEsystem: the CUFE system suffers a major drop inprecision on the test set, while the CP13 systemslose in recall.
For more details, we refer the readerto the system description papers.In addition to the official rankings, it is also in-teresting to analyze system performance for dif-ferent types of mistakes.
Note that here we arenot interested in the annotation classification byaction type.
Instead, we automatically assign er-rors to one of the following categories: punctu-ation errors;errors involving Alif and Ya; and allother errors.
Punctuation errors account for 39%of all errors in the data5.
Table 7 shows the perfor-mance of the teams in three settings: with punctu-ation errors removed; with Alif /Ya errors removed;and when both punctuation and Alif /Ya errors areremoved.
Observe that when punctuation errorsare not taken into account, the CUFE team getsthe first ranking (for each the results of the best-performing system were chosen).7 Analysis of System OutputWe conducted a couple of experiments to analyzethe task challenges and system errors.The Most and Least Challenging SentencesWe examined some of the most, and the least chal-lenging parts of the test data for the shared tasksystems.
To identify these subsets, we ranked allsentences using their average sentence-level F1score and selected the top and bottom 50 sen-tences.
Our manual examination of these two5For example, there are a lot of missing periods at the endof a sentence that may be due to the fact that the data wascollected online.43Rank Team P R F11 CLMB-1 73.34 63.23 67.912 CLMB-2 70.86 62.21 66.253 CUFE-1 87.49 52.63 65.734 CMUQ-1 77.97 56.35 65.425 CLMB-3 71.45 60.00 65.226 QCRI-1 71.70 56.86 63.437 GWU-1 75.47 52.98 62.258 GWU-2 75.34 52.99 62.229 QCRI-2 62.86 60.32 61.5710 YAM-1 63.52 57.61 60.4211 QCRI-3 60.66 59.28 59.9612 TECH-1 73.46 50.56 59.8913 TECH-2 73.50 50.53 59.8814 TECH-3 72.34 50.51 59.4915 CP13-2 76.85 47.33 58.5816 CP13-1 77.85 38.77 51.7617 GLTW-1 75.15 23.15 35.4018 GLTW-2 69.80 12.33 20.96Table 7: Official results on the test set.
Column 1shows the system rank according to the F1score.sets shows that the differences between them re-late to both the density and the type of errors.The more challenging sentences (with the lowestsystem performance) contain more errors in gen-eral, and their errors tend to be complex and chal-lenging, e.g., the correction of the erroneous two-token stringI?HX@ (?dt ?t) requires a charac-ter deletion and a merge to produceI?X@ (Ad?t).In contrast the less challenging sentences tend tohave fewer and simpler errors such as the commonAlif/Ya errors.System Combination We took the 18 systems?output and conducted two system combination ex-periments: (a) an oracle upper-bound estimationand (b) a simple majority vote system combina-tion.
For these experiments we isolated and eval-uated each sentence output individually to form anew combined system output.In the oracle experiment, we combined differ-ent systems by selecting the output of the best per-forming system for each individual sentence.
Forthat, we evaluated sentences individually for eachsystem and chose the system output with the high-est F1score.
The combined output holds the bestoutput of all systems for the test set.
This is anoracle system combination which allows us to es-timate an upper-bound combination of all 18 sys-tems.Rank (test) Rank (dev) Team P R F11 2 CLMB-1 72.22 62.79 67.182 3 CLMB-2 69.49 61.73 65.383 1 CUFE-1 94.11 53.74 68.424 4 CMUQ-1 76.17 56.59 64.945 5 CLMB-3 69.71 59.42 64.156 6 QCRI-1 70.83 57.34 63.387 9 GWU-1 73.15 53.18 61.598 10 GWU-2 73.01 53.13 61.509 8 QCRI-2 62.21 61.30 61.7510 14 YAM-1 57.81 59.19 58.4911 12 QCRI-3 60.47 60.65 60.5612 13 TECH-1 70.86 50.04 58.6613 15 TECH-2 70.66 49.65 58.3214 16 TECH-3 70.65 48.83 57.7515 7 CP13-2 74.85 54.15 62.8416 11 CP13-1 75.73 51.33 61.1917 17 GLTW-1 73.83 22.80 34.8418 18 GLTW-2 67.85 11.09 19.06Table 8: Results on the development set.Columns 1 and 2 show the rank of the system ac-cording to F1score obtained on the test set shownin Table 7 and the development set, respectively.In the majority vote experiment, we combinedsystem output based on majority vote of varioussystems at sentence level.
For every sentence,we choose the output that is agreed by most sys-tems.
If all systems have different output for a sen-tence, we back-off to the best performing system(CLMB-1).Table 10 compares the results of these twoexperiments against the best performing system(CLMB-1).
We observe a large boost of perfor-mance in the oracle experiment.
This promis-ing result reflects the complementary nature of thedifferent methods that have been applied to theshared task, and it motivates further research onsystem combination.
The result for the majority-vote system combination is very close to theCLMB-1?s performance.
This is not surpris-ing; since, for 92% of sentences, there was nosentence-level agreement among systems.
As a re-sult, the combined system was very close to theback-off CLMB-1 system.8 ConclusionWe have described the framework and results ofthe first shared task on automatic correction ofArabic, which used data from the QALB corpus.The shared task received 18 systems submissions44TeamNo punc.
errors No Alif/Ya errors No punc.No Alif/Ya errorsP R F1P R F1P R F1CLMB-1 82.63 72.50 77.24 64.05 50.86 56.70 76.99 49.91 60.56CMUQ-1 82.89 68.69 75.12 68.32 40.51 50.86 74.25 41.46 53.21CP13-2 80.51 59.97 68.74 65.09 28.00 39.16 68.67 25.34 37.02CUFE-1 85.22 78.79 81.88 83.34 36.21 50.48 80.63 63.25 70.89GLTW-1 65.18 34.84 45.41 48.52 15.29 23.26 49.25 26.78 34.70GWU-1 76.28 64.17 69.70 64.67 39.61 49.13 59.07 41.48 48.74QCRI-1 76.74 74.93 75.82 59.66 41.90 49.23 63.22 55.10 58.88TECH-1 81.23 62.99 70.95 59.39 34.59 43.72 64.93 35.69 46.06YAM-1 77.38 63.99 70.05 50.77 43.43 46.81 64.63 34.71 45.17Table 9: Results on the test set in different settings: with punctuation errors removed from evaluation;normalization errors removed; and when both punctuation and normalization errors are removed.
Onlythe best output from each team is shown.System Precision Recall F1Oracle 83.25 68.72 75.29Majority-Vote 73.96 62.88 67.97CLMB-1 73.34 63.23 67.91Table 10: Comparing the best performing systemwith two experimental hybrid systems.from nine teams in six countries.
We are pleasedwith the extent of participation, the quality of re-sults and the diversity of approaches.
We plan torelease the output of all systems.
Such dataset andall the methods used in this shared task are ex-pected to introduce new directions in automaticcorrection of Arabic.
We feel motivated to ex-tend the shared task?s framework and text domainto conduct new research competitions in the nearfuture.9 AcknowledgmentsWe would like to thank the organizing commit-tee of EMNLP-2014 and its Arabic NLP work-shop and also the shared task participants for theirideas and support.
We thank Al Jazeera News (andespecially, Khalid Judia) for providing the usercomments portion of the QALB corpus.
We alsothank the QALB project annotators: Hoda Fathy,Dhoha Abid, Mariem Fekih, Anissa Jrad, HodaIbrahim, Noor Alzeer, Samah Lakhal, Jihene Wefi,Elsherif Mahmoud and Hossam El-Husseini.
Thispublication was made possible by grants NPRP-4-1058-1-168 and YSREP-1-018-1-004 from theQatar National Research Fund (a member of theQatar Foundation).
The statements made hereinare solely the responsibility of the authors.
NizarHabash performed most of his contribution to thispaper while he was at the Center for Computa-tional Learning Systems at Columbia University.ReferencesA.
Alfaifi and E. Atwell.
2012.
Arabic Learner Cor-pora (ALC): A Taxonomy of Coding Errors.
In The8th International Computing Conference in Arabic.M.
Attia, P. Pecina, Y. Samih, K. Shaalan, and J. vanGenabith.
2012.
Improved Spelling Error Detectionand Correction for Arabic.
In Proceedings of COL-ING.M.
Attia, M. Al-Badrashiny, and M. Diab.
2014.GWU-HASP: Hybrid Arabic Spelling and Punctua-tion Corrector.
In Proceedings of EMNLP Workshopon Arabic Natural Language Processing: QALBShared Task.T.
Buckwalter.
2002.
Buckwalter Arabic Morpho-logical Analyzer Version 1.0.T.
Buckwalter.
2004.
Buckwalter Arabic Morpho-logical Analyzer Version 2.0.D.
Dahlmeier and H. T. Ng.
2012.
Better Evaluationfor Grammatical Error Correction.
In Proceedingsof NAACL.R.
Dale and A. Kilgarriff.
2011.
Helping Our Own:The HOO 2011 Pilot Shared Task.
In Proceedings ofthe 13th European Workshop on Natural LanguageGeneration.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
A Re-port on the Preposition and Determiner Error Cor-rection Shared Task.
In Proceedings of the NAACLWorkshop on Innovative Use of NLP for BuildingEducational Applications.45A.
El Kholy and N. Habash.
2012.
Orthographic andmorphological processing for English?Arabic sta-tistical machine translation.
Machine Translation,26(1-2).S.
Farwaneh and M. Tamimi.
2012.
Arabic Learn-ers Written Corpus: A Resource for Research andLearning.
The Center for Educational Resources inCulture, Language and Literacy.N.
Habash and O. Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and Morphologi-cal Disambiguation in One Fell Swoop.
In Proceed-ings of ACL.N.
Habash, A. Soudi, and T. Buckwalter.
2007.On Arabic Transliteration.
In A. van den Boschand A. Soudi, editors, Arabic Computational Mor-phology: Knowledge-based and Empirical Methods.Springer.N.
Habash, O. Rambow, and R. Roth.
2009.MADA+TOKAN: A Toolkit for Arabic Tokeniza-tion, Diacritization, Morphological Disambiguation,POS Tagging, Stemming and Lemmatization.
InProceedings of the Second International Conferenceon Arabic Language Resources and Tools.N.
Habash.
2010.
Introduction to Arabic Natural Lan-guage Processing.
Morgan & Claypool Publishers.Y.
Hassan, M. Aly, and A. Atiya.
2014.
ArabicSpelling Correction using Supervised Learning.
InProceedings of EMNLP Workshop on Arabic Natu-ral Language Processing: QALB Shared Task.S.
Jeblee, H. Bouamor, W. Zaghouani, and K. Oflazer.2014.
CMUQ@QALB-2014: An SMT-based Sys-tem for Automatic Arabic Error Correction.
In Pro-ceedings of EMNLP Workshop on Arabic NaturalLanguage Processing: QALB Shared Task.D.
Mostefa, O. Asbayou, and R. Abbes.
2014.
TECH-LIMED System Description for the Shared Task onAutomatic Arabic Error Correction.
In Proceedingsof EMNLP Workshop on Arabic Natural LanguageProcessing: QALB Shared Task.H.
Mubarak and K. Darwish.
2014.
Automatic Cor-rection of Arabic Text: a Cascaded Approach.
InProceedings of EMNLP Workshop on Arabic Natu-ral Language Processing: QALB Shared Task.M.
Nawar and M. Ragheb.
2014.
Fast and RobustArabic Error Correction System.
In Proceedingsof EMNLP Workshop on Arabic Natural LanguageProcessing: QALB Shared Task.H.
T. Ng, S. M. Wu, Y. Wu, Ch.
Hadiwinoto, andJ.
Tetreault.
2013.
The CoNLL-2013 Shared Taskon Grammatical Error Correction.
In Proceedingsof CoNLL: Shared Task.H.
T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.Susanto, and C. Bryant.
2014.
The CoNLL-2014Shared Task on Grammatical Error Correction.
InProceedings of CoNLL: Shared Task.O.
Obeid, W. Zaghouani, B. Mohit, N. Habash,K.
Oflazer, and N. Tomeh.
2013.
A Web-based An-notation Framework For Large-Scale Text Correc-tion.
In The Companion Volume of the Proceedingsof IJCNLP 2013: System Demonstrations.
AsianFederation of Natural Language Processing.R.
Parker, D. Graff, K. Chen, J. Kong, and K. Maeda.2009.
Arabic Gigaword Fourth Edition.
LDC Cata-log No.
: LDC2009T30, ISBN: 1-58563-532-4.A.
Pasha, M. Al-Badrashiny, M. Diab, A. El Kholy,R.
Eskander, N. Habash, M. Pooleery, O. Rambow,and R. Roth.
2014.
MADAMIRA: A Fast, Compre-hensive Tool for Morphological Analysis and Dis-ambiguation of Arabic.
In Proceedings of the NinthInternational Conference on Language Resourcesand Evaluation (LREC).A.
Rozovskaya, M. Sammons, J. Gioja, and D. Roth.2011.
University of Illinois System in HOO TextCorrection Shared Task.
In Proceedings of the Eu-ropean Workshop on Natural Language Generation(ENLG).A.
Rozovskaya, K.-W. Chang, M. Sammons, andD.
Roth.
2013.
The University of Illinois Systemin the CoNLL-2013 Shared Task.
In Proceedings ofCoNLL Shared Task.A.
Rozovskaya, N. Habash, R. Eskander, N. Farra, andW.
Salloum.
2014.
The Columbia System in theQALB-2014 Shared Task on Arabic Error Correc-tion.
In Proceedings of EMNLP Workshop on Ara-bic Natural Language Processing: QALB SharedTask.N.
Tomeh, N. Habash, R. Eskander, and J.
Le Roux.2014.
A Pipeline Approach to Supervised ErrorCorrection for the QALB-2014 Shared Task.
In Pro-ceedings of EMNLP Workshop on Arabic NaturalLanguage Processing: QALB Shared Task.W.
Zaghouani, B. Mohit, N. Habash, O. Obeid,N.
Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani,and K. Oflazer.
2014.
Large Scale Arabic Error An-notation: Guidelines and Framework.
In Proceed-ings of the Ninth International Conference on Lan-guage Resources and Evaluation (LREC).
EuropeanLanguage Resources Association (ELRA).T.
Zerrouki, K. Alhawiti, and A. Balla.
2014.
Au-tocorrection Of Arabic Common Errors For LargeText Corpus.
In Proceedings of EMNLP Workshopon Arabic Natural Language Processing: QALBShared Task.46Appendix A: Sample annotation fileBelow is the complete list of correction actions for the example in Table 1 as they appear in the trainingand evaluation data.
The first two columns are the error index linking to Table 1 and the original word,respectively.
Only the column titled Correction Action is in the training and evaluation data.
The twonumbers following the A specify the start and end positions of the sentence token string to change.Following that (and delimited by |||) are the action type and the correction string.
The last three fieldsare irrelevant to this discussion.Error Index Original Word Correction Action#1 ?Y?
A 2 3|||Edit|||?Y?|||REQUIRED|||-NONE-|||0#2?K @Q?
A 5 6|||Edit|||?Z @Q?|||REQUIRED|||-NONE-|||0#3?Y?
A 6 7|||Edit||| ?Y?|||REQUIRED|||-NONE-|||0#4??Qj??
@ ?
A 9 11|||Merge|||??Qj??
@?|||REQUIRED|||-NONE-|||0#5 A 11 11|||Add_before|||.|||REQUIRED|||-NONE-|||0#6 ?G @B A 11 12|||Edit|||??KB|||REQUIRED|||-NONE-|||0#7IJ?
?
A 13 15|||Merge|||IJ?
?|||REQUIRED|||-NONE-|||0#8 ??
?JK.A 15 16|||Edit|||????@|||REQUIRED|||-NONE-|||0#9?
@ A 18 19|||Edit|||?
@|||REQUIRED|||-NONE-|||0#10 ??
??B@ A 23 24|||Edit|||??
??B@|||REQUIRED|||-NONE-|||0#11 A 24 24|||Add_before|||,|||REQUIRED|||-NONE-|||0#12?A?
?
A 24 26|||Merge|||?A?
?|||REQUIRED|||-NONE-|||0#13 @?YJ.KA 26 27|||Edit|||?YJ.K|||REQUIRED|||-NONE-|||0#14?
@ A 27 28|||Edit|||?
@|||REQUIRED|||-NONE-|||0#15 A 31 31|||Add_before|||,|||REQUIRED|||-NONE-|||0#16 A?
A 32 33|||Delete||||||REQUIRED|||-NONE-|||0#17 ??
A 33 34|||Delete||||||REQUIRED|||-NONE-|||0#18 Yg A 34 35|||Edit|||Yg@?|||REQUIRED|||-NONE-|||0#19?JJ?B@ A 36 37|||Edit|||?JJ?B@|||REQUIRED|||-NONE-|||0#20 ?
??JK.A 38 39|||Edit|||?
??K|||REQUIRED|||-NONE-|||0#21 ?K@ A 39 40|||Edit|||?K@|||REQUIRED|||-NONE-|||0#22 A 41 41|||Add_before|||?
@|||REQUIRED|||-NONE-|||0#23 ??
?JK A 41 42|||Edit|||???JK|||REQUIRED|||-NONE-|||0#24?
@ A 42 43|||Edit|||?@|||REQUIRED|||-NONE-|||0#25?BA?
???m'A 45 46|||Split|||?B A?
???m'|||REQUIRED|||-NONE-|||0#26 ?JJ?
@ A 46 47|||Edit|||?JJ?
@|||REQUIRED|||-NONE-|||047
