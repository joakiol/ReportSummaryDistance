Proc.
EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing, pages 1?10,Avignon, France, April 24 2012. c?2012 Association for Computational LinguisticsPreservation of Recognizability forWeighted Linear Extended Top-Down Tree Transducers?Nina Seemann and Daniel Quernheim and Fabienne Braune and Andreas MalettiUniversity of Stuttgart, Institute for Natural Language Processing{seemanna,daniel,braunefe,maletti}@ims.uni-stuttgart.deAbstractAn open question in [FU?LO?P, MALETTI,VOGLER: Weighted extended tree trans-ducers.
Fundamenta Informaticae 111(2),2011] asks whether weighted linear ex-tended tree transducers preserve recogniz-ability in countably complete commuta-tive semirings.
In this contribution, thequestion is answered positively, which isachieved with a construction that utilizesinside weights.
Due to the completenessof the semiring, the inside weights alwaysexist, but the construction is only effectiveif they can be effectively determined.
It isdemonstrated how to achieve this in a num-ber of important cases.1 IntroductionSyntax-based statistical machine translation(Knight, 2007) created renewed interest in treeautomata and tree transducer theory (Fu?lo?pand Vogler, 2009).
In particular, it sparkedresearch on extended top-down tree transduc-ers (Graehl et al, 2009), which are top-downtree transducers (Rounds, 1970; Thatcher, 1970)in which the left-hand sides can contain several(or no) input symbols.
A recent contributionby Fu?lo?p et al (2011) investigates the theoreticalproperties of weighted extended tree transduc-ers over countably complete and commutativesemirings (Hebisch and Weinert, 1998; Golan,1999).
Such semirings permit sums of countablymany summands, which still obey the usualassociativity, commutativity, and distributivitylaws.
We will use the same class of semirings.?
All authors were financially supported by the EMMYNOETHER project MA / 4959 / 1-1 of the German ResearchFoundation (DFG).Input?
Parser ?
TM ?
LM ?
OutputFigure 1: Syntax-based machine translation pipeline.Extended top-down tree transducers are used astranslation models (TM) in syntax-based machinetranslation.
In the standard pipeline (see Figure 1;LM is short for language model) the translationmodel is applied to the parses of the input sen-tence, which can be represented as a recogniz-able weighted forest (Fu?lo?p and Vogler, 2009).In practice, only the best or the n-best parses areused, but in principle, we can use the recogniz-able weighted forest of all parses.
In either case,the translation model transforms the input treesinto a weighted forest of translated output trees.A class of transducers preserves recognizabilityif for every transducer of the class and each rec-ognizable weighted forest, this weighted forestof translated output trees is again recognizable.Fu?lo?p et al (2011) investigates which extendedtop-down tree transducers preserve recognizabil-ity under forward (i.e., the setting previously de-scribed) and backward application (i.e., the set-ting, in which we start with the output trees andapply the inverse of the translation model), but thequestion remained open for forward applicationof weighted linear extended top-down tree trans-ducers [see Table 1 for an overview of the exist-ing results for forward application due to Engel-friet (1975) in the unweighted case and Fu?lo?p etal.
(2010) and Fu?lo?p et al (2011) for the weightedcase].
In conclusion, Fu?lo?p et al (2011) ask: ?Arethere a commutative semiring S that is count-ably complete wrt.
?, a linear wxttM [weightedextended top-down tree transducer with regularlook-ahead; see Section 4], and a recognizable1model preserves regularityunweightedln-XTOP 3l-XTOP 3l-XTOPR 3XTOP 7weightedln-XTOP 3l-XTOP 3l-XTOPR 3XTOP 7Table 1: Overview of the known results due to Engel-friet (1975) and Fu?lo?p et al (2011) and our results inboxes.weighted tree language ?
such that M(?)
[for-ward application] is not recognizable?
Or evenharder, are there S and M with the same prop-erties such that M(1?)
[1?
is the weighted forestin which each tree has weight 1] is not recogniz-able?
?In this contribution, we thus investigate preser-vation of recognizability (under forward applica-tion) for linear extended top-down tree transduc-ers with regular look-ahead (Engelfriet, 1977),which are equivalent to linear weighted extendedtree transducers by Fu?lo?p et al (2011).
We showthat they always preserve recognizability, thusconfirming the implicit hypothesis of Fu?lo?p et al(2011).
The essential tool for our construction isthe inside weight (Lari and Young, 1990; Graehlet al, 2008) of the states of the weighted treegrammar (Alexandrakis and Bozapalidis, 1987)representing the parses.
The inside weight of astate q is the sum of all weights of trees acceptedin this state.
In our main construction (see Sec-tion 5) we first compose the input weighted treegrammar with the transducer (input restriction).This is particularly simple since we just abusethe look-ahead of the initial rules.
In a secondstep, we normalize the obtained transducer, whichyields the standard product construction typicallyused for input restriction.
Finally, we project tothe output by basically eliminating the left-handsides.
In this step, the inside weights of statesbelonging to deleted subtrees are multiplied tothe production weight.
Due to the completenessof the semiring, the inside weights always ex-ist, but the infinite sums have to be computed ef-fectively for the final step of the construction tobe effective.
This problem is addressed in Sec-tion 6, where we show several methods to effec-tively compute or approximate the inside weightsfor all states of a weighted tree grammar.2 NotationOur weights will be taken from a commuta-tive semiring (A,+, ?, 0, 1), which is an algebraicstructure of two commutative monoids (A,+, 0)and (A, ?, 1) such that ?
distributes over + and0 ?
a = 0 for all a ?
A.
An infinitary sum opera-tion?is a family (?I)I where I is a countableindex set and?I : AI ?
A.
Given f : I ?
A,we write?i?I f(i) instead of?I f .
The semi-ring together with the infinitary sum operation?is countably complete (Eilenberg, 1974; Hebischand Weinert, 1998; Golan, 1999; Karner, 2004) iffor all countable sets I and ai ?
A with i ?
I?
?i?I ai = am + an if I = {m,n},?
?i?I ai =?j?J(?i?Ijai)if I =?j?J Ijfor countable sets J and Ij with j ?
J suchthat Ij ?
Ij?
= ?
for all different j, j?
?
J ,and?
a ?
(?i?I ai)=?i?I(a ?
ai) for all a ?
A.For such a semiring, we let a?
=?i?N ai forevery a ?
A.
In the following, we assume that(A,+, ?, 0, 1) is a commutative semiring that iscountably complete with respect to?.Our trees have node labels taken from an al-phabet ?
and leaves might also be labeled by el-ements of a set V .
Given a set T , we write ?
(T )for the set{?
(t1, .
.
.
, tk) | k ?
N, ?
?
?, t1, .
.
.
, tk ?
T} .The set T?
(V ) of ?-trees with V -leaves is definedas the smallest set T such that V ?
?
(T ) ?
T .We write T?
for T?(?).
For each tree t ?
T?
(V )we identify nodes by positions.
The root of t hasposition ?
and the position iw with i ?
N andw ?
N?
addresses the position w in the i-th di-rect subtree at the root.
The set of all positionsin t is pos(t).
We write t(w) for the label (takenfrom ?
?
V ) of t at position w ?
pos(t).
Sim-ilarly, we use t|w to address the subtree of t thatis rooted in position w, and t[u]w to represent thetree that is obtained from replacing the subtree t|wat w by u ?
T?
(V ).
For a given set L ?
?
?
Vof labels, we letposL(t) = {w ?
pos(t) | t(w) ?
L}2be the set of all positions whose label belongsto L. We also write posl(t) instead of pos{l}(t).We often use the set X = {x1, x2, .
.
. }
of vari-ables and its finite subsets Xk = {x1, .
.
.
, xk}for every k ?
N to label leaves.
Let Vbe a set potentially containing some variablesof X .
The tree t ?
T?
(V ) is linear if|posx(t)| ?
1 for every x ?
X .
Moreover,var(t) = {x ?
X | posx(t) 6= ?}
collects allvariables that occur in t. Given a finite set Q andT ?
T?
(V ), we letQ[T ] = {q(t) | q ?
Q, t ?
T} .We will treat elements ofQ[T ] (in which elementsofQ are always used as unary symbols) as specialtrees of T?
?Q(V ).
A substitution ?
is a mapping?
: X ?
T?
(V ).
When applied to t ?
T?
(V ),it returns the tree t?, which is obtained from tby replacing all occurrences of x ?
X (in par-allel) by ?(x).
This can be defined recursivelyby x?
= ?
(x) for all x ?
X , v?
= v for allv ?
V \X , and ?
(t1, .
.
.
, tk)?
= ?
(t1?, .
.
.
, tk?
)for all ?
?
?
and t1, .
.
.
, tk ?
T?
(V ).3 Weighted Tree GrammarsIn this section, we will recall weighted treegrammars (Alexandrakis and Bozapalidis, 1987)[see (Fu?lo?p and Vogler, 2009) for a modern treat-ment and a complete historical account].
In gen-eral, weighted tree grammars (WTGs) offer an ef-ficient representation of weighted forests, whichare sets of trees such that each individual treeis equipped with a weight.
The representationis even more efficient than packed forests (Mi etal., 2008) and moreover can represent an infinitenumber of weighted trees.
To avoid confusionbetween the nonterminals of a parser, which pro-duces the forests considered here, and our WTGs,we will refer to the nonterminals of our WTG asstates.Definition 1.
A weighted tree grammar (WTG) isa system (Q,?, q0, P ) where?
Q is a finite set of states (nonterminals),?
?
is the alphabet of symbols,?
q0 ?
Q is the starting state, and?
P is a finite set of productions qa?
t, whereq ?
Q, a ?
A, and t ?
T?
(Q).Example 2.
We illustrate our notation on theWTG Gex = (Q,?, qs, P ) where?
Q = {qs, qnp, qprp, qn, qadj},?
?
contains ?S?, ?NP?, ?VP?, ?PP?, ?DT?,?NN?, ?N?, ?VBD?, ?PRP?, ?ADJ?, ?man?,?hill?, ?telescope?, ?laughs?, ?the?, ?on?,?with?, ?old?, and ?young?, and?
P contains the productionsqs1.0?
S(qnp,VP(VBD(laughs))) (?1)qnp0.4?
NP(qnp,PP(qprp, qnp))qnp0.6?
NP(DT(the), qn) (?2)qprp0.5?
PRP(on)qprp0.5?
PRP(with)qn0.3?
N(qadj , qn)qn0.3?
NN(man) (?3)qn0.2?
NN(hill)qn0.2?
NN(telescope)qadj0.5?
ADJ(old)qadj0.5?
ADJ(young)It produces a weighted forest representing sen-tences about young and old men with telescopeson hills.In the following, let G = (Q,?, q0, P ) be aWTG.
For every production ?
= qa?
t in P , welet wtG(?)
= a.
The semantics of G is definedwith the help of derivations.
Let ?
?
T?
(Q) bea sentential form, and let w ?
posQ(?)
be suchthat w is the lexicographically smallest Q-labeledposition in ?.
Then ?
?
?G ?
[t]w if ?
(w) = q. Fora sequence ?1, .
.
.
, ?n ?
P of productions, welet wtG(?1 ?
?
?
?n) =?ni=1 wtG(?i).
For everyq ?
Q and t ?
T?
(Q), we letwtG(q, t) =??1,...,?n?Pq?
?1G ????
?nG twtG(?1 ?
?
?
?n) .The WTG G computes the weighted forestLG : T?
?
A such that LG(t) = wtG(q0, t) forevery t ?
T?.
Two WTGs are equivalent if theycompute the same weighted forest.
Since produc-tions of weight 0 are useless, we often omit them.Example 3.
For the WTG Gex of Example 2 wedisplay a derivation with weight 0.18 for the sen-tence ?the man laughs?
in Figure 2.The notion of inside weights (Lari and Young,1990) is well-established, and Maletti and Satta3qs ?
?1GSqnp VPVBDlaughs??2GSNPDTtheqnVPVBDlaughs?
?3GSNPDTtheNNmanVPVBDlaughsFigure 2: Derivation with weight 1.0 ?
0.6 ?
0.3.
(2009) consider them for WTGs.
Let us recall thedefinition.Definition 4.
The inside weight of state q ?
Q isinG(q) =?t?T?wtG(q, t) .In Section 6 we demonstrate how to computeinside weights.
Finally, let us introduce WTGs innormal form.
The WTG G is in normal form ift ?
?
(Q) for all its productions qa?
t in P .
Thefollowing theorem was proven by Alexandrakisand Bozapalidis (1987) as Proposition 1.2.Theorem 5.
For every WTG there exists anequivalent WTG in normal form.Example 6.
The WTG Gex of Example 2 is notnormalized.
To illustrate the normalization step,we show the normalization of the production ?2,which is replaced by the following three produc-tions:qnp0.6?
NP(qdt, qn) qdt1.0?
DT(qt)qt1.0?
the .4 Weighted linear extended treetransducersThe model discussed in this contribution is an ex-tension of the classical top-down tree transducer,which was introduced by Rounds (1970) andThatcher (1970).
Here we consider a weightedand extended variant that additionally has regularlook-ahead.
The weighted top-down tree trans-ducer is discussed in (Fu?lo?p and Vogler, 2009),and extended top-down tree transducers werestudied in (Arnold and Dauchet, 1982; Knight andGraehl, 2005; Knight, 2007; Graehl et al, 2008;Graehl et al, 2009).
The combination (weightedextended top-down tree transducer) was recentlyinvestigated by Fu?lo?p et al (2011), who also con-sidered (weighted) regular look-ahead, which wasfirst introduced by Engelfriet (1977) in the un-weighted setting.Definition 7.
A linear extended top-downtree transducer with full regular look-ahead(l-XTOPRf ) is a system (S,?,?, s0, G,R) where?
S is a finite set of states,?
?
and ?
are alphabets of input and outputsymbols, respectively,?
s0 ?
S is an initial state,?
G = (Q,?, q0, P ) is a WTG, and?
R is a finite set of weighted rules of the form`a??
r where?
a ?
A is the rule weight,?
` ?
S[T?
(X)] is the linear left-handside,?
?
: var(`)?
Q is the look-ahead, and?
r ?
T?
(S[var(`)]) is the linear right-hand side.In the following, let M = (S,?,?, s0, G,R)be an l-XTOPRf .
We assume that the WTG Gcontains a state > such that wtG(>, t) = 1 forevery t ?
T?.
In essence, this state representsthe trivial look-ahead.
If ?
(x) = > for everyrule `a??
r ?
R and x ?
var(r) (respectively,x ?
var(`)), then M is an l-XTOPR (respectively,l-XTOP).
l-XTOPR and l-XTOP coincide exactlywith the models of Fu?lo?p et al (2011), and in thelatter model we drop the look-ahead component ?and the WTG G completely.Example 8.
The rules of our running examplel-XTOP Mex (over the input and output alpha-bet ?, which is also used by the WTG Gex of Ex-ample 2) are displayed in Figure 3.Next, we present the semantics.
Without lossof generality, we assume that we can distin-guish states from input and output symbols (i.e.,S ?
(?
?
?)
= ?).
A sentential form of M is atree of SF(M) = T?(Q[T?]).
Let ?
= `a??
r bea rule of R. Moreover, let ?, ?
?
SF(M) be sen-tential forms and w ?
N?
be the lexicographicallysmallest position in posQ(?).
We write ?b?M,?
?if there exists a substitution ?
: X ?
T?
such that?
?
= ?[`?]w,?
?
= ?[r?
]w, and?
b = a ?
?x?var(`) wtG(?
(x), ?
(x)).4s0SNPx1 x2VPx3?
0.6SNPs1x1s2x2VPs3x3???
0.4Ss1x1VPs3x3s2NADJx1x2?
0.7NADJs5x1s2x2???
0.3s2x2s1NPx1 x2?
0.5NPs1x1s2x2???
0.5s1x1s1DTthe?
1.0DTthes3VBDlaughs?
1.0VBDlaughss2PPx1 x2?
1.0PPs4x1s1x2s2NNman /hill /telescope?
1.0NNman /hill /telescopes4PRPon /with?
1.0PRPon /withFigure 3: Example rules of an l-XTOP.
We collapsed rules with the same left-hand side as well as several lexicalitems to save space.s0SNPNPDTtheNNmanPPPRPonNPDTtheNNhillVPVBDlaughs0.4?MSs1NPDTtheNNmanVPs3VBDlaughs0.5?MSNPs1DTthes2NNmanVPs3VBDlaughs?
?MSNPDTtheNNmanVPVBDlaughsFigure 4: Derivation with weight 0.4 ?
0.5 ?
1.0 (rules omitted).The tree transformation ?M computed byM is de-fined by?M (t, u) =?
?1,...,?n?Rs0(t)a1?M,?1 ??
?an?M,?nua1 ?
.
.
.
?
anfor every t ?
T?
and u ?
T?.Example 9.
A sequence of derivation steps of thel-XTOP Mex is illustrated in Figure 4.
The trans-formation it computes is capable of deleting thePP child of every NP-node with probability 0.4 aswell as deleting the ADJ child of every N-nodewith probability 0.3.A detailed exposition to unweighted l-XTOPRis presented by Arnold and Dauchet (1982) andGraehl et al (2009).5 The constructionIn this section, we present the main constructionof this contribution, in which we will construct aWTG for the forward application of another WTGvia an l-XTOPR.
Let us first introduce the mainnotions.
Let L : T?
?
A be a weighted forestand ?
: T??T?
?
A be a weighted tree transfor-mation.
Then the forward application of L via ?yields the weighted forest ?
(L) : T?
?
A suchthat (?
(L))(u) =?t?T?L(t) ?
?
(t, u) for ev-ery u ?
T?.
In other words, to compute theweight of u in ?
(L), we consider all input trees tand multiply their weight in L with their trans-lation weight to u.
The sum of all those prod-ucts yields the weight for u in ?(L).
In the par-ticular setting considered in this contribution, theweighted forest L is computed by a WTG and theweighted tree transformation ?
is computed by anl-XTOPR.
The question is whether the resultingweighted forest ?
(L) can be computed by a WTG.Our approach to answer this question con-sists of three steps: (i) composition, (ii) nor-malization, and (iii) range projection, whichwe address in separate sections.
Our input is5qs ?Sqnp qvp?SNPqnp qppqvp ?2SNPqnp qppVPVBDqvqs ?Sqnp qvp?SNPqdt qnqvp ?2SNPqdt qnVPVBDqvFigure 5: Two derivations (without production andgrammar decoration) with weight 0.4 [top] and0.6 [bottom] of the normalized version of theWTG Gex (see Example 10).the WTG G?
= (Q?,?, q?0, P?
), which com-putes the weighted forest L = LG?
, andthe l-XTOPR M = (S,?,?, s0, G,R) withG = (Q,?, q0, P ), which computes the weightedtree transformation ?
= ?M .
Without loss of gen-erality, we suppose thatG andG?
contain a specialstate > such that wtG(>, t) = wtG?
(>, t) = 1for all t ?
T?.
Moreover, we assume that theWTG G?
is in normal form.
Finally, we assumethat s0 is separated, which means that the initialstate of M does not occur in any right-hand side.Our example l-XTOP Mex has this property.
Allthese restrictions can be assumed without loss ofgenerality.
Finally, for every state s ?
S, we letRs = {`a??
r ?
R | `(?)
= s} .5.1 CompositionWe combine the WTG G?
and the l-XTOPR Minto a single l-XTOPRf M?
that computes?M ?
(t, u) = LG?
(t) ?
?M (t, u) = L(t) ?
?
(t, u)for every t ?
T?
and u ?
T?.
To this end, weconstructM ?
= (S,?,?, s0, G?G?, (R \Rs0) ?R?
)such that G ?
G?
is the classical product WTG[see Proposition 5.1 of (Berstel and Reutenauer,1982)] and for every rule `a??
r in Rs0 and?
: var(`)?
Q?, the rule`a?wtG?
(q?0,`?)???????????
ris in R?, where ??
(x) = ??
(x), ?(x)?
for everyx ?
var(`).Example 10.
Let us illustrate the construction onthe WTG Gex of Example 2 and the l-XTOP Mexof Example 8.
According to our assumptions,Gex should first be normalized (see Theorem 5).We have two rules in Rs0 and they have the sameleft-hand side `.
It can be determined easily thatwtG?ex(qs, `?)
6= 0 only if?
?(x1)?(x2)?
(x3) = qnpqppqv or?
?(x1)?(x2)?
(x3) = qdtqnqv.Figure 5 shows the two corresponding derivationsand their weights.
Thus, the s0-rules are replacedby the 4 rules displayed in Figure 6.Theorem 11.
For every t ?
T?
and u ?
T?, wehave ?M ?
(t, u) = L(t) ?
?
(t, u).Proof.
We prove an intermediate property foreach derivation of M .
Lets0(t)b1?M,?1 ?
?
?bn?M,?n ube a derivation of M .
Let ?1 = `a1??
r be thefirst rule, which trivially must be in Rs0 .
Then forevery ?
: var(`)?
Q?, there exists a derivations0(t)c1?M ?,?
?1 ?2b2?M ?,?2 ?
?
?bn?M ?,?n uin M ?
such thatc1 = b1?wtG?
(q?0, `?)??x?var(`)wtG?(?
(x), ??
(x)) ,where ??
: var(`) ?
T?
is such that t = `?
?.Since we sum over all such derivations and??
: var(`)?Q?wtG?
(q?0, `?)
??x?var(`)wtG?(?
(x), ??
(x))= wtG?
(q?0, t) = LG?
(t)by a straightforward extension of Lemma 4.1.8of (Borchardt, 2005), we obtain that the deriva-tions in M ?
sum to LG?
(t) ?
b1 ?
.
.
.
?
bn as desired.The main property follows trivially from the in-termediate result.5.2 NormalizationCurrently, the weights of the input WTG areonly on the initial rules and in its look-ahead.Next, we use essentially the same method asin the previous section to remove the look-ahead from all variables that are not deleted.Let M ?
= (S,?,?, s0, G ?
G?, R) be thel-XTOPRf constructed in the previous section and6s0SNPx1 x2VPx3?
?0.6 ?
cSNPs1x1s2x2VPs3x3??
?0.4 ?
cSs1x1VPs3x3Figure 6: 4 new l-XTOPRf rules, where ?
and c areeither (i) ?(x1)?(x2)?
(x3) = qnpqppqv and c = 0.4or (ii) ?(x1)?(x2)?
(x3) = qdtqnqv and c = 0.6 (seeExample 10).s0SNPx1 x2VPx3?
?0.4 ?
0.4S?s1, qnp?x1VP?s3, qv?x3??
?0.4 ?
0.6S?s1, qdt?x1VP?s3, qv?x3Figure 7: New l-XTOPR rules, where ?
(x2) = qpp[left] and ?
(x2) = qn [right] (see Figure 6).?
= `a??
r ?
R be a rule with ?
(x) = ?>, q?
?for some q?
?
Q?
\ {>} and x ?
var(r).
Notethat ?
(x) = ?>, q??
for some q?
?
Q?
for allx ?
var(r) since M is an l-XTOPR.
Then weconstruct the l-XTOPRf M??
(S ?
S ?Q?,?,?, s0, G?G?, (R \ {?})
?R?
)such that R?
contains the rule `a???
r?, where??(x?)
={?>,>?
if x = x??(x?)
otherwisefor all x?
?
var(`) and r?
is obtained from r by re-placing the subtree s(x) with s ?
S by ?s, q??
(x).Additionally, for every rule `??a??????
r??
in Rs and?
: var(`??)?
Q?, the rule`??a???wtG?
(q?,`???)??????????????
r?
?is in R?, where ????
(x) = ????
(x), ?(x)?
for ev-ery x ?
var(`).
This procedure is iterated untilwe obtain an l-XTOPR M ??.
Clearly, the iterationmust terminate since we do not change the ruleshape, which yields that the size of the potentialrule set is bounded.Theorem 12.
The l-XTOPR M ??
and thel-XTOPRf M?
are equivalent.
?s2, qn?NADJx1x2?
?0.32 ?
0.5?s2, qn?x2?s1, qnp?NPx1 x2???|??
?0.5 ?
0.4?s1, qnp?x1??
?0.5 ?
0.6?s1, qdt?x1Figure 8: New l-XTOPR rules, where ?
(x1) is eitherqold or qyoung , ??
(x2) = qpp, and ???
(x2) = qn.Proof.
It can be proved that the l-XTOPRf con-structed after each iteration is equivalent to itsinput l-XTOPRf in the same fashion as in Theo-rem 11 with the only difference that the rule re-placement now occurs anywhere in the derivation(not necessarily at the beginning) and potentiallyseveral times.
Consequently, the finally obtainedl-XTOPR M ??
is equivalent to M ?.Example 13.
Let us reconsider the l-XTOPRf con-structed in the previous section and apply the nor-malization step.
The interesting rules (i.e., thoserules la??
r where var(r) 6= var(l)) are dis-played in Figures 7 and 8.5.3 Range projectionWe now have an l-XTOPR M ??
with rules R?
?computing ?M ??
(t, u) = L(t) ?
?
(t, u).
In the fi-nal step, we simply disregard the input and projectto the output.
Formally, we want to construct aWTG G??
such thatLG??
(u) =?t?T?
?M ??
(t, u) =?t?T?L(t) ?
?
(t, u)for every u ?
T?.
Let us suppose that G is theWTG inside M ??.
Recall that the inside weight ofstate q ?
Q isinG(q) =?t?T?wtG(q, t) .We construct the WTGG??
= (S ?
S ?Q?,?, s0, P??
)such that `(?)c?
r?
is in P ??
for every rule`a??
r ?
R?
?, wherec = a ??x?var(`)\var(r)inG(?
(x))and r?
is obtained from r by removing the vari-ables of X .
If the same production is constructedfrom several rules, then we add the weights.
Notethat the WTG G??
can be effectively computed ifinG(q) is computable for every state q.7qs qprpqnp qn qadjFigure 9: Dependency graph of the WTG Gex.Theorem 14.
For every u ?
T?, we haveLG??
(u) =?t?T?L(t) ?
?
(t, u) = (?
(L))(u) .Example 15.
The WTG productions for the rulesof Figures 7 and 8 ares00.4?0.4?
S(?s1, qnp?,VP(?s3, qv?))s00.4?0.6?
S(?s1, qdt?,VP(?s3, qv?
))?s2, qn?0.3?0.3?
?s2, qn?
?s1, qnp?0.5?0.4?
?s1, qnp?
?s1, qnp?0.5?0.6?
?s1, qdt?
.Note that all inside weights are 1 in our exam-ple.
The first production uses the inside weightof qpp, whereas the second production uses the in-side weight of qn.
Note that the third productioncan be constructed twice.6 Computation of inside weightsIn this section, we address how to effectively com-pute the inside weight for every state.
If the WTGG = (Q,?, q0, P ) permits only finitely manyderivations, then for every q ?
Q, the insideweight inG(q) can be computed according to Def-inition 4 because wtG(q, t) = 0 for almost allt ?
T?.
If P contains (useful) recursive rules,then this approach does not work anymore.
OurWTG Gex of Example 2 has the following two re-cursive rules:qnp0.4?
NP(qnp,PP(qprp, qnp)) (?4)qn0.3?
N(qadj , qn) .
(?5)The dependency graph of Gex, which is shown inFigure 9, has cycles, which yields that Gex per-mits infinitely many derivations.
Due to the com-pleteness of the semiring, even the infinite sum ofDefinition 4 is well-defined, but we still have tocompute it.
We will present two simple methodsto achieve this: (a) an analytic method and (b) anapproximation in the next sections.6.1 Analytic computationIn simple cases we can compute the inside weightusing the stars a?, which we defined in Section 2.Let us first list some interesting countably com-plete semirings for NLP applications and theircorresponding stars.?
Probabilities: (R?
?0,+, ?, 0, 1) where R?
?0contains all nonnegative real numbersand ?, which is bigger than every realnumber.
For every a ?
R?
?0 we havea?
={11?a if 0 ?
a < 1?
otherwise?
VITERBI: ([0, 1],max, ?, 0, 1) where [0, 1] isthe (inclusive) interval of real numbers be-tween 0 and 1.
For every 0 ?
a ?
1 we havea?
= 1.?
Tropical: (R?
?0,min,+,?, 0) wherea?
= 0 for every a ?
R??0.?
Tree unification: (2T?
(X1),?,unionsq, ?, {x1})where 2T?
(X1) = {L | L ?
T?
(X1)} andunionsq is unification (where different occurrencesof x1 can be replaced differently) extendedto sets as usual.
For every L ?
T?
(Xk) wehave L?
= {x1} ?
(L unionsq L).We can always try to develop a regular expres-sion (Fu?lo?p and Vogler, 2009) for the weightedforest recognized by a certain state, in which wethen can drop the actual trees and only computewith the weights.
This is particularly easy if ourWTG has only left- or right-recursive productionsbecause in this case we obtain classical regularexpressions (for strings).
Let us consider produc-tion ?5.
It is right-recursive.
On the string level,we obtain the following unweighted regular ex-pression for the string language generated by qn:L(qadj)?
(man | hill | telescope)where L(qadj) = {old, young} is the set of stringsgenerated by qadj .
Correspondingly, we can de-rive the inside weight by replacing the generatedstring with the weights used to derive them.
Forexample, the production ?5, which generates thestate qadj , has weight 0.3.
We obtain the expres-sioninG(qn) = (0.3 ?
inG(qadj))?
?
(0.3 + 0.2 + 0.2) .8Example 16.
If we calculate in the probabilitysemiring and inG(qadj) = 1, theninG(qn) =11?
0.3?
(0.3 + 0.2 + 0.2) = 1 ,as expected (since our productions induce a prob-ability distribution on all trees generated fromeach state).Example 17.
If we calculate in the tropical semi-ring, then we obtaininG(qn) = min(0.3, 0.2, 0.2) = 0.2 .It should be stressed that this method onlyallows us to compute inG(q) in very simplecases (e.g., WTG containing only left- or right-recursive productions).
The production ?4 hasa more complicated recursion, so this simplemethod cannot be used for our full example WTG.However, for extremal semirings the insideweight always coincides with a particular deriva-tion.
Let us also recall this result.
The semiring isextremal if a+ a?
?
{a, a?}
for all a, a?
?
A. TheVITERBI and the tropical semiring are extremal.Recall thatinG(q) =?t?T?wtG(q, t)=?t?T???1,...,?n?Pq?
?1G ????
?nG twtG(?1 ?
?
?
?n) ,which yields that inG(q) coincides with thederivation weight wtG(?1 ?
?
?
?n) of some deriva-tion q ?
?1G ?
?
?
?
?nG t for some t ?
T?.
Inthe VITERBI semiring this is the highest scor-ing derivation and in the tropical semiring it isthe lowest scoring derivation (mind that in theVITERBI semiring the production weights aremultiplied in a derivation, whereas they are addedin the tropical semiring).
There are efficient algo-rithms (Viterbi, 1967) that compute those deriva-tions and their weights.6.2 Numerical ApproximationNext, we show how to obtain a numerical ap-proximation of the inside weights (up to anydesired precision) in the probability semiring,which is the most important of all semiringsdiscussed here.
A similar approach was usedby Stolcke (1995) for context-free grammars.
Tokeep the presentation simple, let us suppose thatG = (Q,?, q0, P ) is in normal form (see The-orem 5).
The method works just as well in thegeneral case.We first observe an important property of theinside weights.
For every state q ?
QinG(q) =?qa??
(q1,...,qn)?Pa ?
inG(q1) ?
.
.
.
?
inG(qn) ,which can trivially be understood as a system ofequations (where each inG(q) with q ?
Q is avariable).
Since there is one such equation foreach variable inG(q) with q ?
Q, we have asystem of |Q| non-linear polynomial equations in|Q| variables.Several methods to solve non-linear systems ofequations are known in the numerical calculus lit-erature.
For example, the NEWTON-RAPHSONmethod allows us to iteratively compute the rootsof any differentiable real-valued function, whichcan be used to solve our system of equations be-cause we can compute the JACOBI matrix for oursystem of equations easily.
Given a good startingpoint, the NEWTON-RAPHSON method assuresquadratic convergence to a root.
A good start-ing point can be obtained, for example, by bisec-tion (Corliss, 1977).
Another popular root-findingapproximation is described by Brent (1973).Example 18.
For the WTG of Example 2 we ob-tain the following system of equations:inG(qs) = 1.0 ?
inG(qnp)inG(qnp) = 0.4 ?
inG(qnp) ?
inG(qprp) ?
inG(qnp)+ 0.6 ?
inG(qn)inG(qn) = 0.3 ?
inG(qadj) ?
inG(qn)+ 0.3 + 0.2 + 0.2inG(qadj) = 0.5 + 0.5inG(qprp) = 0.5 + 0.5 .Together with inG(qn) = 1, which we alreadycalculated in Example 16, the only interestingvalue isinG(qs) = inG(qnp) = 0.4 ?
inG(qnp)2 + 0.6 ,which yields the roots inG(qnp) = 1 andinG(qnp) = 1.5.
The former is the desired solu-tion.
As before, this is the expected solution.9ReferencesAthanasios Alexandrakis and Symeon Bozapalidis.1987.
Weighted grammars and Kleene?s theorem.Inf.
Process.
Lett., 24(1):1?4.Andre?
Arnold and Max Dauchet.
1982.
Morphismeset bimorphismes d?arbres.
Theoret.
Comput.
Sci.,20(1):33?93.Jean Berstel and Christophe Reutenauer.
1982.
Rec-ognizable formal power series on trees.
Theoret.Comput.
Sci., 18(2):115?148.Bjo?rn Borchardt.
2005.
The Theory of RecognizableTree Series.
Ph.D. thesis, Technische Universita?tDresden.Richard P. Brent.
1973.
Algorithms for Minimizationwithout Derivatives.
Series in Automatic Computa-tion.
Prentice Hall, Englewood Cliffs, NJ, USA.George Corliss.
1977.
Which root does the bisectionalgorithm find?
SIAM Review, 19(2):325?327.Samuel Eilenberg.
1974.
Automata, Languages, andMachines ?
Volume A, volume 59 of Pure and Ap-plied Math.
Academic Press.Joost Engelfriet.
1975.
Bottom-up and top-down treetransformations ?
a comparison.
Math.
SystemsTheory, 9(3):198?231.Joost Engelfriet.
1977.
Top-down tree transducerswith regular look-ahead.
Math.
Systems Theory,10(1):289?303.Zolta?n Fu?lo?p and Heiko Vogler.
2009.
Weighted treeautomata and tree transducers.
In Manfred Droste,Werner Kuich, and Heiko Vogler, editors, Hand-book of Weighted Automata, EATCS Monographson Theoret.
Comput.
Sci., chapter 9, pages 313?403.
Springer.Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.2010.
Preservation of recognizability for syn-chronous tree substitution grammars.
In Proc.
1stWorkshop Applications of Tree Automata in Natu-ral Language Processing, pages 1?9.
Associationfor Computational Linguistics.Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.2011.
Weighted extended tree transducers.
Fun-dam.
Inform., 111(2):163?202.Jonathan S. Golan.
1999.
Semirings and their Appli-cations.
Kluwer Academic, Dordrecht.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
Comput.
Linguist.,34(3):391?427.Jonathan Graehl, Mark Hopkins, Kevin Knight, andAndreas Maletti.
2009.
The power of extendedtop-down tree transducers.
SIAM J. Comput.,39(2):410?430.Udo Hebisch and Hanns J. Weinert.
1998.
Semirings?
Algebraic Theory and Applications in ComputerScience.
World Scientific.Georg Karner.
2004.
Continuous monoids and semi-rings.
Theoret.
Comput.
Sci., 318(3):355?372.Kevin Knight and Jonathan Graehl.
2005.
An over-view of probabilistic tree transducers for naturallanguage processing.
In Proc.
6th Int.
Conf.
Com-putational Linguistics and Intelligent Text Process-ing, volume 3406 of LNCS, pages 1?24.
Springer.Kevin Knight.
2007.
Capturing practical naturallanguage transformations.
Machine Translation,21(2):121?133.Karim Lari and Steve J.
Young.
1990.
The esti-mation of stochastic context-free grammars usingthe inside-outside algorithm.
Computer Speech andLanguage, 4(1):35?56.Andreas Maletti and Giorgio Satta.
2009.
Parsing al-gorithms based on tree automata.
In Proc.
11th Int.Workshop Parsing Technologies, pages 1?12.
Asso-ciation for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
46th Ann.
Meeting ofthe ACL, pages 192?199.
Association for Computa-tional Linguistics.William C. Rounds.
1970.
Mappings and grammarson trees.
Math.
Systems Theory, 4(3):257?287.Andreas Stolcke.
1995.
An efficient probabilisticcontext-free parsing algorithm that computes prefixprobabilities.
Comput.
Linguist., 21(2):165?201.James W. Thatcher.
1970.
Generalized2 sequentialmachine maps.
J. Comput.
System Sci., 4(4):339?367.Andrew J. Viterbi.
1967.
Error bounds for convo-lutional codes and an asymptotically optimum de-coding algorithm.
IEEE Trans.
Inform.
Theory,13(2):260?269.10
