Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 355?363,Sydney, July 2006. c?2006 Association for Computational LinguisticsFully Automatic Lexicon Expansionfor Domain-oriented Sentiment AnalysisHiroshi Kanayama Tetsuya NasukawaTokyo Research Laboratory, IBM Japan, Ltd.1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan{hkana,nasukawa}@jp.ibm.comAbstractThis paper proposes an unsupervisedlexicon building method for the detec-tion of polar clauses, which convey pos-itive or negative aspects in a specificdomain.
The lexical entries to be ac-quired are called polar atoms, the min-imum human-understandable syntacticstructures that specify the polarity ofclauses.
As a clue to obtain candidatepolar atoms, we use context coherency,the tendency for same polarities to ap-pear successively in contexts.
Usingthe overall density and precision of co-herency in the corpus, the statisticalestimation picks up appropriate polaratoms among candidates, without anymanual tuning of the threshold values.The experimental results show that theprecision of polarity assignment withthe automatically acquired lexicon was94% on average, and our method is ro-bust for corpora in diverse domains andfor the size of the initial lexicon.1 IntroductionSentiment Analysis (SA) (Nasukawa and Yi,2003; Yi et al, 2003) is a task to recognizewriters?
feelings as expressed in positive ornegative comments, by analyzing unreadablylarge numbers of documents.
Extensive syn-tactic patterns enable us to detect sentimentexpressions and to convert them into seman-tic structures with high precision, as reportedby Kanayama et al (2004).
From the exam-ple Japanese sentence (1) in the digital cam-era domain, the SA system extracts a senti-ment representation as (2), which consists ofa predicate and an argument with positive (+)polarity.
(1) Kono kamera-ha subarashii-to omou.
?I think this camera is splendid.?
(2) [+] splendid(camera)SA in general tends to focus on subjec-tive sentiment expressions, which explicitly de-scribe an author?s preference as in the aboveexample (1).
Objective (or factual) expres-sions such as in the following examples (3) and(4) may be out of scope even though they de-scribe desirable aspects in a specific domain.However, when customers or corporate usersuse SA system for their commercial activities,such domain-specific expressions have a moreimportant role, since they convey strong orweak points of the product more directly, andmay influence their choice to purchase a spe-cific product, as an example.
(3) Kontorasuto-ga kukkiri-suru.
?The contrast is sharp.?
(4) Atarashii kishu-ha zuumu-mo tsuite-iru.
?The new model has a zoom lens, too.
?This paper addresses the Japanese ver-sion of Domain-oriented Sentiment Analysis,which identifies polar clauses conveying good-ness and badness in a specific domain, in-cluding rather objective expressions.
Buildingdomain-dependent lexicons for many domainsis much harder work than preparing domain-independent lexicons and syntactic patterns,because the possible lexical entries are toonumerous, and they may differ in each do-main.
To solve this problem, we have devisedan unsupervised method to acquire domain-dependent lexical knowledge where a user hasonly to collect unannotated domain corpora.The knowledge to be acquired is a domain-dependent set of polar atoms.
A polar atom isa minimum syntactic structure specifying po-larity in a predicative expression.
For exam-ple, to detect polar clauses in the sentences (3)355and (4)1, the following polar atoms (5) and (6)should appear in the lexicon:(5) [+] kukkiri-suru?to be sharp?
(6) [+] tsuku ?
zuumu-ga?to have ?
zoom lens-NOM?The polar atom (5) specified the positive po-larity of the verb kukkiri-suru.
This atom canbe generally used for this verb regardless ofits arguments.
In the polar atom (6), on theother hand, the nominative case of the verbtsuku (?have?)
is limited to a specific noun zu-umu (?zoom lens?
), since the verb tsuku doesnot hold the polarity in itself.
The automaticdecision for the scopes of the atoms is one ofthe major issues.For lexical learning from unannotated cor-pora, our method uses context coherency interms of polarity, an assumption that polarclauses with the same polarity appear suc-cessively unless the context is changed withadversative expressions.
Exploiting this ten-dency, we can collect candidate polar atomswith their tentative polarities as those adja-cent to the polar clauses which have beenidentified by their domain-independent polaratoms in the initial lexicon.
We use both intra-sentential and inter-sentential contexts to ob-tain more candidate polar atoms.Our assumption is intuitively reasonable,but there are many non-polar (neutral) clausesadjacent to polar clauses.
Errors in sentencedelimitation or syntactic parsing also result infalse candidate atoms.
Thus, to adopt a can-didate polar atom for the new lexicon, somethreshold values for the frequencies or ratiosare required, but they depend on the type ofthe corpus, the size of the initial lexicon, etc.Our algorithm is fully automatic in thesense that the criteria for the adoption of po-lar atoms are set automatically by statisticalestimation based on the distributions of co-herency: coherent precision and coherent den-sity.
No manual tuning process is required,so the algorithm only needs unannotated do-main corpora and the initial lexicon.
Thusour learning method can be used not only bythe developers of the system, but also by end-users.
This feature is very helpful for users to1The English translations are included only for con-venience.analyze documents in new domains.In the next section, we review related work,and Section 3 describes our runtime SA sys-tem.
In Section 4, our assumption for unsu-pervised learning, context coherency and itskey metrics, coherent precision and coherentdensity are discussed.
Section 5 describes ourunsupervised learning method.
Experimentalresults are shown in Section 6, and we concludein Section 7.2 Related WorkSentiment analysis has been extensively stud-ied in recent years.
The target of SA in thispaper is wider than in previous work.
For ex-ample, Yu and Hatzivassiloglou (2003) sepa-rated facts from opinions and assigned polari-ties only to opinions.
In contrast, our systemdetects factual polar clauses as well as senti-ments.Unsupervised learning for sentiment analy-sis is also being studied.
For example, Hatzi-vassiloglou and McKeown (1997) labeled ad-jectives as positive or negative, relying on se-mantic orientation.
Turney (2002) used col-location with ?excellent?
or ?poor?
to obtainpositive and negative clues for document clas-sification.
In this paper, we use contextualinformation which is wider than for the con-texts they used, and address the problem ofacquiring lexical entries from the noisy clues.Inter-sentential contexts as in our approachwere used as a clue also for subjectivity anal-ysis (Riloff and Wiebe, 2003; Pang and Lee,2004), which is two-fold classification into sub-jective and objective sentences.
Compared toit, this paper solves a more difficult problem:three-fold classification into positive, negativeand non-polar expressions using imperfect co-herency in terms of sentiment polarity.Learning methods for phrase-level sentimentanalysis closely share an objective of our ap-proach.
Popescu and Etzioni (2005) achievedhigh-precision opinion phrases extraction byusing relaxation labeling.
Their method itera-tively assigns a polarity to a phrase, relying onsemantic orientation of co-occurring words inspecific relations in a sentence, but the scopeof semantic orientation is limited to within asentence.
Wilson et al (2005) proposed su-pervised learning, dividing the resources into356Documentto analyze -SentenceDelimitation ...????
?Sentences?Proposition DetectionPropositionsClauses?Polarity Assignment+?PolaritiesPolar ClausesModalityPatternsConjunctivePatterns*PolarAtoms-Figure 1: The flow of the clause-level SA.prior polarity and context polarity, which aresimilar to polar atoms and syntactic patternsin this paper, respectively.
Wilson et al pre-pared prior polarities from existing resources,and learned the context polarities by usingprior polarities and annotated corpora.
There-fore the prerequisite data and learned dataare opposite from those in our approach.
Wetook the approach used in this paper becausewe want to acquire more domain-dependentknowledge, and context polarity is easier toaccess in Japanese2.
Our approach and theirwork can complement each other.3 Methodology of Clause-level SAAs Figure 1 illustrates, the flow of our sen-timent analysis system involves three steps.The first step is sentence delimitation: the in-put document is divided into sentences.
Thesecond step is proposition detection: proposi-tions which can form polar clauses are identi-fied in each sentence.
The third step is polarityassignment: the polarity of each propositionis examined by considering the polar atoms.This section describes the last two processes,which are based on a deep sentiment analy-sis method analogous to machine translation(Kanayama et al, 2004) (hereafter ?the MTmethod?
).3.1 Proposition DetectionOur basic tactic for clause-level SA is the high-precision detection of polar clauses based ondeep syntactic analysis.
?Clause-level?
meansthat only predicative verbs and adjectives such2For example, indirect negation such as caused bya subject ?nobody?
or a modifier ?seldom?
is rare inJapanese.as in (7) are detected, and adnominal (attribu-tive) usages of verbs and adjectives as in (8)are ignored, because utsukushii (?beautiful?)
in(8) does not convey a positive polarity.
(7) E-ga utsukushii.
?The picture is beautiful.?
(8) Utsukushii hito-ni aitai.
?I want to meet a beautiful person.
?Here we use the notion of a proposition as aclause without modality, led by a predicativeverb or a predicative adjective.
The proposi-tions detected from a sentence are subject tothe assignment of polarities.Basically, we detect a proposition only atthe head of a syntactic tree3.
However, thislimitation reduces the recall of sentiment anal-ysis to a very low level.
In the example (7)above, utsukushii is the head of the tree, whilethose initial clauses in (9) to (11) below arenot.
In order to achieve higher recall whilemaintaining high precision, we apply two typesof syntactic patterns, modality patterns andconjunctive patterns4, to the tree structuresfrom the full-parsing.
(9) Sore-ha utsukushii-to omou.
?I think it is beautiful.?
(10) Sore-ha utsukushiku-nai.
?It is not beautiful.?
(11) Sore-ga utsukushii-to yoi.
?I hope it is beautiful.
?Modality patterns match some auxiliaryverbs or corresponding sentence-final expres-sions, to allow for specific kinds of modalityand negation.
One of the typical patterns is[ v to omou] (?I think v ?
)5, which allows ut-sukushii in (9) to be a proposition.
Also nega-tion is handled with a modality pattern, suchas [ v nai] (?not v ?).
In this case a neg fea-ture is attached to the proposition to identifyutsukushii in (10) as a negated proposition.On the other hand, no proposition is identi-fied in (11) due to the deliberate absence ofa pattern [ v to yoi] (?I hope v ?).
We useda total of 103 domain-independent modalitypatterns, most of which are derived from the3This is same as the rightmost part of the sentencesince all Japanese modification is directed left to right.4These two types of patterns correspond to auxil-iary patterns in the MT method, and can be appliedindependent of domains.5 v denotes a verb or an adjective.357coordinative (roughly ?and?
)-te, -shi, -ueni, -dakedenaku, -nominarazucausal (roughly ?because?
)-tame, -kara, -nodeadversative (roughly ?but?
)-ga, -kedo, -keredo, - monono, -nodagaTable 1: Japanese conjunctions used for con-junctive patterns.MT method, and some patterns are manuallyadded for this work to achieve higher recall.Another type of pattern is conjunctive pat-terns, which allow multiple propositions in asentence.
We used a total of 22 conjunctivepatterns also derived from the MT method, asexemplified in Table 1.
In such cases of coordi-native clauses and causal clauses, both clausescan be polar clauses.
On the other hand, noproposition is identified in a conditional clausedue to the absence of corresponding conjunc-tive patterns.3.2 Polarity Assignment Using PolarAtomsTo assign a polarity to each proposition, po-lar atoms in the lexicon are compared to theproposition.
A polar atom consists of po-larity, verb or adjective, and optionally, itsarguments.
Example (12) is a simple polaratom, where no argument is specified.
Thisatom matches any proposition whose head isutsukushii.
Example (13) is a complex polaratom, which assigns a negative polarity to anyproposition whose head is the verb kaku andwhere the accusative case is miryoku.
(12) [+] utsukushii?to be beautiful?
(13) [?]
kaku ?
miryoku-wo?to lack ?
attraction-ACC?A polarity is assigned if there exists a polaratom for which verb/adjective and the argu-ments coincide with the proposition, and oth-erwise no polarity is assigned.
The oppositepolarity of the polar atom is assigned to aproposition which has the neg feature.We used a total of 3,275 polar atoms, mostof which are derived from an English sentimentlexicon (Yi et al, 2003).According to the evaluation of the MTmethod (Kanayama et al, 2004), high-precision sentiment analysis had been achievedusing the polar atoms and patterns, where thesplendidlight have-zoomsmall-LCD ?
satisfied?high-priceIff?Inter-sententialContext66Intra-sententialContextFigure 2: The concept of the intra- and inter-sentential contexts, where the polarities areperfectly coherent.
The symbol ???
denotesthe existence of an adversative conjunction.system never took positive sentiment for neg-ative and vice versa, and judged positive ornegative to neutral expressions in only about10% cases.
However, the recall is too low, andmost of the lexicon is for domain-independentexpressions, and thus we need more lexical en-tries to grasp the positive and negative aspectsin a specific domain.4 Context CoherencyThis section introduces the intra- and inter-sentential contexts in which we assume contextcoherency for polarity, and describes some pre-liminary analysis of the assumption.4.1 Intra-sentential andInter-sentential ContextThe identification of propositions describedin Section 3.1 clarifies our viewpoint of thecontexts.
Here we consider two types ofcontexts: intra-sentential context and inter-sentential context.
Figure 2 illustrates thecontext coherency in a sample discourse (14),where the polarities are perfectly coherent.
(14) Kono kamera-ha subarashii-to omou.
?I think this camera is splendid.
?Karui-shi, zuumu-mo tsuite-iru.
?It?s light and has a zoom lens.
?Ekishou-ga chiisai-kedo, manzoku-da.
?Though the LCD is small, I?m satisfied.
?Tada, nedan-ga chotto takai.
?But, the price is a little high.
?The intra-sentential context is the link be-tween propositions in a sentence, which aredetected as coordinative or causal clauses.
Ifthere is an adversative conjunction such as-kedo (?but?)
in the third sentence in (14), aflag is attached to the relation, as denotedwith ???
in Figure 2.
Though there are dif-ferences in syntactic phenomena, this is sim-358shikashi (?however?
), demo (?but?
), sorenanoni(?even though?
), tadashi (?on condition that?
),dakedo (?but?
), gyakuni (?on the contrary?
),tohaie (?although?
), keredomo (?however?
),ippou (?on the other hand?
)Table 2: Inter-sentential adversative expres-sions.Domain Post.
Sent.
Len.digital cameras 263,934 1,757,917 28.3movies 163,993 637,054 31.5mobile phones 155,130 609,072 25.3cars 159,135 959,831 30.9Table 3: The corpora from four domainsused in this paper.
The ?Post.?
and ?Sent.
?columns denote the numbers of postings andsentences, respectively.
?Len.?
is the averagelength of sentences (in Japanese characters).ilar to the semantic orientation proposed byHatzivassiloglou and McKeown (1997).The inter-sentential context is the link be-tween propositions in the main clauses of pairsof adjacent sentences in a discourse.
The po-larities are assumed to be the same in theinter-sentential context, unless there is an ad-versative expression as those listed in Table 2.If no proposition is detected as in a nominalsentence, the context is split.
That is, there isno link between the proposition of the previoussentence and that of the next sentence.4.2 Preliminary Study on ContextCoherencyWe claim these two types of context can beused for unsupervised learning as clues to as-sign a tentative polarity to unknown expres-sions.
To validate our assumption, we con-ducted preliminary observations using variouscorpora.4.2.1 CorporaThroughout this paper we used Japanesecorpora from discussion boards in four differ-ent domains, whose features are shown in Ta-ble 3.
All of the corpora have clues to theboundaries of postings, so they were suitableto identify the discourses.4.2.2 Coherent PrecisionHow strong is the coherency in the con-text proposed in Section 4.1?
Using the polarclauses detected by the SA system with theinitial lexicon, we observed the coherent pre-cision of domain d with lexicon L, defined as:cp(d, L) = #(Coherent)#(Coherent)+#(Conflict) (15)where #(Coherent) and #(Conflict) are oc-currence counts of the same and opposite po-larities observed between two polar clauses asobserved in the discourse.
As the two polarclauses, we consider the following types:Window.
A polar clause and the nearest po-lar clause which is found in the precedingn sentences in the discourse.Context.
Two polar clauses in the intra-sentential and/or inter-sentential contextdescribed in Section 4.1.
This is the view-point of context in our method.Table 4 shows the frequencies of coherentpairs, conflicting pairs, and the coherent pre-cision for half of the digital camera domaincorpus.
?Baseline?
is the percentage of posi-tive clauses among the polar clauses6.For the ?Window?
method, we tested forn=0, 1, 2, and ?.
?0?
means two propositionswithin a sentence.
Apparently, the larger thewindow size, the smaller the cp value.
Whenthe window size is ??
?, implying anywherewithin a discourse, the ratio is larger than thebaseline by only 2.7%, and thus these typesof coherency are not reliable even though thenumber of clues is relatively large.?Context?
shows the coherency of the twotypes of context that we considered.
Thecp values are much higher than those in the?Window?
methods, because the relationshipsbetween adjacent pairs of clauses are handledmore appropriately by considering syntactictrees, adversative conjunctions, etc.
The cpvalues for inter-sentential and intra-sententialcontexts are almost the same, and thus bothcontexts can be used to obtain 2.5 times moreclues for the intra-sentential context.
In therest of this paper we will use both contexts.We also observed the coherent precision foreach domain corpus.
The results in the cen-ter column of Table 5 indicate the numberis slightly different among corpora, but all ofthem are far from perfect coherency.6If there is a polar clause whose polarity is unknown,the polarity is correctly predicted with at least 57.0%precision by assuming ?positive?.359Model Coherent Conflict cp(d, L)Baseline 57.0%Windown = 0 3,428 1,916 64.1%n = 1 11,448 6,865 62.5%n = 2 16,231 10,126 61.6%n = ?
26,365 17,831 59.7%Contextintra.
2,583 996 72.2%inter.
3,987 1,533 72.2%both 6,570 2,529 72.2%Table 4: Coherent precision with various view-points of contexts.Domain cp(d, L) cd(d, L)digital cameras 72.2% 7.23%movies 76.7% 18.71%mobile phones 72.9% 7.31%cars 73.4% 7.36%Table 5: Coherent precision and coherent den-sity for each domain.4.2.3 Coherent DensityBesides the conflicting cases, there are manymore cases where a polar clause does not ap-pear in the polar context.
We also observedthe coherent density of the domain d with thelexicon L defined as:cd(d, L) = #(Coherent)#(Polar) (16)This indicates the ratio of polar clauses thatappear in the coherent context, among all ofthe polar clauses detected by the system.The right column of Table 5 shows the co-herent density in each domain.
The moviedomain has notably higher coherent densitythan the others.
This indicates the sentimentexpressions are more frequently used in themovie domain.The next section describes the method ofour unsupervised learning using this imperfectcontext coherency.5 Unsupervised Learning forAcquisition of Polar AtomsFigure 3 shows the flow of our unsupervisedlearning method.
First, the runtime SA sys-tem identifies the polar clauses, and the can-didate polar atoms are collected.
Then, eachcandidate atom is validated using the two met-rics in the previous section, cp and cd, whichare calculated from all of the polar clausesfound in the domain corpus.DomainCorpus d-InitialLexicon L*SA6PolarClausescontext-?UCandidatePolar Atomsf(a), p(a), n(a)cd(d, L)cp(d, L)?test6R?N ??
test-?- ?
- NewLexiconFigure 3: The flow of the learning process.ID Candidate Polar Atom f(a) p(a) n(a)1* chiisai ?to be small?
3,014 226 2272 shikkari-suru ?to be firm?
246 54 103 chiisai ?
bodii-ga 11 4 0?to be small ?
body-NOM?4* todoku ?
mokuyou-ni 2 0 2?to be delivered?on Thursday?Table 6: Examples of candidate polar atomsand their frequencies.
?*?
denotes that itshould not be added to the lexicon.
f(a), p(a),and n(a) denote the frequency of the atom andin positive and negative contexts, respectively.5.1 Counts of Candidate Polar AtomsFrom each proposition which does not have apolarity, candidate polar atoms in the form ofsimple atoms (just a verb or adjective) or com-plex atoms (a verb or adjective and its right-most argument consisting of a pair of a nounand a postpositional) are extracted.
For eachcandidate polar atom a, the total appearancesf(a), and the occurrences in positive contextsp(a) and negative contexts n(a) are counted,based on the context of the adjacent clauses(using the method described in Section 4.1).If the proposition has the neg feature, the po-larity is inverted.
Table 6 shows examples ofcandidate polar atoms with their frequencies.5.2 Determination for Adding toLexiconAmong the located candidate polar atoms,how can we distinguish true polar atoms,which should be added to the lexicon, fromfake polar atoms, which should be discarded?As shown in Section 4, both the coherentprecision (72-77%) and the coherent density(7-19%) are so small that we cannot rely oneach single appearance of the atom in the po-lar context.
One possible approach is to setthe threshold values for frequency in a polarcontext, max(p(a), n(a)) and for the ratio ofappearances in polar contexts among the to-360tal appearances, max(p(a),n(a))f(a) .
However, theoptimum threshold values should depend onthe corpus and the initial lexicon.In order to set general criteria, here we as-sume that a true positive polar atom a shouldhave higher p(a)f(a) than its average i.e.
coher-ent density, cd(d, L+a), and also have higherp(a)p(a)+n(a) than its average i.e.
coherent preci-sion, cp(d, L+a) and these criteria should bemet with 90% confidence, where L+a is theinitial lexicon with a added.
Assuming the bi-nomial distribution, a candidate polar atom isadopted as a positive polar atom7 if both (17)and (18) are satisfied8.q > cd(d, L),wherep(a)?k=0f(a)Ckqk(1?
q)f(a)?k = 0.9(17)r > cp(d, L) or n(a) = 0,wherep(a)?k=0p(a)+n(a)Ckrk(1?
r)p(a)+n(a)?k= 0.9(18)We can assume cd(d, L+a) ' cd(d, L), andcp(d, L+a) ' cp(d, L) when L is large.
Wecompute the confidence interval using approx-imation with the F-distribution (Blyth, 1986).These criteria solve the problems in mini-mum frequency and scope of the polar atomssimultaneously.
In the example of Table 6, thesimple atom chiisai (ID=1) is discarded be-cause it does not meet (18), while the complexatom chiisai ?
bodii-ga (ID=3) is adoptedas a positive atom.
shikkari-suru (ID=2)is adopted as a positive simple atom, eventhough 10 cases out of 64 were observed in thenegative context.
On the other hand, todoku?
mokuyou-ni (ID=4) is discarded because itdoes not meet (17), even though n(a)f(a) = 1.0,i.e.
always observed in negative contexts.6 Evaluation6.1 Evaluation by Polar AtomsFirst we propose a method of evaluation of thelexical learning.7The criteria for the negative atoms are analogous.8nCr notation is used here for combination (nchoose k).Annotator BPositive Neutral NegativeAnno- Positive 65 11 3tator Neutral 3 72 0A Negative 1 4 41Table 7: Agreement of two annotators?
judg-ments of 200 polar atoms.
?=0.83.It is costly to make consistent and large?gold standards?
in multiple domains, espe-cially in identification tasks such as clause-level SA (cf.
classification tasks).
Thereforewe evaluated the learning results by asking hu-man annotators to classify the acquired polaratoms as positive, negative, and neutral, in-stead of the instances of polar clauses detectedwith the new lexicon.
This can be done be-cause the polar atoms themselves are informa-tive enough to imply to humans whether theexpressions hold positive or negative meaningsin the domain.To justify the reliability of this evaluationmethod, two annotators9 evaluated 200 ran-domly selected candidate polar atoms in thedigital camera domain.
The agreement resultsare shown in Table 7.
The manual classifi-cation was agreed upon in 89% of the casesand the Kappa value was 0.83, which is highenough to be considered consistent.Using manual judgment of the polar atoms,we evaluated the performance with the follow-ing three metrics.Type Precision.
The coincidence rate of thepolarity between the acquired polar atomand the human evaluators?
judgments.
Itis always false if the evaluators judged itas ?neutral.
?Token Precision.
The coincidence rate ofthe polarity, weighted by its frequency inthe corpus.
This metric emulates the pre-cision of the detection of polar clauseswith newly acquired poler atoms, in theruntime SA system.Relative Recall.
The estimated ratio of thenumber of detected polar clauses with theexpanded lexicon to the number of de-tected polar clauses with the initial lex-9For each domain, we asked different annotatorswho are familiar with the domain.
They are not theauthors of this paper.361Domain # Type Token RelativePrec.
Prec.
Recalldigital cameras 708 65% 96.5% 1.28movies 462 75% 94.4% 1.19mobile phones 228 54% 92.1% 1.13cars 487 68% 91.5% 1.18Table 8: Evaluation results with our method.The column ?#?
denotes the number of polaratoms acquired in each domain.icon.
Relative recall will be 1 when nonew polar atom is acquired.
Since the pre-cision was high enough, this metric canbe used for approximation of the recall,which is hard to evaluate in extractiontasks such as clause-/phrase-level SA.6.2 Robustness for DifferentConditions6.2.1 Diversity of CorporaFor each of the four domain corpora, the an-notators evaluated 100 randomly selected po-lar atoms which were newly acquired by ourmethod, to measure the precisions.
Relativerecall is estimated by comparing the numbersof detected polar clauses from randomly se-lected 2,000 sentences, with and without theacquired polar atoms.
Table 8 shows the re-sults.
The token precision is higher than 90%in all of the corpora, including the movie do-main, which is considered to be difficult for SA(Turney, 2002).
This is extremely high preci-sion for this task, because the correctness ofboth the extraction and polarity assignmentwas evaluated simultaneously.
The relative re-call 1.28 in the digital camera domain meansthe recall is increased from 43%10 to 55%.
Thedifference was smaller in other domains, butthe domain-dependent polar clauses are muchinformative than general ones, thus the high-precision detection significantly enhances thesystem.To see the effects of our method, we con-ducted a control experiment which used pre-set criteria.
To adopt the candidate atom a,the frequency of polarity, max(p(a), n(a)) wasrequired to be 3 or more, and the ratio of po-larity, max(p(a),n(a))f(a) was required to be higherthan the threshold ?.
Varying ?
from 0.05 to10The human evaluation result for digital camera do-main (Kanayama et al, 2004).6?
-Relative recallTokenprecision0.511.0 1.1 1.2??
?
= 0.05?
?
= 0.1???
= 0.3??????
= 0.8?
?digital cameras??
?
= 0.05??
= 0.1??
?
= 0.3??????
?movies(our method)?YFigure 4: Relative recall vs. token precisionwith various preset threshold values ?
for thedigital camera and movie domains.
The right-most star and circle denote the performance ofour method.0.8, we evaluated the token precision and therelative recall in the domains of digital cam-eras and movies.
Figure 4 shows the results.The results showed both relative recall andtoken precision were lower than in our methodfor every ?, in both corpora.
The optimum ?was 0.3 in the movie domain and 0.1 in thedigital camera domain.
Therefore, in this pre-set approach, a tuning process is necessary foreach domain.
Our method does not requirethis tuning, and thus fully automatic learningwas possible.Unlike the normal precision-recall tradeoff,the token precision in the movie domain gotlower when the ?
is strict.
This is due to thefrequent polar atoms which can be acquiredat the low ratios of the polarity.
Our methoddoes not discard these important polar atoms.6.2.2 Size of the Initial LexiconWe also tested the performance while vary-ing the size of the initial lexicon L. We pre-pared three subsets of the initial lexicon, L0.8,L0.5, and L0.2, removing polar atoms ran-domly.
These lexicons had 0.8, 0.5, 0.2 timesthe polar atoms, respectively, compared toL.
Table 9 shows the precisions and recallsusing these lexicons for the learning process.Though the cd values vary, the precision wasstable, which means that our method was ro-bust even for different sizes of the lexicon.
Thesmaller the initial lexicon, the higher the rela-tive recall, because the polar atoms which wereremoved from L were recovered in the learningprocess.
This result suggests the possibility of362lexicon cd Token Prec.
Relative Rec.L 7.2% 96.5% 1.28L0.8 6.1% 97.5% 1.41L0.5 3.9% 94.2% 2.10L0.2 3.6% 84.8% 3.55Table 9: Evaluation results for various sizes ofthe initial lexicon (the digital camera domain).the bootstrapping method from a small initiallexicon.6.3 Qualitative EvaluationAs seen in the agreement study, the polaratoms used in our study were intrinsicallymeaningful to humans.
This is because theatoms are predicate-argument structures de-rived from predicative clauses, and thus hu-mans could imagine the meaning of a polaratom by generating the corresponding sen-tence in its predicative form.In the evaluation process, some interestingresults were observed.
For example, a nega-tive atom nai ?
kerare-ga (?to be free fromvignetting?)
was acquired in the digital cam-era domain.
Even the evaluator who was fa-miliar with digital cameras did not know theterm kerare (?vignetting?
), but after looking upthe dictionary she labeled it as negative.
Ourlearning method could pick up such technicalterms and labeled them appropriately.Also, there were discoveries in the erroranalysis.
An evaluator assigned positive to aru?
kamera-ga (?to have camera?)
in the mobilephone domain, but the acquired polar atomhad the negative polarity.
This was actuallyan insight from the recent opinions that manyusers want phones without camera functions11.7 ConclusionWe proposed an unsupervised method to ac-quire polar atoms for domain-oriented SA, anddemonstrated its high performance.
The lex-icon can be expanded automatically by us-ing unannotated corpora, and tuning of thethreshold values is not required.
Thereforeeven end-users can use this approach to im-prove the sentiment analysis.
These featuresallow them to do on-demand analysis of morenarrow domains, such as the domain of digital11Perhaps because cameras tend to consume batterypower and some users don?t need them.cameras of a specific manufacturer, or the do-main of mobile phones from the female users?point of view.ReferencesC.
R. Blyth.
1986.
Approximate binomial confi-dence limits.
Journal of the American StatisticalAsscoiation, 81(395):843?855.Vasileios Hatzivassiloglou and Kathleen R. McKe-own.
1997.
Predicting the semantic orientationof adjectives.
In Proceedings of the 35th ACLand the 8th EACL, pages 174?181.Hiroshi Kanayama, Tetsuya Nasukawa, and HideoWatanabe.
2004.
Deeper sentiment analysis us-ing machine translation technology.
In Proceed-ings of the 20th COLING, pages 494?500.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Senti-ment analysis: Capturing favorability using nat-ural language processing.
In Proceedings of theSecond K-CAP, pages 70?77.Bo Pang and Lillian Lee.
2004.
A sentimentaleducation: Sentiment analysis using subjectiv-ity summarization based on minimum cuts.
InProceedings of the 42nd ACL, pages 271?278.Ana-Maria Popescu and Oren Etzioni.
2005.
Ex-tracting product features and opinions fromreviews.
In Proceedings of HLT/EMNLP-05,pages 339?346.Ellen Riloff and Janyee Wiebe.
2003.
Learning ex-traction patterns for subjective expressions.
InProceedings of EMNLP-03, pages 105?112.Peter D. Turney.
2002.
Thumbs up or thumbsdown?
Semantic orientation applied to unsuper-vised classification of reviews.
In Proceedings ofthe 40th ACL, pages 417?424.Theresa Wilson, Janyce Wiebe, and Paul Hoff-mann.
2005.
Recognizing contextual polarity inphrase-level sentiment analysis.
In Proceedingsof HLT/EMNLP-05, pages 347?354.Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu,and Wayne Niblack.
2003.
Sentiment analyzer:Extracting sentiments about a given topic usingnatural language processing techniques.
In Pro-ceedings of the Third IEEE International Con-ference on Data Mining, pages 427?434.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separatingfacts from opinions and identifying the polarityof opinion sentences.
In Proceedings of EMNLP-2003, pages 129?136.363
