Named Entity Discovery Using Comparable News ArticlesYusuke SHINYAMA and Satoshi SEKINEComputer Science DepartmentNew York University715, Broadway, 7th FloorNew York, NY, 10003yusuke@cs.nyu.edu, sekine@cs.nyu.eduAbstractIn this paper we describe a way to discoverNamed Entities by using the distribution ofwords in news articles.
Named Entity recog-nition is an important task for today?s naturallanguage applications, but it still suffers fromdata sparseness.
We used an observation that aNamed Entity is likely to appear synchronouslyin several news articles, whereas a commonnoun is less likely.
Exploiting this characteris-tic, we successfully obtained rare Named Enti-ties with 90% accuracy just by comparing timeseries distributions of a word in two newspa-pers.
Although the achieved recall is not suf-ficient yet, we believe that this method can beused to strengthen the lexical knowledge of aNamed Entity tagger.1 IntroductionRecently, Named Entity (NE) recognition has beengetting more attention as a basic building block forpractical natural language applications.
A NamedEntity tagger identifies proper expressions such asnames, locations and dates in sentences.
We aretrying to extend this to an Extended Named Entitytagger, which additionally identifies some commonnouns such as disease names or products.
We be-lieve that identifying these names is useful for manyapplications such as information extraction or ques-tion answering (Sekine et al, 2002).Normally a Named Entity tagger uses lexical orcontextual knowledge to spot names which appearin documents.
One of the major problem of this taskis its data sparseness.
Names appear very frequentlyin regularly updated documents such as news arti-cles or web pages.
They are, however, much morevaried than common nouns, and changing contin-uously.
Since it is hard to construct a set of pre-defined names by hand, usually some corpus basedapproaches are used for building such taggers.However, as Zipf?s law indicates, most of thenames which occupy a large portion of vocabularyare rarely used.
So it is hard for Named Entitytagger developers to keep up with a contemporaryset of words, even though a large number of docu-ments are provided for learning.
There still mightbe a ?wild?
noun which doesn?t appear in the cor-pora.
Several attempts have been made to tacklethis problem by using unsupervised learning tech-niques, which make vast amount of corpora avail-able to use.
(Strzalkowski and Wang, 1996) and(Collins and Singer, 1999) tried to obtain either lex-ical or contextual knowledge from a seed given byhand.
They trained the two different kind of knowl-edge alternately at each iteration of training.
(Yan-garber et al, 2002) tried to discover names with asimilar method.
However, these methods still sufferin the situation where the number of occurrences ofa certain name is rather small.2 Synchronicity of NamesIn this paper we propose another method tostrengthen the lexical knowledge for Named Entitytagging by using synchronicity of names in com-parable documents.
One can view a ?comparable?document as an alternative expression of the samecontent.
Now, two document sets where each doc-ument of one set is associated with one in the otherset, is called a ?comparable?
corpus.
A compara-ble corpus is less restricted than a parallel corpusand usually more available.
Several different news-papers published on the same day report lots of thesame events, therefore contain a number of compa-rable documents.
One can also take another view ofa comparable corpus, which is a set of paraphraseddocuments.
By exploiting this feature, one can ex-tract paraphrastic expressions automatically fromparallel corpora (Barzilay and McKeown, 2001) orcomparable corpora (Shinyama and Sekine, 2003).Named Entities in comparable documents haveone notable characteristic: they tend to be preservedacross comparable documents because it is gener-ally difficult to paraphrase names.
We think thatit is also hard to paraphrase product names or dis-ease names, so they will also be preserved.
There-fore, if one Named Entity appears in one document,it should also appear in the comparable document.0510152025300  50  100  150  200  250  300  350  400FrequencyDateLATWPNYTREUTEThe occurrence of the word ?yigal?0204060801001201400  50  100  150  200  250  300  350  400FrequencyDateLATWPNYTREUTEThe occurrence of the word ?killed?Figure 1: The occurrence of two words in 1995Consequently, if one has two sets of documentswhich are associated with each other, the distribu-tion of a certain name in one document set shouldlook similar to the distribution of the name in theother document set.We tried to use this characteristic of Named En-tities to discover rare names from comparable newsarticles.
We particularly focused on the time seriesdistribution of a certain word in two newspapers.We hypothesized that if a Named Entity is usedin two newspapers, it should appear in both news-papers synchronously, whereas other words don?t.Since news articles are divided day by day, it is easyto obtain the time series distribution of words ap-pearing in each newspaper.Figure 2 shows the time series distribution of thetwo words ?yigal?
and ?killed?, which appeared inseveral newspapers in 1995.
The word ?yigal?
(thename of the man who killed Israeli Prime MinisterYitzhak Rabin on Nov. 7, 1995) has a clear spike.There were a total of 363 documents which includedthe word that year and its occurrence is synchronousbetween the two newspapers.
In contrast, the word?killed?, which appeared in 21591 documents, isspread over all the year and has no clear character-istic.3 ExperimentTo verify our hypothesis, we conducted an experi-ment to measure the correlation between the occur-rence of Named Entity and its similarity of time se-ries distribution between two newspapers.First, we picked a rare word, then obtained itsdocument frequency which is the number of articleswhich contain the word.
Since newspaper articlesare provided separately day by day, we sampled thedocument frequency for each day.
These numbersform, for one year for example, a 365-element in-teger vector per newspaper.
The actual number ofnews articles is oscillating weekly, however, we nor-malized this by dividing the number of articles con-taining the word by the total number of all articleson that day.
At the end we get a vector of fractionswhich range from 0.0 to 1.0.Next we compared these vectors and calculatedthe similarity of their time series distributions acrossdifferent news sources.
Our basic strategy was touse the cosine similarity of two vectors as the likeli-hood of the word?s being a Named Entity.
However,several issues arose in trying to apply this directly.Firstly, it is not always true that the same event is re-ported on the same day.
An actual newspaper some-times has a one or two-day time lag depending onthe news.
To alleviate this effect, we applied a sim-ple smoothing to each vector.
Secondly, we neededto focus on the salient use of each word, otherwise acommon noun which constantly appears almost ev-ery day has an undesirable high similarity betweennewspapers.
To avoid this, we tried to intensify theeffect of a spike by comparing the deviation of thefrequency instead of the frequency itself.
This waywe can degrade the similarity of a word which has a?flat?
distribution.In this section we first explain a single-word ex-periment which detects Named Entities that consistof one word.
Next we explain a multi-word exper-iment which detects Named Entities that consist ofexactly two words.3.1 Single-word ExperimentIn a single-word experiment, we used two one-year newspapers, Los Angeles Times and Reuters in1995.
First we picked a rare word which appearedin either newspaper less than 100 times throughoutthe year.
We only used a simple tokenizer and con-verted all words into lower case.
A part of speechtagger was not used.
Then we obtained the docu-ment frequency vector for the word.
For each wordw which appeared in newspaper A, we got the doc-ument frequency at date t:fA(w, t) = dfA(w, t)/NA(t)where dfA(w, t) is the number of documents whichcontain the word w at date t in newspaper A. Thenormalization constant NA(t) is the number of allarticles at date t. However comparing this valuebetween two newspapers directly cannot capture atime lag.
So now we apply smoothing by the fol-lowing formula to get an improved version of fA:f ?A(w, t) =?
?W?i?Wr|i|fA(w, t+ i)Here we give each occurrence of a word a ?stretch?which sustains for W days.
This way we can cap-ture two occurrences which appear on slightly dif-ferent days.
In this experiment, we used W = 2and r = 0.3, which sums up the numbers in a 5-daywindow.
It gives each occurrence a 5-day stretchwhich is exponentially decreasing.Then we make another modification to f ?A bycomputing the deviation of f ?A to intensify a spike:f ?
?A(w, t) =f ?A(w, t)?
f?
?A?where f?
?A and ?
is the average and the standard de-viation of f ?A(w):f?
?A =?t f ?A(w, t)T?
=?
?t (f ?A(w, t)?
f?
?A)2TT is the number of days used in the experiment, e.g.T = 365 for one year.
Now we have a time seriesvector FA(w) for word w in newspaper A:FA(w) = {f ?
?A(w, 1), f ?
?A(w, 2), ..., f ?
?A(w, T )}Similarly, we calculated another time seriesFB(w) for newspaper B.
Finally we computedsim(w), the cosine similarity of two distributionsof the word w with the following formula:sim(w) = FA(w) ?
FB(w)|FA(w)||FB(w)|Since this is the cosine of the angle formed bythe two vectors, the obtained similarity ranges from?1.0 to 1.0.
We used sim(w) as the Named Entityscore of the word and ranked these words by thisscore.
Then we took the highly ranked words asNamed Entities.3.2 Multi-word ExperimentWe also tried a similar experiment for compoundwords.
To avoid chunking errors, we picked allconsecutive two-word pairs which appeared in bothnewspapers, without using any part of speech taggeror chunker.
Word pairs which include a pre-definedstop word such as ?the?
or ?with?
were eliminated.As with the single-word experiment, we measuredthe similarity between the time series distributionsfor a word pair in two newspapers.
One differentpoint is that we compared three newspapers 1 ratherthan two, to gain more accuracy.
Now the rankingscore sim(w) given to a word pair is calculated asfollows:sim(w) = simAB(w)?
simBC(w)?
simAC(w)where simXY (w) is the similarity of the distribu-tions between two newspapers X and Y , which canbe computed with the formula used in the single-word experiment.
To avoid incorrectly multiply-ing two negative similarities, a negative similarityis treated as zero.4 Evaluation and DiscussionTo evaluate the performance, we ranked 966 sin-gle words and 810 consecutive word pairs which arerandomly selected.
We measured how many NamedEntities are included in the highly ranked words.We manually classified as names the words in thefollowing categories used in IREX (Sekine and Isa-hara, 2000): PERSON, ORGANIZATION, LOCA-TION, and PRODUCT.
In both experiments, we re-garded a name which can stand itself as a correctNamed Entity, even if it doesn?t stretch to the entirenoun phrase.4.1 Single-word ExperimentTable 1 shows an excerpt of the ranking result.
Foreach word, the type of the word, the document fre-quency and the similarity (score) sim(w) is listed.Obvious typos are classified as ?typo?.
One can ob-serve that a word which is highly ranked is morelikely a Named Entity than lower ones.
To show thiscorrelation clearly, we plot the score of the wordsand the likelihood of being a Named Entity in Fig-ure 2.
Since the actual number of the words isdiscrete, we computed the likelihood by countingNamed Entities in a 50-word window around thatscore.Table 3 shows the number of obtained Named En-tities.
By taking highly ranked words (sim(w) ?1For the multi-word experiment, we used Los AngelesTimes, Reuters, and New York Times.Word Type Freq.
Scoresykesville LOCATION 4 1.000khamad PERSON 4 1.000zhitarenko PERSON 6 1.000sirica PERSON 9 1.000energiyas PRODUCT 4 1.000hulya PERSON 5 1.000salvis PERSON 5 0.960geagea PERSON 27 0.956bogdanor PERSON 6 0.944gomilevsky PERSON 6 0.939kulcsar PERSON 15 0.926carseats noun 17 0.912wilsons PERSON 32 0.897yeud ORGANIZATION 10 0.893yigal PERSON 490 0.878bushey PERSON 10 0.874pardew PERSON 17 0.857yids PERSON 5 0.844bordon PERSON 113 0.822... ... ... ...katyushas PRODUCT 56 0.516solzhenitsyn PERSON 81 0.490scheuer PERSON 9 0.478morgue noun 340 0.456mudslides noun 151 0.420rump noun 642 0.417grandstands noun 42 0.407overslept verb 51 0.401lehrmann PERSON 13 0.391... ... ... ...willowby PERSON 3 0.000unknowable adj 48 0.000taubensee PERSON 22 0.000similary (typo) 3 0.000recommitment noun 12 0.000perorations noun 3 0.000orenk PERSON 2 0.000malarkey PERSON 34 0.000gherardo PERSON 5 0.000dcis ORGANIZATION 3 0.000... ... ... ...merritt PERSON 149 -0.054echelon noun 97 -0.058plugging verb 265 -0.058normalcy noun 170 -0.063lovell PERSON 238 -0.066provisionally adv 74 -0.068sails noun 364 -0.075rekindled verb 292 -0.081sublime adj 182 -0.090afflicts verb 168 -0.116stan PERSON 994 -0.132Table 1: Ranking Result (Single-word)00.20.40.60.810  0.2  0.4  0.6  0.8LikelihoodScoreFigure 2: Relationship of the score and the likeli-hood of being a Named Entity (Single-word).
Thehorizontal axis shows the score of a word.
The ver-tical axis shows the likelihood of being a NE.
Onecan see that the likelihood of NE increases as thescore of a word goes up.
However there is a hugepeak near the score zero.0.6), we can discover rare Named Entities with 90%accuracy.
However, one can notice that there is ahuge peak near the score sim(w) = 0.
This meansthat many Named Entities still remain in the lowerscore.
Most such Named Entities only appeared inone newspaper.
Named Entities given a score lessthan zero were likely to refer to a completely differ-ent entity.
For example, the word ?Stan?
can be usedas a person name but was given a negative score, be-cause this was used as a first name of more than 10different people in several overlapping periods.Also, we took a look at highly ranked wordswhich are not Named Entities as shown in Table 2.The words ?carseats?, ?tiremaker?, or ?neurotripic?happened to appear in a small number of articles.Each of these articles and its comparable counter-parts report the same event, but both of them usethe same word probably because there was no othersuccinct expression to paraphrase these rare words.This way these three words made a high spike.
Theword ?officeholders?
was misrecognized due to theWord Type Freq.
Scorecarseats noun 17 0.9121tiremaker noun 21 0.8766officeholders noun 101 0.8053neurotrophic adj 11 0.7850mishandle verb 12 0.7369Table 2: Errors (Single-word)Words NEsAll words 966 462 (48%)sim(w) ?
0.6 102 92 (90%)sim(w) ?
0 511 255 (50%)Table 3: Obtained NEs (Single-word)Word pairs NEsAll word pairs 810 60 (7%)sim(w) ?
0.05 27 11 (41%)sim(w) ?
0 658 30 (5%)Table 4: Obtained NEs (Multi-word)repetition of articles.
This word appeared a lot oftimes and some of them made the spike very sharp,but it turned out that the document frequency wasundesirably inflated by the identical articles.
Theword ?mishandle?
was used in a quote by a per-son in both articles, which also makes a undesirablespike.4.2 Multi-word ExperimentIn the multi-word experiment, the accuracy of theobtained Named Entities was lower than in thesingle-word experiment as shown in Table 4, al-though correlation was still found between the scoreand the likelihood.
This is partly because there werefar fewer Named Entities in the test data.
Also,many word pairs included in the test data incorrectlycapture a noun phrase boundary, which may con-tain an incomplete Named Entity.
We think that thisproblem can be solved by using a chunk of wordsinstead of two consecutive words.
Another notableexample in the multi-word ranking is a quoted wordpair from the same speech.
Since a news articlesometime quotes a person?s speech literally, suchword pairs are likely to appear at the same time inboth newspapers.
However, since multi-word ex-pressions are much more varied than single-wordones, the overall frequency of multi-word expres-sions is lower, which makes such coincidence easilystand out.
We think that this kind of problem can bealleviated to some degree by eliminating completelyidentical sentences from comparable articles.The obtained ranking of word pairs are listed inTable 5.
The relationship between the score of wordpairs and the likelihood of being Named Entities isplotted in Figure 3.5 Conclusion and Future WorkIn this paper we described a novel way to discoverNamed Entities by using the time series distributionWord Type Freq.
Scorethai nation ORG.
82 0.425united network ORG.
31 0.290government open - 87 0.237club royale ORG.
32 0.142columnist pat - 81 0.111muslim minister - 28 0.079main antenna - 22 0.073great escape PRODUCT 32 0.059american black - 38 0.051patrick swayze PERSON 112 0.038finds unacceptable - 19 0.034mayor ron PERSON 49 0.032babi yar LOCATION 34 0.028bet secret - 97 0.018u.s.
passport - 58 0.017thursday proposed - 60 0.014atlantic command ORG.
30 0.013prosecutors asked - 73 0.011unmistakable message - 25 0.010fallen hero - 12 0.008american electronics ORG.
65 0.007primary goal - 138 0.007beach boys ORG.
119 0.006amnon rubinstein PERSON 31 0.005annual winter - 43 0.004television interviewer - 123 0.003outside simpson - 76 0.003electronics firm - 39 0.002sanctions lifted - 83 0.001netherlands antilles LOCATION 29 0.001make tough - 60 0.000permanent exhibit - 17 0.000Table 5: Ranking Result (Multi-word)00.20.40.60.810  0.02  0.04  0.06  0.08  0.1LikelihoodScoreFigure 3: Relationship of the score and the likeli-hood of being a Named Entity (Multi-word).
Thehorizontal axis shows the score of a word.
The ver-tical axis shows the likelihood of being a NE.of names.
Since Named Entities in comparable doc-uments tend to appear synchronously, one can find aNamed Entity by looking for a word whose chrono-logical distribution is similar among several compa-rable documents.
We conducted an experiment withseveral newspapers because news articles are gener-ally sorted chronologically, and they are abundant incomparable documents.
We confirmed that there issome correlation between the similarity of the timeseries distribution of a word and the likelihood ofbeing a Named Entity.We think that the number of obtained Named En-tities in our experiment was still not enough.
Sowe expect that better performance in actual NamedEntity tagging can be achieved by combining thisfeature with other contextual or lexical knowledge,mainly used in existing Named Entity taggers.6 AcknowledgmentsThis research was supported in part by the De-fense Advanced Research Projects Agency as partof the Translingual Information Detection, Extrac-tion and Summarization (TIDES) program, un-der Grant N66001-001-1-8917 from the Space andNaval Warfare Systems Center, San Diego, and bythe National Science Foundation under Grant ITS-00325657.
This paper does not necessarily reflectthe position of the U.S. Government.ReferencesRegina Barzilay and Kathleen R. McKeown.
2001.Extracting paraphrases from a parallel corpus.
InProceedings of ACL/EACL 2001.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
InProceedings of EMNLP 1999.Satoshi Sekine and Hitoshi Isahara.
2000.
IREX:IR and IE evaluation-based project in Japanese.In Proceedings of LREC 2000.Satoshi Sekine, Kiyoshi Sudo, and Chikashi No-bata.
2002.
Extended named entity hierarchy.
InProceedings of LREC 2002.Yusuke Shinyama and Satoshi Sekine.
2003.
Para-phrase acquisition for information extraction.
InProceedings of International Workshop on Para-phrasing 2003.Tomek Strzalkowski and Jin Wang.
1996.
A self-learning universal concept spotter.
In Proceed-ings of COLING 1996.Roman Yangarber, Winston Lin, and Ralph Grish-man.
2002.
Unsupervised learning of general-ized names.
In Proceedings of COLING 2002.
