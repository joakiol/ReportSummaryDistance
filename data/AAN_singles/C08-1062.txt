Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 489?496Manchester, August 2008PNR2: Ranking Sentences with Positive and Negative Reinforcementfor Query-Oriented Update SummarizationAbstractQuery-oriented update summarization isan emerging summarization task veryrecently.
It brings new challenges to thesentence ranking algorithms that requirenot only to locate the important andquery-relevant information, but also tocapture the new information whendocument collections evolve.
In thispaper, we propose a novel graph basedsentence ranking algorithm, namely PNR2,for update summarization.
Inspired by theintuition that ?a sentence receives apositive influence from the sentences thatcorrelate to it in the same collection,whereas a sentence receives a negativeinfluence from the sentences thatcorrelates to it in the different (perhapspreviously read) collection?, PNR2models both the positive and the negativemutual reinforcement in the rankingprocess.
Automatic evaluation on theDUC 2007 data set pilot taskdemonstrates the effectiveness of thealgorithm.1 IntroductionThe explosion of the WWW has brought with it avast board of information.
It has become virtuallyimpossible for anyone to read and understandlarge numbers of individual documents that areabundantly available.
Automatic documentsummarization provides an effective means to?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.manage such an exponentially increasedcollection of information and to supportinformation seeking and condensing goals.The main evaluation forum that providesbenchmarks for researchers working ondocument summarization to exchange their ideasand experiences is the Document UnderstandingConferences (DUC).
The goals of the DUCevaluations are to enable researchers toparticipate in large-scale experiments upon thestandard benchmark and to increase theavailability of appropriate evaluation techniques.Over the past years, the DUC evaluations haveevolved gradually from single-documentsummarization to multi-document summarizationand from generic summarization to query-oriented summarization.
Query-oriented multi-document summarization initiated in 2005 aimsto produce a short and concise summary for acollection of topic relevant documents accordingto a given query that describes a user?s particularinterests.Previous summarization tasks are all targetedon a single document or a static collection ofdocuments on a given topic.
However, thedocument collections can change (actually grow)dynamically when the topic evolves over time.New documents are continuously added into thetopic during the whole lifecycle of the topic andnormally they bring the new information into thetopic.
To cater for the need of summarizing adynamic collection of documents, the DUCevaluations piloted update summarization in 2007.The task of update summarization differs fromprevious summarization tasks in that the latteraims to dig out the salient information in a topicwhile the former cares the information not onlysalient but also novel.Up to the present, the predominant approachesin document summarization regardless of thenature and the goals of the tasks have still beenbuilt upon the sentence extraction framework.Li Wenjie1, Wei Furu1,2, Lu Qin1, He Yanxiang21Department of ComputingThe Hong Kong Polytechnic University, HK{csfwei, cswjli, csluqin}@comp.polyu.edu.hk2Department of Computer Scienceand Technology, Wuhan University, China{frwei, yxhe@whu.edu.cn}489Under this framework, sentence ranking is theissue of most concern.
In general, two kinds ofsentences need to be evaluated in updatesummarization, i.e.
the sentences in an early (old)document collection A (denoted by SA) and thesentences in a late (new) document collection B(denoted by SB).
Given the changes from SA to SB,an update summarization approach may beconcerned about four ranking issues: (1) rank SAindependently; (2) re-rank SA after SB comes; (3)rank SB independently; and (4) rank SB given thatSA is provided.
Among them, (4) is of mostconcern.
It should be noting that both (2) and (4)need to consider the influence from the sentencesin the same and different collections.In this study, we made an attempt to capturethe intuition that?A sentence receives a positive influence fromthe sentences that correlate to it in the samecollection, whereas a sentence receives anegative influence from the sentences thatcorrelates to it in the different collection.
?We represent the sentences in A or B as a textgraph constructed using the same approach aswas used in Erkan and Radev (2004a, 2004b).Different from the existing PageRank-likealgorithms adopted in document summarization,we propose a novel sentence ranking algorithm,called PNR2 (Ranking with Positive and NegativeReinforcement).
While PageRank models thepositive mutual reinforcement among thesentences in the graph, PNR2 is capable ofmodeling both positive and negativereinforcement in the ranking process.The remainder of this paper is organized asfollows.
Section 2 introduces the background ofthe work presented in this paper, includingexisting graph-based summarization models,descriptions of update summarization and time-based ranking solutions with web graph and textgraph.
Section 3 then proposes PNR2, a sentenceranking algorithm based on positive and negativereinforcement and presents a query-orientedupdate summarization model.
Next, Section 4reports experiments and evaluation results.Finally, Section 5 concludes the paper.2 Background and Related Work2.1 Previous Work in Graph-basedDocument SummarizationGraph-based ranking algorithms such asGoogle?s PageRank (Brin and Page, 1998) andKleinberg?s HITS (Kleinberg, 1999) have beensuccessfully used in the analysis of the linkstructure of the WWW.
Now they are springingup in the community of document summarization.The major concerns in graph-basedsummarization researches include how to modelthe documents using text graph and how totransform existing web page ranking algorithmsto their variations that could accommodatevarious summarization requirements.Erkan and Radev (2004a and 2004b)represented the documents as a weightedundirected graph by taking sentences as verticesand cosine similarity between sentences as theedge weight function.
An algorithm calledLexRank, adapted from PageRank, was appliedto calculate sentence significance, which wasthen used as the criterion to rank and selectsummary sentences.
Meanwhile, Mihalcea andTarau (2004) presented their PageRank variation,called TextRank, in the same year.
Besides, theyreported experimental comparison of threedifferent graph-based sentence rankingalgorithms obtained from Positional PowerFunction, HITS and PageRank (Mihalcea andTarau, 2005).
Both HITS and PageRankperformed excellently.Likewise, the use of PageRank family was alsovery popular in event-based summarizationapproaches (Leskovec et al, 2004; Vanderwendeet al, 2004; Yoshioka and Haraguchi, 2004; Li etal., 2006).
In contrast to conventional sentence-based approaches, newly emerged event-basedapproaches took event terms, such as verbs andaction nouns and their associated named entitiesas graph nodes, and connected nodes accordingto their co-occurrence information or semanticdependency relations.
They were able to providefiner text representation and thus could be infavor of sentence compression which wastargeted to include more informative contents in afixed-length summary.
Nevertheless, theseadvantages lied on appropriately defining andselecting event terms.All above-mentioned representative work wasconcerned with generic summarization.
Later on,graph-based ranking algorithms were introducedin query-oriented summarization too when thisnew challenge became a hot research topicrecently.
For example, a topic-sensitive versionof PageRank was proposed in (OtterBacher et al,2005).
The same algorithm was followed by Wanet al (2006) and Lin et al (2007) who furtherinvestigated on its application in query-orientedupdate summarization.4902.2 The DUC 2007 Update SummarizationTask DescriptionThe DUC 2007 update summarization pilot taskis to create short (100 words) multi-documentsummaries under the assumption that the readerhas already read some number of previousdocuments.
Each of 10 topics contains 25documents.
For each topic, the documents aresorted in chronological order and then partitionedinto three collections, ?A?, ?B?
and ?C?.
Theparticipants are then required to generate (1) asummary for ?A?
; (2) an update summary for?B?
assuming documents in ?A?
have alreadybeen read; and (3) an update summary for ?C?assuming documents in ?A?
and ?B?
havealready been read.
Growing out of the DUC 2007,the Text Analysis Conference (TAC) 2008planed to keep only the DUC 2007 task (1) and(2).Each topic collection in the DUC 2007 (willalso in the TAC 2008) is accompanied with aquery that describes a user?s interests and focuses.System-generated summaries should include asmany responses relevant to the given query aspossible.
Here is a query example from the DUC2007 document collection ?D0703A?.<topic><num> D0703A </num><title> Steps toward introduction of theEuro.
</title><narr> Describe steps taken and worldwidereaction prior to introduction of the Euro onJanuary 1, 1999.
Include predictions andexpectations reported in the press.
</narr></topic>                                          [D0703A]Update summarization is definitely a time-related task.
An appropriate ranking algorithmmust be the one capable of coping with thechange or the time issues.2.3 Time-based Ranking Solutions withWeb Graph and Text GraphGraph based models in document summarizationare inspired by the idea behind web graph modelswhich have been successfully used by currentsearch engines.
As a matter of fact, adding timedimension into the web graph has beenextensively studied in recent literature.Basically, the evolution in the web graph stemsfrom (1) adding new edges between two existingnodes; (2) adding new nodes in the existing graph(consequently adding new edges between theexisting nodes and the new nodes or among thenew nodes); and (3) deleting existing edges ornodes.
Berberich et al (2004 and 2005)developed two link analysis methods, i.e.
T-RankLight and T-Rank, by taking into account twotemporal aspects, i.e.
freshness (i.e.
timestamp ofmost recent update) and activity (i.e.
update rates)of the pages and the links.
They modeled the webas an evolving graph in which each nodes andedges (i.e.
web pages and hyperlinks) wereannotated with time information.
The timeinformation in the graph indicated different kindsof events in the lifespan of the nodes and edges,such as creation, deletion and modifications.Then they derived a subgraph of the evolvinggraph with respect to the user?s temporal interest.Finally, the time information of the nodes and theedges were used to modify the random walkmodel as was used in PageRank.
Specifically,they used it to modify the random jumpprobabilities (in both T-Rank Light and T-Rank)and the transition probabilities (in T-Rank only).Meanwhile, Yu et al (2004 and 2005)introduced a time-weighted PageRank, calledTimedPageRank, for ranking in a network ofscientific publications.
In their approach,citations were weighted based on their ages.
Thena post-processing step decayed the authority of apublication based on the publication?s age.
Later,Yang et al (2007) proposed TemporalRank,based on which they computed the pageimportance from two perspectives: theimportance from the current web graph snapshotand the accumulated historical importance fromprevious web graph snapshot.
They used a kineticmodel to interpret TemporalRank and showed itcould be regarded as a solution to an ordinarydifferential equation.In conclusion, Yu et al tried to cope with theproblem that PageRank favors over old pageswhose in-degrees are greater than those of newpages.
They worked on a static single snapshot ofthe web graph, and their algorithm could workwell on all pages in the web graph.
Yang et al,on the other hand, worked on a series of webgraphs at different snapshots.
Their algorithmwas able to provide more robust ranking of theweb pages, but could not alleviate the problemcarried by time dimension at each web graphsnapshot.
This is because they directly appliedthe original PageRank to rank the pages.
In otherwords, the old pages still obtained higher scoreswhile the newly coming pages still got lowerscores.
Berberich et al focused their efforts onthe evolution of nodes and edges in the webgraph.
However, their algorithms did not work491when the temporal interest of the user (or query)was not available.As for graph based update summarization,Wan (2007) presented the TimedTextRankalgorithm by following the same idea presentedin the work of Yu et al Given three collections ofchronologically ordered documents, Lin et al(2007) proposed to construct the TimeStampedgraph (TSG) graph by incrementally adding thesentences to the graph.
They modified theconstruction of the text graph, but the rankingalgorithm was the same as the one proposed byOtterBacher et alNevertheless, the text graph is different fromthe web graph.
The evolution in the text graph islimited to the type (2) in the web graph.
Thenodes and edges can not be deleted or modifiedonce they are inserted.
In other words, we areonly interested in the changes caused when newsentences are introduced into the existing textgraph.
As a result, the ideas from Berberich et alcannot be adopted directly in the text graph.Similarly, the problem in web graph as stated inthe work of Yu et al (i.e.
?new pages, which maybe of high quality, have few or no in-links andare left behind.?)
does not exist in the text graphat all.
More precisely, the new coming sentencesare equally treated as the existing sentences, andthe degree (in or out) of the new sentences arealso equally accumulated as the old sentences.Directly applying the ideas from the work of Yuet al does not always make sense in the textgraph.
Recall that the main task for sentenceranking in update summarization is to rank SBgiven SA.
So the idea from Yang et al is also notapplicable.In fact, the key points include not onlymaximizing the importance in the current newdocument collection but also minimizing theredundancy to the old document collection whenranking the sentences for update summarization.Time dimension does contribute here, but it is notthe only way to consider the changes.
Unlike theweb graph, the easily-captured contentinformation in a text graph can provide additionalmeans to analyze the influence of the changes.To conclude the previous discussions, addingtemporal information to the text graph is differentfrom it in the web graph.
Capturing operations(such as addition, deletion, modification of webpages and hyperlinks) is most concerned in theweb graph; however, prohibiting redundantinformation from the old documents is the mostcritical issue in the text graph.3 Positive and Negative ReinforcementRanking for Update SummarizationExisting document summarization approachesbasically follow the same processes: (1) firstcalculate the significance of the sentences withreference to the given query with/without usingsome sorts of sentence relations; (2) then rank thesentences according to certain criteria andmeasures; (3) finally extract the top-ranked butnon-redundant sentences from the originaldocuments to create a summary.
Under thisextractive framework, undoubtedly the twocritical processes involved are sentence rankingand sentence selection.
In the following sections,we will first introduce the sentence rankingalgorithm based on ranking with positive andnegative reinforcement, and then we present thesentence selection strategy.3.1 Ranking with Positive and NegativeReinforcement (PNR2)Previous graph-based sentence rankingalgorithms is capable to model the fact that asentence is important if it correlates to (many)other important sentences.
We call this positivemutual reinforcement.
In this paper, we study twokinds of reinforcement, namely positive andnegative reinforcement, among two documentcollections, as illustrated in Figure 1.Figure 1 Positive and Negative ReinforcementIn Figure 1, ?A?
and ?B?
denote two documentcollections about the same topics (?A?
is the olddocument collection, ?B?
is the new documentcollection), SA and SB denote the sentences in?A?
and ?B?.
We assume:1.
SA performs positive reinforcement on itsown internally;2.
SA performs negative reinforcement on SBexternally;3.
SB performs negative reinforcement on SAexternally;4.
SB performs positive reinforcement on itsown internally.Positive reinforcement captures the intuitionthat a sentence is more important if it associatesto the other important sentences in the samecollection.
Negative reinforcement, on the otherhand, reflects the fact that a sentence is lessA B + +--492important if it associates to the importantsentences in the other collection, since such asentence might repeat the same or very similarinformation which is supposed to be included inthe summary generated for the other collection.Let RA and RB denote the ranking of thesentences in A and B, the reinforcement can beformally described as??????+??+??=?+??+??=++BkBBBkABAkBAkBABkAAAkApRMRMRpRMRMRrr2)(2)(2)1(1)(1)(1)1(??????
(1)where the four matrices MAA, MBB, MAB and MBAare the affinity matrices of the sentences in SA, inSB, from SA to SB and from SB to SA.??????=2211???
?W  is a weight matrix to balance thereinforcement among different sentences.
Noticethat 0, 21 <??
such that they perform negativereinforcement.
Aprand Bprare two bias vectors,with 1,0 21 << ??
as the damping factors.
[ ]11?=nA npr , where n is the order of MAA.
Bprisdefined in the same way.
We will further definethe affinity matrices in section 3.2 later.
With theabove reinforcement ranking equation, it is alsotrue that1.
A sentence in SB correlates to many newsentences in SB is supposed to receive a highranking from RB, and2.
A sentence in SB correlates to many oldsentences in SA is supposed to receive a lowranking from RB.Let [ ]TBA RRR =  and [ ]TBA ppp rrr ?
?= 21 ??
, thenthe above iterative equation (1) corresponds tothe linear system,( ) pRMI r=??
(2)where, ??????=BBBAABAAMMMMM2211???
?.Up to now, the PNR2 is still query-independent.That means only the content of the sentences isconsidered.
However, for the tasks of query-oriented summarization, the reinforcement shouldobviously bias to the user?s query.
In this work,we integrate query information into PNR2 bydefining the vector pr  as ( )qsrelp ii |=r , where( )qsrel i |  denotes the relevance of the sentence sito the query q.To guarantee the solution of the linear systemEquation (2), we make the following twotransformations on M. First M is normalized bycolumns.
If all the elements in a column are zero,we replace zero elements with n1  (n is the totalnumber of the elements in that column).
Second,M is multiplied by a decay factor ?
( 10 <<?
),such that each element in M is scaled down butthe meaning of M will not be changed.Finally, Equation (2) is rewritten as,( ) pRMI r=???
?
(3)The matrix ( )MI ???
is a strictly diagonallydominant matrix now, and the solution of thelinear system Equation (3) exists.3.2 Sentence Ranking based on PNR2We use the above mentioned PNR2 framework torank the sentences in both SA and SBsimultaneously.
Section 3.2 defines the affinitymatrices and presents the ranking algorithm.The affinity (i.e.
similarity) between twosentences is measured by the cosine similarity ofthe corresponding two word vectors, i.e.
[ ] ( )ji sssimjiM ,, =                     (4)where ( )jijijisssssssim rrrr??=,.
However, whencalculating the affinity matrices MAA and MBB, thesimilarity of a sentence to itself is defined as 0,i.e.
[ ] ( )??
?=?= jijisssimjiM ji0,,              (5)Furthermore, the relevance of a sentence to thequery q is defined as( )qsqsqsreliii rrrr?
?=,                     (6)Algorithm 1.
RankSentence(SA, SB, q)Input: The old sentence set SA, the newsentence set SB, and the query q.Output: The ranking vectors R of SA and SB.1: Construct the affinity matrices, and set theweight matrix W;2: Construct the matrix ( )MIA ?
?= ?
.3: Choose (randomly) the initial non-negativevectors TR ]11[)0( L= ;4: 0?k , 0??
;5: Repeat6:     ( )?
?< >++ ?
?= ij ij kjijkjijiijki RaRapaR )()1()1( 1 r ;7:     ( ))()1(max kk RR ???
+ ;8:  )1( +kR is normalized such that the maximalelement in )1( +kR is 1.4939:     1+?
kk ;10: Until ?<?
1;11: )(kRR ?
;12: Return.Now, we are ready to adopt the Gauss-Seidelmethod to solve the linear system Equation (3),and an iterative algorithm is developed to rankthe sentences in SA and SB.After sentence ranking, the sentences in SBwith higher ranking will be considered to beincluded in the final summary.3.3 Sentence Selection by RemovingRedundancyWhen multiple documents are summarized, theproblem of information redundancy is moresevere than it is in single documentsummarization.
Redundancy removal is a must.Since our focus is designing effective sentenceranking approach, we apply the following simplesentence selection algorithm.Algorithm 2.
GenerateSummary(S, length)Input: sentence collection S (ranked indescending order of significance) and length(the given summary length limitation)Output: The generated summary ?{}??
;?l length;For i ?
0 to |S| dothreshold ?
( )( )?
?ssssim i   ,max ;If threshold <= 0.92 doisU???
;ll ?
- ( )islen ;If ( l <= 0) break;EndEndReturn ?
.4 Experimental Studies4.1 Data Set and Evaluation MetricsThe experiments are set up on the DUC 2007update pilot task data set.
Each collection ofdocuments is accompanied with a querydescription representing a user?s informationneed.
We simply focus on generating a summaryfor the document collection ?B?
given that the1?
is a pre-defined small real number as theconvergence threshold.2In fact, this is a tunable parameter in the algorithm.We use the value of 0.9 by our intuition.user has read the document collection ?A?, whichis a typical update summarization task.Table 1 below shows the basic statistics of theDUC 2007 update data set.
Stop-words in bothdocuments and queries are removed 3  and theremaining words are stemmed by PorterStemmer 4 .
According to the task definition,system-generated summaries are strictly limitedto 100 English words in length.
We incrementallyadd into a summary the highest ranked sentenceof concern if it doesn?t significantly repeat theinformation already included in the summaryuntil the word limitation is reached.A BAverage number of documents 10 10Average number of sentences 237.6 177.3Table 1.
Basic Statistics of DUC2007 Update Data SetAs for the evaluation metric, it is difficult tocome up with a universally accepted method thatcan measure the quality of machine-generatedsummaries accurately and effectively.
Manyliteratures have addressed different methods forautomatic evaluations other than human judges.Among them, ROUGE5 (Lin and Hovy, 2003) issupposed to produce the most reliable scores incorrespondence with human evaluations.
Giventhe fact that judgments by humans are time-consuming and labor-intensive, and moreimportant, ROUGE has been officially adoptedfor the DUC evaluations since 2005, like theother researchers, we also choose it as theevaluation criteria.In the following experiments, the sentencesand the queries are all represented as the vectorsof words.
The relevance of a sentence to thequery is calculated by cosine similarity.
Noticethat the word weights are normally measured bythe document-level TF*IDF scheme inconventional vector space models.
However, webelieve that it is more reasonable to use thesentence-level inverse sentence frequency (ISF)rather than document-level IDF when dealingwith sentence-level text processing.
This hasbeen verified in our early study.4.2 Comparison of Positive and NegativeReinforcement Ranking StrategyThe aim of the following experiments is toinvestigate the different reinforcement rankingstrategies.
Three algorithms (i.e.
PR(B),3A list of 199 words is used to filter stop-words.4http://www.tartarus.org/~martin/PorterStemmer.5ROUGE version 1.5.5 is used.494PR(A+B), PR(A+B/A)) are implemented asreference.
These algorithms are all based on thequery-sensitive LexRank (OtterBacher et al,2005).
The differences are two-fold: (1) thedocument collection(s) used to build the textgraph are different; and (2) after ranking, thesentence selection strategies are different.
Inparticular, PR(B) only uses the sentences in ?B?to build the graph, and the other two consider thesentences in both ?A?
and in ?B?.
Only thesentences in ?B?
are considered to be selected inPR(B) and PR(A+B/A), but all the sentences in?A?
and ?B?
have the same chance to be selectedin PR(A+B).
Only the sentences from B areconsidered to be selected in the final summariesin PNR2 as well.
In the following experiments,the damping factor is set to 0.85 in the first threealgorithms as the same in PageRank.
The weightmatrix W is set to ???????
?15.05.01in the proposedalgorithm (i.e.
PNR2) and 5.021 == ??
.
We haveobtained reasonable good results with the decayfactor ?
between 0.3 and 0.8.
So we set it to 0.5in this paper.Notice that the three PageRank-like graph-based ranking algorithms can be viewed as onlythe positive reinforcement among the sentences isconsidered, while both positive and negativereinforcement are considered in PNR2 asmentioned before.
Table 2 below shows theresults of recall scores of ROUGE-1, ROUGE-2and ROUGE-SU4 along with their 95%confidential internals within square brackets.ROUGE-1ROUGE-2ROUGE-SU4PR(B) 0.3323 [0.3164,0.3501]0.0814[0.0670,0.0959]0.11650.1053,0.1286]PR(A+B) 0.3059 [0.2841,0.3256]0.0746[0.0613,0.0893]0.1064[0.0938,0.1186]PR(A+B/A) 0.3376 [0.3186,0.3572]0.0865[0.0724,0.1007]0.1222[0.1104,0.1304]PNR2 0.3616 [0.3464,0.3756]0.0895[0.0810,0.0987]0.1291[0.1208,0.1384]Table 2.
Experiment ResultsWe come to the following three conclusions.First, it is not surprising that PR(B) andPR(A+B/A) outperform PR(A+B), because theupdate task obviously prefers the sentences fromthe new documents (i.e.
?B?).
Second,PR(A+B/A) outperforms PR(B) because thesentences in ?A?
can provide useful informationin ranking the sentences in ?B?, although we donot select the sentences ranked high in ?A?.
Third,PNR2 achieves the best performance.
PNR2 isabove PR(A+B/A) by 7.11% of ROUGE-1,3.47% of ROUGE-2, and 5.65% of ROUGE-SU4.This result confirms the idea and algorithmproposed in this work.4.3 Comparison with DUC 2007 SystemsTwenty-four systems have been submitted to theDUC for evaluation in the 2007 update task.Table 3 compares our PNR2 with them.
Forreference, we present the following representativeROUGE results of (1) the best and worstparticipating system performance, and (2) theaverage ROUGE scores (i.e.
AVG).
We can theneasily locate the positions of the proposed modelsamong them.PNR2 Mean Best / WorstROUGE-1 0.3616 0.3262 0.3768/0.2621ROUGE2 0.0895 0.0745 0.1117/0.0365ROUGE-SU4 0.1291 0.1128 0.1430/0.0745Table 3.
System Comparison4.4 DiscussionIn this work, we use the sentences in the samesentence set for positive reinforcement andsentences in the different set for negativereinforcement.
Precisely, the old sentencesperform negative reinforcement over the newsentences while the new sentences performpositive reinforcement over each other.
This isreasonable although we may have a morecomprehensive alternation.
Old sentences mayexpress old topics, but they may also expressemerging new topics.
Similarly, new sentencesare supposed to express new topics, but they mayalso express the continuation of old topics.
As aresult, it will be more comprehensive to classifythe whole sentences (both new sentences and oldsentences together) into two categories, i.e.
oldtopics oriented sentences and new topic orientedsentences, and then to apply these two sentencesets in the PNR2 framework.
This will be furtherstudied in our future work.Moreover, in the update summarization task,the summary length is restricted to about 100words.
In this situation, we find that sentencesimplification is even more important in ourinvestigations.
We will also work on this issue inour forthcoming studies.5 ConclusionIn this paper, we propose a novel sentenceranking algorithm, namely PNR2, for updatesummarization.
As our pilot study, we simplyassume to receive two chronologically ordereddocument collections and evaluate the summaries495generated for the collection given later.
WithPNR2, sentences from the new (i.e.
late)document collection perform positivereinforcement among each other but they receivenegative reinforcement from the sentences in theold (i.e.
early) document collection.
Positive andnegative reinforcement are concernedsimultaneously in the ranking process.
As a result,PNR2 favors the sentences biased to the sentencesthat are important in the new collection andmeanwhile novel to the sentences in the oldcollection.
As a matter of fact, this positive andnegative ranking scheme is general enough andcan be used in many other situations, such associal network analysis etc.AcknowledgementsThe research work presented in this paper waspartially supported by the grants from RGC ofHKSAR (Project No: PolyU5217/07E), NSF ofChina (Project No: 60703008) and the HongKong Polytechnic University (Project No: A-PA6L).ReferencesKlaus Berberich, Michalis Vazirgiannis, and GerhardWeikum.
2004.
G.T-Rank: Time-Aware AuthorityRanking.
In Algorithms and Models for the Web-Graph: Third International Workshop, WAW, pp131-141.Klaus Berberich, Michalis Vazirgiannis, and GerhardWeikum.
2005.
Time-Aware Authority Ranking.Journal of Internet Mathematics, 2(3): 301-332.Klaus Lorenz Berberich.
2004.
Time-aware andTrend-based Authority Ranking.
Master Thesis,Saarlandes  University, Germany.Sergey Brin and Lawrence Page.
1998.
The Anatomyof a Large-scale Hypertextual Web Search Engine.Computer Networks and ISDN Systems, 30(1-7):107-117.Gunes Erkan and Dragomir R. Radev.
2004a.LexPageRank: Prestige in Multi-Document TextSummarization, in Proceedings of EMNLP, pp365-371.Gunes Erkan and Dragomir R. Radev.
2004b.LexRank: Graph-based Centrality as Salience inText Summarization, Journal of ArtificialIntelligence Research 22:457-479.Jon M. Kleinberg.
1999.
Authoritative Sources inHyperlinked Environment, Journal of the ACM,46(5):604-632.Jure Leskovec, Marko Grobelnik and Natasa Milic-Frayling.
2004.
Learning Sub-structures ofDocument Semantic Graphs for DocumentSummarization, in Proceedings of LinkKDDWorkshop, pp133-138.Wenjie Li, Mingli Wu, Qin Lu, Wei Xu and ChunfaYuan.
2006.
Extractive Summarization using Intra-and Inter-Event Relevance, in Proceedings ofACL/COLING, pp369-376.Chin-Yew Lin and Eduard Hovy.
2003.
AutomaticEvaluation of Summaries Using N-gram Co-occurrence Statistics, in Proceedings of HLT-NAACL, pp71-78.Ziheng Lin, Tat-Seng Chua, Min-Yen Kan, Wee SunLee, Long Qiu, and Shiren Ye.
2007.
NUS at DUC2007: Using Evolutionary Models for Text.
InProceedings of Document UnderstandingConference (DUC) 2007.Rada Mihalcea and Paul Tarau.
2004.
TextRank -Bringing Order into Text, in Proceedings ofEMNLP, pp404-411.Rada Mihalcea.
2004.
Graph-based RankingAlgorithms for Sentence Extraction, Applied toText Summarization, in Proceedings of ACL(Companion Volume).Jahna OtterBacher, Gunes Erkan, Dragomir R. Radev.2005.
Using Random Walks for Question-focusedSentence Retrieval, in Proceedings ofHLT/EMNLP, pp915-922.Lucy Vanderwende, Michele Banko and ArulMenezes.
2004.
Event-Centric SummaryGeneration, in Working Notes of DUC 2004.Xiaojun Wan, Jianwu Yang and Jianguo Xiao.
2006.Using Cross-Document Random Walks for Topic-Focused Multi-Document Summarization, inProceedings of the 2006 IEEE/WIC/ACMInternational Conference on Web Intelligence,pp1012-1018.Xiaojun Wan.
2007.
TimedTextRank: Adding theTemporal Dimension to Multi-documentSummarization.
In Proceedings of 30th ACMSIGIR, pp 867-868.Lei Yang, Lei Qi, Yan-Ping Zhao, Bin Gao, and Tie-Yan Liu.
2007.
Link Analysis using Time Series ofWeb Graphs.
In Proceedings of CIKM?07.Masaharu Yoshioka and Makoto Haraguchi.
2004.Multiple News Articles Summarization based onEvent Reference Information, in Working Notes ofNTCIR-4.Philip S. Yu, Xin Li, and Bing Liu.
2004.
On theTemporal Dimension of Search.
In Proceedings ofthe 13th International World Wide Web Conferenceon Alternate Track Papers and Posters, pp 448-449.Philip S. Yu, Xin Li, and Bing Liu.
2005.
Adding theTemporal Dimension to Search ?
A Case Study inPublication Search.
In Proceedings of the 2005IEEE/WIC/ACM International Conference on WebIntelligence.496
