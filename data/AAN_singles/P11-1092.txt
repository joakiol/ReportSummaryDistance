Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 915?923,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsGrammatical Error Correction with Alternating Structure OptimizationDaniel Dahlmeier1 and Hwee Tou Ng1,21NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore{danielhe,nght}@comp.nus.edu.sgAbstractWe present a novel approach to grammaticalerror correction based on Alternating Struc-ture Optimization.
As part of our work, weintroduce the NUS Corpus of Learner En-glish (NUCLE), a fully annotated one mil-lion words corpus of learner English availablefor research purposes.
We conduct an exten-sive evaluation for article and preposition er-rors using various feature sets.
Our exper-iments show that our approach outperformstwo baselines trained on non-learner text andlearner text, respectively.
Our approach alsooutperforms two commercial grammar check-ing software packages.1 IntroductionGrammatical error correction (GEC) has been rec-ognized as an interesting as well as commerciallyattractive problem in natural language process-ing (NLP), in particular for learners of English asa foreign or second language (EFL/ESL).Despite the growing interest, research has beenhindered by the lack of a large annotated corpus oflearner text that is available for research purposes.As a result, the standard approach to GEC has beento train an off-the-shelf classifier to re-predict wordsin non-learner text.
Learning GEC models directlyfrom annotated learner corpora is not well explored,as are methods that combine learner and non-learnertext.
Furthermore, the evaluation of GEC has beenproblematic.
Previous work has either evaluated onartificial test instances as a substitute for real learnererrors or on proprietary data that is not available toother researchers.
As a consequence, existing meth-ods have not been compared on the same test set,leaving it unclear where the current state of the artreally is.In this work, we aim to overcome both problems.First, we present a novel approach to GEC basedon Alternating Structure Optimization (ASO) (Andoand Zhang, 2005).
Our approach is able to trainmodels on annotated learner corpora while still tak-ing advantage of large non-learner corpora.
Sec-ond, we introduce the NUS Corpus of Learner En-glish (NUCLE), a fully annotated one million wordscorpus of learner English available for research pur-poses.
We conduct an extensive evaluation for ar-ticle and preposition errors using six different fea-ture sets proposed in previous work.
We com-pare our proposed ASO method with two baselinestrained on non-learner text and learner text, respec-tively.
To the best of our knowledge, this is thefirst extensive comparison of different feature setson real learner text which is another contributionof our work.
Our experiments show that our pro-posed ASO algorithm significantly improves overboth baselines.
It also outperforms two commercialgrammar checking software packages in a manualevaluation.The remainder of this paper is organized as fol-lows.
The next section reviews related work.
Sec-tion 3 describes the tasks.
Section 4 formulates GECas a classification problem.
Section 5 extends this tothe ASO algorithm.
The experiments are presentedin Section 6 and the results in Section 7.
Section 8contains a more detailed analysis of the results.
Sec-tion 9 concludes the paper.9152 Related WorkIn this section, we give a brief overview on relatedwork on article and preposition errors.
For a morecomprehensive survey, see (Leacock et al, 2010).The seminal work on grammatical error correc-tion was done by Knight and Chander (1994) on arti-cle errors.
Subsequent work has focused on design-ing better features and testing different classifiers,including memory-based learning (Minnen et al,2000), decision tree learning (Nagata et al, 2006;Gamon et al, 2008), and logistic regression (Lee,2004; Han et al, 2006; De Felice, 2008).
Workon preposition errors has used a similar classifica-tion approach and mainly differs in terms of the fea-tures employed (Chodorow et al, 2007; Gamon etal., 2008; Lee and Knutsson, 2008; Tetreault andChodorow, 2008; Tetreault et al, 2010; De Felice,2008).
All of the above works only use non-learnertext for training.Recent work has shown that training on anno-tated learner text can give better performance (Hanet al, 2010) and that the observed word used bythe writer is an important feature (Rozovskaya andRoth, 2010b).
However, training data has eitherbeen small (Izumi et al, 2003), only partly anno-tated (Han et al, 2010), or artificially created (Ro-zovskaya and Roth, 2010b; Rozovskaya and Roth,2010a).Almost no work has investigated ways to combinelearner and non-learner text for training.
The onlyexception is Gamon (2010), who combined featuresfrom the output of logistic-regression classifiers andlanguage models trained on non-learner text in ameta-classifier trained on learner text.
In this work,we show a more direct way to combine learner andnon-learner text in a single model.Finally, researchers have investigated GEC inconnection with web-based models in NLP (Lapataand Keller, 2005; Bergsma et al, 2009; Yi et al,2008).
These methods do not use classifiers, but relyon simple n-gram counts or page hits from the Web.3 Task DescriptionIn this work, we focus on article and preposition er-rors, as they are among the most frequent types oferrors made by EFL learners.3.1 Selection vs.
Correction TaskThere is an important difference between training onannotated learner text and training on non-learnertext, namely whether the observed word can be usedas a feature or not.
When training on non-learnertext, the observed word cannot be used as a feature.The word choice of the writer is ?blanked out?
fromthe text and serves as the correct class.
A classifieris trained to re-predict the word given the surround-ing context.
The confusion set of possible classesis usually pre-defined.
This selection task formula-tion is convenient as training examples can be cre-ated ?for free?
from any text that is assumed to befree of grammatical errors.
We define the more re-alistic correction task as follows: given a particularword and its context, propose an appropriate correc-tion.
The proposed correction can be identical to theobserved word, i.e., no correction is necessary.
Themain difference is that the word choice of the writercan be encoded as part of the features.3.2 Article ErrorsFor article errors, the classes are the three articles a,the, and the zero-article.
This covers article inser-tion, deletion, and substitution errors.
During train-ing, each noun phrase (NP) in the training data is onetraining example.
When training on learner text, thecorrect class is the article provided by the humanannotator.
When training on non-learner text, thecorrect class is the observed article.
The context isencoded via a set of feature functions.
During test-ing, each NP in the test set is one test example.
Thecorrect class is the article provided by the human an-notator when testing on learner text or the observedarticle when testing on non-learner text.3.3 Preposition ErrorsThe approach to preposition errors is similar to ar-ticles but typically focuses on preposition substitu-tion errors.
In our work, the classes are 36 frequentEnglish prepositions (about, along, among, around,as, at, beside, besides, between, by, down, during,except, for, from, in, inside, into, of, off, on, onto,outside, over, through, to, toward, towards, under,underneath, until, up, upon, with, within, without),which we adopt from previous work.
Every prepo-sitional phrase (PP) that is governed by one of the91636 prepositions is one training or test example.
Weignore PPs governed by other prepositions.4 Linear Classifiers for GrammaticalError CorrectionIn this section, we formulate GEC as a classificationproblem and describe the feature sets for each task.4.1 Linear ClassifiersWe use classifiers to approximate the unknown rela-tion between articles or prepositions and their con-texts in learner text, and their valid corrections.
Thearticles or prepositions and their contexts are repre-sented as feature vectors X ?
X .
The correctionsare the classes Y ?
Y .In this work, we employ binary linear classifiersof the form uTX where u is a weight vector.
Theoutcome is considered +1 if the score is positive and?1 otherwise.
A popular method for finding u isempirical risk minimization with least square regu-larization.
Given a training set {Xi, Yi}i=1,...,n, weaim to find the weight vector that minimizes the em-pirical loss on the training datau?
= arg minu(1nn?i=1L(uTXi, Yi) + ?
||u||2),(1)where L is a loss function.
We use a modification ofHuber?s robust loss function.
We fix the regulariza-tion parameter ?
to 10?4.
A multi-class classifica-tion problem with m classes can be cast as m binaryclassification problems in a one-vs-rest arrangement.The prediction of the classifier is the class with thehighest score Y?
= arg maxY ?Y (uTY X).
In earlierexperiments, this linear classifier gave comparableor superior performance compared to a logistic re-gression classifier.4.2 FeaturesWe re-implement six feature extraction methodsfrom previous work, three for articles and three forprepositions.
The methods require different lin-guistic pre-processing: chunking, CCG parsing, andconstituency parsing.4.2.1 Article Errors?
DeFelice The system in (De Felice, 2008) forarticle errors uses a CCG parser to extract arich set of syntactic and semantic features, in-cluding part of speech (POS) tags, hypernymsfrom WordNet (Fellbaum, 1998), and namedentities.?
Han The system in (Han et al, 2006) relies onshallow syntactic and lexical features derivedfrom a chunker, including the words before, in,and after the NP, the head word, and POS tags.?
Lee The system in (Lee, 2004) uses a con-stituency parser.
The features include POStags, surrounding words, the head word, andhypernyms from WordNet.4.2.2 Preposition Errors?
DeFelice The system in (De Felice, 2008) forpreposition errors uses a similar rich set of syn-tactic and semantic features as the system forarticle errors.
In our re-implementation, we donot use a subcategorization dictionary, as thisresource was not available to us.?
TetreaultChunk The system in (Tetreault andChodorow, 2008) uses a chunker to extractfeatures from a two-word window around thepreposition, including lexical and POS n-grams, and the head words from neighboringconstituents.?
TetreaultParse The system in (Tetreault et al,2010) extends (Tetreault and Chodorow, 2008)by adding additional features derived from aconstituency and a dependency parse tree.For each of the above feature sets, we add the ob-served article or preposition as an additional featurewhen training on learner text.5 Alternating Structure OptimizationThis section describes the ASO algorithm and showshow it can be used for grammatical error correction.5.1 The ASO algorithmAlternating Structure Optimization (Ando andZhang, 2005) is a multi-task learning algorithm thattakes advantage of the common structure of multiplerelated problems.
Let us assume that we have m bi-nary classification problems.
Each classifier ui is a917weight vector of dimension p. Let ?
be an orthonor-mal h ?
p matrix that captures the common struc-ture of the m weight vectors.
We assume that eachweight vector can be decomposed into two parts:one part that models the particular i-th classificationproblem and one part that models the common struc-tureui = wi + ?Tvi.
(2)The parameters [{wi,vi},?]
can be learned by jointempirical risk minimization, i.e., by minimizing thejoint empirical loss of the m problems on the train-ing datam?l=1(1nn?i=1L((wl + ?Tvl)TXli, Yli)+ ?
||wl||2).
(3)The key observation in ASO is that the problemsused to find ?
do not have to be same as the targetproblems that we ultimately want to solve.
Instead,we can automatically create auxiliary problems forthe sole purpose of learning a better ?.Let us assume that we have k target problems andm auxiliary problems.
We can obtain an approxi-mate solution to Equation 3 by performing the fol-lowing algorithm (Ando and Zhang, 2005):1.
Learn m linear classifiers ui independently.2.
Let U = [u1,u2, .
.
.
,um] be the p ?
m matrixformed from the m weight vectors.3.
Perform Singular Value Decomposition (SVD) onU : U = V1DV T2 .
The first h column vectors of V1are stored as rows of ?.4.
Learn wj and vj for each of the target problems byminimizing the empirical risk:1nn?i=1L((wj + ?Tvj)TXi, Yi)+ ?
||wj ||2 .5.
The weight vector for the j-th target problem is:uj = wj + ?Tvj .5.2 ASO for Grammatical Error CorrectionThe key observation in our work is that the selectiontask on non-learner text is a highly informative aux-iliary problem for the correction task on learner text.For example, a classifier that can predict the pres-ence or absence of the preposition on can be help-ful for correcting wrong uses of on in learner text,e.g., if the classifier?s confidence for on is low butthe writer used the preposition on, the writer mighthave made a mistake.
As the auxiliary problems canbe created automatically, we can leverage the powerof very large corpora of non-learner text.Let us assume a grammatical error correction taskwith m classes.
For each class, we define a bi-nary auxiliary problem.
The feature space of theauxiliary problems is a restriction of the originalfeature space X to all features except the observedword: X\{Xobs}.
The weight vectors of the aux-iliary problems form the matrix U in Step 2 of theASO algorithm from which we obtain ?
throughSVD.
Given ?, we learn the vectors wj and vj ,j = 1, .
.
.
, k from the annotated learner text usingthe complete feature space X .This can be seen as an instance of transfer learn-ing (Pan and Yang, 2010), as the auxiliary problemsare trained on data from a different domain (non-learner text) and have a slightly different featurespace (X\{Xobs}).
We note that our method is gen-eral and can be applied to any classification problemin GEC.6 Experiments6.1 Data SetsThe main corpus in our experiments is the NUS Cor-pus of Learner English (NUCLE).
The corpus con-sists of about 1,400 essays written by EFL/ESL uni-versity students on a wide range of topics, like en-vironmental pollution or healthcare.
It contains overone million words which are completely annotatedwith error tags and corrections.
All annotations havebeen performed by professional English instructors.We use about 80% of the essays for training, 10% fordevelopment, and 10% for testing.
We ensure thatno sentences from the same essay appear in both thetraining and the test or development data.
NUCLEis available to the community for research purposes.On average, only 1.8% of the articles and 1.3%of the prepositions in NUCLE contain an error.This figure is considerably lower compared to otherlearner corpora (Leacock et al, 2010, Ch.
3) andshows that our writers have a relatively high profi-ciency of English.
We argue that this makes the taskconsiderably more difficult.
Furthermore, to keepthe task as realistic as possible, we do not filter the918test data in any way.In addition to NUCLE, we use a subset of theNew York Times section of the Gigaword corpus1and the Wall Street Journal section of the Penn Tree-bank (Marcus et al, 1993) for some experiments.We pre-process all corpora using the following tools:We use NLTK2 for sentence splitting, OpenNLP3for POS tagging, YamCha (Kudo and Matsumoto,2003) for chunking, the C&C tools (Clark and Cur-ran, 2007) for CCG parsing and named entity recog-nition, and the Stanford parser (Klein and Manning,2003a; Klein and Manning, 2003b) for constituencyand dependency parsing.6.2 Evaluation MetricsFor experiments on non-learner text, we report ac-curacy, which is defined as the number of correctpredictions divided by the total number of test in-stances.
For experiments on learner text, we reportF1-measureF1 = 2?Precision?
RecallPrecision + Recallwhere precision is the number of suggested correc-tions that agree with the human annotator dividedby the total number of proposed corrections by thesystem, and recall is the number of suggested cor-rections that agree with the human annotator dividedby the total number of errors annotated by the humanannotator.6.3 Selection Task Experiments on WSJ TestDataThe first set of experiments investigates predictingarticles and prepositions in non-learner text.
Thisprimarily serves as a reference point for the correc-tion task described in the next section.
We trainclassifiers as described in Section 4 on the Giga-word corpus.
We train with up to 10 million train-ing instances, which corresponds to about 37 millionwords of text for articles and 112 million words oftext for prepositions.
The test instances are extractedfrom section 23 of the WSJ and no text from theWSJ is included in the training data.
The observedarticle or preposition choice of the writer is the class1LDC2009T132www.nltk.org3opennlp.sourceforge.netwe want to predict.
Therefore, the article or prepo-sition cannot be part of the input features.
Our pro-posed ASO method is not included in these experi-ments, as it uses the observed article or prepositionas a feature which is only applicable when testing onlearner text.6.4 Correction Task Experiments on NUCLETest DataThe second set of experiments investigates the pri-mary goal of this work: to automatically correctgrammatical errors in learner text.
The test instancesare extracted from NUCLE.
In contrast to the previ-ous selection task, the observed word choice of thewriter can be different from the correct class and theobserved word is available during testing.
We inves-tigate two different baselines and our ASO method.The first baseline is a classifier trained on the Gi-gaword corpus in the same way as described in theselection task experiment.
We use a simple thresh-olding strategy to make use of the observed wordduring testing.
The system only flags an error if thedifference between the classifier?s confidence for itsfirst choice and the confidence for the observed wordis higher than a threshold t. The threshold parame-ter t is tuned on the NUCLE development data foreach feature set.
In our experiments, the value for tis between 0.7 and 1.2.The second baseline is a classifier trained on NU-CLE.
The classifier is trained in the same way asthe Gigaword model, except that the observed wordchoice of the writer is included as a feature.
The cor-rect class during training is the correction providedby the human annotator.
As the observed word ispart of the features, this model does not need an ex-tra thresholding step.
Indeed, we found that thresh-olding is harmful in this case.
During training, theinstances that do not contain an error greatly out-number the instances that do contain an error.
To re-duce this imbalance, we keep all instances that con-tain an error and retain a random sample of q percentof the instances that do not contain an error.
Theundersample parameter q is tuned on the NUCLEdevelopment data for each data set.
In our experi-ments, the value for q is between 20% and 40%.Our ASO method is trained in the following way.We create binary auxiliary problems for articles orprepositions, i.e., there are 3 auxiliary problems for919articles and 36 auxiliary problems for prepositions.We train the classifiers for the auxiliary problems onthe complete 10 million instances from Gigaword inthe same ways as in the selection task experiment.The weight vectors of the auxiliary problems formthe matrixU .
We perform SVD to get U = V1DV T2 .We keep all columns of V1 to form ?.
The targetproblems are again binary classification problemsfor each article or preposition, but this time trainedon NUCLE.
The observed word choice of the writeris included as a feature for the target problems.
Weagain undersample the instances that do not containan error and tune the parameter q on the NUCLE de-velopment data.
The value for q is between 20% and40%.
No thresholding is applied.We also experimented with a classifier that istrained on the concatenated data from NUCLE andGigaword.
This model always performed worse thanthe better of the individual baselines.
The reason isthat the two data sets have different feature spaceswhich prevents simple concatenation of the trainingdata.
We therefore omit these results from the paper.7 ResultsThe learning curves of the selection task experi-ments on WSJ test data are shown in Figure 1.
Thethree curves in each plot correspond to different fea-ture sets.
Accuracy improves quickly in the be-ginning but improvements get smaller as the sizeof the training data increases.
The best results are87.56% for articles (Han) and 68.25% for prepo-sitions (TetreaultParse).
The best accuracy for ar-ticles is comparable to the best reported results of87.70% (Lee, 2004) on this data set.The learning curves of the correction task ex-periments on NUCLE test data are shown in Fig-ure 2 and 3.
Each sub-plot shows the curves ofthree models as described in the last section: ASOtrained on NUCLE and Gigaword, the baseline clas-sifier trained on NUCLE, and the baseline classifiertrained on Gigaword.
For ASO, the x-axis showsthe number of target problem training instances.
Thefirst observation is that high accuracy for the selec-tion task on non-learner text does not automaticallyentail high F1-measure on learner text.
We also notethat feature sets with similar performance on non-learner text can show very different performance on0.680.700.720.740.760.780.800.820.840.860.881000  10000  100000  1e+06  1e+07ACCURACYNumber of training examplesGIGAWORD DEFELICEGIGAWORD HANGIGAWORD LEE(a) Articles0.250.300.350.400.450.500.550.600.650.701000  10000  100000  1e+06  1e+07ACCURACYNumber of training examplesGIGAWORD DEFELICEGIGAWORD TETRAULTCHUNKGIGAWORD TETRAULTPARSE(b) PrepositionsFigure 1: Accuracy for the selection task on WSJtest data.learner text.
The second observation is that train-ing on annotated learner text can significantly im-prove performance.
In three experiments (articlesDeFelice, Han, prepositions DeFelice), the NUCLEmodel outperforms the Gigaword model trained on10 million instances.
Finally, the ASO models showthe best results.
In the experiments where the NU-CLE models already perform better than the Giga-word baseline, ASO gives comparable or slightlybetter results (articles DeFelice, Han, Lee, preposi-tions DeFelice).
In those experiments where neitherbaseline shows good performance (TetreaultChunk,TetreaultParse), ASO results in a large improvementover either baseline.
The best results are 19.29% F1-measure for articles (Han) and 11.15% F1-measurefor prepositions (TetreaultParse) achieved by theASO model.9200.020.040.060.080.100.120.140.160.181000  10000  100000  1e+06  1e+07F1Number of training examplesASONUCLEGIGAWORD(a) DeFelice0.020.040.060.080.100.120.140.160.180.201000  10000  100000  1e+06  1e+07F1Number of training examplesASONUCLEGIGAWORD(b) Han0.020.030.040.050.060.070.080.091000  10000  100000  1e+06  1e+07F1Number of training examplesASONUCLEGIGAWORD(c) LeeFigure 2: F1-measure for the article correction task on NUCLE test data.
Each plot shows ASO and twobaselines for a particular feature set.0.000.010.020.030.040.050.060.070.080.090.101000  10000  100000  1e+06  1e+07F1Number of training examplesASONUCLEGIGAWORD(a) DeFelice0.000.010.020.030.040.050.060.070.080.090.101000  10000  100000  1e+06  1e+07F1Number of training examplesASONUCLEGIGAWORD(b) TetreaultChunk0.000.020.040.060.080.100.121000  10000  100000  1e+06  1e+07F1Number of training examplesASONUCLEGIGAWORD(c) TetreaultParseFigure 3: F1-measure for the preposition correction task on NUCLE test data.
Each plot shows ASO andtwo baselines for a particular feature set.8 AnalysisIn this section, we analyze the results in more detailand show examples from our test set for illustration.Table 1 shows precision, recall, and F1-measurefor the best models in our experiments.
ASOachieves a higher F1-measure than either baseline.We use the sign-test with bootstrap re-sampling forstatistical significance testing.
The sign-test is a non-parametric test that makes fewer assumptions thanparametric tests like the t-test.
The improvements inF1-measure of ASO over either baseline are statis-tically significant (p < 0.001) for both articles andprepositions.The difficulty in GEC is that in many cases, morethan one word choice can be correct.
Even with athreshold, the Gigaword baseline model suggests toomany corrections, because the model cannot makeuse of the observed word as a feature.
This results inlow precision.
For example, the model replaces asArticlesModel Prec Rec F1Gigaword (Han) 10.33 21.81 14.02NUCLE (Han) 29.48 12.91 17.96ASO (Han) 26.44 15.18 19.29PrepositionsModel Prec Rec F1Gigaword (TetreaultParse ) 4.77 14.81 7.21NUCLE (DeFelice) 13.84 5.55 7.92ASO (TetreaultParse) 18.30 8.02 11.15Table 1: Best results for the correction task on NU-CLE test data.
Improvements for ASO over eitherbaseline are statistically significant (p < 0.001) forboth tasks.with by in the sentence ?This group should be cate-gorized as the vulnerable group?, which is wrong.In contrast, the NUCLE model learns a bias to-wards the observed word and therefore achieveshigher precision.
However, the training data is921smaller and therefore recall is low as the model hasnot seen enough examples during training.
This isespecially true for prepositions which can occur in alarge variety of contexts.
For example, the preposi-tion in should be on in the sentence ?...
psychologyhad an impact in the way we process and managetechnology?.
The phrase ?impact on the way?
doesnot appear in the NUCLE training data and the NU-CLE baseline fails to detect the error.The ASO model is able to take advantage of boththe annotated learner text and the large non-learnertext, thus achieving overall high F1-measure.
Thephrase ?impact on the way?, for example, appearsmany times in the Gigaword training data.
With thecommon structure learned from the auxiliary prob-lems, the ASO model successfully finds and correctsthis mistake.8.1 Manual EvaluationWe carried out a manual evaluation of the best ASOmodels and compared their output with two com-mercial grammar checking software packages whichwe call System A and System B.
We randomly sam-pled 1000 test instances for articles and 2000 testinstances for prepositions and manually categorizedeach test instance into one of the following cate-gories: (1) Correct means that both human and sys-tem flag an error and suggest the same correction.If the system?s correction differs from the humanbut is equally acceptable, it is considered (2) BothOk.
If the system identifies an error but fails to cor-rect it, we consider it (3) Both Wrong, as both thewriter and the system are wrong.
(4) Other Errormeans that the system?s correction does not resultin a grammatical sentence because of another gram-matical error that is outside the scope of article orpreposition errors, e.g., a noun number error as in?all the dog?.
If the system corrupts a previouslycorrect sentence it is a (5) False Flag.
If the hu-man flags an error but the system does not, it is a(6) Miss.
(7) No Flag means that neither the humanannotator nor the system flags an error.
We calculateprecision by dividing the count of category (1) by thesum of counts of categories (1), (3), and (5), and re-call by dividing the count of category (1) by the sumof counts of categories (1), (3), and (6).
The resultsare shown in Table 2.
Our ASO method outperformsboth commercial software packages.
Our evalua-ArticlesASO System A System B(1) Correct 4 1 1(2) Both Ok 16 12 18(3) Both Wrong 0 1 0(4) Other Error 1 0 0(5) False Flag 1 0 4(6) Miss 3 5 6(7) No Flag 975 981 971Precision 80.00 50.00 20.00Recall 57.14 14.28 14.28F1 66.67 22.21 16.67PrepositionsASO System A System B(1) Correct 3 3 0(2) Both Ok 35 39 24(3) Both Wrong 0 2 0(4) Other Error 0 0 0(5) False Flag 5 11 1(6) Miss 12 11 15(7) No Flag 1945 1934 1960Precision 37.50 18.75 0.00Recall 20.00 18.75 0.00F1 26.09 18.75 0.00Table 2: Manual evaluation and comparison withcommercial grammar checking software.tion shows that even commercial software packagesachieve low F1-measure for article and prepositionerrors, which confirms the difficulty of these tasks.9 ConclusionWe have presented a novel approach to grammati-cal error correction based on Alternating StructureOptimization.
We have introduced the NUS Corpusof Learner English (NUCLE), a fully annotated cor-pus of learner text.
Our experiments for article andpreposition errors show the advantage of our ASOapproach over two baseline methods.
Our ASO ap-proach also outperforms two commercial grammarchecking software packages in a manual evaluation.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA)of Singapore.922ReferencesR.K.
Ando and T. Zhang.
2005.
A framework for learn-ing predictive structures from multiple tasks and un-labeled data.
Journal of Machine Learning Research,6.S.
Bergsma, D. Lin, and R. Goebel.
2009.
Web-scale n-gram models for lexical disambiguation.
In Proceed-ings of IJCAI.M.
Chodorow, J. Tetreault, and N.R.
Han.
2007.
De-tection of grammatical errors involving prepositions.In Proceedings of the 4th ACL-SIGSEM Workshop onPrepositions.S.
Clark and J.R. Curran.
2007.
Wide-coverage effi-cient statistical parsing with CCG and log-linear mod-els.
Computational Linguistics, 33(4).R.
De Felice.
2008.
Automatic Error Detection in Non-native English.
Ph.D. thesis, University of Oxford.C.
Fellbaum, editor.
1998.
WordNet: An electronic lexi-cal database.
MIT Press, Cambridge,MA.M.
Gamon, J. Gao, C. Brockett, A. Klementiev, W.B.Dolan, D. Belenko, and L. Vanderwende.
2008.
Usingcontextual speller techniques and language modelingfor ESL error correction.
In Proceedings of IJCNLP.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing: A meta-classifier approach.In Proceedings of HLT-NAACL.N.R.
Han, M. Chodorow, and C. Leacock.
2006.
De-tecting errors in English article usage by non-nativespeakers.
Natural Language Engineering, 12(02).N.R.
Han, J. Tetreault, S.H.
Lee, and J.Y.
Ha.
2010.Using an error-annotated learner corpus to develop anESL/EFL error correction system.
In Proceedings ofLREC.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-hara.
2003.
Automatic error detection in the Japaneselearners?
English spoken data.
In Companion Volumeto the Proceedings of ACL.D.
Klein and C.D.
Manning.
2003a.
Accurate unlexical-ized parsing.
In Proceedings of ACL.D.
Klein and C.D.
Manning.
2003b.
Fast exact inferencewith a factored model for natural language processing.Advances in Neural Information Processing Systems(NIPS 2002), 15.K.
Knight and I. Chander.
1994.
Automated posteditingof documents.
In Proceedings of AAAI.T Kudo and Y. Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL.M.
Lapata and F. Keller.
2005.
Web-based models fornatural language processing.
ACM Transactions onSpeech and Language Processing, 2(1).C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault.2010.
Automated Grammatical Error Detection forLanguage Learners.
Morgan & Claypool Publishers,San Rafael,CA.J.
Lee and O. Knutsson.
2008.
The role of PP attachmentin preposition generation.
In Proceedings of CICLing.J.
Lee.
2004.
Automatic article restoration.
In Proceed-ings of HLT-NAACL.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of English:The Penn Treebank.
Computational Linguistics, 19.G.
Minnen, F. Bond, and A. Copestake.
2000.
Memory-based learning for article generation.
In Proceedingsof CoNLL.R.
Nagata, A. Kawai, K. Morihiro, and N. Isu.
2006.A feedback-augmented method for detecting errors inthe writing of learners of English.
In Proceedings ofCOLING-ACL.S.J.
Pan and Q. Yang.
2010.
A survey on transfer learn-ing.
IEEE Transactions on Knowledge and Data En-gineering, 22(10).A.
Rozovskaya and D. Roth.
2010a.
Generating con-fusion sets for context-sensitive error correction.
InProceedings of EMNLP.A.
Rozovskaya and D. Roth.
2010b.
Training paradigmsfor correcting errors in grammar and usage.
In Pro-ceedings of HLT-NAACL.J.
Tetreault and M. Chodorow.
2008.
The ups and downsof preposition error detection in ESL writing.
In Pro-ceedings of COLING.J.
Tetreault, J.
Foster, and M. Chodorow.
2010.
Usingparse features for preposition selection and error de-tection.
In Proceedings of ACL.X.
Yi, J. Gao, and W.B.
Dolan.
2008.
A web-based En-glish proofing system for English as a second languageusers.
In Proceedings of IJCNLP.923
