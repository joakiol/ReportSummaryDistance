Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 200?205,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsJoint Training of Dependency Parsing Filters throughLatent Support Vector MachinesColin CherryInstitute for Information TechnologyNational Research Council Canadacolin.cherry@nrc-cnrc.gc.caShane BergsmaCenter for Language and Speech ProcessingJohns Hopkins Universitysbergsma@jhu.eduAbstractGraph-based dependency parsing can be spedup significantly if implausible arcs are elim-inated from the search-space before parsingbegins.
State-of-the-art methods for arc fil-tering use separate classifiers to make point-wise decisions about the tree; they label tokenswith roles such as root, leaf, or attaches-to-the-left, and then filter arcs accordingly.
Be-cause these classifiers overlap substantially intheir filtering consequences, we propose totrain them jointly, so that each classifier canfocus on the gaps of the others.
We inte-grate the various pointwise decisions as latentvariables in a single arc-level SVM classifier.This novel framework allows us to combinenine pointwise filters, and adjust their sensi-tivity using a shared threshold based on arclength.
Our system filters 32% more arcs thanthe independently-trained classifiers, withoutreducing filtering speed.
This leads to fasterparsing with no reduction in accuracy.1 IntroductionA dependency tree represents syntactic relationshipsbetween words using directed arcs (Mel?c?uk, 1987).Each token in the sentence is a node in the tree,and each arc connects a head to its modifier.
Thereare two dominant approaches to dependency pars-ing: graph-based and transition-based, where graph-based parsing is understood to be slower, but oftenmore accurate (McDonald and Nivre, 2007).In the graph-based setting, a complete searchfinds the highest-scoring tree under a model that de-composes over one or two arcs at a time.
Much ofthe time for parsing is spent scoring each poten-tial arc in the complete dependency graph (John-son, 2007), one for each ordered word-pair in thesentence.
Potential arcs are scored using rich linearmodels that are discriminatively trained to maximizeparsing accuracy (McDonald et al, 2005).
The vastmajority of these arcs are bad; in an n-word sen-tence, only n of the n2 potential arcs are correct.
Ifmany arcs can be filtered before parsing begins, thenthe entire process can be sped up substantially.Previously, we proposed a cascade of filters toprune potential arcs (Bergsma and Cherry, 2010).One stage of this cascade operates one token at atime, labeling each token t according to various rolesin the tree:?
Not-a-head (NaH ): t is not the head of any arc?
Head-to-left (HtL{1/5/*}): t?s head is to itsleft within 1, 5 or any number of words?
Head-to-right (HtR{1/5/*}): as head-to-left?
Root (Root): t is the root node, which elimi-nates arcs according to projectivitySimilar to Roark and Hollingshead (2008), each rolehas a corresponding binary classifier.
These token-role classifierswere shown to be more effective thanvine parsing (Eisner and Smith, 2005; Dreyer etal., 2006), a competing filtering scheme that filtersarcs based on their length (leveraging the observa-tion that most dependencies are short).In this work, we propose a novel filtering frame-work that integrates all the information used intoken-role classification and vine parsing, but of-fers a number of advantages.
In our previous work,classifier decisions would often overlap: differenttoken-role classifiers would agree to filter the samearc.
Based on this observation, we propose a jointtraining framework where only the most confident200HtR16?NaH3?
HtR*6?HtR56?Bob1?
ate2?
the3?
pizza4?
with5?
his6?
fork8??NN?
VBD?
DT?
NN?
IN?
POS?
NN?HtL16?salad7?NN?(T)?
(T)?(T)?(F)?
(F)?Figure 1: The dotted arc can be filtered by labeling any of theboxed roles as True; i.e., predicting that the head the3 is not thehead of any arc, or that the modifier his6 attaches elsewhere.Role truth values, derived from the gold-standard tree (in grey),are listed adjacent to the boxes, in parentheses.classifier is given credit for eliminating an arc.
Theidentity of the responsible classifier is modeled asa latent variable, which is filled in during trainingusing a latent SVM (LSVM) formulation.
Our useof an LSVM to assign credit during joint trainingdiffers substantially from previous LSVM applica-tions, which have induced latent linguistic structures(Cherry and Quirk, 2008; Chang et al, 2010) or sen-tence labels (Yessenalina et al, 2010).In our framework, each classifier learns to fo-cus on the cases where the other classifiers are lessconfident.
Furthermore, the integrated approach di-rectly optimizes for arc-filtering accuracy (ratherthan token-labeling fidelity).
We trade-off filteringprecision/recall using two hyperparameters, whilethe previous approach trained classifiers for eightdifferent tasks resulting in sixteen hyperparameters.Ultimately, the biggest gains in filter quality areachieved when we jointly train the token-role classi-fiers together with a dynamic threshold that is basedon arc length and shared across all classifiers.2 Joint Training of Token RolesIn our previous system, filtering is conducted bytraining a separate SVM classifier for each of theeight token-roles described in Section 1.
Each clas-sifier uses a training set with one example per tree-bank token, where each token is assigned a binarylabel derived from the gold-standard tree.
Figure 1depicts five of the eight token roles, along with theirtruth values.
The role labelers can be tuned for highprecision with label-specific cost parameters; theseare tuned separately for each classifier.
At test time,each of the eight classifiers assigns a binary labelto each of the n tokens in the sentence.
Potentialarcs are then filtered from the complete dependencygraph according to these token labels.
In Figure 1,a positive assignment to any of the indicated token-roles is sufficient to filter the dotted arc.In the current work, we maintain almost the sametest-time framework, but we alter training substan-tially, so that the various token-role classifiers aretrained jointly.
To do so, we propose a classifica-tion scheme focused on arcs.1 During training, eacharc is assigned a filtering event as a latent variable.Events generalize the token-roles from our previoussystem (e.g.
NaH 3,HtR?6).
Events are assigned bi-nary labels during filtering; positive events are saidto be detected.
In general, events can correspondto any phenomenon, so long as the following holds:For each arc a, we must be able to deterministicallyconstruct the set Za of all events that would filtera if detected.2 Figure 1 shows that Zthe3?his6 ={NaH 3,HtR?6,HtR56,HtR16,HtL16}.To detect events, we maintain the eight token-roleclassifiers from the previous system, but they be-come subclassifiers of our joint system.
For no-tational convenience, we pack them into a singleweight vector w?.
Thus, the event z = NaH 3 is de-tected only if w?
?
??
(NaH 3) > 0, where ??
(z) is z?sfeature vector.
Given this notation, we can cast thefiltering decision for an arc a as a maximum.
Wefilter a only if:f(Za) > 0 where f(Za) = maxz?Za[w?
?
??
(z)](1)We have reformulated our problem, which previ-ously involved a number of independent token clas-sifiers, as a single arc classifier f()with an innermaxover latent events.
Note the asymmetry inherent in(1).
To filter an arc,[w?
?
??
(z) > 0]must hold for atleast one z ?
Za; but to keep an arc,[w?
?
??
(z) ?
0]must hold for all z ?
Za.
Also note that tokenshave completely disappeared from our formalism:the classifier is framed only in terms of events andarcs; token-roles are encapsulated inside events.To provide a large-margin training objective forour joint classifier, we adapt the latent SVM (Felzen-1A joint filtering formalism for CFG parsing or SCFG trans-lation would likewise focus on hyper-edges or spans.2This same requirement is also needed by the previous,independently-trained filters at test time, so that arcs can be fil-tered according to the roles assigned to tokens.201szwalb et al, 2010; Yu and Joachims, 2009) to ourproblem.
Given a training set A of (a, y) pairs,where a is an arc in context and y is the correct filterlabel for a (1 to filter, 0 otherwise), LSVM trainingselects w?
to minimize:12||w?||2+?
(a,y)?ACy max[0, 1 + f(Za|?y)?
f(Za|y)](2)where Cy is a label-specific regularization parame-ter, and the event set Z is now conditioned on thelabel y: Za|1 = Za, and Za|0 = {Nonea}.
Noneais a rejection event, which indicates that a is notfiltered.
The rejection event slightly alters our de-cision rule; rather than thresholding at 0, we nowfilter a only if f(Za) > w?
?
??(Nonea).
One can set??(Nonea)?
?
for all a to fix the threshold at 0.Though not convex, (2) can be solved to a lo-cal minimum with an EM-like alternating minimiza-tion procedure (Felzenszwalb et al, 2010; Yu andJoachims, 2009).
The learner alternates betweenpicking the highest-scoring latent event z?a ?
Za|yfor each example (a, y), and training a multiclassSVM to solve an approximation to (2) where Za|y isreplaced with {z?a}.
Intuitively, the first step assignsthe event z?a to a, making z?a responsible for a?s ob-served label.
The second step optimizes the model toensure that each z?a is detected, leading to the desiredarc-filtering decisions.
As the process iterates, eventassignment becomes increasingly refined, leading toa more accurate joint filter.The resulting joint filter has only two hyper-parameters: the label-specific cost parameters C1and Co.
These allow us to tune our system for highprecision by increasing the cost of misclassifying anarc that should not be filtered (C1  Co).Joint training also implicitly affects the relativecosts of subclassifier decisions.
By minimizing anarc-level hinge loss with latent events (which in turncorrespond to token-roles), we assign costs to token-roles based on arc accuracy.
Consequently, 1) Atoken-level decision that affects multiple arcs im-pacts multiple instances of hinge loss, and 2) Noextra credit (penalty) is given for multiple decisionsthat (in)correctly filter the same arc.
Therefore, anNaH decision that filters thirty arcs is given moreweight than an HtL5 decision that filters only one(Item 1), unless those thirty arcs are already filteredNaH3?=?0.5?The1?
big2?
dog3?
chased4?
the5?
cat6?DT?
ADJ?
NN?
VBD?
DT?
NN?1.0?
1.1?
0.6?
0.3?
0.2?Figure 2: A hypothetical example of dynamic threshold-ing, where a weak assertion that dog3 should not be a head`w?
?
??
(NaH 3) = 0.5?is sufficient to rule out two arcs.
Eacharc?s threshold`w?
?
??
(Nonea)?is shown next to its arrow.by higher-scoring subclassifiers (Item 2).3 Accounting for Arc LengthWe can extend our system by expanding our eventset Z.
By adding an arc-level event Vinea to eachZa, we can introduce a vine filter to prune long arcs.Similarly, we have already introduced another arc-level event, the rejection event Nonea.
By assign-ing features to Nonea, we learn a dynamic thresh-old on all filters, which considers properties of thearc before acting on any other event.
We parameter-ize both Vinea and Nonea with the same two fea-tures, inspired by tag-specific vine parsing (Eisnerand Smith, 2005):{Bias : 1HeadTag ModTag Dir(a) : Len(a)}where HeadTag ModTag Dir(a) concatenates thepart-of-speech tags of a?s head and modifier tokensto its direction (left or right), and Len(a) gives theunsigned distance between a?s head and modifier.In the context of Vinea, these two features al-low the system to learn tag-pair-specific limits onarc length.
In the context of Nonea, these featuresprotect short arcs and arcs that connect frequently-linked tag-pairs, allowing our token-role filters to bemore aggressive on arcs that do not have these char-acteristics.
The dynamic threshold also alters ourinterpretation of filtering events: where before theywere either active or inactive, events are now as-signed scores, which are compared with the thresh-old to make final filtering decisions (Figure 2).33Because tokens and arcs are scored independently and cou-pled only through score comparison, the impact of Vinea andNonea on classification speed should be no greater than doingvine and token-role filtering in sequence.
In practice, it is noslower than running token-role filtering on its own.2024 ExperimentsWe extract dependency structures from the PennTreebank using the head rules of Yamada and Mat-sumoto (2003).4 We divide the Treebank into train(sections 2?21), development (22) and test (23).
Wepart-of-speech tag our data using a perceptron taggersimilar to the one described by Collins (2002).
Thetraining set is tagged with jack-knifing: the data issplit into 10 folds and each fold is tagged by a sys-tem trained on the other 9 folds.
Development andtest sets are tagged using the entire training set.We train our joint filter using an in-house latentSVM framework, which repeatedly calls a multi-class exponentiated gradient SVM (Collins et al,2008).
LSVM training was stopped after 4 itera-tions, as determined during development.5 For thetoken-role classifiers, we re-implement the Bergsmaand Cherry (2010) feature set, initializing w?
withhigh-precision subclassifiers trained independentlyfor each token-role.
Vine and None subclassifiersare initialized with a zero vector.
At test time, weextract subclassifiers from the joint weight vector,and use them as parameters in the filtering tools ofBergsma and Cherry (2010).6Parsing experiments are carried out using theMST parser (McDonald et al, 2005),7 which wehave modified to filter arcs before carrying out fea-ture extraction.
It is trained using 5-best MIRA(Crammer and Singer, 2003).Following Bergsma and Cherry (2010), we mea-sure intrinsic filter quality with reduction, the pro-portion of total arcs removed, and coverage, the pro-portion of true arcs retained.
For parsing results, wepresent dependency accuracy, the percentage of to-kens that are assigned the correct head.4.1 Impact of Joint TrainingOur technical contribution consists of our proposedjoint training scheme for token-role filters, along4As implemented at http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html5The LSVM is well on its way to convergence: fewer than3% of arcs have event assignments that are still in flux.6http://code.google.com/p/arcfilter/.
Since ourcontribution is mainly in better filter training, we were able touse the arcfilter (testing) code with only small changes.
We haveadded our new joint filter, along with the Joint P1 model to thearcfilter package, labeled as ultra filters.7http://sourceforge.net/projects/mstparser/Indep.
JointSystem Cov.
Red.
Cov.
Red.Token 99.73 60.5 99.71 59.0+ Vine 99.62 68.6 99.69 63.3+ None N/A 99.76 71.6Table 1: Ablation analysis of intrinsic filter quality.with two extensions: the addition of vine filters(Vine) and a dynamic threshold (None).
Using pa-rameters determined to perform well during devel-opment,8 we examine test-set performance as we in-corporate each of these components.
For the token-role and vine subclassifiers, we compare against anindependently-trained ensemble of the same classi-fiers.9 Note that None cannot be trained indepen-dently, as its shared dynamic threshold considers arcand token views of the data simultaneously.
Resultsare shown in Table 1.Our complete system outperforms all variants interms of both coverage and reduction.
However, onecan see that neither joint system is able to outper-form its independently-trained counter-part withoutthe dynamic threshold provided by None.
This isbecause the desirable credit-assignment propertiesof our joint training procedure are achieved throughduplication (Zadrozny et al, 2003).
That is, theLSVM knows that a specific event is important be-cause it appears in event sets Za for many arcs fromthe same sentence.
WithoutNone, the filtering deci-sions implied by each copy of an event are identical.Because these replicated events are associated witharcs that are presented to the LSVM as independentexamples, they appear to be not only important, butalso low-variance, and therefore easy.
This leads tooverfitting.
We had hoped that the benefits of jointtraining would outweigh this drawback, but our re-sults show that they do not.
However, in addition toits other desirable properties (protecting short arcs),the dynamic threshold imposed byNone restores in-dependence between arcs that share a common event(Figure 2).
This alleviates overfitting and enablesstrong performance.8C0=1e-2, C1=1e-59Each subclassifier is a token-level SVM trained with token-role labels extracted from the training treebank.
Using develop-ment data, we search over regularization parameters so that eachclassifier yields more than 99.93% arc-level coverage.203Filter Intrinsic MST-1 MST-2Filter Cov.
Red.
Time Acc.
Sent/sec* Acc.
Sent/sec*None 100.00 00.0 0s 91.28 16 92.05 10B&C R+L 99.70 54.1 7s 91.24 29 92.00 17Joint P1 99.76 71.6 7s 91.28 38 92.06 22B&C R+L+Q 99.43 78.3 19s 91.23 35 91.98 22Joint P2 99.56 77.9 7s 91.29 44 92.05 25Table 2: Parsing with jointly-trained filters outperforms independently-trained filters (R+L), as well as a more complexcascade (R+L+Q).
*Accounts for total time spent parsing and applying filters, averaged over five runs.4.2 Comparison to the state of the artWe directly compare our filters to those of Bergsmaand Cherry (2010) in terms of both intrinsic fil-ter quality and impact on the MST parser.
TheB&C system consists of three stages: rules (R), lin-ear token-role filters (L) and quadratic arc filters(Q).
The Q stage uses rich arc-level features simi-lar to those of the MST parser.
We compare againstindependently-trained token-role filters (R+L), aswell as the complete cascade (R+L+Q), using themodels provided online.10 Our comparison points,Joint P1 and P2 were built by tuning our completejoint system to roughly match the coverage valuesof R+L and R+L+Q on development data.11 Resultsare shown in Table 2.Comparing Joint P1 to R+L, we can see that fora fixed set of pointwise filters, joint training witha dynamic threshold outperforms independent train-ing substantially.
We achieve a 32% improvementin reduction with no impact on coverage and no in-crease in filtering overhead (time).Comparing Joint P2 to R+L+Q, we see that JointP2 achieves similar levels of reduction with far lessfiltering overhead; our filters take only 7 secondsto apply instead of 19.
This increases the speed ofthe (already fast) filtered MST-1 parser from 35 sen-tences per second to 44, resulting in a total speed-up of 2.75 with respect to the unfiltered parser.
Theimprovement is less impressive for MST-2, wherethe overhead for filter application is a less substan-tial fraction of parsing time; however, our trainingframework also has other benefits with respect toR+L+Q, including a single unified training algo-10Results are not identical to those reported in our previouspaper, due to our use of a different part-of-speech tagger.
Notethat parsing accuracies for the B&C systems have improved.11P1: C0=1e-2, C1=1e-5, P2: C0=1e-2, C1=2e-5rithm, fewer hyper-parameters and a smaller test-time memory footprint.
Finally, the jointly trainedfilters have no impact on parsing accuracy, whereboth B&C filters have a small negative effect.The performance of Joint-P2+MST-2 is compa-rable to the system of Huang and Sagae (2010),who report a parsing speed of 25 sentences persecond and an accuracy of 92.1 on the same testset, using a transition-based parser enhanced withdynamic-programming state combination.12 Graph-based and transition-based systems tend to make dif-ferent types of errors (McDonald and Nivre, 2007).Therefore, having fast, accurate parsers for both ap-proaches presents an opportunity for large-scale, ro-bust parser combination.5 ConclusionWe have presented a novel use of latent SVMtechnology to train a number of filters jointly,with a shared dynamic threshold.
By training afamily of dependency filters in this manner, eachsubclassifier focuses on the examples where it ismost needed, with our dynamic threshold adjust-ing filter sensitivity based on arc length.
This al-lows us to outperform a 3-stage filter cascade interms of speed-up, while also reducing the im-pact of filtering on parsing accuracy.
Our filter-ing code and trained models are available online athttp://code.google.com/p/arcfilter.
Inthe future, we plan to apply our joint training tech-nique to other rich filtering regimes (Zhang et al,2010), and to other NLP problems that combine thepredictions of overlapping classifiers.12The usual caveats for cross-machine, cross-implementationspeed comparisons apply.204ReferencesShane Bergsma and Colin Cherry.
2010.
Fast and accu-rate arc filtering for dependency parsing.
In COLING.Ming-Wei Chang, Dan Goldwasser, Dan Roth, and VivekSrikumar.
2010.
Discriminative learning over con-strained latent representations.
In HLT-NAACL.Colin Cherry and Chris Quirk.
2008.
Discriminative,syntactic language modeling through latent SVMs.
InAMTA.Michael Collins, Amir Globerson, Terry Koo, XavierCarreras, and Peter L. Bartlett.
2008.
Exponentiatedgradient algorithms for conditional random fields andmax-margin markov networks.
JMLR, 9:1775?1822.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In EMNLP.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
JMLR,3:951?991.Markus Dreyer, David A. Smith, and Noah A. Smith.2006.
Vine parsing and minimum risk reranking forspeed and precision.
In CoNLL.Jason Eisner and Noah A. Smith.
2005.
Parsing with softand hard constraints on dependency length.
In IWPT.Pedro F. Felzenszwalb, Ross B. Girshick, DavidMcAllester, and Deva Ramanan.
2010.
Object detec-tion with discriminatively trained part based models.IEEE Transactions on Pattern Analysis and MachineIntelligence, 32(9).Liang Huang and Kenji Sagae.
2010.
Dynamic program-ming for linear-time incremental parsing.
In ACL.Mark Johnson.
2007.
Transforming projective bilexicaldependency grammars into efficiently-parsable CFGswith unfold-fold.
In ACL.Ryan McDonald and Joakim Nivre.
2007.
Characteriz-ing the errors of data-driven dependency parsing mod-els.
In EMNLP-CoNLL.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In ACL.Igor A. Mel?c?uk.
1987.
Dependency syntax: theory andpractice.
State University of New York Press.Brian Roark and Kristy Hollingshead.
2008.
Classifyingchart cells for quadratic complexity context-free infer-ence.
In COLING.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InIWPT.Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010.Multi-level structured models for document-level sen-timent classification.
In EMNLP.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural SVMs with latent variables.
InICML.Bianca Zadrozny, John Langford, and Naoki Abe.
2003.Cost-sensitive learning by cost-proportionate exampleweighting.
In Third IEEE International Conference onData Mining.Yue Zhang, Byung-Gyu Ahn, Stephen Clark, Curt VanWyk, James R. Curran, and Laura Rimell.
2010.Chart pruning for fast lexicalised-grammar parsing.
InEMNLP.205
