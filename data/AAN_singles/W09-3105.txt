Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 23?26,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPActive Learning of Extractive Reference Summariesfor Lecture Speech SummarizationJustin Jian Zhang and Pascale FungHuman Language Technology CenterDepartment of Electronic and Computer EngineeringUniversity of Science and Technology (HKUST)Clear Water Bay,Hong Kong{zjustin,pascale}@ece.ust.hkAbstractWe propose using active learning for tag-ging extractive reference summary of lec-ture speech.
The training process offeature-based summarization model usu-ally requires a large amount of train-ing data with high-quality reference sum-maries.
Human production of such sum-maries is tedious, and since inter-labeleragreement is low, very unreliable.
Ac-tive learning helps assuage this problem byautomatically selecting a small amount ofunlabeled documents for humans to handcorrect.
Our method chooses the unla-beled documents according to the similar-ity score between the document and thecomparable resource?PowerPoint slides.After manual correction, the selected doc-uments are returned to the training pool.Summarization results show an increasinglearning curve of ROUGE-L F-measure,from 0.44 to 0.514, consistently higherthan that of using randomly chosen train-ing samples.Index Terms: active learning, summarization1 IntroductionThe need for the summarization of classroom lec-tures, conference speeches, political speeches isever increasing with the advent of remote learning,distributed collaboration and electronic archiving.These user needs cannot be sufficiently met byshort abstracts.
In recent years, virtually all sum-marization systems are extractive - compiling bul-let points from the document using some saliencycriteria.
Reference summaries are often manu-ally compiled by one or multiple human annota-tors (Fujii et al, 2008; Nenkova et al, 2007).
Un-like for speech recognition where the referencesentence is clear and unambiguous, and unlikefor machine translation where there are guidelinesfor manual translating reference sentences, thereis no clear guideline for compiling a good ref-erence summary.
As a result, one of the mostimportant challenges in speech summarization re-mains the difficulty to compile, evaluate and thusto learn what a good summary is.
Human judgestend to agree on obviously good and very badsummaries but cannot agree on borderline cases.Consequently, annotator agreement is low.
Refer-ence summary generation is a tedious and low ef-ficiency task.
On the other hand, supervised learn-ing of extractive summarization requires a largeamount of training data of reference summaries.To reduce the amount of human annotation effortand improve annotator agreement on the referencesummaries, we propose that active learning (selec-tive sampling) is one possible solution.Active learning has been applied to NLP taskssuch as spoken language understanding (Tur et al,2005), information extraction (Shen et al, 2004),and text classification (Lewis and Catlett, 1994;McCallum and Nigam, 1998; Tong and Koller,2002).
Different from supervised learning whichneeds the entire corpus with manual labeling re-sult, active learning selects the most useful exam-ples for labeling and requires manual labeling oftraining dataset to re-train model.In this paper, we suggest a framework of refer-ence summary annotation with relatively high in-ter labeler agreement based on the rhetorical struc-ture in presentation slides.
Based on this frame-work, we further propose a certainty-based activelearning method to alleviate the burden of humanannotation of training data.The rest of this paper is organized as follows:Section 2 depicts the corpus for our experiments,the extractive summarizer, and outlines the acous-tic/prosodic, and linguistic feature sets for repre-senting each sentence.
Section 3 depicts how to23compile reference summaries with high inter la-beler agreement by using the RDTW algorithmand our active learning algorithm for tagging ex-tractive reference summary.
We describe our ex-periments and evaluate the results in Section 4.Our conclusion follows in Section 5.2 Experimental Setup2.1 The CorpusOur lecture speech corpus (Zhang et al, 2008)contains 111 presentations recorded from theNCMMSC2005 and NCMMSC2007 conferencesfor evaluating our approach.
The man-ual transcriptions and the comparable corpus?PowerPoint slides are also collected.
Each presen-tation lasts for 15 minutes on average.
We select71 of the 111 presentations with well organizedPowerPoint slides that always have clear sketchesand evidently aligned with the transcriptions.
Weuse about 90% of the lecture corpus from the 65presentations as original unlabeled data U and theremaining 6 presentations as held-out test set.
Werandomly select 5 presentations from U as ourseed presentations.
Reference summaries of theseed presentations and the presentations of test setare generated from the PowerPoint slides and pre-sentation transcriptions using RDTW followed bymanual correction, as described in Section 3.2.2 SVM Classifier and the Feature SetWhile (Ribeiro and de Matos, 2007) has shownthat MMR (maximum marginal relevance) ap-proach is superior to feature-based classifica-tion for summarizing Portuguese broadcast newsdata, another work on Japanese lecture speechdrew the opposite conclusion (Fujii et al, 2008)that feature-based classification method is bet-ter.
Therefore we continue to use the feature-based method in our work.
We consider the ex-tractive summarization as a binary classificationproblem, we predict whether each sentence of thelecture transcription should be in a summary ornot.
We use Radial Basis Function (RBF) ker-nel for constructing SVM classifier, which is pro-vided by LIBSVM, a library for support vectormachines (Chang and Lin, 2001).
We representeach sentence by a feature vector which consists ofacoustic features: duration of the sentence, aver-age syllable Duration, F0 information features, en-ergy information features; and linguistic features:length of the sentence counted by word and TFIDFinformation features, as shown in (Zhang et al,2008).
We then build the SVM classifier as oursummarizer based on these sentence feature vec-tors.3 Active Learning for Tagging ReferenceSummary and SummarizationSimilar to (Hayama et al, 2005; Kan, 2007), wehave previously proposed how presentation slidesare used to compile reference summaries automat-ically (Zhang et al, 2008).
The motivations be-hind this procedure are:?
presentation slides are compiled by the au-thors themselves and therefore provide agood standard summary of their work;?
presentation slides contain the hierarchicalrhetorical structure of lecture speech as the ti-tles, subtitles, page breaks, bullet points pro-vide an enriched set of discourse informationthat are otherwise not apparent in the spokenlecture transcriptions.We propose a Relaxed Dynamic Time Warping(RDTW) procedure, which is identical to Dy-namic Programming and Edit Distance, to alignsentences from the slides to those in the lecturespeech transcriptions, resulting in automaticallyextracted reference summaries.We calculate the similarity scoresmatrix Sim = (sij), where sij =similarity(Senttrans[i], Sentslides[j]), be-tween the sentences in the transcription andthe sentences in the slides.
We then obtainthe distance matrix Dist = (dij), wheredij = 1?sij .
We calculate the initial warp path P:P = (pini1 , ..., pinin , ..., piniN ) by DTW, where pininis represented by sentence pair(iinin , jinin ): onefrom transcription, the other from slides.
Con-sidering that the lecturer often doesn?t follow theflow of his/her slides strictly, we adopt RelaxedDynamic Time Warping (RDTW) for finding theoptimal warp path, by the following equation.????
?ioptn = iininjoptn =jinin +Cargminj=jinin ?Cdioptn ,j(1)We consider the transcription sentences on thispath as reference summary sentences.
We thenobtain the optimal path (popt1 , ..., poptn , ..., poptN ),where poptn is represented by (ioptn , joptn ) and C24is the capacity to relax the path.
We then selectthe sentences ioptn of the transcription whose sim-ilarity scores of sentence pairs: (ioptn , joptn ), arehigher than the pre-defined threshold as the refer-ence summary sentences.
The advantage of usingthese summaries as references is that it circum-vents the disagreement between multiple humanannotators.We have compared these reference summariesto human-labeled summaries.
When asked to ?se-lect the most salient sentences for a summary?, wefound that inter-annotator agreement ranges from30% to 50% only.
Sometimes even a single per-son might choose different sentences at differenttimes (Nenkova et al, 2007).
However, when in-structed to follow the structure and points in thepresentation slides, inter-annotator agreement in-creased to 80%.
The agreement between auto-matically extracted reference summary and hu-mans also reaches 75%.
Based on this high degreeof agreement, we generate reference summariesby asking a human to manually correct those ex-tracted by the RDTW algorithm.
Our referencesummaries therefore make for more reliable train-ing and test data.For a transcribed presentation D with a se-quence of recognized sentences {s1, s2, ..., sN},we want to find the sentences to be classifiedas summary sentences by using the salient sen-tence classification function c().
In a probabilis-tic framework, the extractive summarization taskis equivalent to estimating P (c(?
?s n) = 1|D) ofeach sentence sn, where ?
?s n is the feature vec-tor with acoustic and linguistic features of the sen-tence sn.We propose an active learning approach where asmall set of transcriptions as seeds with referencesummaries, created by the RDTW algorithm andhuman correction, are used to train the seed modelfor the summarization classifier, and then the clas-sifier is used to label data from a unlabel pool.
Ateach iteration, human annotators choose the unla-beled documents whose similarity scores betweenthe extracted summary sentences and the Power-Point slides sentences are top-N highest for label-ing summary sentences.
Formally, this approachis described in Algorithm 1.Given document D: {s1, s2, ..., sN}, we cal-culate the similarity score between the extractedsummary sentences: {s?1, s?2, ..., s?K} and the Pow-erPoint slide sentences: {ppts1, ppts2, ..., pptsL},by equation 2.Scoresim(D) = 1KK?n=1L?j=1Sim(s?n, pptsj) (2)4 Experimental Results and EvaluationAlgorithm 1 Active learning for tagging extrac-tive reference summary and summarizationInitializationFor an unlabeled data set: Uall, i = 0(1) Randomly choose a small set of data X{i}from Uall; U{i} = Uall ?X{i}(2) Manually label each sentence in X{i} assummary or non-summary by RDTW and hu-man correction and save these sentences andtheir labels in L{i}Active Learning Process(3) X{i} = null(4) Train the classifier M{i} using L{i}(5) Test U{i} by M{i}(6) Calculate similarity score of given docu-ment D between the extracted summary sen-tences and the PowerPoint slides sentences byequation 2(7) Select the documents with top-five highestsimilarity scores from U{i}(8) Save selected samples into X{i}(9) Manually correct each sentence label inX{i} as summary or non-summary(10) L{i + 1} = L{i} + X{i}(11) U{i + 1} = U{i} ?X{i}(12) Evaluate M{i} on the testing set E(13) i = i+1, and repeat from (3) until U{i} isempty or M{i} obtains satisfying performance(14) M{i} is produced and the process endsWe start our experiments by randomly choosingsix documents for manual labeling.
We graduallyincrease the training data pool by choosing fivemore documents each time for manual correction.We carry out two sets of experiments for compar-ing our algorithm and random selection.
We evalu-ate the summarizer by ROUGE-L (summary-levelLongest Common Subsequence) F-measure (Lin,2004).The performance of our algorithm is illustratedby the increasing ROUGE-L F-measure curve inFigure 1.
It is shown to be consistently higher than25Figure 1: Active learning vs. random selectionusing randomly chosen samples.
We also find thatby using only 51 documents for training, the per-formance of the summarization model achievedby our approach is better than that of the modeltrained by random selection using all 65 presen-tations (0.514 vs. 0.512 ROUGE-L F-measure).This shows that our active learning approach re-quires 22% less training data.
Besides, acousticfeatures can improve the performance of activelearning of speech summarization.
Without acous-tic features, our summarizer only performs 0.47ROUGE-L F-measure.5 Conclusion and DiscussionIn this paper, we propose using active learning re-duce the need for human annotation for taggingextractive reference summary of lecture speechsummarization.
We use RDTW to extract sen-tences from transcriptions according to Power-Point slides, and these sentences are then handcorrected as reference summaries.
The unlabeleddocuments are selected whose similarity scoresbetween the extracted summary sentences and thePowerPoint slides sentences are top-N highest forlabeling summary sentences.
We then use an SVMclassifier to extract summary sentences.
Summa-rization results show an increasing learning curveof F-measure, from 0.44 to 0.514, consistentlyhigher than that of using randomly chosen train-ing data samples.
Besides, acoustic features playa significant role in active learning of speech sum-marization.
In our future work, we will try to ap-ply different criteria, such as uncertainty-based orcommittee-based criteria, for selecting samples tobe labeled, and compare the effectiveness of them.6 AcknowledgementsThis work is partially supported by GRF612806 ofthe Hong Kong RGC.ReferencesC.C.
Chang and C.J.
Lin.
2001.
LIBSVM: a library for sup-port vector machines.
Software available at http://www.csie.
ntu.
edu.
tw/cjlin/libsvm, 80:604?611.Y.
Fujii, K. Yamamoto, N. Kitaoka, and S. Nakagawa.
2008.Class Lecture Summarization Taking into Account Con-secutiveness of Important Sentences.
In Proceedings ofInterspeech, pages 2438?2441.T.
Hayama, H. Nanba, and S. Kunifuji.
2005.
Alignmentbetween a technical paper and presentation sheets usinga hidden markov model.
In Active Media Technology,2005.
(AMT 2005).
Proceedings of the 2005 InternationalConference on, pages 102?106.M.Y.
Kan. 2007.
SlideSeer: A digital library of aligneddocument and presentation pairs.
In Proceedings of the7th ACM/IEEE-CS joint conference on Digital libraries,pages 81?90.
ACM New York, NY, USA.D.D.
Lewis and J. Catlett.
1994.
Heterogeneous uncertaintysampling for supervised learning.
In Proceedings of theEleventh International Conference on Machine Learning,pages 148?156.
Morgan Kaufmann.C.Y.
Lin.
2004.
Rouge: A Package for Automatic Evalua-tion of Summaries.
Proceedings of the Workshop on TextSummarization Branches Out (WAS 2004), pages 25?26.A.
McCallum and K. Nigam.
1998.
Employing EM in Pool-based Active Learning for Text Classification.
In Proceed-ings of ICML, pages 350?358.A.
Nenkova, R. Passonneau, and K. McKeown.
2007.
ThePyramid Method: Incorporating human content selectionvariation in summarization evaluation.
ACM Transactionson Speech and Language Processing (TSLP), 4(2).R.
Ribeiro and D.M.
de Matos.
2007.
Extractive Summa-rization of Broadcast News: Comparing Strategies for Eu-ropean Portuguese.
Lecture Notes in Computer Science,4629:115.D.
Shen, J. Zhang, J. Su, G. Zhou, and C.L.
Tan.
2004.Multi-criteria-based Active Learning for Named EntityRecognition.
In Proceedings of 42th Annual Meeting ofthe Association for Computational Linguistics.
Associa-tion for Computational Linguistics Morristown, NJ, USA.S.
Tong and D. Koller.
2002.
Support vector machine ac-tive learning with applications to text classification.
TheJournal of Machine Learning Research, 2:45?66.G.
Tur, D. Hakkani-Tr, and R. E. Schapiro.
2005.
Combin-ing Active and Semi-supervised Learning for Spoken Lan-guage Understanding.
Speech Communications, 45:171?186.J.J.
Zhang, S. Huang, and P. Fung.
2008.
RSHMM++ forextractive lecture speech summarization.
In IEEE SpokenLanguage Technology Workshop, 2008.
SLT 2008, pages161?164.26
