Some empirical findings on dialogue management and domain ontologies indialogue systems ?
Implications from an evaluation of BIRDQUESTAnnika Flycht-ErikssonDepartment of Computer andInformation ScienceLinko?ping University, Swedenannfl@ida.liu.seArne Jo?nssonDepartment of Computer andInformation ScienceLinko?ping University, Swedenarnjo@ida.liu.seAbstractIn this paper we present implicationsfor development of dialogue systems,based on an evaluation of the systemBIRDQUEST which combine dialogue in-teraction with information extraction.
Anumber of issues detected during theevaluation concerning primarily dialoguemanagement, and domain knowledge rep-resentation and use are presented and dis-cussed.1 IntroductionIn the field of Question Answering (Q&A), Infor-mation extraction (IE) techniques have been usedsuccessfully when it comes to handling simple fac-toid questions, but the Q&A approach has yet notreached the level of sophistication for handling con-nected dialogue as is present in dialogue systems tai-lored to background systems with structured data.Dialogue capabilities allow for more precise formu-lation of information requests and more natural in-teraction.
The challenge is to combine the IE tech-niques and some of the features of Q& approacheswith dialogue systems (Burger et al, 2001).
By asuccessful combination of these techniques, userswould be allowed to access information derivedfrom a large set of, initially unstructured, docu-ments, using dialogue functionalities, such as a di-alogue history and clarification requests.We have developed a first version of such a com-bined system, BIRDQUEST (Jo?nsson and Merkel,2003), which supports dialogue interaction to accesstextual data in a bird encyclopaedia.
The source datais initially provided as unstructured text but refinedwith IE techniques to be used within a dialogue sys-tem framework.
As a basis for many of the tasksin the system domain knowledge represented in anontology is utilised.To assess the approach and get insights into whatareas need further improvement an evaluation of thesystem has been carried out.
In this paper the resultsof this evaluation are presented together with a dis-cussion of implications for development of dialoguesystems with focus on dialogue management and theuse of domain ontologies.2 Combining IE with dialogue interactionin a systemCombining dialogue interaction with informationextraction has several benefits; dialogue is a naturaland efficient means of interaction and with IE tech-niques information can be retrieved from unstruc-tured information sources that are otherwise hard tomanage and search for a user.
A possible way ofmerging these two in a practical system is to havetwo components, an information processing compo-nent and an interaction component that, as a basisfor their tasks, use a set of shared knowledge sourcesthat define the scope of the language and domain.2.1 The Information Processing ComponentThe Information Processing Component takes col-lections of unstructured or semistructured docu-ments and transforms them into structured informa-tion that can be used by the Interaction Componentin the interaction with the user.
The transforma-tion utilise IE techniques, and the documents areanalysed in several ways going through lexical andmorphological, syntactical, and semantical analy-sis (Sullivan, 2001).A wide variety of pattern extraction rules areused to identify the relevant information as slots andfillers.
The objective is to fill the database with rel-evant information and ignore text segments that donot meet the needs of the users.
Figure 1 illustrateshow unstructured text is transformed into slot andfiller type information in the database.Original textBlack-throated diverGavia arctica58-73 cm, wingspan 110-130 cm.In breeding plumage the head is grayand the throat is black, the sidesof the throat striped in black andwhite.
[...]Extracted informationNAME: Black-throated diverLATIN NAME: Gavia arcticaMAX WING: 130MIN WING: 110MAX HEIGHT: 73MIN HEIGHT: 58BR PLUMAGE: ?the head is gray and thethroat is black, the sidesof the throat striped inblack and white.
?Figure 1: Original text passage from the text bookand the corresponding entry in the database (trans-lated from Swedish).2.2 The Interaction ComponentThe Interaction Component is responsible for the di-alogue with the user.
It collaborates with the userto produce a query and access the structured infor-mation sources to retrieve an answer to the query.The interaction component in BIRDQUEST is basedon the MALIN framework (Dahlba?ck et al, 1999).MALIN is a modularised dialogue system and itseparates dialogue management (DM) from domainknowledge management (DKM) (Flycht-Erikssonand Jo?nsson, 2000).
The former handles the dia-logue whereas the latter handles access to variousbackground information sources.The Dialogue Manager is responsible for control-ling the flow of the dialogue by deciding how thesystem should respond to a user utterance.
Thisis done by inspecting and contextually specifyingthe information structure produced by an interpreta-tion module.
The MALIN dialogue model classifiesthe discourse segments by general speech act cate-gories, such as question (Q) and answer (A), ratherthan specialised (cf.
(Hagen, 1999)), or domain re-lated (Alexandersson and Reithinger, 1995).
The di-alogue manager instead utilise the focal parametersto control interaction (cf.
(Jokinen et al, 1998; De-necke, 1997; Jo?nsson, 1995)).
In MALIN dialoguehistory is represented in dialogue objects with a pa-rameter termed Objects, which identify a set of pri-mary referents, and the parameter Properties whichdenote a complex predicate ascribed to this set.
InBIRDQUEST Objects are normally birds and Proper-ties model information about the birds, such as ap-pearance, number of eggs and feed.The Domain knowledge manager receives re-quests from the dialogue manager and process themfurther using domain knowledge, for example, dis-ambiguation and mapping of vague concepts to onesmore suitable for database access.
It then retrievesand coordinates information from available informa-tion sources, such as data and knowledge bases.
Ifa request is under-specified or contains inconsisten-cies from the domain knowledge manager?s point ofview, a specification of what clarifying informationis needed will be returned to the dialogue managerto help the formulation of a clarification question tothe user.2.3 Knowledge sourcesAs a basis for the processing of documents and userqueries a number of knowledge sources are utilised.Some are highly specialised and only used by one ora few submodules of a component, for example thedialogue model in the Interaction Component, whileothers are more general and used for several tasks inboth components.
These shared knowledge sourcescomprise lexicon, grammar, and domain ontologies.Building lexicon and grammars to be used for dif-ferent tasks also involves several challenges but willnot be further discussed in this paper.The term ontology is used very differently in var-ious areas of computer science, ranging from sim-ple taxonomies, meta data schemes, to logical the-ories.
A general and commonly used definitiongiven by Gruber (1993) is that ?An ontology is aformal, explicit specification of a shared conceptu-alisation?.
A more practical view is to consideran ontology as ?a world model used as a com-putational resource for solving a particular set ofproblems?
(Mahesh and Nirenburg, 1995), i.e.
adatabase with information about what categories (orconcepts) exist in the world/domain, what propertiesthey have, and how they are related to one another.An ontology provides a common vocabulary thatcan be used to state facts and formulate questionsabout the domain.
Constructing an ontology that canbe shared by the Information Processing Componentand the Interaction Component then gives us a pos-sible way to bridge users?
expression and queries tothe information contained in the unstructured docu-ments.3 Constructing the domain ontologyA challenge when constructing a shared domain on-tology lies in capturing and including two differentconceptualisations of the domain, the one present inthe information sources and the one users have.
Theshared ontology for the BIRDQUEST system was de-veloped based on the analysis of two different typesof empirical material, a bird encyclopaedia and aquestion corpus.
The corpus consists of more than250 questions about birds.
It was collected by TheSwedish Public Service Television Company on aweb site for one of their nature programs, where thepublic could send in questions, i.e.
it is not a dia-logue corpus.The analysis of the empirical material focusedon identifying objects and properties, which in turnwere organised using hyponym relations.
Fromthe encyclopaedia a conceptualisation underlyingthe structure and presentation of information thatwere to be extracted by the Information Process-ing Component was constructed.
The result was asystem-oriented domain ontology representing ex-perts?
view of the domain.
The question corpusyielded a user-oriented conceptualisation of the do-main, thus providing a non-expert view of the do-main useful for the interaction component.
Thesetwo conceptualisations were then merged to form ashared domain ontology for all components of thesystem.The users?
view of the domain as reflected in thequestions seemed to correspond to the one found inthe reference book, most objects and properties werethe same, but there were two aspects that deviated.The first concerned the classification of birds and thesecond the granularity of the properties of birds.  Users sometimes utilised another way of cat-egorising birds from the biologically orientedtaxonomy in the reference book, talking about?Spring birds?, ?Small birds?, ?Migratorybirds?, and ?Birds of prey?
instead of orders,families, kins etc.  In many cases the properties of the birds weremore general than the terms used in the book,for example questions about a bird?s appear-ance, e.g.
What does a European Robin looklike?
which includes plumage, size, bodyshape, description of beak and feet, etc.Since the two conceptualisations had many ob-jects and properties in common and these were re-lated in similar ways they could be integrated in thefollowing way (cf.
figure 2).
Taking the system-oriented ontology as a starting point the new cate-gories of birds found in the question corpora wereadded.
Allowing multiple inheritance new links be-tween existing categories and new categories wereadded.
Note, for example, how the new category?Small bird?
is introduced and a new link is added to?Finches?
in figure 2.
In a similar manner the vagueproperties were introduced and linked to the exist-ing properties.
This is illustrated in figure 2 wheretwo new levels are introduced, ?Wingspan?
and?Length?
are sub-properties of the property ?Size?,which in turn is a sub-property of the property ?Ap-pearance?.4 Evaluating BIRDQUESTAs stated above BIRDQUEST was developed basedon a corpus of questions.
For further developmentof BIRDQUEST, we needed to assess its strengthsand limitations during dialogues with real users.
Anevaluation of the system was thus performed withHyponym/MeronymInstance OfObject instanceSystem conceptUser conceptFamilySmall birdBirdSpeciesOrderCardinality: 0..1Range:Number Range: StringBirdGeographic locationDomain:Range:Migratory birdDomain:OBJECTSPROPERTIES BirdDomain:Range: ValueCardinality: 0..NWingspanLength EclipseplumageSummerplumageWinterplumagePlumageMigratorybirdCardinality: 0..NDistributionRELATIONSBreeding WinterdistributiondistributionRangePineGrosbeakFinchesSongbirdsAppearanceSizeFigure 2: A part of the integrated ontology representing the conceptualisations of both bird encyclopaediaand users.the goal of detecting problems concerning inter-pretation, dialogue management, and representationand use of domain knowledge.4.1 Data collectionBIRDQUEST is intended to be used by casual userswithout previous experience of dialogue systems orextensive knowledge of birds.
It was therefore eval-uated in a walk-up and use situation similar to a realuse situation during a day when the public was in-vited to the university.
In that respect the situationresembles that of Gustafson and Bell (2000), thoughslightly more controlled.We had six machines running BIRDQUEST during2 hours and 30 minutes and collected dialogues from27 users.
They received minimal instructions in ad-vance, they were only told that the system can an-swer questions on Nordic birds, that it understandsSwedish, and that the dialogue would be recorded.The resulting corpus consisting of 27 dialogueshave a total number of 518 user utterances, with amean of 19 for each user.
However, with individ-ual differences, for instance, three users posing morethan 40 utterances to the system and three users pos-ing less than 5.Personal data about age, gender, interest in birds,and knowledge of birds were collected together witheach dialogue.
The users where of varying age, 5female and 22 male.
Most of them had no inter-est in birds, nor any knowledge of birds.
Thus, de-spite having no interest in birds, they were fairlyrepresentative of the intended users.
Besides thelogged dialogue, the users were also asked to fillout a small questionnaire on how they liked to usethe system.
Most users thought the system was funto use, on a 10-graded scale we had a mean of 7.1.The users also though that it was fairly easy to useBIRDQUEST, mean 6.1.
On the question how theyliked the system we had a score of 4.7, i.e.
the usersneither disliked nor liked BIRDQUEST.4.2 Corpus annotation and initial analysisAs we had no predefined tasks we did not havea situation that allowed for a controlled evalua-tion, as e.g.
PARADISE (Walker et al, 1998) orPROMISE (Beringer et al, 2002).
Instead we useda combination of quantitative and qualitative ap-proaches to analyse the collected dialogue corpus.The dialogues were tagged in order to provide statis-tics over successful and problematic information ex-changes.The user utterances were categorised as in Ta-ble 1 and the categorisation of responses fromBIRDQUEST is presented in Table 2.Table 1 shows that approximately half of the usersutterances (48%) were involved in successful infor-mation exchanges where the user initiated an infor-Table 1: User utterancesNo of Percentage ofutterances user utterancesInterpretableRequests 189 37%Cooperative CRResponses 55 11%UncooperativeCR responses 11 2%Out of scope 121 23%Mis-interpreted 141 27%Table 2: System utterancesNo of Percentage ofutterances system utterancesSuccessful resp.
180 35%Clarification req.
70 13%Incorrect resp.
15 3%Incorrect focus 16 3%Error message 240 46%mation request or answered a clarification requestfrom the system.
We also see that 25% of theuser?s utterances are erroneous in some way and thatBIRDQUEST failed on 141 utterances, as will be fur-ther discussed in section 5.From Table 2 we see that BIRDQUEST presented180 successful responses.
A successful responseis a response where BIRDQUEST presents infor-mation found in the database.
A response wherethe bird encyclopaedia does not include the infor-mation and BIRDQUEST responds e.g.
Informationon wing span is missing for magpie.
is, however,also considered successful.
The reason being thatBIRDQUEST successfully accessed the database andpresented whatever information was there, includ-ing cases where there was no information in thedatabase.
Among the 180 there are 55 such re-sponses, so they are not rare, and shows one of themany interesting problems we encountered in thedevelopment of a dialogue system based on infor-mation extraction from a text book.4.2.1 ClarificationsNotable is that a fair amount of the dialoguemoves involve clarifications.
The system initiates70 clarification sub-dialogues in order to transform avague information request to a specific question, asexemplified by the excerpt below 1.U17: What do water fowls look like?S18: Choose one of the following Water fowls: BarnacleGoose, Tufted Duck, Bean Goose, Whooper Swan,Gadwall,...U19: gadwallS20: You can receive information about the size andplumage of a Gadwall.
Please specify what you areinterested in.U21: sizeS22: A Gadwall has a length of 56 cm from beak tip totail.Information about wingspan is missing for GadwallThe basis for this type of clarification is domainknowledge collected from the domain ontology.
Ut-terance U17 is under specified since the object, thebird family ?Water fowls?, can refer to a number ofdifferent species of birds, and the property ?Appear-ance?, is vague.
To pose clarification question S18,information about which species belong to the givenfamily is gathered from the ontology and the user isasked to chose one of them.
Next, in S20, the ontol-ogy is accessed to retrieve the sub-properties of ap-pearance.
When the user has chosen a specific prop-erty (U21) the request is sufficiently specified.
Theontology is used to find the sub-properties of ?Size?and these are then used to access the database andthe result is presented to the user (S22).The users responded cooperatively to 55 clarifi-cation requests from the system and incorrectly 11times.
A typical example of the latter is seen below.S22: You can receive information about size andplumage of a Blue Tit.
Please specify what you areinterested in.U23: blue titDialogue management, such as clarification sub-dialogues, thus plays an important role for the per-formance of BIRDQUEST.Contextual interpretation and dialogue historymanagement are other important dialogue phenom-ena from MALIN that are frequently utilised in thedialogues.
Managing dialogue history is, however,not trivial.
There are 16 cases in the corpus, termedIncorrect focus in Table 2, when BIRDQUESTpresents doubtful responses because of how dia-logue history is handled, as will be further discussedin section 5.1.1All examples are translations of excerpts from the Swedishdialogue corpus.4.2.2 Utterances out of scope for BIRDQUESTApproximately half of the non-successful user ut-terances (23% of all user utterances) were ques-tions that BIRDQUEST will never be able to an-swer.
Beringer et al (2002) use the term incooper-ative user for users who ?fall out of the role or pur-posely misuse the system.
?, and propose to excludethem in evaluations.
We include such users in ourcorpus, but group them together in a wider categorycalled Out of scope.Out of Scope utterances include user requests forinformation that is outside the scope of the applica-tion, such as How do you kill crows?, or socialisationutterances (Gustafson and Bell, 2000) such as Howare you?.
Utterances can also be out of the database?scope, e.g.
How high does a magpie fly?
is such anutterance since there is no information on how highbirds fly in the Bird encyclopaedia.
These type ofrequests are further discussed in section 5.5The reason for grouping such utterances togetheris that BIRDQUEST can never present informationto them.
Instead, we need to add a number ofwell-designed responses informing the user on thesystem?s abilities.
Utterances that are out of thesystem?s scope require different types of responsesfrom the system, and the corpus gave us valuable in-sights on the importance of system help messagesdescribing what BIRDQUEST can and cannot do.4.2.3 Utterances where BIRDQUEST failsFinally, there are those utterances where the sys-tem failed, i.e.
those where an answer can be foundin the encyclopaedia, but where BIRDQUEST failsto present a successful response for various reasons.Such utterances comprise 27% of the users?
input.We have further analysed these and categorisedthem as being 1) spelling mistakes, 2) lexical gaps,or 3) grammatically out of scope, as seen in Table 3.Table 3 includes only utterances that can be success-fully responded to, not, for instance, misspellings inutterances that are out of the systems?
scope.Table 3 only gives a very brief indication on thenature of non-interpretable utterances in the corpus.For instance, each utterance is tagged as being of onetype only, with misspellings having highest priorityand missing grammar rules the lowest.
Furthermore,there could be several misspellings in one utterance.It is also the case that the categories overlap, i.e.Table 3: User utterances not interpreted byBIRDQUESTNo of Percentage ofutterances system utterancesMisspelled 28 5%Not in lexicon 64 12%Not in grammar 50 10%utterances can belong to more than one category, e.g.a misspelled word can also be missed in the lexi-con.
There are three such utterances tagged mis-spelled that also contain words not in the lexicon.
11of the utterances tagged misspelled cannot be han-dled because of missing grammar rules.
Thus, witha spelling checker, BIRDQUEST can handle another14 utterances, i.e.
misspelling on its own only ac-counts for 2,5% of the utterances that BIRDQUESTcannot handle.5 Implications for developmentOur main goal was to collect data for further de-velopment.
Utterances in the categories involvingunsuccessful information exchanges, such as incor-rect responses and mis-interpretations, were there-fore analysed more closely.5.1 Problematic focus managementFocus management and methods to determine topicshifts play an important role in many dialogue sys-tems.
Our corpus includes a variety of problematiccases, that need further consideration, both on howfocus is handled in the dialogue manager and howthe grammar rules are constructed.When a user poses a question that gives rise toa partially interpreted request the dialogue managerseeks the dialogue history to see if it can be contex-tually specified.
This works well in most cases whenutterances are elliptic but in some cases, especiallywhen the interpreter has failed to produce a completeinterpretation, it can cause faulty responses, see theexample below.U35: How old does a common gull get?S36: Information about the greatest age of a CommonGull:Greatest age: 29 years and 7 monthsU37: What migratory birds exist?S38: Information about range is missing for CommonGull.The reason for not being able to correctly inter-preting U37 is that migratory birds is not a well de-fined concept in the bird encyclopaedia 2.
Thus, onlya partial interpretation with the property ?Range?,based on interpreting ?exist?
as connected to a loca-tion, is produced.
The partial interpretation is thencombined with the object ?Common gull?
from U35and an incorrect response is produced in S38.A more difficult problem to handle is when therehas been a long segment of uninterpreted user ut-terances often after sequences of misspellings or re-quests outside the scope of the system.
Normally,such sequences of utterances are contextually inter-preted utilising the dialogue history and a successfulresponse can be presented to the user.
There are,however, also ?faulty?
cases, as seen in the dialoguefragment below3.
Here, the user is trying to shiftfocus and thus the property ?Feed?
should not havebeen inherited from the previous question (U11).
Amore appropriate response in S18 would have beenYou can receive information about a song thrush?sflight, feed, breeding location, appearance,... Pleasespecify what you are interested in.U11: What do the honey buzzard eatS12: Information about feed for a Honey Buzzard:Main feed consists of Honeycomb,[...]U13: What birds can imitate speechS14: Failed to interpret the question.
Please try againU15: Talking birdsS16: Failed to interpret the question.
Please try againU17: Song thrushS18: Information about feed for a Song Thrush:Worms, larvas and mollusc etc[...]The problem of dialogue history and fo-cus shifts does not have a simple solution.Leceuche et al (2000) use focus rules based on re-lations between the various things in the domainto track focus shifts.
A similar approach is to en-code the domain in a type hierarchy and use under-specified feature structures to determine clarificationrequests (Denecke, 1997).
Jokinen et al (1998) usea topic model based on a tree organisation of domainknowledge to detect topic shifts.
Such approacheswould correctly handle the dialogue above and starta new focus space after U13, as there is no relation2Many bird species do not migrate every year and oftensome exemplars stay when most birds of that specie migrateetc.
cf.
section 5.4.3The Swedish name of Song thrush, (Sw. taltrast), can beliterally translated as ?Talking thrush?between the items in U11-S12 and U13.
However,as we assume that it is a good idea to keep itemsin focus even after user utterances with no previousrelation to the current focus (Jo?nsson, 1995), suchmechanisms are not applicable here.
For instance, ifthe user instead typed ?U13b: magpi?, we would nothave a relation between the items in the utterances,and can then not be able to correctly respond if theuser corrected that to ?U15b: magpie?.Another approach is proposed byChu-Carroll (2000) who distinguish between taskinitiatives and discourse initiatives that are identifiedbased on cues.
These are then utilised to adoptdifferent interaction strategies.
In BIRDQUESTone solution could be to keep focus for discourseinitiatives and drop focus for new task initiatives.In the dialogue above, U13, is to be considered anew task initiative whereas U13b is not a new taskinitiative and focus is maintained.5.2 Unnecessary clarificationsSince information extraction is used to populate thedatabase with information there can be holes init, i.e.
for some properties values are missing dueto shortcomings in the information extraction com-ponent or lack of information in the original textsource.
This causes unnecessary clarifications.
Inthe example below a more appropriate answer toU19 would have been to give information about sizeinstead of making a clarification since there is no in-formation about plumage in the database.U19: What does a mute swan look like?S20: You can receive information about the size andplumage of a Mute Swan.
Please specify what youare interested in.U21: What do the plumage of a mute swan look like?S22: Information about eclipse plumage is missing forMute Swan.Information about winter plumage is missing forMute Swan.Information about summer plumage is missing forMute Swan.Information about breeding plumage is missing forMute Swan.The database has many such missing values asvery often there was no information in the informa-tion source.
It is, thus, not possible to adopt generaldialogue strategies, for instance, for clarification re-quests.
Instead, the domain knowledge base must beconsulted to decide on a proper dialogue strategy foreach property.In BIRDQUEST the unnecessary clarifications canbe dealt with through extended co-operation be-tween the dialogue manager and the domain knowl-edge manager.
When a vague property is encoun-tered the dialogue manager can send the request tothe domain knowledge manager for information onsuitable clarifications.
By traversal of the ontologythe DKM can produce a set of sub-properties whichcan be used to access the database.
Should therebe only a few pieces of information these can bereturned directly as an answer to the request.
Oth-erwise the proper clarification is decided based onthe relation between the given property and the sub-properties which produced the values.Note, however, that such a strategy can violate thelearning aspect discussed below.5.3 Partial and empty answersA problem related to unnecessary clarifications arehow partial and empty answers should be presentedto the user when a vague property has been mappedto several sub-properties, for example ?Plumage?
inS22 in the example above, or ?Size?
in the examplebelow.S2: You can receive information about the size andplumage of a Magpie.
Please specify what you areinterested in.U3: sizeS4: Information about wingspan is missing for Magpie.A Magpie has a length of 46 cm from beak tip totail.In the case of empty answers, S22 above, an alter-native could be to just state that Information aboutplumage is missing for mute swan, and for partialanswers the sub-properties with missing informationcould be omitted.
However, including all the sub-properties have a point in so far as it helps the userlearn what type of information the system can pro-vide.
A solution is to make a compromise and makethe system adaptive.
This means that the first time avague property is mapped to a set of sub-propertiesall of them are presented regardless of whether theyhave values or not, thereafter empty answers can beomitted.5.4 Ontological interpretation failuresSome seemingly simple interpretation failures thathave been tagged as missing information in the lexi-con are actually more complicated and relates to therepresentation of domain knowledge in the ontology.For example, users tend to ask what colour a spe-cific bird have, see U1 below4.
However, in the birdencyclopaedia colour is linked to the body parts ofa bird.
The complex relation between the concept?Bird?
and the property ?Colour?
via the body partsthus have to be captured in the ontology so that thisknowledge can be used for interpretation and dia-logue management.U1: What colour is a blue tit?S2: Failed to interpret the question.
Please try againWith more information about how concepts andproperties are related, for example, through a chainof hypernym and meronym relations, requests likeU1 could be dealt with.
Since a bird consists of bodyparts and body parts have colours the missing link inthe request is a body part and based on this a clarifi-cation could be produced A bird has several coloursfor different body parts.
What part of the blue tit areyou interested in?Migratory birds, as discussed in section 5.1, isanother example when the ontology can be used tostipulate a certain interpretation.
With the help ofan ornithologist, we can define what migrating be-haviour constitutes a migratory bird and add that tothe ontology.
Used by the information extractioncomponent, we can easily pick those birds that aremigratory birds, and as we utilise the same ontologyfor the interaction component, we can interpret suchrequests.
Note, however, that this goes beyond ourinitial goal of only utilising the bird encyclopaedia.5.5 Questions outside database coverageUsers often posed questions that were outside thedatabase coverage.
A successful interpretation ofsuch a request and an appropriate response is seenin the dialogue below:U1: What is the wingspan of an Albatross?S2: There is no information on non-Nordic birds.
Theavailable birds are listed in the menu ?Birds a-o?
?.Please try againThe same strategy could be used to handle similarquestions concerning non-Nordic birds like canariesand ostriches.
However, that would require additionof information of non-Nordic birds to the ontology4Many of the requests for appearance can be handled by pre-senting a picture of the bird.
However, the pictures in our birdencyclopaedia are copyrighted and can therefore not be pre-sented.collected from other sources than the bird encyclo-pedia.However, in most cases the requests concernedproperties that are not covered by the database butusers often ask about, for example ?Weight?
and?Flight-speed?
as in How much does a Sea Gullweigh?
or How fast can an Eagle fly?.
The commonresponse to these types of questions were Failed tointerpret the question.
Please try again or in somecases a partial interpretation was made which led toinappropriate responses.
A more desirable responsewould be to give more informative error messagesand explain to the user that it cannot answer ques-tions about these topics.Extending the ontology could help give informa-tive answers when the questions are outside databasecoverage.
The properties similar to those in thedatabase, such as ?Weight?, ?Flight-speed?, couldbe added to the ontology as user-oriented proper-ties.
Since the DKM always have to map this typeof properties to the system-oriented sub-propertiesbefore database access it could conclude that, if auser-oriented property do not have any user-orientedsub-properties, it is outside database coverage andan appropriate answer can be given.
If these prop-erties were related to others, for example, ?Weight?is a sub-property of ?Appearance?, the system couldeven suggest some of the sibling properties, in thiscase ?Size?
and ?Plumage?.Another strategy is to have BIRDQUEST respondwith help phrases explaining how to pose valid re-quests, as is done in Targeted Help (Gorrell et al,2002).
Targeted help is used for improving user be-haviour in speech interfaces.
It utilises the SLM-based recognition and categorised help messagetemplates to present targeted help when the gram-mar based recogniser fails.
Thus, a system mustlearn the most common types of mistakes which inturn must be classified to provide a targeted help.Unfortunately, we do not yet have a large enoughBIRDQUEST corpus for such classification.6 SummaryIn this paper we have presented an evaluation ofa dialogue system that was developed to access adatabase built from information automatically ex-tracted from a text book.
The results from our eval-uation show that it is possible to develop such a sys-tem and that users staying within the boundaries ofthe application will get useful information.Dialogue is important for the interaction as wellas a shared ontology for both information extractionand interaction.
The evaluation also revealed a num-ber of challenging issues, especially regarding, sys-tem help messages, dialogue management, problemswith gaps in the database due to incomplete informa-tion and how to utilise a domain ontology.7 AcknowledgementMany thanks to Frida Ande?n, Lars Degerstedt andSara Norberg for interesting discussions and workon the development and implementation of theBIRDQUEST system.
This research is financed byVinnova, Swedish Agency for Innovation SystemsReferencesJan Alexandersson and Norbert Reithinger.
1995.
De-signing the dialogue component in a speech translationsystem.
In Proceedings of the Ninth Twente Workshopon Language Technology (TWLT-9), pages 35?43.Nicole Beringer, Ute Kartal, Katerina Louka, FlorianSchiel, and Uli Tu?rk.
2002.
Promise - a procedurefor multimodal interactive system evaluation.
In Pro-ceedings of the Workshop ?Multimodal Resources andMultimodal Systems Evaluation?.
Las Palmas, GranCanaria, Spain.J.
Burger, C. Cardie, V. Chaudhri, R. Gaizauskas,S.
Harabagiu, D. Israel, C. Jacquemin, C. Y. Lin,S.
Maiorano, G. Miller, D. Moldovan, B. Og-den, J. Prager, E. Riloff, A. Singhal, R. Shrihari,T.
Strzalkowski, E. Voorhees, and R. Weishedel.2001.
Issues, tasks and program structures toroadmap research in question & answering (Q&A).http://wwwnlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper v2.doc.Jennifer Chu-Carroll.
2000.
MIMIC: An adaptivemixed initiative spoken dialogue system for informa-tion queries.
In Proceedings of 6th Applied NaturalLanguage Processing Conference, pages 97?104.Nils Dahlba?ck, Annika Flycht-Eriksson, Arne Jo?nsson,and Pernilla Qvarfordt.
1999.
An architecture formulti-modal natural dialogue systems.
In Proceedingsof ESCA Tutorial and Research Workshop (ETRW)on Interactive Dialogue in Multi-Modal Systems, Ger-many.Matthias Denecke.
1997.
An information-based ap-proach for guiding multi-modal human-computer-interaction.
In IJCAI?97, Nagoya, Japan, pages 1036?1041.Annika Flycht-Eriksson and Arne Jo?nsson.
2000.
Dia-logue and domain knowledge management in dialoguesystems.
In 1st SIGdial Workshop on Discourse andDialogue, Hong Kong.Genevieve Gorrell, Ian Lewin, and Manny Rainer.
2002.Adding intelligent help to mixed initiative spoken dia-logue systems.
In Proceedings of ICSLP 2002.Tom R. Gruber.
1993.
A translation approach toportable ontology specification.
Knowledge Acquisi-tion, 5:199?220.Joakim Gustafson and Linda Bell.
2000.
Speech tech-nology on trial: Experiences from the august system.Natural Language Engineering, 6(3-4):273?286.Eli Hagen.
1999.
An approach to mixed initiative spo-ken information retrieval dialogue.
User modeling andUser-Adapted Interaction, 9(1-2):167?213.Arne Jo?nsson and Magnus Merkel.
2003.
Some issues indialogue-based question-answering.
In Working Notesfrom AAAI Spring Symposium, Stanford.Kristiina Jokinen, Hideki Tanaka, and Akio Yokoo.1998.
Context management with topics for spoken di-alogue systems.
In Proceedings of the 36th AnnualMeeting of the Association of Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics, COLING-ACL?98, Montreal, pages631?637.Arne Jo?nsson.
1995.
Dialogue actions for naturallanguage interfaces.
In Proceedings of IJCAI-95,Montre?al, Canada.Renaud Leceuche, Dave Robertson, Catherine Barry,and Chris Mellish.
2000.
Evaluating focus theoriesfor dialogue management.
International Journal onHuman-Computer Studies, 52:23?76.Kavi Mahesh and Sergei Nirenburg.
1995.
A situated on-tology for practical NLP.
In Proceedings of IJCA?95Workshop on Basic Ontological Issues in KnowledgeSharing, Montreal, Canada.Dan Sullivan.
2001.
Document Warehousing and TextMining.
John Wiley & Sons.Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,and Alicia Abella.
1998.
Paradise: A framework forevaluating spoken dialogue agents.
In Mark Maybury& Wolfgang Wahlster, editor, Readings in IntelligentUser Interfaces.
Morgan Kaufmann.
