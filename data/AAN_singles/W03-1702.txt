Class Based Sense Definition Model for Word Sense Tagging and DisambiguationTracy LinDepartment of Communication EngineeringNational Chiao Tung University,1001, Ta Hsueh Road, Hsinchu, 300, Taiwan, ROCtracylin@cm.nctu.edu.twJason S. ChangDepartment of Computer ScienceNational Tsing Hua University101, Kuangfu Road, Hsinchu, 300, Taiwan, ROCjschang@cs.nthu.edu.twAbstractWe present an unsupervised learningstrategy for word sense disambiguation(WSD) that exploits multiple linguisticresources including a parallel corpus, a bi-lingual machine readable dictionary, and athesaurus.
The approach is based on ClassBased Sense Definition Model (CBSDM)that generates the glosses and translationsfor a class of word senses.
The model canbe applied to resolve sense ambiguity forwords in a parallel corpus.
That sensetagging procedure, in effect, produces asemantic bilingual concordance, whichcan be used to train WSD systems for thetwo languages involved.
Experimental re-sults show that CBSDM trained onLongman Dictionary of ContemporaryEnglish, English-Chinese Edition(LDOCE E-C) and Longman Lexicon ofContemporary English (LLOCE) is veryeffectively in turning a Chinese-Englishparallel corpus into sense tagged data fordevelopment of WSD systems.1.
IntroductionWord sense disambiguation has been an importantresearch area for over 50 years.
WSD is crucial formany applications, including machine translation,information retrieval, part of speech tagging, etc.Ide and Veronis (1998) pointed out the two majorproblems of WSD: sense tagging and data sparse-ness.
On one hand, tagged data are very difficult tocome by, since sense tagging is considerably moredifficult than other forms of linguistic annotation.On the other hand, although the data sparseness isa common problem, it is especially severe forWSD.
The problems were attacked in various ways.Yarowsky (1992) showed a class-based approachunder which a very large untagged corpus and the-saurus can be used effectively for unsupervisedtraining for noun homograph disambiguation.However, the method does not offer a method thatexplicitly produces sense tagged data for any givensense inventory.
Li and Huang (1999) described asimilar unsupervised approach for Chinese textbased on a Chinese thesaurus.
As noted in Meri-aldo (1994), even minimal hand tagging improvedon the results of unsupervised methods.
Yarowsky(1995) showed that the learning strategy of boot-strapping from small tagged data led to results ri-valing supervised training methods.
Li and Li(2002) extended the approach by using corpora intwo languages to bootstrap the learning process.They showed bilingual bootstrapping is even moreeffective.
The bootstrapping approach is limited bylack of a systematic procedure of preparing seeddata for any word in a given sense inventory.
Theapproach also suffers from errors propagating fromone iteration into the next.
Li and HuangAnother alternative involves using a parallelcorpus as a surrogate for tagged data.
Gale, Churchand Yarowsky (1992) exploited the so-called onesense per translation constraint for WSD.
Theyreported high precision rates of a WSD system fortwo-way disambiguation of six English nounsbased on their translations in an English-FrenchParallel corpus.
However, when working with aparticular sense inventory, there is no obvious wayto know whether the one sense per translation con-straint holds or how to determine the relevanttranslations automatically.Diab and Resnik (2002) extended the transla-tion-based learning strategy with a weakened con-straint that many instances of a word in a parallelcorpus often correspond to lexically varied but se-mantically consistent translations.
They proposedto group those translations into a target set, whichcan be automatically tagged with correct sensesbased on the hypernym hierarchy of WordNet.Diab and Resnik?s work represents a departurefrom previous unsupervised approaches in that noseed data is needed and explicit tagged data areproduced for a given sense inventory (WordNet intheir case).
The system trained on the tagged datawas shown to be on a par with the best ?supervisedtraining?
systems in SENSEVAL-2 competition.However, Diab and Resnik?s method is only appli-cable to nominal WordNet senses.
Moreover, themethod is seriously hampered by noise and seman-tic inconsistency in a target set.
Worse still, it isnot always possible to rely on the hypernym hier-archy for tagging a target set.
For instance, therelevant senses of the target set of {serve, tee off}for the Chinese counterpart  [faqiu] do nothave a common hypernym:Sense 15serve ?
(put the ball into play; as in games like tennis)?
move ?
(have a turn; make one?s move in a game)Sense 1Tee off ?
(strike a golf ball from a tee at the start of a game)?
play ?
(participating in game or sports)?
compete ?
(compete for something)This paper describes a new WSD approach tosimultaneously attack the problems of tagging anddata sparseness.
The approach assumes the avail-ability of a parallel corpus of text written in E (thefirst language, L1+) and C (the second language,L2), an L1 to L2 bilingual machine readable dic-tionary M, and a L1 thesaurus T. A so-called Mu-tually Assured Resolution of Sense Algorithm(MARS) and Class Based Sense Definition Model(CBSDM) are proposed to identify the word sensesin I for each word in a semantic class of words L inT.
Unlike Diab and Resnik, we do not apply theMARS algorithm directly to target sets to avoidthe noisy words therein.
The derived classes sensesand their relevant glosses in L1 and L2 make itpossible to build Class Based Sense Definition andTranslation Models (CBSDM and CBSTM), whichsubsequently can be applied to assign sense tags towords in a parallel corpus.The main idea is to exploit the defining L1 andL2 words in the glosses to resolve the sense ambi-+This has nothing to do with the direction of translation and isnot to be confused with the native and second language dis-tinction made in the literature of Teaching English As a Sec-ond Language (TESL) and Computer Assisted LanguageLearning.guity.
For instance, for the class containing ?serve?and ?tee off,?
the approach exploits common defin-ing words, including ?ball?
and ?game?
in tworelevant serve-15 and tee off-1 to assign the cor-rect senses to ?serve?
and ?tee off.?
The characterbigram  [faqiu] in an English-ChineseMRD:serve v 10 [I?
; T1] to begin play by striking  (theball) to the opponent  (LDOCE E-C p.1300),would make it possible to align and sense tag?serve?
or ?tee off?
in a parallel corpus such as thebilingual citations in Example 1:(1C)(1E) drink a capful before teeing off at each hole.
(Source: Sinorama, 1999, Nov. Issue, p.15, WhoPlayed the First Stroke?
).That effectively attaches semantic information tobilingual citations and turns a parallel corpus into aBilingual Semantic Concordance (BSC).
The BSCenables us to simultaneously attack two criticalWSD problems of sense tagging difficulties anddata sparseness, thus provides an effective ap-proach to WSD.
BSC also embodies a projectionof the sense inventory from L1 onto L2, thus cre-ates a new sense inventory and semantic concor-dance for L2.
If I is based on WordNet for English,it is then possible to obtain an L2 WordNet.
Thereare many additional applications of BSC, includingbilingual lexicography, cross language informationretrieval, and computer assisted language learning.The remainder of the paper is organized as fol-lows: Sections 2 and 3 lay out the approach anddescribe the MARS and SWAT algorithms.
Sec-tion 4 describes experiments and evaluation.
Sec-tion 5 contains discussion and we conclude inSection 6.2.
Class Based Sense Definition ModelWe will first illustrate our approach with an exam-ple.
A formal treatment of the approach will followin Section 2.2.2.1 An exampleTo make full use of existing machine readable dic-tionaries and thesauri, some kind of linkage andintegration is necessary (Knight and Luk, 1994).Therefore, we are interested in linking thesaurusclasses and MRD senses: Given a thesaurus class S,it is important that the relevant senses for eachword w in S is determined in a MRD-based senseinventory I.
We will show such linkage is usefulfor WSD and is feasible, based solely on the wordsof the glosses in I.
For instance, given the follow-ing set of word (N060) in Longman Lexicon ofContemporary English  (McArthur 1992):L = {difficult, hard, stiff, tough, arduous, awkward}.Although those words are highly ambiguous,the juxtaposition immediately brings to mind therelevant senses.
Specifically for the sense inven-tory of LDOCE E-C, the relevant senses for L areas follows:Therefore, we have the intended senses, SS = {difficult-1, hard-2, stiff-6, tough-4, arduous-1, awk-ward-2}.It is reasonable to assume each sense in I is ac-companied by a sense definition written in thesame language (L1).
We use D(S) to denote theglosses of S. Therefore we haveD(S) = ?not easy; hard to do, make, understand, etc.
;  diffi-cult to do or understand; difficult to do; difficult to do; noteasy; demanding effort; needing much effort; difficult; notwell made for use; difficult to use; causing difficulty;?The intuition of bringing out the intendedsenses of semantically related words can be for-malized by Class Based Sense Definition Model(CBSDM), which is a micro language model gen-erating D(S), the glosses of S in I.
For simplicity,we assume an unigram language model P(d) thatgenerates the content words d in the glosses of S.Therefore, we haveD(S) = ?easy hard do make understand difficult do under-stand difficult do difficult do easy demanding effort need-ing much effort difficult well made use difficult use causingdifficulty?If we have the relevant senses, it is a simplematter of counting to estimate P(d).
Conversely,with P(d) available to us, we can pick the relevantsense of S in I which is most likely generated byP(d).
The problem of learning the model P(d) lenditself nicely to an iterative relaxation method suchas the Expectation and Maximization Algorithm(Dempster, Laird, Rubin, 1977).Initially, we assume all senses of S word in I isequally likely and use all the defining wordstherein to estimate P(d) regardless of whether theyare relevant.
For LDOCE senses, initial estimate ofthe relevant glosses is as follows:D(S) = ?easy hard do make understand people unfriendlyquarrelling pleased ?
firm stiff broken pressed bent diffi-cult do understand forceful needing using force bodymind ?bent painful moving moved ?
strong weakenedsuffer uncomfortable conditions cut worn bro-ken ?needing effort difficult lacking skill moving bodyparts body CLUMSY made use difficult use causing diffi-culty?Table 1.
The initial CBSDM for n-word list {difficult,hard, stiff, tough, arduous, awkward} based on the rele-vant and irrelevant LDOCE senses, n = 6.Defining word d Count, k P(d) = k/nDifficult 5 0.83Effort 3 0.50Understand 2 0.33Bad 2 0.33Bent 2 0.33Body 2 0.33Broken 2 0.33Difficulty 2 0.33Easy 2 0.33Firm 2 0.33Hard 2 0.33Moving 2 0.33Needing 2 0.33Water 2 0.33As evident from Table 1, the initial estimates ofP(d) are quite close to the true probability distribu-tion (based on the relevant senses only).
The threetop ranking defining words ?difficult,?
?effort,?
and?understand?
appear in glosses of relevant senses,and not in irrelevant senses.
Admittedly, there arestill some noisy, irrelevant words such as ?bent?and ?broken.?
But they do not figure prominentlyin the model from the start and will fade out gradu-ately with successive iterations of re-estimation.We estimate the probability of a particular sense sbeing in S by P(D(s)), the probability of its glossunder P(d).
For intance, we haveP(hard-1) = P(D(hard-1)) = P(?firm and stiff; which ??
),P(hard-2) = P(D(hard-2)) = P(?difficult to do or understand?
).On the other hand, we re-estimate the probabil-ity P(d) of a defining word d under CBSDM byhow often d appears in a sense s and P(s).
P(d) ispositively prepositional to the frequency of d inD(s) and to the value of P(s).
Under that re-estimation scheme, the defining words in relevantsenses will figure more prominently in CBSDM,leading to more accurate estimation for probabilityof s being in S. For instance, in the first round,?difficult?
in the gloss of hard-2 will weigh twicemore than ?firm?
in the gloss of irrelevant hard-1,leading to relatively higher unigram probability for?difficult.?
That in turn makes hard-2 even moreprobable than hard-1.
See Table 2.Table 2.
First round estimates for P(s), the probability ofsense s in S.Sense* Definition P(s)hard-1 firm and stiff; which can-not easily be broken0.2857hard-2 difficult to do or under-stand0.7143stiff-1 not easily bent 0.2857stiff-6 difficult to do 0.7143* in LDOCE.
** Assuming )(max)()(dPsPsDd?
?Often the senses in I are accompanied withglosses written in a second language (L2); exclu-sively (as in a simple bilingual word list) or addi-tionally (as in LDOCE E-C).
Either way, the wordsin L2 glosses can be incorporated into D(s) andP(d).
For instance, the character unigrams and/oroverlapping bigrams in the Mandarin glosses of Sin LDOCE E-C and their appearance counts andprobability are shown in Table 3.Table 3.
Classes Based Sense Translation Model for{difficult-1, hard-2, stiff-6, tough-4, arduous-1, awk-ward-2} in LDOCE*.We call the part of CBSDM that are involvedwith words written in L2, Class Based SenseTranslation Model.
CBSTM trained on a thesaurusand a bilingual MRD can be exploited to alignwords and translation counter part as well as toassign word sense in a parallel corpus.
For instance,given a pair of aligned sentences in a parallel cor-pus:(2E) A scholar close to Needham analyses the reasonsthat he was able to achieve this huge work asbeing due to a combination of factors thatwould be hard to find in any other person.
(Source: 1990, Dec Issue Page 24, Giving Jus-tice Back to China --Dr. Joseph Needham andthe History of Science and Civilisation in China)It is possible to apply CBSTM to obtain the fol-lowing pair of translation equivalent, (  [nan],?hard?)
and, at the same time, determine the in-tended sense.
For instance, we can label the cita-tion with hard-2LDOCE, leading to the followingquadruple:(3) (hard,  [nan], hard-2LDOCE , (2C, 2E))After we have done this for all pairs of word andtranslation counterpart, we would in effect estab-lish a Bilingual Semantic Concordance (BSC).2.2 The ModelWe assume that there is a Class Based Sense Defi-nition Model, which can be viewed as a languagemodel that generates the glosses for a class ofsenses S. Assume that we are given L, the words ofS but not explicitly the intended senses S. In addi-tion, we are given a sense inventory I in the formof an MRD with the regular glosses, which arewritten in L1 and/or L2.
We are concerned withtwo problems: (1) Unsupervised training of M,CBSDM for S; (2) Determining S by identifying arelevant sense in I, if existing, for each word in L.Those two problems can be solved based onMaximum Likelihood Principle: Finding M and Ssuch that M generates the glosses of S with maxi-mum probability.
For that, we utilize the Expecta-tion and Maximization Algorithm to derive M andS through Mutually Assured Resolution of SenseAlgorithm (MARS) given below:Mutual Assured Resolution of Sense AlgorithmDetermine the intended sense for each of a set of seman-tic related words.Input: (1) Class of words L = {w1 w2 ?wn};(2) Sense inventory I.Output: (1) Senses S from I for words in L;(2) CBSTM M from L1 to L2.1.
Initially, we assume that each of the senses wi,j, j =1, mi in I is equally probable to be in S with prob-abilityiji,1),|(mLiwP = , j = 1, mi; where mi isthe number of senses in I for the word wi.2.
Estimate CBSDM P(d | L) for L ,,),(),|()|(kj,i,ji,kj,maxnddEQLiwPLdP i?=where d is a unigram or overlapping bigram in L1or L2, di,j,k = the kth word in D(wi,j), and EQ(x, y)= 1, if x = y and 0 otherwise;3.
Re-estimate P(wi,j | i,L) according to di,j,k , k = 1,n i,j :,)|P(15.0)|P(5.0),|(P kj,i,ji,kj,i,ji,1 max ?+=kk LdnLdLiw?==i,1ji,1ji,1ji, ),|(P),|(P),|P(mjLiwLiwLiw ;4.
Repeat Steps 2 and 3 until the values of P(d | L) andP(wi,j | i, L) converge;5.
For each i, find the most probable sense wi,j* ,j*=argmaxj P(wi,j | i, L) ;6.
Output S = { wi,j* | j*=argmax j P(wi,j | i, L)} ;7.
Estimate and output CBSTM for L,ntcILcP ni?=?=,1j*,i )()|( ,where c is a unigram or overlapping bigram in L2and ti,j is the L2 gloss of wi,j.Note that the purpose of Step 2 is to estimate how likelya word will appear in the definition of S based on thedefinining word for the senses, wi,j  and relevant prob-ability P(wi,j | i,L).
This likelihood of the word d beingused to define senses in questions is subsequently usedto re-estimate P(wi,j | i,L), the likelihood of the jth sense,wi,j of wi being in the intended senses of L.3.
Application to Word Sense TaggingArmed with the Class Based Sense TranslationModel, we can attack the word alignment andsense tagging problems simultaneously.
Each wordin a pair of aligned sentences in a parallel corpuswill be considered and assigned a counterparttranslation and intended sense in the given contextthrough the proposed algorithm below:Simutaneous Word Alignment and Tagging Algorithm (SWAT)Align and sense tag words in a give sentence and trans-lation.Input: (1) Pair of sentences (E, C);(2) Word w, POS p in question;(3) Sense Inventory I;(4) CBSTM, P(c|L).Output:  (1) Translation c of w in C;(2) Intended sense s for w.1.
Perform part of speech tagging on E;2.
Proceed if w with part of speech p is found in theresults of tagging E;3.
For all classes L to which (w, p) belongs and allwords c in C:,)|(maxmaxarg*),(?????
?= LcPLcwLINKL( )*)|(maxarg* LcPcc= ,where LINK(x, y) means x and y are two wordaligned based on Competitive Linking Align-ment4.
Output c* as the translation;5.
Output the sense of w in L* as the intended sense.To make sense tagging more precise, it is advisableto place constraint on the translation counterpart cof w. SWAT considers only those translations cthat has been linked with w based the CompetitiveLinking Algorithm (Melamed 1997) and logarith-mic likelihood ratio (Dunning 1993).Table 4.
The experimental results of assigning LDOCEsenses to classes of LLOCE.4.
Experiments and evaluationIn order to assess the feasibility of the proposedapproach, we carried out experiments and evalua-tion on an implementation of MARS and SWATbased on LDOCE E-C, LLOCE, and Sinorama.First experiment was involved with the train-ability of CBSDM and CBSTM via MARS.
Thesecond experiment was involved with the effec-tiveness of using SWAT and CBSTM to annotate aparallel corpus with sense information.
Evaluationwas done on a set of 14 nouns, verbs, adjectives,and adverbs studies in previous work.
The set in-cludes the nouns ?bass,?
?bow,?
?cone,?
?duty,??gallery,?
?mole,?
?sentence,?
?slug,?
?taste,??star,?
?interest,?
?issue,?
the adjective ?hard,?and the verb ?serve.
?Table 5.
Evaluation of the MARS Algorithm based on12 nouns, 1 verb, 1 adjective in LDOCE.Word Pos #Senses #Done #Correct Prec(LB*)Prec.Bass N 4 1 1 0.25 1.00Bow N 5 2 2 0.25 1.00Cone N 3 3 2 0.33 0.67Duty N 2 2 2 0.13 1.00Galley N 3 3 2 0.33 0.67Mole N 3 2 2 0.33 1.00Sentence N 2 2 2 1.00 1.00Slug N 2 2 2 0.20 1.00Taste N 6 1 1 0.17 1.00Star N 8 2 2 0.13 1.00Interest N 6 4 4 0.17 1.00Issue N 7 4 3 0.14 0.75Serve V 13 4 2 0.08 0.50Hard A 12 2 2 0.08 1.00Avg.
4.14 1.36 1.29 0.26 0.90* The lower bound of precision of picking one sense in random.Table 6.
Experimental results of sense tagging the Sinoramaparallel Corpus.Word Instance #done #correct PrecisionStar 173 86 82 0.95Hard 325 37 33 0.894.1 Experiment 1: Training CBSDMWe applied MARS to assign LDOCE senses toword classes in LLOCE.
Some results related tothe test set are shown in Tables 4.
The evaluationin Tables indicates that MARS assigns LDOCEsenses to an LLOCE class with a high average pre-cision rate of 90%.4.2 Experiment 2: Sense TaggingWe applied SWAT to sense tag English words insome 50,000 reliably aligned sentence pairs in Si-norama parallel Corpus based on LDOCE senseinventory.
The results are shown in Tables 6.Evaluation indicates an average precision rate ofaround 90%.5.
DiscussionThe proposed approach offers a new method forautomatic learning for the task of word sense dis-ambiguation.
The class based approach attacks theproblem of tagging and data sparseness in a waysimilar to the Yarowsky approach (1992) based onthesaurus categories.
We differ from theYarowsky?s approach, in the following ways:i.
The WSD problem is solved for two languages in-stead of one within a single sense inventory.
Fur-thermore, an explicit sense tagged corpus isproduced in the process.ii.
It is possible to work with any number of sense in-ventories.iii.
The method is applicable not only to nouns butalso to adjectives and verbs, since it does not relyon topical context, which is effective only fornouns as pointed out by Towell and Voorhees(1998).The approach is very general and modular andcan work in conjunction with a number of learningstrategies for word sense disambiguation(Yarowsky, 1995; Li and Li, 2002).6.
ConclusionIn this paper, we present the Mutual Assured Reso-lution of Sense (MARS) Algorithm for assigningrelevant senses to word classes in a given senseinventory (i.e.
LDOCE or WordNet).
We also de-scribe the SWAT Algorithm for automatic sensetagging of a parallel corpus.We carried out experiments on an implementa-tion of the MARS and SWAT Algorithms for allthe senses in LDOCE and LLOCE.
Evaluation on aset of 14 highly ambiguous words showed thatvery high precision CBSDM and CBSTM can beconstructed.
High applicability and precision rateswere achieved, when applying CBSTM to sensetagging of a Chinese-English parallel corpus.A number of interesting future directions pre-sent themselves.
First, it would be interesting tosee how effectively we can broaden the coverageof CBSTM via backing off smoothing.
Second, aCBSTM trained directly on a parallel corpus wouldbe more effective in word alignment and sensetagging.
The approach of training CBSTM on theL2 glosses in a bilingual MRD may lead to occa-sional mismatch between MRD translations and in-context translations.
Third, there is a lack of re-search for a more abstractive and modular repre-sentation of sense differences and commonality.There is potential of developing Sense DefinitionModel to identify and represent semantic and sty-listic differentiation reflected in the MRD glossespointed out in DiMarco, Hirst and Stede (1993).Last but not the least, it would be interesting toapply MARS to both LDOCE E-C and WordNetand project WordNet?s sense inventory to a sen-cond language via CBSDM and a parallel corpus,thus creating a Chinese WordNet and semanticconcordance.AcknowledgementWe acknowledge the support for this study throughgrants from National Science Council and Ministryof Education, Taiwan (NSC 90-2411-H-007-033-MC and MOE EX-91-E-FA06-4-4).ReferencesDagan, Ido; A. Itai, and U. Schwall (1991).
Two lan-guages are more informative than one.
Proceedingsof the 29th Annual Meeting of the Association forComputational Linguistics, 18-21 June 1991, Berke-ley, California, 130-137.Dempster, A., N. Laird, and D. Rubin (1977).
Maxi-mum likelihood from incomplete data via the EM al-gorithm.
Journal of the Royal Statistical Society,Series B, 39(1):1?38.Diab, M. and  P. Resnik, (2002).
An UnsupervisedMethod for Word Sense Tagging using Parallel Cor-pora, Proceedings of ACL, 255-262.DiMarco, C., G. Hirst, M. Stede, (1993).
"The semanticand stylistic differentiation of synonyms and near-synonyms."
In: Working notes of the AAAI SpringSymposium on Building Lexicons for Machine Trans-lation.
Stanford University.Dunning, T (1993) Accurate methods for the statisticsof surprise and coincidence, Computational Linguis-tics 19:1, 61-75.Gale, W., K. Church, and D. Yarowsky, (1992).
UsingBilingual Materials to Develop Word Sense Disam-biguation Methods.
In Proceedings, Fourth Interna-tional Conference on Theoretical andMethodological Issues in Machine Translation.Montreal, 101-112, 1992.Ide, N. and J. V?ronis (1998).
Word sense disambigua-tion: The state of the art.
Computational Linguistics,24:1, 1-40.Knight, K, and A. Luk, (1994).
Building a Large-ScaleKnowledge Base for Machine Translation, Proc.
ofthe National Conference on Artificial Intelligence(AAAI).Knight, K., I. Chander, M. Haines, V. Hatzivassiloglou,E.
Hovy, M. Iida, S. Luk, A. Okumura, R. Whitney,K.
Yamada, (1994).
"Integrating Knowledge Basesand Statistics in MT, Proc.
of the Conference of theAssociation for Machine Translation in the Americas(AMTA).Leacock, C., G. Towell, and E. Voorhees (1993).
Cor-pus-based statistical sense resolution.
Proceedings ofthe ARPA Human Language Technology Worskshop,San Francisco, Morgan Kaufman.Li, C, and H. Li (2002).
Word Translation Disambigua-tion Using Bilingual Bootstrapping, Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL), Philadelphia, July 2002,343-351.Li, Juanzi and C. Huang (1999).
A Model for WordSense Disambiguation.
In Computational Linguisticsand Chinese Language Processing,4(2), August 1999,pp.1-20McArthur, T. (1992) Longman Lexicon of Contempo-rary English, Longman Group (Far East) Ltd., HongKong.Mei, J. J., et al (1984) Tongyici Cilin, Shanghai, Com-mercial Press.
(in Chinese)Melamed, I.D.
(1997).
"A Word-to-Word Model ofTranslational Equivalence".
In Procs.
of the ACL97.pp 490-497.
Madrid Spain.Merialdo, B, (1994).
Tagging English Text with aProbabilistic Model, Computational Linguistics,20(2):155-171.Miller, G., A, R.. Beckwith, C. Fellbaum, D. Gross andK.J.
Miller.
(1990).
WordNet: An on-line lexical da-tabase.
International Journal of Lexicography, 3(4),235- 244.Proctor, P. (1988) Longman English-Chinese Dictionaryof Contemporary English, Longman Group (Far East)Ltd., Hong Kong.Towell, G. and E. Voorhees.
(1998) DisambiguatingHighly Ambiguous Words.
Computational Linguis-tics, vol.
24, no.
1, 125-146.Yarowsky, D. (1992).
Word sense disambiguation usingstatistical models of Roget's categories trained onlarge corpora.
Proceedings of the 14th InternationalConference on Computational Linguistics,COLING'92, 23-28 August, Nantes, France, 454-460.Yarowsky, D. (1995).
Unsupervised word sense disam-biguation rivaling supervised methods.
Proceedingsof the 33rd Annual Meeting of the Association forComputational Linguistics, 189-196
