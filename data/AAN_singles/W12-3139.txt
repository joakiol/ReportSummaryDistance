Proceedings of the 7th Workshop on Statistical Machine Translation, pages 317?321,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsTowards Effective Use of Training Data in Statistical Machine TranslationPhilipp Koehn and Barry HaddowUniversity of EdinburghEdinburgh, United Kingdom{pkoehn,bhaddow}@inf.ed.ac.ukAbstractWe report on findings of exploiting large datasets for translation modeling, language mod-eling and tuning for the development of com-petitive machine translation systems for eightlanguage pairs.1 IntroductionWe report on experiments carried out for the devel-opment of competitive systems on the datasets of the2012 Workshop on Statistical Machine Translation.Our main focus was directed on the effective useof all the available training data during training oftranslation and language models and tuning.We use the open source machine translation sys-tem Moses (Koehn et al, 2007) and other standardopen source tools, hence all our experiments arestraightforwardly replicable1.Compared to all single system submissions byparticipants of the workshop we achieved the bestBLEU scores for four language pairs (es-en, en-es,cs-en, en-cs), the 2nd best results for two languagepairs (fr-en, de-en), as well as a 3rd place (en-de)and a 5th place (en-fr) for the remaining pairs.
Weimproved upon this in the post-evaluation period forsome of the language pairs by more systematicallyapplying our methods.During the development of our system, we sawmost gains from using large corpora for translationmodel training, especially when using subsamplingtechniques for out-of-domain sets, using large cor-pora for language model training, and larger tuningsets.
We also observed mixed results with alternativetuning methods.
We also experimented with hierar-chical models and semi-supervised training, but didnot achieve any improvements.1Configuration files and instructions are available at http://www.statmt.org/wmt12/uedin/.LP Baseline +UNfr-en 28.2 28.4 (+.2)es-en 29.1 28.9 (?.2)en-fr 28.8 28.7 (?.1)en-es 31.0 30.9 (?.1)LP Baseline +GigaFrEnfr-en 28.7 29.1 (+.4)en-fr 29.3 30.3 (+1.0)Table 1: Gains from larger translation models: UN (about300 million English words), GigaFrEn (about 550 millionEnglish words).We report all results in case-sensitive BLEU (mt-eval13a) on the newstest2011 test set (Callison-Burch et al, 2011).
Please also note that base-line scores vary throughout the paper, since differentmethods were investigated at different time points.2 Better Translation Models2.1 Using Large Training SetsThe WMT evaluation campaign works with thelargest training sets in the field.
Our French-Englishsystems are trained on a parallel corpus with 1,072million French and 934 million English words.Training a system on this amount of data takes abouttwo weeks.The basic data sets for the language pairs are theEuroparl and NewsCommentary corpora consist ofabout 50 million words and 3 million words, respec-tively.
These corpora are quite close to the targetdomain of news reports, and give quite good results.Table 1 shows the gains from using the much largerUN (about 300 million words) and GigaFrEn cor-pora (about 550 million words).From these results, it is not clear if the UN is help-ful, but the GigaFrEn corpus gives large gains (+0.4BLEU and +1.0 BLEU).317Model 1 Moore-LewisLP Base- Before After Before Afterline 10% 50% 10% 50% 10% 50% 10% 50%fr-en 29.3 28.5(?.8) 29.1(?.2) 28.6(?.7) 28.9(?.4) 29.1(?.2) 29.6(+.3) 29.1(?.2) 29.4(+.1)en-fr 30.1 29.1(?1.0) 30.1(?.0) 29.3(?.8) 29.8(?.3) 29.9(?.2) 30.2(+.1) 29.9(?.2) 30.1(?.0)es-en 29.0 28.9(?.1) 29.0(?.0) 29.0(?.0) 29.0(?.0) 29.0(?.0) 29.1(+.1) 29.4(+.4) 29.2(+.2)en-es 30.9 30.9(?.0) 31.0(+.1) 30.8(?.1) 30.7(?.2) 31.4(+.5) 31.5(+.6) 31.5(+.6) 31.3(+.4)Table 2: Subsampling UN and GigaFrEn corpora using Model 1 and Moore-Lewis filtering, before and after wordalignment2.2 SubsamplingWe experimented with two different types of sub-sampling techniques ?
Model 1, similar to that usedby Schwenk et al (2011), and modified Moore-Lewis (Axelrod et al, 2011) ?
for the language pairses-en, en-es, fr-en and en-fr.
In each case the ideawas to include the NewsCommentary and Europarlcorpora in their entirety, and to score the sentencesin the remaining corpora (the selection corpus) usingone of the two measures, adding either the top 10%or top 50% of the selection corpus to the trainingdata.For Model 1 filtering, we trained IBM Model 1on Europarl and NewsCommentary concatenated, inboth directions, and scored the sentences in the se-lection corpus using the length-normalised sum ofthe IBM Model scores.
For the modified Moore-Lewis filtering, we trained two 5-gram languagemodels for source and target, the first on 5M sen-tences from the news2011 monolingual data, andthe second on 5M words from the selection corpus,using the same vocabulary.
The modified Moore-Lewis score for a sentence is the sum of the sourceand target?s perplexity difference for the two lan-guage models.For the Spanish experiments, the selection corpuswas the UN data, whilst for the French experimentsit was the UN data and the GigaFrEn data, concate-nated and with duplicates removed.The results of the subsampling are shown in Ta-ble 2, where the BLEU scores are averaged over2 tuning runs.
The conclusion was that modifiedMoore-Lewis subsampling was effective (and wasused in our final submissions), but Model 1 sam-pling made no difference for the Spanish systems,and was harmful for the French systems.3 Better Language ModelsIn previous years, we were not able to make useof the monolingual LDC Gigaword corpora due tolack of sufficiently powerful computing resources.These corpora exist for English (4.3 billion words),Spanish (1.1 billion words), and French (0.8 billionwords).
With the acquisition of large memory ma-chines2, we were now able to train language modelson this data.
Use of these large language models dur-ing decoding is aided by more efficient storage andinference (Heafield, 2011).Still, even with that much RAM it is not possi-ble to train a language model with SRILM (Stolke,2002) in one pass.
Hence, we broke up the train-ing corpus by source (New York Times, WashingtonPost, ...) and trained separate language model foreach.
The largest individual corpus was the EnglishNew York Times portion which consists of 1.5 billionwords and took close to 100GB of RAM.
We alsotrained individual language models for each year ofWMT12?s monolingual corpus.We interpolated the language models using theSRILM toolkit.
The toolkit has a limit of 10 lan-guage models to be merged at once, so we had to in-terpolate sub-groups of some of the language models(the WMT12 monolingual news models) first.
It isnot clear if this is harmful, but building separate lan-guage model for each source and year and interpo-late those many more models did hurt significantly.Table 3 shows that we gain around half a BLEUpoint into Spanish and French, as well as German?English, and around one and a half BLEU points forthe other language pairs into English.2Dell Poweredge R710, equipped with two 6-core IntelXeon X5660 CPUs running at 2.8GHz, with each core able torun two threads (24 threads total), six 3TB disks and 144GBRAM, and cost ?6000.318LP Baseline +LDC Gigade-en 21.9 22.4 (+.5)cs-en 24.2 25.6 (+1.4)fr-en 29.1 31.0 (+1.9)es-en 29.1 30.7 (+1.6)en-es 31.5 31.8 (+.3)en-fr 30.3 30.8 (+.5)Table 3: Using the LDC Gigaword corpora to train largerlanguage models.LP Baseline Big-Tunede-en 21.4 21.6 (+.2)fr-en 28.4 28.7 (+.3)es-en 28.9 29.0 (+.1)cs-en 23.9 24.1 (+.2)en-de 15.8 15.9 (+.1)en-fr 28.7 29.2 (+.5)en-es 30.9 31.2 (+.2)en-cs 17.2 17.4 (+.2)Table 4: Using a larger tuning set (7567 sentences) bycombining newstest 2008 to 2010.4 Better Tuning4.1 Bigger Tuning SetsIn recent experiments, mainly geared towards usingmuch larger feature sets, we learned that larger tun-ing sets may give better and more stable results.
Wetested this hypothesis here as well.By concatenating the sets from three years (2008-2010), we constructed a tuning set of 7567 sentencesper language.
Table 4 shows that we gain on averageabout +0.2 BLEU points.4.2 Pairwise Ranked OptimizationWe recently added an implementation of the pair-wise ranked optimization (PRO) tuning method(Hopkins and May, 2011) to Moses as an alterna-tive to Och?s (2003) minimum error rate training(MERT).
We checked if this method gives us betterresults.
Table 5 shows a mixed picture.
PRO givesslightly shorter translations, probably because it op-timises sentence rather than corpus BLEU, which hasa noticeable effect on the BLEU score.
For 2 lan-guage pairs we see better results, for 4 worse, andfor 1 there is no difference.
On other data and lan-LP MERT PRO PRO-MERTde-en 21.7 (1.01) 21.9 (1.00) +.2 21.7 (1.01) ?.0es-en 29.1 (1.02) 29.1 (1.01) ?.0 29.1 (1.02) ?.0cs-en 24.2 (1.03) 24.5 (1.00) +.3 24.2 (1.03) ?.0en-de 16.0 (1.00) 15.7 (0.96) ?.3 16.0 (1.00) ?.0en-fr 29.3 (0.98) 28.9 (0.96) ?.4 29.3 (0.98) ?.0en-es 31.5 (0.98) 31.3 (0.97) ?.2 31.4 (0.98) ?.1en-cs 17.4 (0.97) 16.9 (0.92) ?.5 17.3 (0.97) ?.1Table 5: Replacing the line search method of MERT withpairwise ranked optimization (PRO).guage conditions we have observed better and morestable results with PRO.We tried to use PRO to generate starting points forMERT optimization.
Theoretically this will lead tobetter optimization on the tuning set, since MERToptimization steps on PRO weights will never leadto worse results on the sampled n-best lists.
Thismethod (PRO-MERT in the table) applied here,however, did not lead to significantly different re-sults than plain MERT.5 What did not WorkNot everything we tried worked out.
Notably, twopromising directions ?
hierarchical models andsemi-supervised learning ?
did not yield any im-provements.
It is not clear if we failed or if themethods failed, but we will investigate this furtherin future work.5.1 Hierarchical ModelsHierarchical models (Chiang, 2007) have been sup-ported already for a few years by Moses, and theygive significantly better performance for Chinese?English over phrase-based models.
While we havenot yet seen benefits for many other language pairs,the eight language pairs of WMT12 allowed us tocompare these two models more extensively, alsoin view of recent enhancements resulting in bettersearch accuracy.Since hierarchical models are much larger(roughly 10 times bigger), we trained hierarchicalmodels on downsized training data for most lan-guage pairs.
For Spanish and French, this ex-cludes UN and GigaFrEn; for Czech some partsof the CzEng corpus were excluded based on theirlower language model interpolation weights relative319LP Phrase Downsized Hierarchicalde-en 21.6 same 21.4 (?.2)fr-en 28.7 27.9 27.6 (?.3)es-en 29.0 28.9 28.4 (?.5)cs-en 24.1 22.4 22.0 (?.4)en-de 15.9 same 15.5 (?.4)en-fr 29.2 28.8 28.0 (?.8)en-es 31.2 30.8 30.4 (?.4)en-cs 17.4 16.2 15.6 (?.6)Table 6: Hierarchical phrase models vs. baseline phrase-based models.to their size.Table 6 shows inferior performance for all lan-guage pairs (by about half a BLEU point), althoughresults for German?English are close (?0.2 BLEU).5.2 Semi-Supervised LearningOther research groups have reported improvementsusing semi-supervised learning methods to cre-ate synthetic parallel data from monolingual data(Schwenk et al, 2008; Abdul-Rauf and Schwenk,2009; Bertoldi and Federico, 2009; Lambert et al,2011).
The idea is to translate in-domain monolin-gual data with a baseline system and filter the resultfor use as an additional parallel corpus.Table 7 shows out results when trying to emulatethe approach of Lambert et al (2011).
We translatethe some of the 2011 monolingual news data (139million words for French and 100 million words forEnglish) from the target language into the sourcelanguage with a baseline system trained on Europarland News Commentary.
Adding all the obtaineddata hurts (except for minimal improvements overa small French-English system).
When we filteredout half of the sentences based on translation scores,results were even worse.AcknowledgmentsThis work was supported by the EuroMatrixPlusproject funded by the European Commission (7thFramework Programme).ReferencesAbdul-Rauf, S. and Schwenk, H. (2009).
On theuse of comparable corpora to improve SMT per-Setup Baseline +synthetic +syn-halffr-en ep+nc 28.0 28.1 (+.1) 28.0 (?.0)+un 28.7 28.6 (?.1) 28.5 (?.2)en-fr ep+nc 28.8 28.2 (?.6) 28.1 (?.7)+un 29.3 28.9 (?.4) 28.9 (?.4)Table 7: Using semi-supervised methods to add syntheticparallel data to a baseline system trained on Europarl(ep)m News Commentary (nc) and United Nations (un).We added all generated data (synthetic) or filtered out halfbased on model scores (syn-half).formance.
In Proceedings of the 12th Confer-ence of the European Chapter of the ACL (EACL2009), pages 16?23, Athens, Greece.
Associationfor Computational Linguistics.Axelrod, A., He, X., and Gao, J.
(2011).
Domainadaptation via pseudo in-domain data selection.In Proceedings of the 2011 Conference on Em-pirical Methods in Natural Language Processing,pages 355?362, Edinburgh, Scotland, UK.
Asso-ciation for Computational Linguistics.Bertoldi, N. and Federico, M. (2009).
Domainadaptation for statistical machine translation withmonolingual resources.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, pages 182?189, Athens, Greece.
Associationfor Computational Linguistics.Callison-Burch, C., Koehn, P., Monz, C., andZaidan, O.
(2011).
Findings of the 2011 work-shop on statistical machine translation.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 22?64, Edinburgh, Scot-land.
Association for Computational Linguistics.Chiang, D. (2007).
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2).Heafield, K. (2011).
Kenlm: Faster and smallerlanguage model queries.
In Proceedings of theSixth Workshop on Statistical Machine Transla-tion, pages 187?197, Edinburgh, Scotland.
Asso-ciation for Computational Linguistics.Hopkins, M. and May, J.
(2011).
Tuning as ranking.In Proceedings of the 2011 Conference on Em-pirical Methods in Natural Language Processing,pages 1352?1362, Edinburgh, Scotland, UK.
As-sociation for Computational Linguistics.320Koehn, P., Hoang, H., Birch, A., Callison-Burch,C., Federico, M., Bertoldi, N., Cowan, B., Shen,W., Moran, C., Zens, R., Dyer, C. J., Bojar, O.,Constantin, A., and Herbst, E. (2007).
Moses:Open source toolkit for statistical machine trans-lation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics Companion Volume Proceedings of the Demoand Poster Sessions, pages 177?180, Prague,Czech Republic.
Association for ComputationalLinguistics.Lambert, P., Schwenk, H., Servan, C., and Abdul-Rauf, S. (2011).
Investigations on translationmodel adaptation using monolingual data.
InProceedings of the Sixth Workshop on StatisticalMachine Translation, pages 284?293, Edinburgh,Scotland.
Association for Computational Linguis-tics.Och, F. J.
(2003).
Minimum error rate training forstatistical machine translation.
In Proceedingsof the 41st Annual Meeting of the Association ofComputational Linguistics (ACL).Schwenk, H., Este`ve, Y., and Rauf, S. A.
(2008).The LIUM Arabic/English statistical machinetranslation system for IWSLT 2008.
In Proceed-ings of the International Workshop on SpokenLanguage Translation (IWSLT), pages 63?68.Schwenk, H., Lambert, P., Barrault, L., Servan, C.,Abdul-Rauf, S., Afli, H., and Shah, K. (2011).Lium?s smt machine translation systems for wmt2011.
In Proceedings of the Sixth Workshop onStatistical Machine Translation, pages 464?469,Edinburgh, Scotland.
Association for Computa-tional Linguistics.Stolke, A.
(2002).
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the Interna-tional Conference on Spoken Language Process-ing.321
