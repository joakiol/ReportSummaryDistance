Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 76?85,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsModeling Coverage for Neural Machine TranslationZhaopeng Tu?Zhengdong Lu?Yang Liu?Xiaohua Liu?Hang Li?
?Noah?s Ark Lab, Huawei Technologies, Hong Kong{tu.zhaopeng,lu.zhengdong,liuxiaohua3,hangli.hl}@huawei.com?Department of Computer Science and Technology, Tsinghua University, Beijingliuyang2011@tsinghua.edu.cnAbstractAttention mechanism has enhanced state-of-the-art Neural Machine Translation(NMT) by jointly learning to align andtranslate.
It tends to ignore past alignmentinformation, however, which often leadsto over-translation and under-translation.To address this problem, we proposecoverage-based NMT in this paper.
Wemaintain a coverage vector to keep trackof the attention history.
The coverage vec-tor is fed to the attention model to help ad-just future attention, which lets NMT sys-tem to consider more about untranslatedsource words.
Experiments show thatthe proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-basedNMT.11 IntroductionThe past several years have witnessed the rapidprogress of end-to-end Neural Machine Transla-tion (NMT) (Sutskever et al, 2014; Bahdanau etal., 2015).
Unlike conventional Statistical Ma-chine Translation (SMT) (Koehn et al, 2003; Chi-ang, 2007), NMT uses a single and large neuralnetwork to model the entire translation process.
Itenjoys the following advantages.
First, the use ofdistributed representations of words can alleviatethe curse of dimensionality (Bengio et al, 2003).Second, there is no need to explicitly design fea-tures to capture translation regularities, which isquite difficult in SMT.
Instead, NMT is capable oflearning representations directly from the trainingdata.
Third, Long Short-Term Memory (Hochre-iter and Schmidhuber, 1997) enables NMT to cap-1Our code is publicly available at https://github.com/tuzhaopeng/NMT-Coverage.ture long-distance reordering, which is a signifi-cant challenge in SMT.NMT has a serious problem, however, namelylack of coverage.
In phrase-based SMT (Koehnet al, 2003), a decoder maintains a coverage vec-tor to indicate whether a source word is translatedor not.
This is important for ensuring that eachsource word is translated in decoding.
The decod-ing process is completed when all source wordsare ?covered?
or translated.
In NMT, there is nosuch coverage vector and the decoding processends only when the end-of-sentence mark is pro-duced.
We believe that lacking coverage mightresult in the following problems in conventionalNMT:1.
Over-translation: some words are unneces-sarily translated for multiple times;2.
Under-translation: some words are mistak-enly untranslated.Specifically, in the state-of-the-art attention-basedNMT model (Bahdanau et al, 2015), generating atarget word heavily depends on the relevant partsof the source sentence, and a source word is in-volved in generation of all target words.
As aresult, over-translation and under-translation in-evitably happen because of ignoring the ?cover-age?
of source words (i.e., number of times asource word is translated to a target word).
Fig-ure 1(a) shows an example: the Chinese word?gu?anb`??
is over translated to ?close(d)?
twice,while ?b`eip`o?
(means ?be forced to?)
is mistak-enly untranslated.In this work, we propose a coverage mechanismto NMT (NMT-COVERAGE) to alleviate the over-translation and under-translation problems.
Basi-cally, we append a coverage vector to the inter-mediate representations of an NMT model, whichare sequentially updated after each attentive read76(a) Over-translation and under-translationgenerated by NMT.
(b) Coverage model alleviates the problems ofover-translation and under-translation.Figure 1: Example translations of (a) NMT without coverage, and (b) NMT with coverage.
In conven-tional NMT without coverage, the Chinese word ?gu?anb`??
is over translated to ?close(d)?
twice, while?b`eip`o?
(means ?be forced to?)
is mistakenly untranslated.
Coverage model alleviates these problems bytracking the ?coverage?
of source words.during the decoding process, to keep track of theattention history.
The coverage vector, when en-tering into attention model, can help adjust the fu-ture attention and significantly improve the over-all alignment between the source and target sen-tences.
This design contains many particular casesfor coverage modeling with contrasting character-istics, which all share a clear linguistic intuitionand yet can be trained in a data driven fashion.
No-tably, we achieve significant improvement even bysimply using the sum of previous alignment prob-abilities as coverage for each word, as a success-ful example of incorporating linguistic knowledgeinto neural network based NLP models.Experiments show that NMT-COVERAGE sig-nificantly outperforms conventional attention-based NMT on both translation and alignmenttasks.
Figure 1(b) shows an example, in whichNMT-COVERAGE alleviates the over-translationand under-translation problems that NMT withoutcoverage suffers from.2 BackgroundOur work is built on attention-based NMT (Bah-danau et al, 2015), which simultaneously con-ducts dynamic alignment and generation of thetarget sentence, as illustrated in Figure 2.
ItFigure 2: Architecture of attention-based NMT.Whenever possible, we omit the source index j tomake the illustration less cluttered.produces the translation by generating one targetword yiat each time step.
Given an input sentencex = {x1, .
.
.
, xJ} and previously generated words{y1, .
.
.
, yi?1}, the probability of generating nextword yiisP (yi|y<i,x) = softmax(g(yi?1, ti, si))(1)where g is a non-linear function, and tiis a decod-ing state for time step i, computed byti= f(ti?1, yi?1, si) (2)Here the activation function f(?)
is a Gated Re-current Unit (GRU) (Cho et al, 2014b), and siis77a distinct source representation for time i, calcu-lated as a weighted sum of the source annotations:si=J?j=1?i,j?
hj(3)where hj= [??h>j;?
?h>j]>is the annotation ofxjfrom a bi-directional Recurrent Neural Net-work (RNN) (Schuster and Paliwal, 1997), and itsweight ?i,jis computed by?i,j=exp(ei,j)?Jk=1exp(ei,k)(4)andei,j= a(ti?1,hj)= v>atanh(Wati?1+ Uahj) (5)is an attention model that scores how well yiandhjmatch.
With the attention model, it avoids theneed to represent the entire source sentence witha single vector.
Instead, the decoder selects partsof the source sentence to pay attention to, thusexploits an expected annotation siover possiblealignments ?i,jfor each time step i.However, the attention model fails to take ad-vantage of past alignment information, which isfound useful to avoid over-translation and under-translation problems in conventional SMT (Koehnet al, 2003).
For example, if a source word istranslated in the past, it is less likely to be trans-lated again and should be assigned a lower align-ment probability.3 Coverage Model for NMTIn SMT, a coverage set is maintained to keep trackof which source words have been translated (?cov-ered?)
in the past.
Let us take x = {x1, x2, x3, x4}as an example of input sentence.
The initial cov-erage set is C = {0, 0, 0, 0} which denotes thatno source word is yet translated.
When a trans-lation rule bp = (x2x3, ymym+1) is applied, weproduce one hypothesis labelled with coverageC = {0, 1, 1, 0}.
It means that the second and thirdsource words are translated.
The goal is to gener-ate translation with full coverage C = {1, 1, 1, 1}.A source word is translated when it is covered byone translation rule, and it is not allowed to betranslated again in the future (i.e., hard coverage).In this way, each source word is guaranteed to betranslated and only be translated once.
As shown,Figure 3: Architecture of coverage-based attentionmodel.
A coverage vector Ci?1is maintained tokeep track of which source words have been trans-lated before time i. Alignment decisions ?iaremade jointly taking into account past alignmentinformation embedded in Ci?1, which lets the at-tention model to consider more about untranslatedsource words.coverage is essential for SMT since it avoids gapsand overlaps in translation of source words.Modeling coverage is also important forattention-based NMT models, since they gener-ally lack a mechanism to indicate whether a cer-tain source word has been translated, and there-fore are prone to the ?coverage?
mistakes: someparts of source sentence have been translated morethan once or not translated.
For NMT models, di-rectly modeling coverage is less straightforward,but the problem can be significantly alleviated bykeeping track of the attention signal during the de-coding process.
The most natural way for doingthat would be to append a coverage vector to theannotation of each source word (i.e., hj), whichis initialized as a zero vector but updated after ev-ery attentive read of the corresponding annotation.The coverage vector is fed to the attention modelto help adjust future attention, which lets NMTsystem to consider more about untranslated sourcewords, as illustrated in Figure 3.3.1 Coverage ModelSince the coverage vector summarizes the atten-tion record for hj(and therefore for a small neigh-bor centering at the jthsource word), it willdiscourage further attention to it if it has beenheavily attended, and implicitly push the atten-tion to the less attended segments of the sourcesentence since the attention weights are normal-ized to one.
This can potentially solve both cover-age mistakes mentioned above, when modeled andlearned properly.78Formally, the coverage model is given byCi,j= gupdate(Ci?1,j, ?i,j,?(hj),?)(6)where?
gupdate(?)
is the function that updates Ci,jaf-ter the new attention ?i,jat time step i in thedecoding process;?
Ci,jis a d-dimensional coverage vector sum-marizing the history of attention till time stepi on hj;?
?
(hj) is a word-specific feature with its ownparameters;?
?
are auxiliary inputs exploited in differentsorts of coverage models.Equation 6 gives a rather general model, whichcould take different function forms for gupdate(?
)and ?(?
), and different auxiliary inputs ?
(e.g.,previous decoding state ti?1).
In the rest of thissection, we will give a number of representativeimplementations of the coverage model, whicheither leverage more linguistic information (Sec-tion 3.1.1) or resort to the flexibility of neural net-work approximation (Section 3.1.2).3.1.1 Linguistic Coverage ModelWe first consider at linguistically inspired modelwhich has a small number of parameters, as wellas clear interpretation.
While the linguistically-inspired coverage in NMT is similar to that inSMT, there is one key difference: it indicates whatpercentage of source words have been translated(i.e., soft coverage).
In NMT, each target word yiis generated from all source words with probabil-ity ?i,jfor source word xj.
In other words, thesource word xjis involved in generating all tar-get words and the probability of generating targetword yiat time step i is ?i,j.
Note that unlikein SMT in which each source word is fully trans-lated at one decoding step, the source word xjispartially translated at each decoding step in NMT.Therefore, the coverage at time step i denotes thetranslated ratio of that each source word is trans-lated.We use a scalar (d = 1) to represent linguis-tic coverage for each source word and employan accumulate operation for gupdate.
The initialvalue of linguistic coverage is zero, which de-notes that the corresponding source word is nottranslated yet.
We iteratively construct linguis-tic coverages through accumulation of alignmentprobabilities generated by the attention model,each of which is normalized by a distinct context-dependent weight.
The coverage of source wordxjat time step i is computed byCi,j= Ci?1,j+1?j?i,j=1?ji?k=1?k,j(7)where ?jis a pre-defined weight which indicatesthe number of target words xjis expected to gener-ate.
The simplest way is to follow Xu et al (2015)in image-to-caption translation to fix ?
= 1 for allsource words, which means that we directly usethe sum of previous alignment probabilities with-out normalization as coverage for each word, asdone in (Cohn et al, 2016).However, in machine translation, different typesof source words may contribute differently to thegeneration of target sentence.
Let us take thesentence pairs in Figure 1 as an example.
Thenoun in the source sentence ?j??ch?ang?
is translatedinto one target word ?airports?, while the adjec-tive ?b`eip`o?
is translated into three words ?wereforced to?.
Therefore, we need to assign a dis-tinct ?jfor each source word.
Ideally, we expect?j=?Ii=1?i,jwith I being the total numberof time steps in decoding.
However, such desiredvalue is not available before decoding, thus is notsuitable in this scenario.Fertility To predict ?j, we introduce the con-cept of fertility, which is firstly proposed in word-level SMT (Brown et al, 1993).
Fertility of sourceword xjtells how many target words xjproduces.In SMT, the fertility is a random variable ?j,whose distribution p(?j= ?)
is determined bythe parameters of word alignment models (e.g.,IBM models).
In this work, we simplify and adaptfertility from the original model and compute thefertility ?jby2?j= N (xj|x) = N ?
?
(Ufhj) (8)where N ?
R is a predefined constant to denotethe maximum number of target words one source2Fertility in SMT is a random variable with a set of fer-tility probabilities, n(?j|xj) = p(?<j,x), which dependson the fertilities of previous source words.
To simplify thecalculation and adapt it to the attention model in NMT, wedefine the fertility in NMT as a constant number, which isindependent of previous fertilities.79Figure 4: NN-based coverage model.word can produce, ?(?)
is a logistic sigmoid func-tion, and Uf?
R1?2nis the weight matrix.
Herewe use hjto denote (xj|x) since hjcontains in-formation about the whole input sentence with astrong focus on the parts surrounding xj(Bah-danau et al, 2015).
Since ?jdoes not depend oni, we can pre-compute it before decoding to mini-mize the computational cost.3.1.2 Neural Network Based Coverage ModelWe next consider Neural Network (NN) basedcoverage model.
When Ci,jis a vector (d > 1) andgupdate(?)
is a neural network, we actually havean RNN model for coverage, as illustrated in Fig-ure 4.
In this work, we take the following form:Ci,j= f(Ci?1,j, ?i,j,hj, ti?1)where f(?)
is a nonlinear activation function andti?1is the auxiliary input that encodes past trans-lation information.
Note that we leave out theword-specific feature function ?(?)
and only takethe input annotation hjas the input to the cov-erage RNN.
It is important to emphasize that theNN-based coverage model is able to be fed witharbitrary inputs, such as the previous attentionalcontext si?1.
Here we only employ Ci?1,jfor pastalignment information, ti?1for past translation in-formation, and hjfor word-specific bias.3Gating The neural function f(?)
can be either asimple activation function tanh or a gating func-tion that proves useful to capture long-distance3In our preliminary experiments, considering more inputs(e.g., current and previous attentional contexts, unnormal-ized attention weights ei,j) does not always lead to bettertranslation quality.
Possible reasons include: 1) the inputscontains duplicate information, and 2) more inputs introducemore back-propagation paths and therefore make it difficultto train.
In our experience, one principle is to only feedthe coverage model inputs that contain distinct information,which are complementary to each other.dependencies.
In this work, we adopt GRU forthe gating activation since it is simple yet power-ful (Chung et al, 2014).
Please refer to (Cho et al,2014b) for more details about GRU.Discussion Intuitively, the two types of modelssummarize coverage information in ?different lan-guages?.
Linguistic models summarize coverageinformation in human language, which has a clearinterpretation to humans.
Neural models encodecoverage information in ?neural language?, whichcan be ?understood?
by neural networks and letthem to decide how to make use of the encodedcoverage information.3.2 Integrating Coverage into NMTAlthough attention based model has the capabil-ity of jointly making alignment and translation, itdoes not take into consideration translation his-tory.
Specifically, a source word that has sig-nificantly contributed to the generation of targetwords in the past, should be assigned lower align-ment probabilities, which may not be the case inattention based NMT.
To address this problem, wepropose to calculate the alignment probabilities byincorporating past alignment information embed-ded in the coverage model.Intuitively, at each time step i in the decodingphase, coverage from time step (i ?
1) serves asan additional input to the attention model, whichprovides complementary information of that howlikely the source words are translated in the past.We expect the coverage information would guidethe attention model to focus more on untranslatedsource words (i.e., assign higher alignment prob-abilities).
In practice, we find that the coveragemodel does fulfill the expectation (see Section 5).The translated ratios of source words from lin-guistic coverages negatively correlate to the cor-responding alignment probabilities.More formally, we rewrite the attention modelin Equation 5 asei,j= a(ti?1,hj, Ci?1,j)= v>atanh(Wati?1+ Uahj+ VaCi?1,j)where Ci?1,jis the coverage of source word xjbe-fore time i. Va?
Rn?dis the weight matrix forcoverage with n and d being the numbers of hid-den units and coverage units, respectively.804 TrainingWe take end-to-end learning for the NMT-COVERAGE model, which learns not only the pa-rameters for the ?original?
NMT (i.e., ?
for encod-ing RNN, decoding RNN, and attention model)but also the parameters for coverage modeling(i.e., ?
for annotation and guidance of attention) .More specifically, we choose to maximize the like-lihood of reference sentences as most other NMTmodels (see, however (Shen et al, 2016)):(?
?, ??)
= arg max?,?N?n=1logP (yn|xn; ?, ?)
(9)No auxiliary objective For the coverage modelwith a clearer linguistic interpretation (Section3.1.1), it is possible to inject an auxiliary objec-tive function on some intermediate representation.More specifically, we may have the following ob-jective:(?
?, ??)
= arg max?,?N?n=1{logP (yn|xn; ?, ?)?
?
{J?j=1(?j?I?i=1?i,j)2; ?
}}where the term{?Jj=1(?j?
?Ii=1?i,j)2; ?
}pe-nalizes the discrepancy between the sum of align-ment probabilities and the expected fertility forlinguistic coverage.
This is similar to the moreexplicit training for fertility as in Xu et al (2015),which encourages the model to pay equal attentionto every part of the image (i.e., ?j= 1).
However,our empirical study shows that the combined ob-jective consistently worsens the translation qualitywhile slightly improves the alignment quality.Our training strategy poses less constraints onthe dependency between ?jand the attention thana more explicit strategy taken in (Xu et al, 2015).We let the objective associated with the transla-tion quality (i.e., the likelihood) to drive the train-ing, as in Equation 9.
This strategy is arguablyadvantageous, since the attention weight on a hid-den state hjcannot be interpreted as the propor-tion of the corresponding word being translated inthe target sentence.
For one thing, the hidden statehj, after the transformation from encoding RNN,bears the contextual information from other partsof the source sentence, and thus loses the rigid cor-respondence with the corresponding word.
There-fore, penalizing the discrepancy between the sumof alignment probabilities and the expected fertil-ity does not hold in this scenario.5 Experiments5.1 SetupWe carry out experiments on a Chinese-Englishtranslation task.
Our training data for the trans-lation task consists of 1.25M sentence pairs ex-tracted from LDC corpora4, with 27.9M Chinesewords and 34.5M English words respectively.
Wechoose NIST 2002 dataset as our development set,and the NIST 2005, 2006 and 2008 datasets as ourtest sets.
We carry out experiments of the align-ment task on the evaluation dataset from (Liu andSun, 2015), which contains 900 manually alignedChinese-English sentence pairs.
We use the case-insensitive 4-gram NIST BLEU score (Papineni etal., 2002) for the translation task, and the align-ment error rate (AER) (Och and Ney, 2003) forthe alignment task.
To better estimate the qual-ity of the soft alignment probabilities generatedby NMT, we propose a variant of AER, namingSAER:SAER = 1?|MA?MS|+ |MA?MP||MA|+ |MS|where A is a candidate alignment, and S and Pare the sets of sure and possible links in the ref-erence alignment respectively (S ?
P ).
M de-notes alignment matrix, and for both MSand MPwe assign the elements that correspond to the ex-isting links in S and P with probabilities 1 whileassign the other elements with probabilities 0.
Inthis way, we are able to better evaluate the qualityof the soft alignments produced by attention-basedNMT.
We use sign-test (Collins et al, 2005) forstatistical significance test.For efficient training of the neural networks, welimit the source and target vocabularies to the mostfrequent 30K words in Chinese and English, cov-ering approximately 97.7% and 99.3% of the twocorpora respectively.
All the out-of-vocabularywords are mapped to a special token UNK.
We setN = 2 for the fertility model in the linguistic cov-erages.
We train each model with the sentencesof length up to 80 words in the training data.
Theword embedding dimension is 620 and the size ofa hidden layer is 1000.
All the other settings arethe same as in (Bahdanau et al, 2015).4The corpora include LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07,LDC2004T08 and LDC2005T06.81# System #Params MT05 MT06 MT08 Avg.1 Moses ?
31.37 30.85 23.01 28.412 GroundHog 84.3M 30.61 31.12 23.23 28.323 + Linguistic coverage w/o fertility +1K 31.26?32.16??24.84?
?29.424 + Linguistic coverage w/ fertility +3K 32.36??32.31??24.91?
?29.865 + NN-based coverage w/o gating (d = 1) +4K 31.94??32.11?
?23.31 29.126 + NN-based coverage w/ gating (d = 1) +10K 31.94??32.16??24.67?
?29.597 + NN-based coverage w/ gating (d = 10) +100K 32.73??32.47??25.23?
?30.14Table 1: Evaluation of translation quality.
d denotes the dimension of NN-based coverages, and ?
and ?indicate statistically significant difference (p < 0.01) from GroundHog and Moses, respectively.
?+?
ison top of the baseline system GroundHog.We compare our method with two state-of-the-art models of SMT and NMT5:?
Moses (Koehn et al, 2007): an open sourcephrase-based translation system with defaultconfiguration and a 4-gram language modeltrained on the target portion of training data.?
GroundHog (Bahdanau et al, 2015): anattention-based NMT system.5.2 Translation QualityTable 1 shows the translation performances mea-sured in BLEU score.
Clearly the proposed NMT-COVERAGE significantly improves the translationquality in all cases, although there are still consid-erable differences among different variants.Parameters Coverage model introduces few pa-rameters.
The baseline model (i.e., GroundHog)has 84.3M parameters.
The linguistic coverageusing fertility introduces 3K parameters (2K forfertility model), and the NN-based coverage withgating introduces 10K?d parameters (6K?d forgating), where d is the dimension of the coveragevector.
In this work, the most complex coveragemodel only introduces 0.1M additional parame-ters, which is quite small compared to the numberof parameters in the existing model (i.e., 84.3M).Speed Introducing the coverage model slowsdown the training speed, but not significantly.When running on a single GPU device Tesla K80,the speed of the baseline model is 960 target wordsper second.
System 4 (?+Linguistic coverage withfertility?)
has a speed of 870 words per second,while System 7 (?+NN-based coverage (d=10)?
)achieves a speed of 800 words per second.5There are recent progress on aggregating multiple mod-els or enlarging the vocabulary(e.g., in (Jean et al, 2015)),but here we focus on the generic models.Linguistic Coverages (Rows 3 and 4): Twoobservations can be made.
First, the simplestlinguistic coverage (Row 3) already significantlyimproves translation performance by 1.1 BLEUpoints, indicating that coverage information isvery important to the attention model.
Second, in-corporating fertility model boosts the performanceby better estimating the covered ratios of sourcewords.NN-based Coverages (Rows 5-7): (1) Gating(Rows 5 and 6): Both variants of NN-based cover-ages outperform GroundHog with averaged gainsof 0.8 and 1.3 BLEU points, respectively.
In-troducing gating activation function improves theperformance of coverage models, which is consis-tent with the results in other tasks (Chung et al,2014).
(2) Coverage dimensions (Rows 6 and 7):Increasing the dimension of coverage models fur-ther improves the translation performance by 0.6point in BLEU score, at the cost of introducingmore parameters (e.g., from 10K to 100K).65.3 Alignment QualityTable 2 lists the alignment performances.
Wefind that coverage information improves atten-tion model as expected by maintaining an annota-tion summarizing attention history on each sourceword.
More specifically, linguistic coverage withfertility significantly reduces alignment errors un-der both metrics, in which fertility plays an impor-tant role.
NN-based coverages, however, does notsignificantly reduce alignment errors until increas-ing the coverage dimension from 1 to 10.
It in-dicates that NN-based models need slightly more6In a pilot study, further increasing the coverage dimen-sion only slightly improved the translation performance.
Onepossible reason is that encoding the relatively simple cover-age information does not require too many dimensions.82(a) Groundhog (b) + NN cov.
w/ gating (d = 10)Figure 5: Example alignments.
Using coverage mechanism, translated source words are less likely tocontribute to generation of the target words next (e.g., top-right corner for the first four Chinese words.
).System SAER AERGroundHog 67.00 54.67+ Ling.
cov.
w/o fertility 66.75 53.55+ Ling.
cov.
w/ fertility 64.85 52.13+ NN cov.
w/o gating (d = 1) 67.10 54.46+ NN cov.
w/ gating (d = 1) 66.30 53.51+ NN cov.
w/ gating (d = 10) 64.25 50.50Table 2: Evaluation of alignment quality.
Thelower the score, the better the alignment quality.dimensions to encode the coverage information.Figure 5 shows an example.
The coveragemechanism does meet the expectation: the align-ments are more concentrated and most impor-tantly, translated source words are less likely toget involved in generation of the target words next.For example, the first four Chinese words are as-signed lower alignment probabilities (i.e., darkercolor) after the corresponding translation ?roma-nia reinforces old buildings?
is produced.5.4 Effects on Long SentencesFollowing Bahdanau et al (2015), we group sen-tences of similar lengths together and computeBLEU score and averaged length of translationfor each group, as shown in Figure 6.
Cho etal.
(2014a) show that the performance of Ground-hog drops rapidly when the length of input sen-tence increases.
Our results confirm these find-ings.
One main reason is that Groundhog pro-duces much shorter translations on longer sen-tences (e.g., > 40, see right panel in Figure 6),and thus faces a serious under-translation prob-lem.
NMT-COVERAGE alleviates this problem byincorporating coverage information into the atten-tion model, which in general pushes the attentionto untranslated parts of the source sentence andimplicitly discourages early stop of decoding.
Itis worthy to emphasize that both NN-based cov-erages (with gating, d = 10) and linguistic cover-ages (with fertility) achieve similar performanceson long sentences, reconfirming our claim that thetwo variants improve the attention model in theirown ways.As an example, consider this source sentence inthe test set:qi?aod?an b?en s`aij`?
p?
?ngj?un d?ef?en 24.3f?en, t?a z`ai s?an zh?ou qi?an ji?esh`ou sh?oush`u, qi?udu`?
z`ai c??
q?
?ji?an 4 sh`eng 8 f`u .Groundhog translates this sentence into:jordan achieved an average score ofeight weeks ahead with a surgical oper-ation three weeks ago .in which the sub-sentence ?, qi?udu`?
z`ai c??
q?
?ji?an4 sh`eng 8 f`u?
is under-translated.
With the (NN-based) coverage mechanism, NMT-COVERAGEtranslates it into:jordan ?s average score points to UNKthis year .
he received surgery beforethree weeks , with a team in the periodof 4 to 8 .83Figure 6: Performance of the generated translations with respect to the lengths of the input sentences.Coverage models alleviate under-translation by producing longer translations on long sentences.in which the under-translation is rectified.The quantitative and qualitative results showthat the coverage models indeed help to allevi-ate under-translation, especially for long sentencesconsisting of several sub-sentences.6 Related WorkOur work is inspired by recent works on im-proving attention-based NMT with techniques thathave been successfully applied to SMT.
Follow-ing the success of Minimum Risk Training (MRT)in SMT (Och, 2003), Shen et al (2016) proposedMRT for end-to-end NMT to optimize model pa-rameters directly with respect to evaluation met-rics.
Based on the observation that attention-based NMT only captures partial aspects of atten-tional regularities, Cheng et al (2016) proposedagreement-based learning (Liang et al, 2006) toencourage bidirectional attention models to agreeon parameterized alignment matrices.
Along thesame direction, inspired by the coverage mecha-nism in SMT, we propose a coverage-based ap-proach to NMT to alleviate the over-translationand under-translation problems.Independent from our work, Cohn et al (2016)and Feng et al (2016) made use of the concept of?fertility?
for the attention model, which is sim-ilar in spirit to our method for building the lin-guistically inspired coverage with fertility.
Cohnet al (2016) introduced a feature-based fertilitythat includes the total alignment scores for the sur-rounding source words.
In contrast, we make pre-diction of fertility before decoding, which worksas a normalizer to better estimate the coverage ra-tio of each source word.
Feng et al (2016) usedthe previous attentional context to represent im-plicit fertility and passed it to the attention model,which is in essence similar to the input-feedmethod proposed in (Luong et al, 2015).
Compar-atively, we predict explicit fertility for each sourceword based on its encoding annotation, and incor-porate it into the linguistic-inspired coverage forattention model.7 ConclusionWe have presented an approach for enhancingNMT, which maintains and utilizes a coveragevector to indicate whether each source word istranslated or not.
By encouraging NMT to pay lessattention to translated words and more attention tountranslated words, our approach alleviates the se-rious over-translation and under-translation prob-lems that traditional attention-based NMT suffersfrom.
We propose two variants of coverage mod-els: linguistic coverage that leverages more lin-guistic information and NN-based coverage thatresorts to the flexibility of neural network approx-imation .
Experimental results show that bothvariants achieve significant improvements in termsof translation quality and alignment quality overNMT without coverage.84AcknowledgementThis work is supported by China National 973project 2014CB340301.
Yang Liu is supportedby the National Natural Science Foundation ofChina (No.
61522204) and the 863 Program(2015AA011808).
We thank the anonymous re-viewers for their insightful comments.References[Bahdanau et al2015] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2015.
Neural machinetranslation by jointly learning to align and translate.ICLR 2015.
[Bengio et al2003] Yoshua Bengio, R?ejean Ducharme,Pascal Vincent, and Christian Janvin.
2003.
A neu-ral probabilistic language model.
JMLR.
[Brown et al1993] Peter E. Brown, Stephen A. DellaPietra, Vincent J. Della Pietra, and Robert L. Mer-cer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
ComputationalLinguistics, 19(2):263?311.
[Cheng et al2016] Yong Cheng, Shiqi Shen, ZhongjunHe, Wei He, Hua Wu, Maosong Sun, and Yang Liu.2016.
Agreement-based Joint Training for Bidirec-tional Attention-based Neural Machine Translation.In IJCAI 2016.
[Chiang2007] David Chiang.
2007.
Hierarchicalphrase-based translation.
CL.
[Cho et al2014a] Kyunghyun Cho, Bart van Merrien-boer, Dzmitry Bahdanau, and Yoshua Bengio.2014a.
On the properties of neural machine trans-lation: encoder?decoder approaches.
In SSST 2014.
[Cho et al2014b] Kyunghyun Cho, Bart van Merrien-boer, Caglar Gulcehre, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014b.
Learningphrase representations using rnn encoder-decoderfor statistical machine translation.
In EMNLP 2014.
[Chung et al2014] Junyoung Chung, Caglar Gulcehre,KyungHyun Cho, and Yoshua Bengio.
2014.
Em-pirical evaluation of gated recurrent neural networkson sequence modeling.
arXiv.
[Cohn et al2016] Trevor Cohn, Cong Duy Vu Hoang,Ekaterina Vylomova, Kaisheng Yao, Chris Dyer,and Gholamreza Haffari.
2016.
IncorporatingStructural Alignment Biases into an AttentionalNeural Translation Model.
In NAACL 2016.
[Collins et al2005] Michael Collins, Philipp Koehn,and Ivona Ku?cerov?a.
2005.
Clause restructuring forstatistical machine translation.
In ACL 2005.
[Feng et al2016] Shi Feng, Shujie Liu, Mu Li, andMing Zhou.
2016.
Implicit distortion and fertil-ity models for attention-based encoder-decoder nmtmodel.
arXiv.
[Hochreiter and Schmidhuber1997] Sepp Hochreiterand J?urgen Schmidhuber.
1997.
Long short-termmemory.
Neural Computation.
[Jean et al2015] S?ebastien Jean, Kyunghyun Cho,Roland Memisevic, and Yoshua Bengio.
2015.
Onusing very large target vocabulary for neural ma-chine translation.
In ACL 2015.
[Koehn et al2003] Philipp Koehn, Franz Josef Och,and Daniel Marcu.
2003.
Statistical phrase-basedtranslation.
In NAACL 2003.
[Koehn et al2007] Philipp Koehn, Hieu Hoang,Alexandra Birch, Chris Callison-Burch, MarcelloFederico, Nicola Bertoldi, Brooke Cowan, WadeShen, Christine Moran, Richard Zens, Chris Dyer,Ondrej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: open source toolkit forstatistical machine translation.
In ACL 2007.
[Liang et al2006] Percy Liang, Ben Taskar, and DanKlein.
2006.
Alignment by agreement.
In NAACL2006.
[Liu and Sun2015] Yang Liu and Maosong Sun.
2015.Contrastive unsupervised word alignment with non-local features.
In AAAI 2015.
[Luong et al2015] Minh-Thang Luong, Hieu Pham,and Christopher D Manning.
2015.
Effective ap-proaches to attention-based neural machine transla-tion.
In EMNLP 2015.
[Och and Ney2003] Franz J. Och and Hermann Ney.2003.
A systematic comparison of various statisti-cal alignment models.
Computational Linguistics,29(1):19?51.
[Och2003] Franz Josef Och.
2003.
Minimum error ratetraining in statistical machine translation.
In ACL2003.
[Papineni et al2002] Kishore Papineni, Salim Roukos,Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: amethod for automatic evaluation of machine trans-lation.
In ACL 2002.
[Schuster and Paliwal1997] Mike Schuster andKuldip K Paliwal.
1997.
Bidirectional recur-rent neural networks.
IEEE Transactions on SignalProcessing, 45(11):2673?2681.
[Shen et al2016] Shiqi Shen, Yong Cheng, ZhongjunHe, Wei He, Hua Wu, Maosong Sun, and Yang Liu.2016.
Minimum Risk Training for Neural MachineTranslation.
In ACL 2016.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals,and Quoc VV Le.
2014.
Sequence to sequencelearning with neural networks.
In NIPS 2014.
[Xu et al2015] Kelvin Xu, Jimmy Ba, Ryan Kiros,Kyunghyun Cho, Aaron Courville, Ruslan Salakhut-dinov, Richard Zemel, and Yoshua Bengio.
2015.Show, Attend and Tell: Neural Image Caption Gen-eration with Visual Attention.
In ICML 2015.85
