Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 129?137,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsLearning Comparable Corpora from Latent Semantic AnalysisSimplified Document SpaceEkaterina Stambolievaeuroscript Luxembourg S.?.
r.l.55, rue de Luxembourg, L-8077Luxembourgekaterina.stambolieva@euroscript.luAbstractFocusing on a systematic Latent SemanticAnalysis (LSA) and Machine Learning (ML)approach, this research contributes to the de-velopment of a methodology for the automaticcompilation of comparable collections of doc-uments.
Its originality lies within the delinea-tion of relevant comparability characteristicsof similar documents in line with an estab-lished definition of comparable corpora.
Theseinnovative characteristics are used to build aLSA vector-based representation of the texts.In accordance with this new reduced in dimen-sionality document space, an unsupervisedmachine learning algorithm gathers similartexts into comparable clusters.
On a monolin-gual collection of less than 100 documents, theproposed approach assigns comparable docu-ments to different comparable corpora withhigh confidence.1 IntroductionThe problem of collecting comparable corpora ischallenging and yet enchanting.
Many can bene-fit from the availability of such corpora as trans-lation professionals, machine learning research-ers and computational linguistics specialists.
Yetthere is not an even consent about the notioncovered by the term comparable corpora.
Thedegree of similarity between comparable corporadocuments has not been formalized strictly andleaves space for different interpretations of simi-larity, contributing to abundant text collectionsof similar and semi-similar documents.
The cur-rent research endeavors to contribute to an ap-proach, which assembles a collection of compa-rable documents that are closely related to eachother on the basis of a strict definition of compa-rable corpora.
The proposed approach incorpo-rates originally a Latent Semantic Analysis tech-nique in order to match similar concepts insteadof words thus contributing to better automaticlearning of comparability between documents.2 Comparable Corpora DefinitionMaia (2003) discusses the characteristics ofcomparable corpora.
Nevertheless, the adopteddefinition of comparable corpora in this study isgiven by McEnery (2003):?Comparable corpora are corpora where seriesof monolingual corpora are collected for a rangeof languages, preferably using the same samplingand frame and with similar balance and repre-sentativeness, to enable the study of those lan-guages in contrast.
?McEnery (2003) characterizes comparablecorpora as ?corpora where series of monolingualcorpora are collected for the range of languages?.In the views of McEnery (2003), a monolingualcorpus is a corpus that is not collected for a rangeof languages, but instead the documents selectedare written in one language.
In the context of thecurrent research, a comparable corpus, a sub-language corpus, can be constructed from docu-ments in one language under the condition theyare compliant with the preferred guidelines pro-vided by McEnery (2003).
These preferredguidelines are similar sampling frame, balanceand representativeness.A document feature corresponding to textsampling is explicated taking into considerationthe domain and genre of the documents.
Addi-129tionally, similar terminology vocabulary insuresgenre correspondence.
Therefore, the same sam-pling scheme in collecting documents is evaluat-ed considering domain and genre and viewed asdocument features.Language is rapidly changing and evolvingthroughout the years (Crystal 2001).
As a result,restricting the time period a document has beenpublished increases the chances of it being com-parable to another one written during the sametime frame.
When events are reported in thenewspaper domain, their date of publication isstrong similarity evidence and is used as a filterbetween weakly comparable and non-comparabletext articles (Skadi?a et al2010a).The question of how representativeness of acorpus is decided upon is answered in differentways depending on the specific corpus purpose.For the purposes of this research, a corpus isconsidered representative when correspondingtexts are similar in size.
As reported by Manningand Sch?tze (1999), a balanced corpus is one,which is assembled ?as to give each subtype oftext a share of the corpus that is proportional tosome predetermined criterion of importance?.Skadina et al(2010b) present a good summaryof the advantages of exploiting comparable cor-pora.
It is discussed that ?they can draw on muchricher, more available and more diverse sourceswhich are produced every day (e.g.
multilingualnews feeds) and are available on the Web inlarge quantities for many languages and do-mains.?
(Skadina et al2010b).3 Related WorkThe most closely-related to machine learningwork that mines comparable corpora is that bySharoff (2010).
His research incorporates intelli-gent self-learning techniques to the compilationof comparable documents.
Unlike other re-searchers that experiment with Cross-LingualInformation Retrieval (CLIR) techniques as inTao and Zhai (2005), Sharoff (2010) estimatesthe document collection?s internal subgroup sys-tem in search for structure.
The possible structureand grouping of a set of documents is most easilydefined by ranked words that are representativefor the subsets in the collection.
Sharoff's ap-proach relies heavily on keywords and keywordestimation.
One thing Sharoff (2010) does notelaborate on in details is the definition of a com-parable corpus.
A possible reason for that is thatunsupervised machine learning approaches pro-duce related sets of documents in an environmentwhere the selection process is automated and notsupervised by any linguistically-dependent rules.What is written by Goeuriot et al(2009) is al-so an influential and relevant material to the cur-rent research.
Their paper is on the compilationof comparable corpora in a specialized domainwith a focus on English and Japanese.
The articleis significant for the reason the authors investi-gate ways of building comparable corpora usingmachine learning classification algorithms,namely Support Vector Machine and C4.5.
Theexperimental setup in the work of Goeuriot et al(2009) relies on manually labeled data, which isthen fed to the machine learning algorithm core.The paper by Goeuriot et al(2009) is directedtowards building a tool to automatically compilecomparable corpora in a predefined set of docu-ments and languages.
The text comparabilitycharacteristics extracted, which allow compari-son between the documents, are external and in-ternal to the textual data.
Goeuriot et al(2009)emphasize on selecting ways to automatic recog-nition of useful features similar texts have andexperiment with these features to test and predicttheir reliability.
The comparability of the docu-ments defined by them is on three levels - type ofdiscourse, topic and domain, focusing on locu-tive, ellocutive and allocutive act labels.Bekavac et al(2004) discuss the grounds of amethodology describing similarity comparison ofunder-resourced monolingual corpora.
Contraryto other methodologies that exploit seed words orseed texts as a basis for search, the researchershave at their disposal two monolingual docu-ments sets from which they aim to mine compa-rable documents.
The advantage of their ap-proach is that it is applicable to texts collectionwritten in one language for the reason that theyare easily mined and compiled from the availabletextual resources nowadays.
The concept behindtheir research is to align comparable documentsthat are found in pre-collected different monolin-gual corpora.
Content features are used to test thedegree to which two texts are similar to eachother in the sense of sharing the same infor-mation and common words.
These features,composition features, need to be representativefor the texts.
The composition features, extractedfrom the data, monitor the size, the format andthe time span of the documents.Clustering based on semantic keyword extrac-tion is performed by Finkelstein et al(2001).This approach is relevant to the current researchas it suggests a different methodology of feedingtexts to machine learning algorithms.
The re-130searchers aim to generate new content based oninput user queries by using context ?
?a body ofwords surrounding a user-selected phrase?
(Finkelstein et al2001).
They emphasise on thesignificance of using context when developingNatural Language Processing (NLP) applica-tions.
The keyword extraction algorithm present-ed relies on a precisely-designed clustering algo-rithm, different than k-means, to recursivelyclean clustering results and present refined statis-tical output.With regards to evaluation metrics of compa-rable corpora, one of the main focuses of theACCURAT Project (Skadina et al2010b) is todesign metrics of comparability estimation be-tween texts.
The ACCURAT researchers (Skadi-na et al2010b) concentrate on the developmentof comparable corpora criteria for different textsand different types of parallelism between thetexts.
Saralegi et al(2008) suggest measuresbased on distribution of topics or time with re-gards to publication dates.
Kilgariff (2001) aimsto measure the level of comparability betweentwo collections of documents.
He focuses addi-tionally on the shortcoming of known corpussimilarity metrics.
He discusses evaluation meth-ods for corpus comparability measures, whichare based on Spearman rank correlation co-efficient, perplexity and cross-entropy, ?2 andothers.
To his knowledge, the ?2 test performs thebest when comparing two sets of documents.
It isimportant to note that the approach adopted byKilgariff (2001) relies on words and n-gram se-quence features.
Not only does he regard thetexts as bag-of-words, but also he incorporates n-gram characteristics in his evaluation metricanalysis.Mining word similarity techniques are dis-cussed in the work of Deerwester et al(1990);Baeza-Yates and Ribeiro-Netto (1999); and Da-gan, Lee and Pereira (1999).
Deerwester et al(1990) incorporate LSA as a technique to identi-fy word relatedness.
LSA ?identifies a number ofmost prominent dimensions in the data, whichare assumed to correspond to ?latent concepts?.?
(Radinsky et al2011).
Radinsky et al(2011)indicate that LSA vector space models are ?diffi-cult to interpret?.
Consequently, the current re-search focuses not only on the incorporation ofLSA to mapping content, but also of the em-ployment of a machine learning technique togroup projected into the two-dimensional spacedocuments into similar clusters.
Baeza-Yates andRibeiro-Netto (1999), as Sharoff (2009) andGoeuriot et al(2010), consider texts as bag-of-words as the least complex word similarity ap-proaches can be incorporated.
Mapping distri-butional similarity, Lee (1999) opts for similarword co-occurrence probability estimation im-provement.
Dagan et al(1999) also aim forbetter estimation of word co-occurrence likeli-hood not based on empirical methods, but in-stead relying on distributional similarity for thegeneration of language models.
WordNet-based and distributional-similarity compari-sons of word similarity are presented in Agirreet al(2009).
They suggest different views ofword relatedness comparison ?
bag-of-words,context windows and syntactic dependencyapproaches.
They describe their findings asyielding best results on known test sets.
Whatis important to be remarked is that their meth-odology requires minor fine-tuning in order togive good results on cross-lingual word simi-larity.4 MethodologyThe novelty of our approach is the incorpora-tion of the Latent Semantic Analysis tech-nique, which matches concepts, or informationunits, from one document to another instead ofapproximating word similarity.
LSA expectsand constructs a new vector-based representa-tion of the documents to be compared.
A con-cept holds not only textual, but also morpho-logical information about each word present inthe texts.
By employing LSA, the documentspace is projected into the two-dimensionalspace in correspondence with the latent rela-tionships between the words in the texts.
In thetwo-dimensional space, clusters of similardocuments are compiled together using a sim-ple, but powerful unsupervised machine learn-ing algorithm, k-means clustering.
Clusteringevaluation metrics such as precision, recall andpurity are employed towards automatic evalua-tion and analysis of the resulting comparablecorpora.In order to compile comparable corpora withthe current settings, a set of pre-collected doc-uments is needed.
From this set of documents,two to five comparable corpora are identifiedand texts with similar topics, domains and fea-tures are assigned to relevant comparable cor-pora.LSA has its known limitations.
It acknowl-edges documents as bags-of-words and mines131the latent relationships between the words inthe bags-of-words.
Working with informationunits overcomes this limitation of LSA.
Theinformation units contain additional linguisticinformation about the syntactic and morpho-logical relationships between words, thereforeforming concepts of these words.
The order ofthe words, or the information units, is not im-perative, therefore it is not controlled by themethodology.LSA allows words to have only one mean-ing thus restricting the robustness of the natu-ral languages.
This limitation is tackled bysuggesting different word sense candidates forwords and constructing a separate informationunit for each promoted word sense.5 Data Feature SelectionThe innovation of the discussed research ap-proach lays in its basic concept of perceivingtexts as bags of interrelated concepts.
The sur-face-form words found in the texts are en-riched with linguistic information that furnish-es better matching procedure of the conceptslying within the texts for comparison.Unlike previous work, which regards docu-ments as bags-of-words (Sharoff 2009,Goeuriot et al2010) the methodology treatsdocuments as collections of concepts, eachconcept containing comparable textual infor-mation.
The concepts are represented by in-formation units.
The process of recognizingsuch units happens at document level, whereeach document is viewed as a separate textwith its own context.
Each information unit isdefined as the inseparable pair of lemma andits context-dependent part-of-speech (POS)tag.
A lemmatization technique is applied totransform the texts into linguistically-simplified versions of the originals, where eachword (infected or not) is substituted by its cor-responding lexeme.As stated before, the information units in-corporate POS output.
A POS tagger is used toprocess the texts before linguistically-simplifying it using lemmatization techniques.The idea of enriching the words by POS infor-mation is not new to the research of NaturalLanguage Processing, but it is new for the re-search of compiling comparable corpora.
Byidentifying the POS information of a sentence,lexical ambiguity is reduced.
The accompany-ing POS tag to each lemma assists the disam-biguation of the information units.
For exam-ple, run as being the action of walking fast hasa verb POS tag opposed to run as the period ofsome event happening has a noun POS tag.
Inthis example, the POS tag provides the neededinformation for disambiguating the two differ-ent meanings of a word.
In the current researchscenario, the POS tagging module 1  emulatesthe results of a basic Word Sense Disambigua-tion technique.Furthermore, the input set of documents istransformed into a set of lists of informationunits as described, where a single list of unitscorresponds to a single document.
When com-pared, the units are matched for correspond-ence both based on the lemma's lexical catego-ry in the sentence and its base form.Another feature, which helps build context re-lated concepts, is the identification of NounPhrases (NP) in the texts.
Noun Phrase recogni-tion is imperative since it further develops thesimple word sense disambiguation method.
Somewords to have a different meaning when occur-ring in a chain of words such as a noun phrase.Unlike the proposed by Su and Babych (2012)approach to NP recognition, NPs are identifiedfollowing linguistically-derived rules, which rep-resent common constructions of the languageunder consideration.
When a NP is identified, itis listed as a new information unit with a corre-sponding NP POS tag.
All POS annotations aswell as lemma information of its constituentwords are removed from the documents' list ofinformation units.6 Experiments6.1 Experimental CorpusA pre-collected corpus of documents, part of theNPs for Events (NP4E) corpus (Hasler et al2006), is used for experimenting.
The NP4E cor-pus is collected for the special purpose of ex-tracting coreference resolution in English.
Never-theless, the structure and the organization of thecorpus are suitable for the needs of acquisition ofa test corpus for the current study.
The NP4Ecorpus contains five different groups of newsarticles based on topic gathered from the Reuters.The news articles are collected in the time frame1 TreeTagger http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/132of two years ?
1996 and 1997 (Rose, Stevensonand Whitehead 2002).
Four of the five NP4Enews article groups are used to compile an exper-imental corpus containing roughly 40000 wordsor 520 words per text.
The chosen experimentalcollection consists of sub-corpora thathave documents comparable to the others in theirsub-corpora based on domain.
The domain ofthese comparable corpora is terrorism, and thefour distinct topics are connected with terrorism,bombing and suicide respectively for events inIsrael, Tajikistan, China and Peru.
In total, theexperimental corpus consists of 77 newswire ar-ticles.
The distribution of the documents in thisselected corpus is 20 on Israel topic, 19 on Tajik-istan topic, 19 for China topic and 19 on Perutopic.
These sub-corpora are referred to as Israel(I), Tajikistan (T), China (C) and Peru (P) on-wards.6.2 Experimental Set-upThe experimental set-up is structured as a chainof two simple procedures.
They are respectivelyan experimental setup data selectionand experimental setup clustering distribution.6.2.1 Data Selection FrameThe data selection frame describes how docu-ment features are selected.
The documents areafterwards preprocessed in order to extract allunderlying text features and binary vectors areconstructed to represent each separate document.The document features on focus consist of allidentified information units enriched with thenoun phrases that were recognized in the texts.The binary vectors then are used as an input tothe LSA algorithm.6.2.2 Cluster DistributionThe number of resulting clusters, or comparablecorpora, should be set in advance for unsuper-vised machine learning algorithms.
An experi-ments with k, k is in the range of 2 to 5, are con-ducted.
Testing with number of clusters greateror equal to two comes logical.
In the case of ex-pecting two resulting clusters, the methodologygroups all similar documents in one comparablecorpus, and withdraws the non-similar docu-ments to the second collection.
When k is chosento be 2 or 3, the resulting comparable corporatend to be weakly-comparable (Skadi?a et al2010a) for the reason the algorithms are forced togather documents with four distinct topics intoonly two or three comparable collections.
It isinteresting to analyze the research methodolo-gy?s performance in the case four output compa-rable corpora are expected, meaning when thelearning algorithm is asked to suggest four com-parable sets of documents.To evaluate clustering performance in terms offorcing the system to split the document collec-tion into more comparable corpora than present,k equals to 5 is also used in the experiments.Consequently, the number of clusters varies be-tween 2 and 5.6.2.3 Evaluation MetricsThree metrics are chosen to evaluate results - thestandard precision and recall, and additionally -purity.
Precision shows how many documents inthe resulting collections are identified correctlyas comparable to the majority of documents on aspecific topic in the cluster.
For example, when16 out of 19 documents are recognized to becomparable to each other, the precision of thisclustering result is 0.84.
Recall shows how manyfalse negatives are identified as comparable to acertain topic-related collection of texts.
The falsenegatives are the documents on a different topic,which the machine learning algorithm falselylists to be comparable to documents on anothertopic.
When 21 documents are grouped in onesimilarity cluster, 19 of them being on a relatedtopic, 3 of them being on another topic, the recallof the learning performance is 0.86.Purity is an evaluation metric used to estimatethe purity of the resulting clusters (Figure 1.).
Acluster is recognized as pure when it contains anumber of documents with the same label (mean-ing they are listed to be comparable to each otherby a human evaluator) and as less as possibledocuments that have a different label from thedominant label (Manning et al2008):Figure 1.
Purity score formulawhere nomcluster i is the number of the majorityclass members in each resulting cluster i, andnoclustrers is the number of resulting clusters, or k.As Manning et al(2008) warn ?High purity iseasy to achieve when the number of clusters islarge - in particular, purity is 1 if each documentgets its own cluster?.
The number of clusters forthe current research is not big.
Nonetheless, theresults are evaluated based on two other metrics.133The other metrics for measuring thecomparability between documents that arechosen for exploitation in the current research,are Mutual Infromation (MI) and NormalizedMutual Infroamtion (NMI).
The formula forNMI is as follows and shown in Figure 2.:Figure 2.
NMI score formulaMI is explained in details in Kalgariff (2001)and (Manning et al2008).
Manning et al(2008)discuss additionally the formula for the entropyH, and NMI.
?
is the group of clusters addressedin the experiments, and C is the group of labels ?namely the different characteristics of the com-parable corpora.In the current scenario, no human evaluation isperformed.
Rather than that the corpus is pre-designed in a way to contain four different com-parable corpora that need not to be manually la-beled6.3 EvaluationResults are obtained after conducting differentset-up experiments.
One set-up focuses on evalu-ating comparable corpus collection having as aninput part of the experimental corpus.
This partcontains documents on two out of the four differ-ent topics.
The two-topic collections are com-piled by combining all combinations possible oftwo topic-based sets together from the four dis-tinct topic sub-corpora.
In this experimental sce-nario, the total of different corpora for evaluationis 6 (according to the combination?s formula )- Peru and China, Peru and Tajikistan, Peru andIsrael, Tajikistan and China, Tajikistan and Isra-el, China and Israel.
Table 1 shows the results ofrunning LSA with k-means clustering on the dis-cussed sub-groups.
As seen on Table 1. the learn-ing algorithm performance is excellent when thenumber of comparable corpora that are expectedis greater than two.
When three or more compa-rable clusters are elected, each similar by topicdocument is grouped with all other documentsthat are comparable to it in the same resultingcomparable corpus.
In the case of expecting threecomparable corpora with Precision and Recallequal to 1.0, one of these corpora contains alldocuments of two different sub-corpora and therest contain all documents of one of the pre-defined experimental sub-corpora.
In the case ofexpecting five comparable corpora with Preci-sion and Recall equal to 1.0, one sub-corpus issplit into two comparable clusters, these clusterscontaining documents on the same topic.
What isinteresting in this experimental set-up are theresults the learning algorithm obtains when itaims to produce only two comparable clusters.For three of the test sets - China and Israel, Peruand China and Tajikistan and Israel, grouping ofdocuments on different topics into the same simi-lar collection is seen.
The lowest results obtainedare for the test set Tajikistan and Israel, where 3of the 19 documents on an Israel topic aregrouped together with the texts on the Tajikistantopic.
The reason behind this automatic learningconfusion originates from the fact the Tajikistanand Israel topic documents contain many similarconcepts, which make good clustering harder toachieve.The purity of the resulting corpora is veryhigh, above 0.9, indicating that comparable doc-uments are identified correctly with high rele-vance.
The only exception is the results on theTajikistan and Israel test set with purity 0.56.This exception occurs because of poor clusteringresults, which have been discussed.Sub-corpusTopic Precision    Recall    Purity2Cl 3Cl 4Cl 5Cl 2Cl 3Cl 4Cl 5ClP Peru 0.84 1 1 1 1 1 1 1 0.921C China 1 1 1 1 0.86 1 1 1P Peru 0.84 1 1 1 1 1 1 1 0.921T Tajikistan 1 1 1 1 0.86 1 1 1P Peru 1 1 1 1 1 1 1 1 1.00I Israel 1 1 1 1 1 1 1 1T Tajikistan 1 1 1 1 1 1 1 1 1.00C China 1 1 1 1 1 1 1 1T Tajikistan 1 1 1 1 0.52 1 1 1 0.56I Israel 0.15 1 1 1 1 1 1 1C China 0.86 1 1 1 1 1 1 1 0.923I Israel 1 1 1 1 0.85 1 1 1Table 1.
Clustering results for test sets of combinations of two topic sub-corpora(nCl pointing to the numbers of clusters identified )134Another set-up focuses on the analysis andevaluation of the results on clusters containingdocuments on three of the four different topics.The same way as the two-topic collections areconstructed, combining three topic sub-corporainto one results in the development of the inputfor the LSA and k-means clustering algorithms.In this experimental scenario, a total of 4 distinctinput collections are compiled -Tajikistan, Israeland China; Tajikistan, Israel and Peru; Peru,China and Israel; and Tajikistan, China and Peru.The results of the learning comparable corporafrom them are listed in Table 2.
As it can be easi-ly seen, the clustering performance is impecca-ble.
Therefore, providing more documents, moredata features, helps identifying better similardocuments applying the proposed research ap-proach.Precision Recall Purity2cl 3cl 4cL 5cl 2cl 3cl 4cl 5clT 1 1 1 1 1 1 1 1C 1 1 1 1 1 1 1 1 1.00I 1 1 1 1 1 1 1 1P 1 1 1 1 1 1 1 1Table 3.
Clustering results on the whole experi-mental corpusMutualInformationH(?)
H(C) NMI2CL 2CL 2CL 2ClPeruChina0.6866 0.9927 1 0.6916PeruTajikistan0.6866 0.9927 1 0.6916PeruIsrael1.0230 1.0074 1.0074 0.9522TajikistanChina1 1 1 1TajikistanIsrael0.0844 0.3912 1.0074 0.1262ChinaIsrael0.6855 0.9744 1.0074 0.6917Table 4.
MI and NMI scores results for test sets ofcombinations of two topic sub-corporaTable 3.
Shows the clustering results when alltexts of the experimental corpus are suggested asan input.
The algorithms once more do not haveproblems collecting the similar documents intocomparable corpora with high precision and re-call.MI and NMI are computed only for the resultspresented in Table 1.
The reasoning behind isthat Table 2.
And Table 3. show perfect cluster-ing results of comparable corpora obtained onthe whole set of input documents described inSection 6.1.The results of the comparable texts groupingare estimated using a clustering quality trade-offmetric, NMI.
Table 4. shows the NMI results ofthe clustering performance on the two-topic col-lections described in the first experimental set-upat the beginning of  Section 6.3.Consequently, the results shown on Table 4.are obtained with respects to the precision, recalland purity scores presented in Table 1.
The NMIscore is evidence of the identified comparablecorpora quality.
As seen on Table 4., the lowestNMI score correspond to the clustering results onthe Peru- and China- topic texts.
As shown onTable 1., the proposed approach is not confidentwhen grouping the Peru- and China- topic textsinto comparable collections.
The results of theNMI metric shown on Table 4. only confirm thisconclusion.
The best results obtained accordingto the NMI score are NMI is dependent on themutual information and the entropy the texts tobe clustered share.
MI is a metric, which esti-mates how the amount of information presentedin the documents affect the clustering output.When the MI score is low, as in the example ofgrouping the Tajikistan- and Israel- topic texts,the information contained in the documents doesnot contribute to highly-comparable clusters ofcorpora.
When the MI score obtained is high, asSub-corpusTopic Precision    Recall    Purity2Cl 3Cl 4CL 5Cl 2Cl 3Cl 4Cl 5ClT Tajikistan 1 1 1 1 1 1 1 1I Israel 1 1 1 1 1 1 1 1 1.00C China 1 1 1 1 1 1 1 1T Tajikistan 1 1 1 1 1 1 1 1I Israel 1 1 1 1 1 1 1 1 1.00P Peru 1 1 1 1 1 1 1 1P Peru 1 1 1 1 1 1 1 1C China 1 1 1 1 1 1 1 1 1.00I Israel 1 1 1 1 1 1 1 1T Tajikistan 1 1 1 1 1 1 1 1C China 1 1 1 1 1 1 1 1 1.00P Peru 1 1 1 1 1 1 1 1Table 2.
Clustering results for test sets of combinations of three topic sub-corpora135in the Tajikistan- and China- topic documentsexperiment, the information in these documentsis a strong evidence of the text relatedness.
Table4.
lists the intermediate calculations of the entro-py based on the available labels H(C) and theresulting clusters H(?
).7 RemarksThe problems identified in the current methodol-ogy are classified into two different groups: textprocessing resources errors and clustering outputerrors.
The processing resources are taken as off-the-shelf modules and the development focus ofthe study in not concentrating on improving theirperformance.
The second type of errors is theclustering errors.
Their size can be reduced byimproving the performance of the text prepro-cessing resources.
Additionally, enhanced clus-tering output evaluation metrics can reveal learn-ing algorithm?s weaknesses and suggest ways forimprovement.8 Future WorkMore can be done in the future to improve theproposed methodology.
One idea for further in-vestigation is experimenting with larger collec-tions of data.
The results on the experimentalcorpus are promising, but the document collec-tion is not big and contains less than 80 texts.
Itwould be interesting to experiment with corporathat consist of hundreds of documents to testclustering performance.
Additionally, a new ex-perimental collection of documents is beingcompiled.
It contains psycholinguistics texts bothin Spanish and English.
As the collection of thisdocument set is still in progress, the results ob-tained on it are not presented in the current pa-per.
These results will be reported in future workpublications.Furthermore, a new translation equivalentsource can be added.
In the case of compilingspecialized collections of comparable docu-ments, a specialized bilingual or multilingualdictionary can prove to be a valuable resource.An untested interesting experimental setup canbe investigating the resulting clustering perfor-mance when more than 50% or more of the mostrelevant lemmas (with noun phrases) are selectedas document features.
A Named Entity Recog-nizer (NER) and a synonymy suggestion modulehave the possibility to serve as good text pro-cessing resources and further improve groupingoutcomes.
In connection with NER, it is interest-ing additionally to investigate if the test corpuscontains local names, which make clustering bet-ter easier.
Lastly, potential source for further de-velopment is the automatic recognition of diasys-tematic text features, such as diachronic, diatopicor diatechnic information.Clustering results of comparable corpora areobtained when the document characteristics arefiltered by best keyword estimation metric -TF.BM25, explained in P?rez-Iglesias et al(2009).
The results show decrease in good clus-tering performance.
A future work aspect is toinvestigate the cause this lower performance.9 ConclusionAn innovative approach to the problem ofcompilation of comparable corpora is described.The approach suggests guidelines to textualcharacteristics selection scheme.
Additionally,the approach incorporates LSA and unsupervisedML techniques.
Different evaluation metrics,such as precision, purity and normalized mutualinformation, are employed to estimate compara-ble corpus clustering results.
These metrics showgood results when evaluating comparable clus-ters from a predefined set of less than 100 docu-ments.
The methodology suggested is applied formonolingual selection of documents; nonethelessit is readily extendable to more languages.ReferencesAgirre, Eneko, Alfonseca, Enrique, Hall, Keith,Kravalova, Jana, Pa?ca, Marius and Soroa, Ai-tor.2009.
A study of Similarity and RelatednessUsing Distributional and WordNet-based ap-proaches.
In  NAACL ?09, pages 19-27.Baeza-Yates, Ricardo and Ribeiro-Neto, Betrhier.1999.
Modern Infromation Retieval, AddisonWesley.Bekavac, Bo?o, Osenova, Petya, Simov, Kiril andTadic, Marco.
2004.
Making Monolingual CorporaComparable: a Case Study of Bulgarian and Croa-tian.
In Proceedings of LREC2004, pages1187-1190, Lisbon.Crystal, David.
2001.
Language and the Internet.Cambidge University, Press.
Cambidge.UK, pages91-93.Dagan, Igo, Lee, Lillian and Pereira, Fernando.
1999.Similarity-based models of word co-occurrenceprobabilities.
Machine Learning.
34(1-3), pages43-69.Deerwester, Scott, Dumais, Susan, Furnas, George,Landauer, Thomas and Harshman, Richard.
1990.Indexing by latent semantic analysis.
Journal of136the Americal Society for Information Science.41(6), pages 391-407.Finkelstein, Lev, Gabrilovich, Evgeniy, Matias, Yos-si, Rivlin, Ehud, Solan, Zach, Wolfman, Gadi andRuppin, Eytan.
2001.
Placing Search in Context:The Concept Revisited.
In WWW?01, pages 406-414.Goeuriot, Lorraine, Emmanuel Morin and B?atriceDaille.
2009.
Compilation of specialized compara-ble corpora in French and Japanese.
In Proceed-ings of the 2nd workshop on Building and Us-ing Comparable Corpora: from Parallel toNon-parallel Corpora, August 06, 2009, Suntec,Singapore.Hasler, Laura, Constantin Orasan and Karin Nau-mann.
2006.
NPs for Events: Experiments in Con-ference Annotation.
In Proceedings of the 5thedition of the International Conference onLanguage Resources and Evaluation(LREC2006),pages 1167-1172, 24-26 May 2006,Genoa, Italy.Ion, Radu, Dan Tufi?, Tiberiu Boro?, Ru Ceau?u andDan ?tef?nescu.
2010.
On-line Compilation ofComparable Corpora and Their Evaluation.
InProceedingds of the 7th International Confer-ence of Formal Approaches to South Slavicand Balkan Languages (FASSBL7), pages 29-34, Dubrovnic, Croatia.Kilgarriff, Adam.
2001.
Comparing corpora.
Interna-tional Journal of Corpus Lingusitics, 6(1), pag-es 97-133.Lee, Lillian.
1999.
Measures of distributional similari-ty.
Proceedings of ACL 1999, pages 25-32.Maia, Belinda.
2003.
What are Comparable Corpora?Electronic resource:http://web.letras.up.pt/bhsmaia/belinda/pubs/CL2003%20workshop.doc.Manning, Christopher D. and Hinrich Sch?tze.
1999.Introduction to Information Retrieval.
Cam-bridge University Press, Cambridge, UK.Manning, Christopher D., Prabhakan Raghavan, andHinrich Sch?tze.
2008.
Introduction to Infor-mation Retrieval, Cambridge University Press,pages 356-358.McEnery, Tony.
2003.
Corpus Linguistics.
In RuslanMitkov, editor, The Handbook of Computation-al Lingustics.
Oxford University Press, Oxford,UK, pages 448-464.Radinsky, Kira, Agichtein, Eugene, Gabrilovich,Evgeniy and Markovitch, Shaul.
2011.
A word at atime: Computing Word Relatedness using Tem-poral Semantic Analysis.
In  WWW?11, pages 337-346.P?rez-Iglesias, Joaqu?n, P?rez-Ag?era, Jos?, Fresno,V?ctor and Feinstein, Yuval.
2009.
Integrating theprobabilistic model BM25/BM25F into Lucene.
InCoRR, abs/0911.5046.Rose, Tony, Mark Stevenson and Miles Whitehead.2002.
The Reuters Corpus Volume 1 ?
from Yes-terday?s News to Tomorrow?s Language Resource.In  Proceedings of  LREC2002, pages 827-833.Sarageli, Xabier., San Vincente, Inaki, Gurrutxaga.Antton 2002.Automatic Extraction of bilingualterms from comparable corpora in a popular sci-ence domain.
In  Proceedings of  the workshopon Comparable Corpora, LREC?08.Sharoff, Serge.
2010.
Analysing similarities and dif-ferences between corpora.
In Proceedings of the7th Conference of Language Technologies(Jezikovne Tehnologije), pages 5-11,Ljubljiana.
Slovenia.Skadi?a, Inguna, Ahmet Aker, Voula Giouli, DanTufis, Robert Gaizauskas, Madara Mieri?a and Ni-kos Mastropavlos.
2010a.
A Collection of Compa-rable Corpora for Under-Resourced Languages.
InInguna Skadi?a and Dan Tufis, editors, HumanLanguage Technologies.
The Baltic Perspec-tive.
Proceedings of the 4th International Con-ference Baltic HLT 2010, pages 161-168.Skadi?a, Inguna, Vasiljeiv, Andrejs, Skadi?
?, Raivis,Gaizauskas, Robert, Tufi?, Dan and Gornostay,Tatiana.
2010b.
Analysis and Evaluation of Com-poarable Corpora for Under Resourced Areas ofMachine Translation.
In Proceedings of the 3rdWorkshop on Building and Using ComparableCorpora.
Applications of Parallel and Com-parable Corpora in Natural Language PEngi-neering and the Humanities, pages 6-14.Su, Fangzhoung and Bogdan Babych.
2012.
Measur-ing Comparability of Documents in Non-ParallelCorpora for Efficient Extraction of (Semi-)ParallelTranslation Equivalents.
In Proceedings of theJoint Workshop on Exploiting Synergies be-tween Information Retrieval and MachineTranslation (ESIRMT) and Hybrid Approach-es to Machine Translation (HyTra), pages 10-19, Avignon, France.Tao, Tao and Cheng Xiang Zhai.
2005.
Mining Com-parable Bilingual Text Corpora for Cross-Language Information Integration.
In Proceedingsof the eleventh ACM SIGKDD internationalconference on Knowledge discovery in datamining, pages 691-696.137
