Verb Phrase Ellipsis detection using Automatically Parsed TextLeif Arda NielsenDepartment of Computer ScienceKing?s College Londonnielsen@dcs.kcl.ac.ukAbstractThis paper describes a Verb Phrase Ellipsis(VPE) detection system, built for robust-ness, accuracy and domain independence.The system is corpus-based, and uses a va-riety of machine learning techniques on freetext that has been automatically parsed us-ing two different parsers.
Tested on a mixedcorpus comprising a range of genres, the sys-tem achieves a 72% F1-score.
It is designedas the first stage of a complete VPE resolu-tion system that is input free text, detectsVPEs, and proceeds to find the antecedentsand resolve them.1 IntroductionEllipsis is a linguistic phenomenon that has re-ceived considerable attention, mostly focusingon its interpretation.
Most work on ellipsis(Fiengo and May, 1994; Lappin, 1993; Dalrym-ple et al, 1991; Kehler, 1993; Shieber et al,1996) is aimed at discerning the procedures andthe level of language processing at which ellipsisresolution takes place, or focuses on ambiguousand difficult cases.
The detection of ellipticalsentences or the identification of the antecedentand elided clauses within them are usually notdealt with, but taken as given.
Noisy or miss-ing input, which is unavoidable in NLP applica-tions, is not dealt with, and neither is focusingon specific domains or applications.
It thereforebecomes clear that a robust, trainable approachis needed.An example of Verb Phrase Ellipsis (VPE),which is detected by the presence of an auxiliaryverb without a verb phrase, is seen in example1.
VPE can also occur with semi-auxiliaries, asin example 2.
(1) John3 {loves his3 wife}2.
Bill3 does1 too.
(2) But although he was terse, he didn?t {rageat me}2 the way I expected him to1.Several steps of work need to be done for el-lipsis resolution :1.
Detecting ellipsis occurrences.
First, elidedverbs need to be found.2.
Identifying antecedents.
For most cases ofellipsis, copying of the antecedent clause isenough for resolution (Hardt, 1997).3.
Resolving ambiguities.
For cases whereambiguity exists, a method for generatingthe full list of possible solutions, and sug-gesting the most likely one is needed.This paper describes the work done on thefirst stage, the detection of elliptical verbs.First, previous work done on tagged corporawill be summarised.
Then, new work on parsedcorpora will be presented, showing the gainspossible through sentence-level features.
Fi-nally, experiments using unannotated data thatis parsed using an automatic parser are pre-sented, as our aim is to produce a stand-alonesystem.We have chosen to concentrate on VP ellipsisdue to the fact that it is far more common thanother forms of ellipsis, but pseudo-gapping, anexample of which is seen in example 3, has alsobeen included due to the similarity of its res-olution to VPE (Lappin, 1996).
Do so/it/thatand so doing anaphora are not handled, as theirresolution is different from that of VPE (Kehlerand Ward, 1999).
(3) John writes plays, and Bill does novels.2 Previous workHardt?s (1997) algorithm for detecting VPE inthe Penn Treebank (see Section 3) achieves re-call levels of 53% and precision of 44%, giv-ing an F11 of 48%, using a simple search tech-1Precision, recall and F1 are defined as :Recall = No(correct ellipses found)No(all ellipses in test) (1)nique, which relies on the parse annotation hav-ing identified empty expressions correctly.In previous work (Nielsen, 2003a; Nielsen,2003b) we performed experiments on the BritishNational Corpus using a variety of machinelearning techniques.
These earlier results arenot directly comparable to Hardt?s, due tothe different corpora used.
The expandedset of results are summarised in Table 1, forTransformation Based Learning (TBL) (Brill,1995), GIS based Maximum Entropy Modelling(GIS-MaxEnt)2 (Ratnaparkhi, 1998), L-BFGSbased Maximum Entropy Modelling (L-BFGS-MaxEnt)3 (Malouf, 2002), Decision Tree Learn-ing (Quinlan, 1993) and Memory Based Learn-ing (MBL) (Daelemans et al, 2002).Algorithm Recall Precision F1TBL 69.63 85.14 76.61Decision Tree 60.93 79.39 68.94MBL 72.58 71.50 72.04GIS-MaxEnt 71.72 63.89 67.58L-BFGS-MaxEnt 71.93 80.58 76.01Table 1: Comparison of algorithmsFor all of these experiments, the training fea-tures consisted of lexical forms and Part ofSpeech (POS) tags of the words in a three wordforward/backward window of the auxiliary be-ing tested.
This context size was determinedempirically to give optimum results, and will beused throughout this paper.
L-BFGS-MaxEntuses Gaussian Prior smoothing optimized forthe BNC data, while GIS-MaxEnt has a sim-ple smoothing option available, but this dete-riorates results and is not used.
Both maxi-mum entropy models were experimented withto determine thresholds for accepting results asVPE; GIS-MaxEnt was set to a 20% confidencethreshold and L-BFGS-MaxEnt to 35%.
MBLwas used with its default settings.While TBL gave the best results, the soft-ware we used (Lager, 1999) ran into memoryproblems and proved problematic with largerdatasets.
Decision trees, on the other hand,Precision = No(correct ellipses found)No(all ellipses found) (2)F1 = 2?
Precision?RecallPrecision+Recall (3)2Downloadable fromhttps://sourceforge.net/projects/maxent/3Downloadable fromhttp://www.nlplab.cn/zhangle/maxent toolkit.htmltend to oversimplify due to the very sparse na-ture of ellipsis, and produce a single rule thatclassifies everything as non-VPE.
This leavesMaximum Entropy and MBL for further exper-iments.3 Corpus descriptionThe British National Corpus (BNC) (Leech,1992) is annotated with POS tags, using theCLAWS-4 tagset.
A range of sections of theBNC, containing around 370k words4 with 645samples of VPE was used as training data.
Theseparate development data consists of around74k words5 with 200 samples of VPE.The Penn Treebank (Marcus et al, 1994)has more than a hundred phrase labels, and anumber of empty categories, but uses a coarsertagset.
A mixture of sections from the WallStreet Journal and Brown corpus were used.The training section6 consists of around 540kwords and contains 522 samples of VPE.
Thedevelopment section7 consists of around 140kwords and contains 150 samples of VPE.4 Experiments using the PennTreebankTo experiment with what gains are possiblethrough the use of more complex data such asparse trees, the Penn Treebank is used for thesecond round of experiments.
The results arepresented as new features are added in a cumu-lative fashion, so each experiment also containsthe data contained in those before it; the closeto punctuation experiment contains the wordsand POS tags from the experiment before it,the next experiment contains all of these plusthe heuristic baseline and so on.Words and POS tagsThe Treebank, besides POS tags and categoryheaders associated with the nodes of the parsetree, includes empty category information.
Forthe initial experiments, the empty category in-formation is ignored, and the words and POStags are extracted from the trees.
The resultsin Table 2 are seen to be considerably poorerthan those for BNC, despite the comparabledata sizes.
This can be accounted for by thecoarser tagset employed.4Sections CS6, A2U, J25, FU6, H7F, HA3, A19, A0P,G1A, EWC, FNS, C8T5Sections EDJ, FR36Sections WSJ 00, 01, 03, 04, 15, Brown CF, CG, CL,CM, CN, CP7Sections WSJ 02, 10, Brown CK, CRAlgorithm Recall Precision F1MBL 50.98 61.90 55.91GIS-MaxEnt 34.64 79.10 48.18L-BFGS-MaxEnt 60.13 76.66 67.39Table 2: Initial results with the TreebankClose to punctuationA very simple feature, that checks for auxiliariesclose to punctuation marks was tested.
Table3 shows the performance of the feature itself,characterised by very low precision, and resultsobtained by using it.
It gives a 3% increase inF1 for GIS-MaxEnt, but a 1.5% decrease for L-BFGS-MaxEnt and 0.5% decrease for MBL.This brings up the point that the individualsuccess rate of the features will not be in directcorrelation with gains in overall results.
Theircontribution will be high if they have high pre-cision for the cases they are meant to address,and if they produce a different set of results fromthose already handled well, complementing theexisting features.
Overlap between features canbe useful to have greater confidence when theyagree, but low precision in the feature can in-crease false positives as well, decreasing perfor-mance.
Also, the small size of the developmentset can contribute to fluctuations in results.Algorithm Recall Precision F1close-to-punctuation 30.06 2.31 4.30MBL 50.32 61.60 55.39GIS-MaxEnt 37.90 79.45 51.32L-BFGS-MaxEnt 57.51 76.52 65.67Table 3: Effects of using the close-to-punctuation featureHeuristic BaselineA simple heuristic approach was developed toform a baseline using only POS data.
Themethod takes all auxiliaries as possible candi-dates and then eliminates them using local syn-tactic information in a very simple way.
Itsearches forwards within a short range of words,and if it encounters any other verbs, adjectives,nouns, prepositions, pronouns or numbers, clas-sifies the auxiliary as not elliptical.
It also doesa short backwards search for verbs.
The forwardsearch looks 7 words ahead and the backwardssearch 3.
Both skip ?asides?, which are taken tobe snippets between commas without verbs inthem, such as : ?...
papers do, however, show...?.
This feature gives a 3.5 - 4.5% improvement(Table 4).Algorithm Recall Precision F1heuristic 48.36 27.61 35.15MBL 55.55 65.38 60.07GIS-MaxEnt 43.13 78.57 55.69L-BFGS-MaxEnt 62.09 77.86 69.09Table 4: Effects of using the heuristic feature(SINV(ADVP-PRD-TPC-2 (RB so) )(VP (VBZ is)(ADVP-PRD (-NONE- *T*-2) ))(NP-SBJ (PRP$ its)(NN balance) (NN sheet) ))Figure 1: Fragment of sentence from TreebankSurrounding categoriesThe next feature added is the categories ofthe previous branch of the tree, and the nextbranch.
So in the example in Figure 1, the pre-vious category of the elliptical verb is ADVP-PRD-TPC-2, and the next category NP-SBJ.The results of using this feature are seen in Ta-ble 5, giving a 1.6 - 3.5% boost.Algorithm Recall Precision F1MBL 58.82 69.23 63.60GIS-MaxEnt 45.09 81.17 57.98L-BFGS-MaxEnt 64.70 77.95 70.71Table 5: Effects of using the surrounding cate-goriesAuxiliary-final VPFor auxiliary verbs parsed as verb phrases (VP),this feature checks if the final element in the VPis an auxiliary or negation.
If so, no main verbcan be present, as a main verb cannot be fol-lowed by an auxiliary or negation.
This featurewas used by Hardt (1993) and gives a 3.4 - 6%boost to performance (Table 6).Algorithm Recall Precision F1Auxiliary-final VP 72.54 35.23 47.43MBL 63.39 71.32 67.12GIS-MaxEnt 54.90 77.06 64.12L-BFGS-MaxEnt 71.89 76.38 74.07Table 6: Effects of using the Auxiliary-final VPfeatureEmpty VPHardt (1997) uses a simple pattern check tosearch for empty VP?s identified by the Tree-bank, (VP (-NONE- *?
*)), which is to say a...(VP (VBZ resembles)(NP (NNS politics) )(ADVP(ADVP (RBR more) )(SBAR (IN than)(S(NP-SBJ (PRP it) )(VP (VBZ does)(VP (-NONE- *?
*)(NP (NN comedy) )))))))Figure 2: Missed pseudo-gapping parse...(CC or)(VP (VB put)(NP (-NONE- *-4) )(PP-PUT (IN into)(NP (NN syndication) )))(, ,)(SBAR-ADV (IN as)(S(NP-SBJ-1 (JJS most)(JJ American) (NNS programs) )(VP (VBP are)(VP (-NONE- *?
*)(NP (-NONE- *-1) )))))))))Figure 3: Missed VPE parseVP which consists only of an empty element.This achieves 60% F1 on our development set.Our findings are in line with Hardt?s, who re-ports 48% F1, with the difference being due tothe different sections of the Treebank used.It was observed that this search may be toorestrictive to catch pseudo-gapping (Figure 2)and some examples of VPE in the corpus (Fig-ure 3).
We modify the search pattern to be ?
(VP(-NONE- *?
*) ...
)?, which is a VP that containsan empty element, but can contain other cate-gories after it as well.
This improves the featureitself by 10% in F1 and gives 10 - 14% improve-ment to the algorithms (Table 7).Algorithm Recall Precision F1Empty VP 54.90 97.67 70.29MBL 77.12 77.63 77.37GIS-MaxEnt 69.93 88.42 78.10L-BFGS-MaxEnt 83.00 88.81 85.81Table 7: Effects of using the improved EmptyVP featureEmpty categoriesFinally, empty category information is includedcompletely, such that empty categories aretreated as words, or leaves of the parse tree,and included in the context.
Table 8 shows thatadding this information results in 2.5 - 4.9% in-crease in F1.Algorithm Recall Precision F1MBL 83.00 79.87 81.41GIS-MaxEnt 76.47 90.69 82.97L-BFGS-MaxEnt 86.27 90.41 88.29Table 8: Effects of using the empty categoriesCross-validationWe perform cross-validation with and withoutthe features developed to measure the improve-ment obtained through their use.
The cross-validation results show a different ranking ofthe algorithms by performance than on the de-velopment set (Table 9), but consistent withthe results for the BNC corpus.
MBL showsconsistent performance, L-BFGS-MaxEnt getssomewhat lower results and GIS-MaxEnt muchlower.
These results indicate that the confi-dence threshold settings of the maximum en-tropy models were over-optimized for the devel-opment data, and perhaps the smoothing for L-BFGS-MaxEnt was as well.
MBL which wasused as-is does not suffer these performancedrops.
The increase in F1 achieved by addingthe features is similar for all algorithms, rangingfrom 17.9 to 19.8%.5 Experiments with AutomaticallyParsed dataThe next set of experiments use the BNC andTreebank, but strip POS and parse information,and parse them automatically using two differ-ent parsers.
This enables us to test what kindof performance is possible for real-world appli-cations.5.1 Parsers usedCharniak?s parser (2000) is a combination prob-abilistic context free grammar and maximumentropy parser.
It is trained on the Penn Tree-bank, and achieves a 90.1% recall and precisionaverage for sentences of 40 words or less.
WhileCharniak?s parser does not generate empty cat-egory information, Johnson (2002) has devel-oped an algorithm that extracts patterns fromthe Treebank which can be used to insert emptyWords + POS + featuresAlgorithm Recall Precision F1 Recall Precision F1MBL 56.05 63.02 59.33 78.32 79.39 78.85GIS-MaxEnt 40.00 57.03 47.02 65.06 68.64 66.80L-BFGS-MaxEnt 60.08 70.77 64.99 78.92 87.27 82.88Table 9: Cross-validation on the Treebankcategories into the parser?s output.
This pro-gram will be used in conjunction with Char-niak?s parser.Robust Accurate Statistical Parsing (RASP)(Briscoe and Carroll, 2002) uses a combina-tion of statistical techniques and a hand-craftedgrammar.
RASP is trained on a range of cor-pora, and uses a more complex tagging system(CLAWS-2), like that of the BNC.
This parser,on our data, generated full parses for 70% of thesentences, partial parses for 28%, while 2% werenot parsed, returning POS tags only.5.2 Reparsing the TreebankThe results of experiments using the twoparsers (Table 10) show generally similar perfor-mance.
Preiss (2003) shows that for the task ofanaphora resolution, these two parsers producevery similar results, which is consistent with ourfindings.
Compared to results on the originaltreebank with similar data (Table 6), the re-sults are low, which is not surprising, given theerrors introduced by the parsing process.
It isnoticeable that the addition of features has lesseffect; 0-6%.The auxiliary-final VP feature (Table 11),which is determined by parse structure, is onlyhalf as successful for RASP.
Conversely, theheuristic baseline, which relies on POS tags, ismore successful for RASP as it has a more de-tailed tagset.
The empty VP feature retains ahigh precision of over 80%, but its recall dropsby 50% to 20%, suggesting that the empty-category insertion algorithm is sensitive to pars-ing errors.Feature Rec Prec F1Charniak close-to-punct 34.00 2.47 4.61heur.
baseline 45.33 25.27 32.45aux-final VP 51.33 36.66 42.77empty VP 20.00 83.33 32.25RASP close-to-punct 71.05 2.67 5.16heur.
baseline 74.34 28.25 40.94aux-final VP 22.36 25.18 23.69Table 11: Performance of features on re-parsedTreebank dataCross-validation results (Table 12) are consis-tent with experiments on the development set.This is due to the fact that settings were notoptimized for the development set, but left asthey were from previous experiments.
Resultshere show better performance for RASP overall.5.3 Parsing the BNCExperiments using parsed versions of the BNCcorpora (Tables 13, 15) show similar results tothe original results (Table 1) but the featuresgenerate only a 3% improvement, suggestingthat many of the cases in the test set can beidentified using similar contexts in the train-ing data and the features do not add extrainformation.
The performance of the features(Table 14) remain similar to those for the re-parsed treebank experiments, except for emptyVP, where there is a 7% drop in F1, due toCharniak?s parser being trained on the Tree-bank only.Feature Rec Prec F1Charniak close-to-punct 48.00 5.52 9.90heur.
baseline 44.00 34.50 38.68aux-final VP 53.00 42.91 47.42empty VP 15.50 62.00 24.80RASP close-to-punct 55.32 4.06 7.57heur.
baseline 84.77 35.15 49.70aux-final VP 16.24 28.57 20.71Table 14: Performance of features on parsedBNC data5.4 Combining BNC and TreebankdataCombining the re-parsed BNC and Treebankdata gives a more robust training set of 1167VPE?s and a development set of 350 VPE?s.The results (Tables 16, 17) show only a 2-3% im-provement when the features are added.
Again,simple contextual information is successful incorrectly identifying most of the VPE?s.It is also seen that the increase in data sizeis not matched by a large increase in perfor-mance.
This may be because simple cases arealready handled, and for more complex casesthe context size limits the usefulness of addeddata.
The differences between the two corporaMBL GIS-MaxEnt L-BFGS-MaxEntRec Prec F1 Rec Prec F1 Rec Prec F1Charniak Words + POS 54.00 62.30 57.85 38.66 79.45 52.01 56.66 71.42 63.19+ features 62.66 71.21 66.66 48.00 70.58 57.14 64.66 72.93 68.55RASP Words + POS 55.92 66.92 60.93 43.42 56.89 49.25 51.63 79.00 62.45+ features 57.23 71.31 63.50 61.84 72.30 66.66 62.74 73.84 67.84Table 10: Results on re-parsed data from the TreebankCharniak RASPAlgorithm Recall Precision F1 Recall Precision F1MBL 58.76 63.35 60.97 61.97 71.50 66.39GIS-MaxEnt 46.22 71.66 56.19 56.58 72.27 63.47L-BFGS-MaxEnt 63.14 71.82 67.20 64.52 69.85 67.08Table 12: Cross-validation on re-parsed TreebankMBL GIS-MaxEnt L-BFGS-MaxEntRec Prec F1 Rec Prec F1 Rec Prec F1Charniak Words + POS 66.50 63.63 65.03 55.00 75.86 63.76 71.00 70.64 70.82+ features 69.00 65.40 67.15 64.00 72.72 68.08 74.00 68.83 71.32RASP Words + POS 61.92 63.21 62.56 64.46 54.04 58.79 65.34 70.96 68.04+ features 71.06 73.29 72.16 73.09 61.01 66.51 70.29 67.29 68.76Table 13: Results on parsed data from the BNCCharniak RASPAlgorithm Recall Precision F1 Recall Precision F1MBL 68.46 66.94 67.69 69.26 73.06 71.11GIS-MaxEnt 61.75 72.63 66.75 67.49 72.37 69.84L-BFGS-MaxEnt 71.10 71.96 71.53 70.68 72.22 71.44Table 15: Cross-validation on parsed BNCMBL GIS-MaxEnt L-BFGS-MaxEntRec Prec F1 Rec Prec F1 Rec Prec F1Charniak Words + POS 62.28 69.20 65.56 54.28 77.86 63.97 65.14 69.30 67.15+ features 65.71 71.87 68.65 63.71 72.40 67.78 70.85 69.85 70.35RASP Words + POS 63.61 67.47 65.48 59.31 55.94 57.37 57.46 71.83 63.84+ features 68.48 69.88 69.17 67.61 71.47 69.48 70.14 72.17 71.14Table 16: Results on parsed data using the combined datasetCharniak RASPAlgorithm Recall Precision F1 Recall Precision F1MBL 66.37 68.57 67.45 68.21 73.62 70.81GIS-MaxEnt 61.43 74.53 67.35 66.22 72.46 69.20L-BFGS-MaxEnt 69.78 71.65 70.70 71.00 73.22 72.09Table 17: Cross-validation on combined datasetmay also limit the relevance of examples fromone to the other.6 Summary and Future workThis paper has presented a robust system forVPE detection.
The data is automaticallytagged and parsed, syntactic features are ex-tracted and machine learning is used to classifyinstances.
This work offers clear improvementover previous work, and is the first to handleun-annotated free text, where VPE detectioncan be done with limited loss of performancecompared to annotated data.?
Three different machine learning algo-rithms, Memory Based Learning, GIS-based and L-BFGS-based maximum en-tropy modeling are used.
They give simi-lar results, with L-BFGS-MaxEnt generallygiving the highest performance.?
Two different parsers were used, Char-niak?s parser and RASP, achieving simi-lar results in experiments, with RASP re-sults being slightly higher.
RASP gen-erates more fine-grained POS info, whileCharniak?s parser generates more reliableparse structures for identifying auxiliary-final VP?s.?
Experiments on the Treebank give 82% F1,with the most informative feature, emptyVP?s, giving 70% F1.?
Re-parsing the Treebank gives 67% F1for both parsers.
Charniak?s parser com-bined with Johnson?s algorithm generatesthe empty VP feature with 32% F1.?
Repeating the experiments by parsingparts of the BNC gives 71% F1, with theempty VP feature further reduced to 25%F1.
Combining the datasets, final resultsof 71-2% F1 are obtained.Further work can be done on extracting gram-matical relation information (Lappin et al,1989; Cahill et al, 2002), or using those pro-vided by RASP, to produce more complicatedfeatures.
While the experiments suggest a per-formance barrier around 70%, it may be worth-while to investigate the performance increasespossible through the use of larger training sets.In the next stage of work, we will use machinelearning methods for the task of finding an-tecedents.
We will also perform a classificationof the cases to determine what percentage canbe dealt with using syntactic reconstruction,and how often more complicated approaches arerequired.ReferencesEric Brill.
1995.
Transformation-based error-driven learningand natural language processing: A case study in part-of-speech tagging.
Computational Linguistics, 21(4):543?565.E.
Briscoe and J. Carroll.
2002.
Robust accurate statisticalannotation of general text.
In Proceedings of the 3rd In-ternational Conference on Language Resources and Eval-uation, Las Palmas, Gran Canaria.Aoife Cahill, Mairead McCarthy, Josef van Genabith, andAndy Way.
2002.
Evaluating automatic f-structure anno-tation for the penn-ii treebank.
In Proceedings of the FirstWorkshop on Treebanks and Linguistic Theories (TLT2002), pages 42?60.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Meeting of the North American Chapter of theACL, pages 132?139.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2002.
Tilburg memory basedlearner, version 4.3, reference guide.
Downloadable fromhttp://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.gz.Mary Dalrymple, Stuart M. Shieber, and Fernando Pereira.1991.
Ellipsis and higher-order unification.
Linguisticsand Philosophy, 14:399?452.Robert Fiengo and Robert May.
1994.
Indices and Identity.MIT Press, Cambridge, MA.Daniel Hardt.
1993.
VP Ellipsis: Form, Meaning, and Pro-cessing.
Ph.D. thesis, University of Pennsylvania.Daniel Hardt.
1997.
An empirical approach to vp ellipsis.Computational Linguistics, 23(4).Mark Johnson.
2002.
A simple pattern-matching algorithmfor recovering empty nodes and their antecedents.
In Pro-ceedings of the 40th Annual Meeting of the Association forComputational Linguistics.Andrew Kehler and Gregory Ward.
1999.
On the seman-tics and pragmatics of ?identifier so?.
In Ken Turner, ed-itor, The Semantics/Pragmatics Interface from Differ-ent Points of View (Current Research in the Seman-tics/Pragmatics Interface Series, Volume I).
Amsterdam:Elsevier.Andrew Kehler.
1993.
A discourse copying algorithm for el-lipsis and anaphora resolution.
In Proceedings of the SixthConference of the European Chapter of the Associationfor Computational Linguistics (EACL-93), Utrecht, theNetherlands.Torbjorn Lager.
1999.
The mu-tbl system: Logic pro-gramming tools for transformation-based learning.
InThird International Workshop on Computational Natu-ral Language Learning (CoNLL?99).
Downloadable fromhttp://www.ling.gu.se/ lager/mutbl.html.Shalom Lappin, I. Golan, and M. Rimon.
1989.
Comput-ing grammatical functions from configurational parse trees.Technical Report 88.268, IBM Science and Technology andScientific Center, Haifa, June.Shalom Lappin.
1993.
The syntactic basis of ellipsis resolu-tion.
In S. Berman and A. Hestvik, editors, Proceedings ofthe Stuttgart Ellipsis Workshop, Arbeitspapiere des Son-derforschungsbereichs 340, Bericht Nr.
29-1992.
Univer-sity of Stuttgart, Stuttgart.Shalom Lappin.
1996.
The interpretation of ellipsis.
InShalom Lappin, editor, The Handbook of ContemporarySemantic Theory, pages 145?175.
Oxford: Blackwell.G.
Leech.
1992.
100 million words of english : The BritishNational Corpus.
Language Research, 28(1):1?13.Robert Malouf.
2002.
A comparison of algorithms for maxi-mum entropy parameter estimation.
In Proceedings of theSixth Conference on Natural Language Learning (CoNLL-2002), pages 49?55.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,M.
Bies, M. Ferguson, K. Katz, and B. Schasberger.1994.
The Penn Treebank: Annotating predicate argu-ment structure.
In Proceedings of the Human LanguageTechnology Workshop.
Morgan Kaufmann, San Francisco.Leif Arda Nielsen.
2003a.
A corpus-based study of verbphrase ellipsis.
In Proceedings of the 6th Annual CLUKResearch Colloquium, pages 109?115.Leif Arda Nielsen.
2003b.
Using machine learning techniquesfor VPE detection.
In Proceedings of RANLP, pages 339?346.Judita Preiss.
2003.
Choosing a parser for anaphora resolu-tion.
In Proceedings of DAARC, pages 175?180.R.
Quinlan.
1993.
C4.5: Programs for Machine Learning.San Mateo, CA: Morgan Kaufmann.Adwait Ratnaparkhi.
1998.
Maximum Entropy Models forNatural Language Ambiguity Resolution.
Ph.D. thesis,University of Pennsylvania.Stuart Shieber, Fernando Pereira, and Mary Dalrymple.1996.
Interactions of scope and ellipsis.
Linguistics andPhilosophy, 19(5):527?552.
