Proceedings of the ACL 2010 Conference Short Papers, pages 200?204,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsTop-Down K-Best A?
ParsingAdam Pauls and Dan KleinComputer Science DivisionUniversity of California at Berkeley{adpauls,klein}@cs.berkeley.eduChris QuirkMicrosoft ResearchRedmond, WA, 98052chrisq@microsoft.comAbstractWe propose a top-down algorithm for ex-tracting k-best lists from a parser.
Ouralgorithm, TKA?
is a variant of the k-best A?
(KA?)
algorithm of Pauls andKlein (2009).
In contrast to KA?, whichperforms an inside and outside pass be-fore performing k-best extraction bottomup, TKA?
performs only the inside passbefore extracting k-best lists top down.TKA?
maintains the same optimality andefficiency guarantees of KA?, but is sim-pler to both specify and implement.1 IntroductionMany situations call for a parser to return a k-best list of parses instead of a single best hypothe-sis.1 Currently, there are two efficient approachesknown in the literature.
The k-best algorithm ofJime?nez and Marzal (2000) and Huang and Chi-ang (2005), referred to hereafter as LAZY, oper-ates by first performing an exhaustive Viterbi in-side pass and then lazily extracting k-best lists intop-down manner.
The k-best A?
algorithm ofPauls and Klein (2009), hereafter KA?, computesViterbi inside and outside scores before extractingk-best lists bottom up.Because these additional passes are only partial,KA?
can be significantly faster than LAZY, espe-cially when a heuristic is used (Pauls and Klein,2009).
In this paper, we propose TKA?, a top-down variant of KA?
that, like LAZY, performsonly an inside pass before extracting k-best liststop-down, but maintains the same optimality andefficiency guarantees as KA?.
This algorithm canbe seen as a generalization of the lattice k-best al-gorithm of Soong and Huang (1991) to parsing.Because TKA?
eliminates the outside pass fromKA?, TKA?
is simpler both in implementation andspecification.1See Huang and Chiang (2005) for a review.2 ReviewBecause our algorithm is very similar to KA?,which is in turn an extension of the (1-best) A?parsing algorithm of Klein and Manning (2003),we first introduce notation and review those twoalgorithms before presenting our new algorithm.2.1 NotationAssume we have a PCFG2 G and an input sen-tence s0 .
.
.
sn?1 of length n. The grammar G hasa set of symbols denoted by capital letters, includ-ing a distinguished goal (root) symbol G. With-out loss of generality, we assume Chomsky nor-mal form: each non-terminal rule r in G has theform r = A ?
B C with weight wr.
Edgesare labeled spans e = (A, i, j).
Inside deriva-tions of an edge (A, i, j) are trees with root non-terminalA, spanning si .
.
.
sj?1.
The weight (neg-ative log-probability) of the best (minimum) insidederivation for an edge e is called the Viterbi in-side score ?
(e), and the weight of the best deriva-tion of G ?
s0 .
.
.
si?1 A sj .
.
.
sn?1 is calledthe Viterbi outside score ?(e).
The goal of a k-best parsing algorithm is to compute the k best(minimum weight) inside derivations of the edge(G, 0, n).We formulate the algorithms in this paperin terms of prioritized weighted deduction rules(Shieber et al, 1995; Nederhof, 2003).
A prior-itized weighted deduction rule has the form?1 : w1, .
.
.
, ?n : wnp(w1,...,wn)?????????
?0 : g(w1, .
.
.
, wn)where ?1, .
.
.
, ?n are the antecedent items of thededuction rule and ?0 is the conclusion item.
Adeduction rule states that, given the antecedents?1, .
.
.
, ?n with weights w1, .
.
.
, wn, the conclu-sion ?0 can be formed with weight g(w1, .
.
.
, wn)and priority p(w1, .
.
.
, wn).2While we present the algorithm specialized to parsingwith a PCFG, this algorithm generalizes to a wide range of200VPs2s3s4s0s2... s5sn-1...VPVBZ NPDT NNs2s3s4VPG(a) (b)(c)VPVP NPs1s2sn-1(d) Gs0NNNPFigure 1: Representations of the different types of itemsused in parsing.
(a) An inside edge item I(VP, 2, 5).
(b)An outside edge item O(VP, 2, 5).
(c) An inside deriva-tion item: D(TVP, 2, 5).
(d) An outside derivation item:Q(TGVP, 1, 2, {(NP, 2, n)}.
The edges in boldface are fron-tier edges.These deduction rules are ?executed?
withina generic agenda-driven algorithm, which con-structs items in a prioritized fashion.
The algo-rithm maintains an agenda (a priority queue ofitems), as well as a chart of items already pro-cessed.
The fundamental operation of the algo-rithm is to pop the highest priority item ?
from theagenda, put it into the chart with its current weight,and apply deduction rules to form any items whichcan be built by combining ?
with items alreadyin the chart.
When the resulting items are eithernew or have a weight smaller than an item?s bestscore so far, they are put on the agenda with pri-ority given by p(?).
Because all antecedents mustbe constructed before a deduction rule is executed,we sometimes refer to particular conclusion itemas ?waiting?
on another item before it can be built.2.2 A?A?
parsing (Klein and Manning, 2003) is an al-gorithm for computing the 1-best parse of a sen-tence.
A?
operates on items called inside edgeitems I(A, i, j), which represent the many pos-sible inside derivations of an edge (A, i, j).
In-side edge items are constructed according to theIN deduction rule of Table 1.
This deduction ruleconstructs inside edge items in a bottom-up fash-ion, combining items representing smaller edgesI(B, i, k) and I(C, k, j) with a grammar rule r =A ?
B C to form a larger item I(A, i, j).
Theweight of a newly constructed item is given by thesum of the weights of the antecedent items andthe grammar rule r, and its priority is given byhypergraph search problems as shown in Klein and Manning(2001).VPNPs1s2s3Gs0NNNPs4s5VPVPNPs1s2s3Gs0NNNPs4s5VPVP NN(a)(b)Figure 2: (a) An outside derivation item before expansion atthe edge (VP, 1, 4).
(b) A possible expansion of the item in(a) using the rule VP?
VP NN.
Frontier edges are marked inboldface.its weight plus a heuristic h(A, i, j).
For consis-tent and admissible heuristics h(?
), this deductionrule guarantees that when an inside edge item isremoved from the agenda, its current weight is itstrue Viterbi inside score.The heuristic h controls the speed of the algo-rithm.
It can be shown that an edge e satisfying?
(e) + h(A, i, j) > ?
(G, 0, n) will never be re-moved from the agenda, allowing some edges tobe safely pruned during parsing.
The more closelyh(e) approximates the Viterbi outside cost ?
(e),the more items are pruned.2.3 KA?The use of inside edge items in A?
exploits the op-timal substructure property of derivations ?
sincea best derivation of a larger edge is always com-posed of best derivations of smaller edges, it isonly necessary to compute the best way of build-ing a particular inside edge item.
When findingk-best lists, this is no longer possible, since we areinterested in suboptimal derivations.Thus, KA?, the k-best extension of A?, mustsearch not in the space of inside edge items,but rather in the space of inside derivation itemsD(TA, i, j), which represent specific derivationsof the edge (A, i, j) using tree TA.
However, thenumber of inside derivation items is exponentialin the length of the input sentence, and even witha very accurate heuristic, running A?
directly inthis space is not feasible.Fortunately, Pauls and Klein (2009) show thatwith a perfect heuristic, that is, h(e) = ?
(e) ?e,A?
search on inside derivation items will onlyremove items from the agenda that participatein the true k-best lists (up to ties).
In orderto compute this perfect heuristic, KA?
makesuse of outside edge items O(A, i, j) which rep-resent the many possible derivations of G ?201IN??
: I(B, i, l) : w1 I(C, l, j) : w2w1+w2+wr+h(A,i,j)???????????????
I(A, i, j) : w1 + w2 + wrIN-D?
: O(A, i, j) : w1 D(TB , i, l) : w2 D(TC , l, j) : w3w2+w3+wr+w1???????????
D(TA, i, j) : w2 + w3 + wrOUT-L?
: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3w1+w3+wr+w2???????????
O(B, i, l) : w1 + w3 + wrOUT-R?
: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3w1+w2+wr+w3???????????
O(C, l, j) : w1 + w2 + wrOUT-D?
: Q(TGA , i, j,F) : w1 I(B, i, l) : w2 I(C, l, j) : w3w1+wr+w2+w3+?(F)????????????????
Q(TGB , i, l,FC) : w1 + wrTable 1: The deduction rules used in this paper.
Here, r is the rule A ?
B C. A superscript * indicates that the rule is usedin TKA?, and a superscript ?
indicates that the rule is used in KA?.
In IN-D, the tree TA is rooted at (A, i, j) and has childrenTB and TC .
In OUT-D, the tree TGB is the tree TGA extended at (A, i, j) with rule r, FC is the list F with (C, l, j) prepended,and ?
(F) isPe?F ?(e).
Whenever the left child I(B, i, l) of an application of OUT-D represents a terminal, the next edge isremoved from F and is used as the new point of expansion.s1 .
.
.
si A sj+1 .
.
.
sn (see Figure 1(b)).Outside items are built using the OUT-L andOUT-R deduction rules shown in Table 1.
OUT-L and OUT-R combine, in a top-down fashion, anoutside edge over a larger span and inside edgeover a smaller span to form a new outside edgeover a smaller span.
Because these rules make ref-erence to inside edge items I(A, i, j), these itemsmust also be built using the IN deduction rulesfrom 1-best A?.
Outside edge items must thus waituntil the necessary inside edge items have beenbuilt.
The outside pass is initialized with the itemO(G, 0, n) when the inside edge item I(G, 0, n) ispopped from the agenda.Once we have started populating outside scoresusing the outside deductions, we can initiate asearch on inside derivation items.3 These itemsare built bottom-up using the IN-D deduction rule.The crucial element of this rule is that derivationitems for a particular edge wait until the exact out-side score of that edge has been computed.
The al-gorithm terminates when k derivation items rootedat (G, 0, n) have been popped from the agenda.3 TKA?KA?
efficiently explores the space of insidederivation items because it waits for the exactViterbi outside cost before building each deriva-tion item.
However, these outside costs and asso-ciated deduction items are only auxiliary quanti-ties used to guide the exploration of inside deriva-tions: they allow KA?
to prioritize currently con-structed inside derivation items (i.e., constructedderivations of the goal) by their optimal comple-tion costs.
Outside costs are thus only necessarybecause we construct partial derivations bottom-up; if we constructed partial derivations in a top-down fashion, all we would need to compute opti-3We stress that the order of computation is entirely speci-fied by the deduction rules ?
we only speak about e.g.
?initi-ating a search?
as an appeal to intuition.mal completion costs are Viterbi inside scores, andwe could forget the outside pass.TKA?
does exactly that.
Inside edge items areconstructed in the same way as KA?, but once theinside edge item I(G, 0, n) has been discovered,TKA?
begins building partial derivations from thegoal outwards.
We replace the inside derivationitems of KA?
with outside derivation items, whichrepresent trees rooted at the goal and expandingdownwards.
These items bottom out in a list ofedges called the frontier edges.
See Figure 1(d)for a graphical representation.
When a frontieredge represents a single word in the input, i.e.
isof the form (si, i, i+ 1), we say that edge is com-plete.
An outside derivation can be expanded byapplying a rule to one of its incomplete frontieredges; see Figure 2.
In the same way that insidederivation items wait on exact outside scores be-fore being built, outside derivation items wait onthe inside edge items of all frontier edges beforethey can be constructed.Although building derivations top-down obvi-ates the need for a 1-best outside pass, it raises anew issue.
When building derivations bottom-up,the only way to expand a particular partial insidederivation is to combine it with another partial in-side derivation to build a bigger tree.
In contrast,an outside derivation item can be expanded any-where along its frontier.
Naively building deriva-tions top-down would lead to a prohibitively largenumber of expansion choices.We solve this issue by always expanding theleft-most incomplete frontier edge of an outsidederivation item.
We show the deduction ruleOUT-D which performs this deduction in Fig-ure 1(d).
We denote an outside derivation item asQ(TGA , i, j,F), where TGA is a tree rooted at thegoal with left-most incomplete edge (A, i, j), andF is the list of incomplete frontier edges exclud-ing (A, i, j), ordered from left to right.
Wheneverthe application of this rule ?completes?
the left-202most edge, the next edge is removed from F andis used as the new point of expansion.
Once allfrontier edges are complete, the item represents acorrectly scored derivation of the goal, explored ina pre-order traversal.3.1 CorrectnessIt should be clear that expanding the left-most in-complete frontier edge first eventually explores thesame set of derivations as expanding all frontieredges simultaneously.
The only worry in fixingthis canonical order is that we will somehow ex-plore the Q items in an incorrect order, possiblybuilding some complete derivation Q?C before amore optimal complete derivation QC .
However,note that all items Q along the left-most construc-tion ofQC have priority equal to or better than anyless optimal complete derivation Q?C .
Therefore,when Q?C is enqueued, it will have lower prioritythan all Q; Q?C will therefore not be dequeued un-til all Q ?
and hence QC ?
have been built.Furthermore, it can be shown that the top-downexpansion strategy maintains the same efficiencyand optimality guarantees as KA?
for all itemtypes: for consistent heuristics h, the first k en-tirely complete outside derivation items are thetrue k-best derivations (modulo ties), and that onlyderivation items which participate in those k-bestderivations will be removed from the queue (up toties).3.2 Implementation DetailsBuilding derivations bottom-up is convenient froman indexing point of view: since larger derivationsare built from smaller ones, it is not necessary toconstruct the larger derivation from scratch.
In-stead, one can simply construct a new tree whosechildren point to the old trees, saving both mem-ory and CPU time.In order keep the same efficiency when build-ing trees top-down, a slightly different data struc-ture is necessary.
We represent top-down deriva-tions as a lazy list of expansions.
The top nodeTGG is an empty list, and whenever we expand anoutside derivation item Q(TGA , i, j,F) with a ruler = A ?
B C and split point l, the resultingderivation TGB is a new list item with (r, l) as thehead data, and TGA as its tail.
The tree can be re-constructed later by recursively reconstructing theparent, and adding the edges (B, i, l) and (C, l, j)as children of (A, i, j).3.3 AdvantagesAlthough our algorithm eliminates the 1-best out-side pass of KA?, in practice, even for k = 104,the 1-best inside pass remains the overwhelmingbottleneck (Pauls and Klein, 2009), and our modi-fications leave that pass unchanged.However, we argue that our implementation issimpler to specify and implement.
In terms of de-duction rules, our algorithm eliminates the 2 out-side deduction rules and replaces the IN-D rulewith the OUT-D rule, bringing the total numberof rules from four to two.The ease of specification translates directly intoease of implementation.
In particular, if high-quality heuristics are not available, it is often moreefficient to implement the 1-best inside pass asan exhaustive dynamic program, as in Huang andChiang (2005).
In this case, one would only needto implement a single, agenda-based k-best extrac-tion phase, instead of the 2 needed for KA?.3.4 PerformanceThe contribution of this paper is theoretical, notempirical.
We have argued that TKA?
is simplerthan TKA?, but we do not expect it to do any moreor less work than KA?, modulo grammar specificoptimizations.
Therefore, we simply verify, likeKA?, that the additional work of extracting k-bestlists with TKA?
is negligible compared to the timespent building 1-best inside edges.We examined the time spent building 100-bestlists for the same experimental setup as Pauls andKlein (2009).4 On 100 sentences, our implemen-tation of TKA?
constructed 3.46 billion items, ofwhich about 2% were outside derivation items.Our implementation of KA?
constructed 3.41 bil-lion edges, of which about 0.1% were outside edgeitems or inside derivation items.
In other words,the cost of k-best extraction is dwarfed by thethe 1-best inside edge computation in both cases.The reason for the slight performance advantageof KA?
is that our implementation of KA?
useslazy optimizations discussed in Pauls and Klein(2009), and while such optimizations could easilybe incorporated in TKA?, we have not yet done soin our implementation.4This setup used 3- and 6-round state-split grammars fromPetrov et al (2006), the former used to compute a heuristicfor the latter, tested on sentences of length up to 25.2034 ConclusionWe have presented TKA?, a simplification to theKA?
algorithm.
Our algorithm collapses the 1-best outside and bottom-up derivation passes ofKA?
into a single, top-down pass without sacri-ficing efficiency or optimality.
This reduces thenumber of non base-case deduction rules, makingTKA?
easier both to specify and implement.AcknowledgementsThis project is funded in part by the NSF undergrant 0643742 and an NSERC Postgraduate Fel-lowship.ReferencesLiang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the International Work-shop on Parsing Technologies (IWPT), pages 53?64.V?
?ctor M. Jime?nez and Andre?s Marzal.
2000.
Com-putation of the n best parse trees for weighted andstochastic context-free grammars.
In Proceedingsof the Joint IAPR International Workshops on Ad-vances in Pattern Recognition, pages 183?192, Lon-don, UK.
Springer-Verlag.Dan Klein and Christopher D. Manning.
2001.
Pars-ing and hypergraphs.
In Proceedings of the Interna-tional Workshop on Parsing Technologies (IWPT),pages 123?134.Dan Klein and Christopher D. Manning.
2003.
A*parsing: Fast exact Viterbi parse selection.
InProceedings of the Human Language TechnologyConference and the North American Associationfor Computational Linguistics (HLT-NAACL), pages119?126.Mark-Jan Nederhof.
2003.
Weighted deductive pars-ing and Knuth?s algorithm.
Computationl Linguis-tics, 29(1):135?143.Adam Pauls and Dan Klein.
2009.
K-best A* parsing.In Proccedings of the Association for ComputationalLinguistics (ACL).Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proccedings of theAssociation for Computational Linguistics (ACL).Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementation ofdeductive parsing.
Journal of Logic Programming,24:3?36.Frank K. Soong and Eng-Fong Huang.
1991.
A tree-trellis based fast search for finding the n best sen-tence hypotheses in continuous speech recognition.In Proceedings of the Workshop on Speech and Nat-ural Language.204
