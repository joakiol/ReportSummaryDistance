Syntactic features for high precision Word Sense DisambiguationDavid Mart?nez, Eneko AgirreIxA NLP GroupUniversity of the Basque CountryDonostia, Spain{jibmaird,eneko}@si.ehu.esLlu?s M?rquezTALP Research CenterPolytechnical University of CataloniaBarcelona, Spainlluism@lsi.upc.esAbstractThis paper explores the contributionof a broad range of syntactic featuresto WSD: grammatical relations codedas the presence of adjuncts/argumentsin isolation or as subcategorizationframes, and instantiated grammaticalrelations between words.
We havetested the performance of syntacticfeatures using two different MLalgorithms (Decision Lists andAdaBoost) on the Senseval-2 data.Adding syntactic features to a basicset of traditional features improvesperformance, especially for AdaBoost.In addition, several methods to buildarbitrarily high accuracy WSDsystems are also tried, showing thatsyntactic features allow for a precisionof 86% and a coverage of 26% or 95%precision and 8% coverage.1.
IntroductionSupervised learning has become the mostsuccessful paradigm for Word SenseDisambiguation (WSD).
This kind of algorithmsfollows a two-step process:1.
Choosing the representation as a set offeatures for the context of occurrence of thetarget word senses.2.
Applying a Machine Learning (ML)algorithm to train on the extracted featuresand tag the target word in the test examples.Current WSD systems attain high performancesfor coarse word sense differences (two or threesenses) if enough training material is available.In contrast, the performance for finer-grainedsense differences (e.g.
WordNet senses as usedin Senseval 2 (Preiss & Yarowsky, 2001)) is farfrom application needs.
Nevertheless, recentwork (Agirre and Martinez, 2001a) shows that itis possible to exploit the precision-coveragetrade-off and build a high precision WSD systemthat tags a limited number of target words with apredefined precision.This paper explores the contribution of abroad set of syntactically motivated features thatranges from the presence of complements andadjuncts, and the detection of subcategorizationframes, up to grammatical relations instantiatedwith specific words.
The performance of thesyntactic features is measured in isolation and incombination with a basic set of local and topicalfeatures (as defined in the literature), and usingtwo ML algorithms: Decision Lists (Dlist) andAdaBoost (Boost).
While Dlist does not attemptto combine the features, i.e.
it takes the strongestfeature only, Boost tries combinations offeatures and also uses negative evidence, i.e.
theabsence of features.Additionally, the role of syntactic features ina high-precision WSD system based on theprecision-coverage trade-off is also investigated.The paper is structured as follows.
Section 2reviews the features previously used in theliterature.
Section 3 defines a basic feature setbased on the preceding review.
Section 4presents the syntactic features as defined in ourwork, alongside the parser used.
In section 5 thetwo ML algorithms are presented, as well as thestrategies for the precision-coverage trade-off.Section 6 shows the experimental setting and theresults.
Finally section 7 draws the conclusionsand summarizes further work.2.
Previous work.Yarowsky (1994) defined a basic set of featuresthat has been widely used (with some variations)by other WSD systems.
It consisted on wordsappearing in a window of ?k positions aroundthe target and bigrams and trigrams constructedwith the target word.
He used words, lemmas,coarse part-of-speech tags and special classes ofwords, such as ?Weekday?.
These features havebeen used by other approaches, with variationssuch as the size of the window, the distinctionbetween open class/closed class words, or thepre-selection of significative words to look up inthe context of the target word.Ng (1996) uses a basic set of features similarto those defined by Yarowsky, but they also usesyntactic information: verb-object and subject-verb relations.
The results obtained by thesyntactic features are poor, and no analysis ofthe features or any reason for the lowperformance is given.Stetina et al (1998) achieve good results withsyntactic relations as features.
They use ameasure of semantic distance based on WordNetto find similar features.
The features areextracted using a statistical parser (Collins,1996), and consist of the head and modifiers ofeach phrase.
Unfortunately, they do not providea comparison with a baseline system that wouldonly use basic features.The Senseval-2 workshop was held inToulouse in July 2001 (Preiss & Yarowsky,2001).
Most of the supervised systems used onlya basic set of local and topical features to traintheir ML systems.
Regarding syntacticinformation, in the Japanese tasks, severalgroups relied on dependency trees to extractfeatures that were used by different models(SVM, Bayes, or vector space models).
For theEnglish tasks, the team from the University ofSussex extracted selectional preferences basedon subject-verb and verb-object relations.
TheJohn Hopkins team applied syntactic featuresobtained using simple heuristic patterns andregular expressions.
Finally, WASP-bench usedfinite-state techniques to create a grammaticalrelation database, which was later used in thedisambiguation process.
The papers in theproceedings do not provide specific evaluationof the syntactic features, and it is difficult toderive whether they were really useful or not.3.
Basic feature setWe have taken a basic feature set widely used inthe literature, divided in topical features andlocal features (Agirre & Martinez, 2001b).Topical features correspond to open-classlemmas that appear in windows of different sizesaround the target word.
In this experiment, weused two different window-sizes: 4 lemmasaround the target (coded as win_lem_4w), andthe lemmas in the sentence plus the 2 previousand 2 following sentences (win_lem_2s).Local features include bigrams and trigrams(coded as big_, trig_ respectively) that containthe target word.
An index (+1, -1, 0) is used toindicate the position of the target in the bigramor trigram, which can be formed by part ofspeech, lemmas or word forms (wf, lem,pos).
We used TnT (Brants, 2000) for PoStagging.For instance, we could extract the followingfeatures for the target word known from thesample sentence below: word form ?whole?occurring in a 2 sentence window (win_wf_2s),the bigram  ?known widely?
where target is thelast word (big_wf_+1) and the trigram ?RB RB N?formed by the two PoS before the target word(trig_pos_+1).
?There is nothing in the whole range of humanexperience more widely known and universally ??4.
Set of Syntactic Features.In order to extract syntactic features from thetagged examples, we needed a parser that wouldmeet the following requirements: free forresearch, able to provide the whole structurewith named syntactic relations (in contrast toshallow parsers), positively evaluated on well-established corpora, domain independent, andfast enough.Three parsers fulfilled all the requirements:Link Grammar (Sleator and Temperley, 1993),Minipar (Lin, 1993) and (Carroll & Briscoe,2001).
We installed the first two parsers, andperformed a set of small experiments (JohnCarroll helped out running his own parser).Unfortunately, we did not have a comparativeevaluation to help choosing the best.
Weperformed a little comparative test, and allparsers looked similar.
At this point we choseMinipar mainly because it was fast, easy toinstall and the output could be easily processed.The choice of the parser did not condition thedesign of the experiments (cf.
section 7).From the output of the parser, we extracteddifferent sets of features.
First, we distinguishbetween direct relations (words linked directlyin the parse tree) and indirect relations (wordsthat are two or more dependencies apart in thesyntax tree, e.g.
heads of prepositional modifiersof a verb).
For example, from ?Henry was listedon the petition as the mayor's attorney?
a directverb-object relation is extracted between listedand Henry and the indirect relation ?head of amodifier prepositional phrase?
between listedand petition.
For each relation we store also itsinverse.
The relations are coded according to theMinipar codes (cf.
Appendix):[Henry obj_word listed][listed objI_word Henry][petition mod_Prep_pcomp-n_N_word listed][listed mod_Prep_pcomp-n_NI_word petition]For instance, in the last relation above, mod_Prepindicates that listed has some prepositionalphrase attached, pcomp-n_N indicates that petitionis the head of the prepositional phrase, Iindicates that it is an inverse relation, and wordthat the relation is between words (as opposed torelations between lemmas).We distinguished two different kinds ofsyntactic relations: instantiated grammaticalrelations (IGR) and grammatical relations (GR).4.1.
Instantiated Grammatical RelationsIGRs are coded as [wordsense relation value]triples, where the value can be either the wordform or the lemma.
Some examples for thetarget noun ?church?
are shown below.
In thefirst example, a direct relation is extracted forthe ?building?
sense, and in the second examplean indirect relation for the ?group of Christians?sense.Example 1: ?...Anglican churches have beendemolished...?
[Church#2 obj_lem  demolish]Example 2: ?...to whip men into a surrender to aparticular churh...?
[Church#1 mod_Prep_pcomp-n_N_lem surrender]4.2.
Grammatical relationsThis kind of features refers to the grammaticalrelation themselves.
In this case, we collectbigrams [wordsense relation] and also n-grams[wordsense relation1 relation2 relation3 ...].
Therelations can refer to any argument, adjunct ormodifier.
N-grams are similar to verbalsubcategorization frames.
At present, they havebeen used only for verbs.
Minipar providessimple subcategorization information in the PoSitself, e.g.
V_N_N for a verb taking twoarguments.
We have defined 3 types of n-grams:?
Ngram1: The subcategorization informationincluded in the PoS data given by Minipar,e.g.
V_N_N.?
Ngram2: The subcategorization informationin ngram1, filtered by the arguments thatactually occur in the sentence.?
Ngram3: Which includes all dependencies inthe parse tree.The three types have been explored in order toaccount for the argument/adjunct distinction,which Minipar does not always assign correctly.In the first case, Minipar?s judgment is takenfrom the PoS.
In the second case the PoS and therelations deemed as arguments are combined(adjuncts are hopefully filtered out, but somearguments might be also discarded).
In the third,all relations (including adjuncts and arguments)are considered.In the example below, the ngram1 featureindicates that the verb has two arguments (i.e.
itis transitive), which is an error of Miniparprobably caused by a gap in the lexicon.
Thengram2 feature indicates simply that it has asubject and no object, and the ngram3 featuredenotes the presence of the adverbial modifier?still?.
Ngram2 and ngram3 try to repair possiblegaps in Minipar?s lexicon.Example: ?His mother was nudging him, but hewas still falling?
[Fall#1 ngram1 V_N_N][Fall#1 ngram2 subj][Fall#1 ngram3 amodstill+subj]5.
ML algorithms.In order to measure the contribution of syntacticrelations, we wanted to test them on several MLalgorithms.
At present we have chosen onealgorithm which does not combine features(Decision Lists) and another which doescombine features (AdaBoost).Despite their simplicity, Decision Lists (Dlistfor short) as defined in Yarowsky (1994) havebeen shown to be very effective for WSD(Kilgarriff & Palmer, 2000).
Features areweighted with a log-likelihood measure, andarranged in an ordered list according to theirweight.
In our case the probabilities have beenestimated using the maximum likelihoodestimate, smoothed adding a small constant (0.1)when probabilities are zero.
Decisions takenwith negative values were discarded (Agirre &Martinez, 2001b).AdaBoost (Boost for short) is a generalmethod for obtaining a highly accurateclassification rule by linearly combining manyweak classifiers, each of which may be onlymoderately accurate (Freund, 1997).
In theseexperiments, a generalized version of the Boostalgorithm has been used, (Schapire, 1999),which works with very simple domainpartitioning weak hypotheses (decision stumps)with confidence rated predictions.
Thisparticular boosting algorithm is able to workefficiently in very high dimensional featurespaces, and has been applied, with significantsuccess, to a number of NLP disambiguationtasks, including word sense disambiguation(Escudero et al, 2000).
Regardingparametrization, the smoothing parameter hasbeen set to the default value (Schapire, 1999),and Boost has been run for a fixed number ofrounds (200) for each word.
No optimization ofthese parameters has been done at a word level.When testing, the sense with the highestprediction is assigned.5.1.
Precision vs. coverage trade-off.A high-precision WSD system can be obtainedat the cost of low coverage, preventing thesystem to return an answer in the lowestconfidence cases.
We have tried two methods onDlists, and one method on Boost.The first method is based on a decision-threshold (Dagan and Itai, 1994): the algorithmrejects decisions taken when the difference ofthe maximum likelihood among the competingsenses is not big enough.
For this purpose, aone-tailed confidence interval was created so wecould state with confidence 1 - ?
that the truevalue of the difference measure was bigger thana given threshold (named ?).
As in (Dagan andItai, 1994), we adjusted the measure to theamount of evidence.
Different values of ?
weretested, using a 60% confidence interval.
Thevalues of ?
range from 0 to 4.
For more detailscheck (Agirre and Martinez, 2001b).The second method is based on featureselection (Agirre and Martinez, 2001a).
Ten-fold cross validation on the training data foreach word was used to measure the precision ofeach feature in isolation.
Thus, the MLalgorithm would be used only on the featureswith precision exceeding a given threshold.
Thismethod has the advantage of being able to setthe desired precision of the final system.In the case of Boost, there was nostraightforward way to apply the first method.The application of the second method did notyield satisfactory results, so we turned todirectly use the support value returned for eachdecision being made.
We first applied athreshold directly on this support value, i.e.discarding decisions made with low supportvalues.
A second approximation, which is theone reported here, applies a threshold over thedifference in the support for the winning senseand the second winning sense.
Still, further workis needed in order to investigate how Boostcould discard less-confident results.6.
Experimental setting and results.We used the Senseval-2 data (73 nouns, verbsand adjectives), keeping the original training andtesting sets.
In order to measure the contributionof syntactic features the following experimentswere devised (not all ML algorithms were usedin all experiments, as specified): contribution ofIGR-type and GR-type relations (Dlist),contribution of syntactic features over acombination of local and topical features (Dlist,Boost), and contribution of syntactic features ina high precision system (Dlist, Boost).Performance is measured as precision andcoverage (following the definitions given inSenseval-2).
We also consider F11 to comparethe overall performance as it gives the harmonicaverage between precision and recall (whererecall is in this case precision times thecoverage).
F1 can be used to select the bestprecision/coverage combination (cf.
section 6.3).6.1.
Results for different sets of syntacticfeatures (Dlist).Table 1 shows the precision, coverage and F1figures for each of the grammatical feature setsas used by the decision list algorithm.Instantiated Grammatical Relations provide verygood precision, but low coverage.
The onlyexceptions are verbs, which get very similarprecision for both kinds of syntactic relations.Grammatical Relations provide lower precisionbut higher coverage.
A combination of bothattains best F1, and is the feature set used insubsequent experiments.1 F1=2*precision*recall/(precision+recall).
In thiscase we use recall=precision*coverage.6.2.
Results for different combinations offeatures (Dlist, Boost)Both ML algorithms were used on syntacticfeatures, local features, a combination oflocal+topical features (also called basic), and acombination of all features (basic+syntax) inturn.
Table 2 shows the F1 figures for eachalgorithm, feature set and PoS.All in all, Boost is able to outperform Dlist inall cases, except for local features.
Syntacticfeatures get worse results than local features.Regarding the contribution of syntacticfeatures to the basic set, the last two columns inTable 2 show a "+" whenever the difference inthe precision over the basic feature set issignificant (McNemar's test).
Dlist is able toscarcely profit from the additional syntacticfeatures (only significant for verbs).
Boostattains significant improvement, showing thatbasic and syntactic features are complementary.The differencealgorithms could beDlist is a conservatithat it only uses theby the first feature tha(abstaining if none ousing a combination osingle-feature classifenegative evidence)positive predictions tDlist.
Since the featcovered and given thaccurate, Boost achieit is a significantapproaching a 100% c6.3.
Precision vs. coverage: high precisionsystems (Dlist, Boost)Figure 1 shows the results for the three methodsto exploit the precision/coverage trade-off inorder to obtain a high-precision system.
For eachmethod two sets of features have been used: thebasic set alne and the combination of bothbasic and syntactic features.The figure reveals an interesting behavior fordifferent coverage ranges.
In the high coveragerange, Boost on basic+syntactic features attainsthe best performance.
In the medium coveragearea, the feature selection method for Dlistobtains the best results, also for basic+syntacticfeatures.
Finally, in the low coverage and highprecision area the decision-threshold method forDlist is able to reach precisions in the high 90?s,with no profit from syntactic features.The two methods to raise precision for Dlistsare very effective.
The decision-thresholde in performancecoverage.
Thes 86% precisionctic features, but.obtain extremelyof low coverage)most predictiveave had problemsalgorithm forions.r coverage overures consistentlying that syntacticIGR GR All-syntaxPoS Prec.
Cov.
F1 Prec.
Cov.
F1 Prec.
Cov.
F1A 81,6 21,8 29,2 70,1 65,4 55,4 70,7 68,9 57,7N 74,6 36,0 38,5 65,4 57,6 47,8 67,6 62,5 52,0V 68,6 32,2 33,4 67,3 41,2 39,2 66,3 52,7 45,4Ov.
72,9 31,9 35,2 67,1 52,1 46,0 67,7 59,5 50,4Table 1: precision and coverage for different sets of syntactic features (percentage).Syntax Local Local+Topical (Basic) Basic + SyntaxPoS MFS Dlist Boost Dlist Boost Dlist Boost Dlist BoostA 59,0 57,7 62,6 66,3 67,5 65,3 66,2 65,4     67,7N 57,1 52,0 60,0 63,6 65,3 63,2 67,9 63,3 69,3+V 40,3 45,4 48,5 51,6 50,1 51,0 51,6   51,2+ 53,9+Ov.
48,2 50,4 55,2 59,4 59,3 58,5 60,7 58,7 62,5+Table 2: F1 results (perc.)
for different feature sets.
?+?
indicates statistical significance over Basic.
between the two MLexplained by the fact thatve algorithm in the sensepositive information givent holds in the test examplef them are applicable).
Byf the predictions of severalrs (using both positive andBoost is able to assigno more test examples thanure space is more widelyat the classifiers are quiteves better recall levels andly better algorithm formethod obtains constant increasup to 93% precision with 7%feature selection method attainwith 26% coverage using syntathere is no further improvementIn this case Dlist is able togood accuracy rates (at the costrestricting to the use of thefeatures.
On the contrary, we hin adjusting the AdaBoostobtaining high precision predictThe figure also shows, fo20%, that the syntactic featallow for better results, confirmoverage WSD system.
features improve the results of the basic set.7.
Conclusions and further work.This paper shows that syntactic featureseffectively contribute to WSD precision.
Wehave extracted syntactic relations using theMinipar parser, but the results should be alsoapplicable to other parsers with similarperformance.
Two kinds of syntactic features aredefined: Instantiated Grammatical Relations(IGR) between words, and GrammaticalRelations (GR) coded as the presence ofadjuncts / arguments in isolation or assubcategorization frames.The experimental results were tried on theSenseval-2 data, comparing two different MLalgorithms (Dlist and Boost) trained both on abasic set of widely used features alone, and on acombination of basic and syntactic features.
Themain conclusions are the following:?
IGR get better precision than GR, but thebest precision/coverage combination(measured with F1) is attained by thecombination of both.?
Boost is able to profit from the addition ofsyntactic features, obtaining better resultsthan Dlist.
This proves that syntacticfeatures contain information that is notpresent in other traditional features.?
Overall the improvement is around twopoints for Boost, with highest increase forverbs.Several methods to exploit the precision-coverage trade-off where also tried:?
The results show that syntactic featuresconsistently improve the results on all datapoints except in the very low coveragerange, confirming the contribution of syntax.?
The results also show that Dlist are suited tobuild a system with high precision: either aprecision of 86% and a coverage of 26%, or95% precision and 8% coverage.Regarding future work, a thorough analysis ofthe quality of each of the syntactic relationsextracted should be performed.
In addition, aword-by-word analysis would be interesting, assome words might profit from specific syntacticfeatures, while others might not.
A preliminaryanalysis has been performed in (Agirre &Martinez, 2001b).Other parsers rather than Minipar could beused.
In particular, we found out that Miniparalways returns unambiguous trees, often makingerroneous attachment decisions.
A parserreturning ambiguous output could be moredesirable.
The results of this paper do notdepend on the parser used, only on the quality ofthe output, which should be at least as good asMinipar.Concerning the performance of the algorithmas compared to other Senseval 2 systems, it isnot the best.
Getting the best results was not theobjective of this paper, but to show that syntacticfeatures are worth including.
We plan toimprove the pre-processing of our systems, thedetection of multiword lexical entries, etc.
whichcould improve greatly the results.
In additionthere can be a number of factors that couldFigure 1: prec./cov.
curve for three high precision methods on basic and basic+syntactic features.0,500,550,600,650,700,750,800,850,900,951,000 0,2 0,4 0,6 0,8 1coverageprecisiondlist threshold basic dlist feat.sel.
basic boost basicdlist threshold basic+synt dlist feat.sel.
basic+synt boost basic+syntdiminish or disguise the improvement in theresults: hand-tagging errors, word sensesmissing from training or testing data, biasedsense distributions, errors in syntactic relations,etc.
Factor out this ?noise?
could show the realextent of the contribution of syntactic features.On the other hand, we are using a highnumber of features.
It is well known that manyML algorithms have problems to scale to highdimensional feature spaces, especially when thenumber of training examples is relatively low (asit is the case for Senseval-2 word senses).Researching on more careful feature selection(which is dependent of the ML algorithm) couldalso improve the contribution of syntacticfeatures, and WSD results in general.
Inaddition, alternative methods to produce a highprecision method based on Boost need to beexplored.Finally, the results on high precision WSDopen the avenue for acquiring further examplesin a bootstrapping framework.AcknowledgementsThis research has been partially funded by McyT(Hermes project TIC-2000-0335-C03-03).
DavidMartinez was funded by the BasqueGovernment, grant AE-BFI:01.245).ReferencesAgirre, E. and D. Martinez.
2001a.
Decision Lists forEnglish and Basque.
Proceedings of theSENSEVAL-2 Workshop.
In conjunction withACL'2001/EACL'2001.
Toulouse, France.Agirre, E. and D. Martinez.
2001b.
Analysis ofsupervised word sense disambiguation systems.
Int.report LSI 11-2001, available from the authors.Brants, T. 2000.
TnT - A Statistical Part-of-SpeechTagger.
In Proc.
of the Sixth Applied NaturalLanguage Processing Conference, Seattle, WA.Carroll, J. and E. Briscoe (2001) `High precisionextraction of grammatical relations'.
In Proceedingsof the 7th ACL/SIGPARSE International Workshopon Parsing Technologies, Beijing, China.
78-89.Collins M. 1996.
A new statistical parser based onbigram lexical dependencies.
In Proceedings of the34th Annual Meeting of the ACL, pages 184-191.Dagan I., and A. Itai.
1994.
Word SenseDisambiguation Using a Second LanguageMonolingual Corpus.
Computational Linguistics20:4, pp.
563--596.Freund Y. and R. E. Schapire.
1997.
A Decision-Theoretic Generalization of On-line Learning andan Application to Boosting.
Journal of Computerand System Sciences, 55(1):119--139.Escudero G., L. M?rquez, G. Rigau.
2000.
BoostingApplied to Word Sense Disambiguation.Proceedings of the 12th European Conference onMachine Learning, ECML 2000.
Barcelona, Spain.Kilgarriff, A. and M. Palmer.
(eds).
2000.
Specialissue on SENSEVAL.
Computer and theHumanities, 34 (1-2).Lin, D. 1993.
Principle Based parsing withoutOvergeneration.
In 31st Annual Meeting of theAssociation for Computational Linguistics.Columbus, Ohio.
pp 112-120.Ng, H. T. and H. B. Lee.
1996.
Integrating MultipleKnowledge Sources to Disambiguate Word Sense:An Exemplar-based Approach.
Proceedings of the34th Annual Meeting of the Association forComputational Linguistics.Preiss, J. and D. Yarowsky.
2001.
Proc.
of theSecond Intl.
Workshop on Evaluating Word SenseDisambiguation Systems (Senseval 2).
In conj.
withACL'2001/EACL'2001.
Toulouse, France.Schapire, R. E. and Y.
Singer.
1999.
ImprovedBoosting Algorithms Using Confidence-ratedPredictions.
Machine Learning, 37(3):297--336.Sleator, D. and D. Temperley.
1993.
Parsing Englishwith a Link Grammar.
Third InternationalWorkshop on Parsing Technologies.Stetina J., S. Kurohashi, M. Nagao.
1998.
GeneralWord Sense Disambiguation Method Based on aFull Sentential Context.
In Usage of WordNet inNatural Language Processing , Proceedings ofCOLING-ACL Workshop.
Montreal (Canada).Yarowsky, D. 1994.
Decision Lists for LexicalAmbiguity Resolution: Application to AccentRestoration in Spanish and French.
Proceedings ofthe 32nd Annual Meeting of the Association forComputational Linguistics, pp.
88--95.Appendix: main Minipar relations.Relation Direct Indirect Descriptionby-subj X  Subj.
with passivesC  X clausal complementCn  X nominalized clausecomp1 X  complement (PP, inf/fin clause) of nounDesc X  descriptionFc X  finite complementI  X see c and fc, dep.
between clause and main verbMod X  ModifierObj X  Objectpcomp-c X  clause of ppPcomp-n X  nominal head of ppPnmod X  postnominal modifier.Pred X  predicative (can be A or N)Sc X  sentential complementSubj X  subjectVrel X  passive verb modifier of nounsFor each relation the acronym, whether it is used as adirect relation or to construct indirect relations, and ashort description are provided.
