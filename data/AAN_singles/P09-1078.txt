Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 692?700,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPA Framework of Feature Selection Methods for Text CategorizationShoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang11Department of Chinese and BilingualStudiesThe Hong Kong Polytechnic University{shoushan.li,churenhuang}@gmail.com2National Laboratory of PatternRecognitionInstitute of AutomationChinese Academy of Sciences{rxia,cqzong}@nlpr.ia.ac.cnAbstractIn text categorization, feature selection (FS) isa strategy that aims at making text classifiersmore efficient and accurate.
However, whendealing with a new task, it is still difficult toquickly select a suitable one from various FSmethods provided by many previous studies.In this paper, we propose a theoreticframework of FS methods based on two basicmeasurements: frequency measurement andratio measurement.
Then six popular FSmethods are in detail discussed under thisframework.
Moreover, with the guidance ofour theoretical analysis, we propose a novelmethod called weighed frequency and odds(WFO) that combines the two measurementswith trained weights.
The experimental resultson data sets from both topic-based andsentiment classification tasks show that thisnew method is robust across different tasksand numbers of selected features.1 IntroductionWith the rapid growth of online information, textclassification, the task of assigning textdocuments to one or more predefined categories,has become one of the key tools forautomatically handling and organizing textinformation.The problems of text classification normallyinvolve the difficulty of extremely highdimensional feature space which sometimesmakes learning algorithms intractable.
Astandard procedure to reduce the featuredimensionality is called feature selection (FS).Various FS methods, such as documentfrequency (DF), information gain (IG), mutualinformation (MI), 2?
-test (CHI), Bi-NormalSeparation (BNS), and weighted log-likelihoodratio (WLLR), have been proposed for the tasks(Yang and Pedersen, 1997; Nigam et al, 2000;Forman, 2003) and make text classification moreefficient and accurate.However, comparing these FS methodsappears to be difficult because they are usuallybased on different theories or measurements.
Forexample, MI and IG are based on informationtheory, while CHI is mainly based on themeasurements of statistic independence.Previous comparisons of these methods havemainly depended on empirical studies that areheavily affected by the experimental sets.
As aresult, conclusions from those studies aresometimes inconsistent.
In order to betterunderstand the relationship between thesemethods, building a general theoreticalframework provides a fascinating perspective.Furthermore, in real applications, selecting anappropriate FS method remains hard for a newtask because too many FS methods are availabledue to the long history of FS studies.
Forexample, merely in an early survey paper(Sebastiani, 2002), eight methods are mentioned.These methods are provided by previous workfor dealing with different text classification tasksbut none of them is shown to be robust acrossdifferent classification applications.In this paper, we propose a framework withtwo basic measurements for theoreticalcomparison of six FS methods which are widelyused in text classification.
Moreover, a novelmethod is set forth that combines the twomeasurements and tunes their influencesconsidering different application domains andnumbers of selected features.The remainder of this paper is organized asfollows.
Section 2 introduces the related work on692feature selection for text classification.
Section 3theoretically analyzes six FS methods andproposes a new FS approach.
Experimentalresults are presented and analyzed in Section 4.Finally, Section 5 draws our conclusions andoutlines the future work.2 Related WorkFS is a basic problem in pattern recognition andhas been a fertile field of research anddevelopment since the 1970s.
It has been provento be effective on removing irrelevant andredundant features, increasing efficiency inlearning tasks, and improving learningperformance.FS methods fall into two broad categories, thefilter model and the wrapper model (John et al,1994).
The wrapper model requires onepredetermined learning algorithm in featureselection and uses its performance to evaluateand determine which features are selected.
Andthe filter model relies on general characteristicsof the training data to select some featureswithout involving any specific learningalgorithm.
There is evidence that wrappermethods often perform better on small scaleproblems (John et al 1994), but on large scaleproblems, such as text classification, wrappermethods are shown to be impractical because ofits high computational cost.
Therefore, in textclassification, filter methods using featurescoring metrics are popularly used.
Below wereview some recent studies of feature selectionon both topic-based and sentiment classification.In the past decade, FS studies mainly focus ontopic-based classification where the classificationcategories are related to the subject content, e.g.,sport or education.
Yang and Pedersen (1997)investigate five FS metrics and report that goodFS methods improve the categorization accuracywith an aggressive feature removal using DF, IG,and CHI.
More recently, Forman (2003)empirically compares twelve FS methods on 229text classification problem instances andproposes a new method called 'Bi-NormalSeparation' (BNS).
Their experimental resultsshow that BNS can perform very well in theevaluation metrics of recall rate and F-measure.But for the metric of precision, it often loses toIG.
Besides these two comparison studies, manyothers contribute to this topic (Yang and Liu,1999; Brank et al, 2002; Gabrilovich andMarkovitch, 2004) and more and more new FSmethods are generated, such as, Gini index(Shang et al, 2007), Distance to Transition Point(DTP) (Moyotl-Hernandez and Jimenez-Salazar,2005), Strong Class Information Words (SCIW)(Li and Zong, 2005) and parameter tuning basedFS for Rocchio classifier (Moschitti, 2003).Recently, sentiment classification has becomepopular because of its wide applications (Pang etal., 2002).
Its criterion of classification is theattitude expressed in the text (e.g., recommendedor not recommended, positive or negative) ratherthan some facts (e.g., sport or education).
To ourbest knowledge, yet no related work has focusedon comparison studies of FS methods on thisspecial task.
There are only some scatteredreports in their experimental studies.
Riloff et al(2006) report that the traditional FS method(only using IG method) performs worse than thebaseline in some cases.
However, Cui et al(2006) present the experiments on the sentimentclassification for large-scale online productreviews to show that using the FS method of CHIdoes not degrade the performance but cansignificantly reduce the dimension of the featurevector.Moreover, Ng et al (2006) examine the FS ofthe weighted log-likelihood ratio (WLLR) on themovie review dataset and achieves an accuracyof 87.1%, which is higher than the result reportedby Pang and Lee (2004) with the same dataset.From the analysis above, we believe that theperformance of the sentiment classificationsystem is also dramatically affected by FS.3 Our FrameworkIn the selection process, each feature (term, orsingle word) is assigned with a score accordingto a score-computing function.
Then those withhigher scores are selected.
These mathematicaldefinitions of the score-computing functions areoften defined by some probabilities which areestimated by some statistic information in thedocuments across different categories.
For theconvenience of description, we give somenotations of these probabilities below.
( )P t : the probability that a document x  containsterm t ;( )iP c : the probability that a document x  doesnot belong to category ic ;( , )iP t c : the joint probability that a document xcontains term t  and also belongs to category ic ;( | )iP c t : the probability that a document x belongsto category ic ?under the condition that it containsterm t.693( | )iP t c : the probability that, a document x doesnot contain term t with the condition that x belongs tocategory ic ;Some other probabilities, such as ( )P t , ( )iP c ,( | )iP t c , ( | )iP t c , ( | )iP c t ,  and ( | )iP c t , aresimilarly defined.In order to estimate these probabilities,statistical information from the training data isneeded, and notations about the training data aregiven as follows:1{ }mi ic = : the set of categories;iA : the number of the documents that contain theterm t  and also belong to category ic ;iB : the number of the documents that contain theterm t  but do not belong to category ic ;iN : the total number of the documents that belongto category ic ;allN : the total number of all documents from thetraining data.iC : the number of the documents that do notcontain the term t  but belong to category ic , i.e.,i iN A?iD : the number of the documents that neithercontain the term t  nor belong to category ic , i.e.,all i iN N B?
?
;In this section, we would analyze theoreticallysix popular methods, namely DF, MI, IG, CHI,BNS, and WLLR.
Although these six FSmethods are defined differently with differentscoring measurements, we believe that they arestrongly related.
In order to connect them, wedefine two basic measurements which arediscussed as follows.The first measurement is to compute thedocument frequency in one category, i.e., iA .The second measurement is the ratio betweenthe document frequencies in one category andthe other categories, i.e., /i iA B .
The terms witha high ratio are often referred to as the terms withhigh category information.These two measurements form the basis for allthe measurements that are used by the FSmethods throughout this paper.
In particular, weshow that DF and MI are using the first andsecond measurement respectively.
Othercomplicated FS methods are combinations ofthese two measurements.
Thus, we regard thetwo measurements as basic, which are referred toas the frequency measurement and ratiomeasurement.3.1 Document Frequency (DF)DF is the number of documents in which a termoccurs.
It is defined as1( )m iiDF A==?The terms with low or high documentfrequency are often referred to as rare orcommon terms, respectively.
It is easy to see thatthis FS method is based on the first basicmeasurement.
It assumes that the terms withhigher document frequency are more informativefor classification.
But sometimes this assumptiondoes not make any sense, for example, the stopwords (e.g., the, a, an) hold very high DF scores,but they seldom contribute to classification.
Ingeneral, this simple method performs very wellin some topic-based classification tasks (Yangand Pedersen, 1997).3.2 Mutual Information (MI)The mutual information between term t  andclass ic  is defined as( | )( , ) log ( )iiP t cI t cP t=And it is estimated aslog ( )( )i alli i i iA NMIA C A B?=+ +Let us consider the following formula (usingBayes theorem)( | ) ( | )( , ) log log( ) ( )i iiiP t c P c tI t cP t P c= =Therefore,( , )= log ( | ) log ( )i i iI t c P c t P c?And it is estimated aslog loglog log1log(1 ) log/i ii i alli i ii allii i allA NMIA B NA B NA NNA B N= ?++= ?
?= ?
+ ?From this formula, we can see that the MI scoreis based on the second basic measurement.
Thismethod assumes that the term with highercategory ratio is more effective for classification.It is reported that this method is biasedtowards low frequency terms and the biasbecomes extreme when ( )P t  is near zero.
It canbe seen in the following formula (Yang andPedersen, 1997)( , ) log( ( | )) log( ( ))i iI t c P t c P t= ?694Therefore, this method might perform badlywhen common terms are informative forclassification.Taking into account mutual information of allcategories, two types of MI score are commonlyused: the maximum score ( )maxI t  and theaverage score ( )avgI t , i.e.,1( ) max { ( , )}mmax i iI t I t c== ,1( ) ( ) ( , )mavg i iiI t P c I t c== ??
.We choose the maximum score since it performsbetter than the average score (Yang and Pedersen,1997).
It is worth noting that the same choice ismade for other methods, including CHI, BNS,and WLLR in this paper.3.3 Information Gain (IG)IG measures the number of bits of informationobtained for category prediction by recognizingthe presence or absence of a term in a document(Yang and Pedersen, 1997).
The function is111( ) { ( ) log ( )}+{ ( )[ ( | ) log ( | )]( )[ ( | ) log ( | )]}mi iimi iimi iiG t P c P cP t P c t P c tP t P c t P c t==== ?+??
?And it is estimated as11 11 1{ log }+( / )[ log ]( / )[ log ]m i iiall allm m i ii alli ii i i im m i ii alli ii i i iN NIGN NA AA NA B A BC CC NC D C D== == == ?+ +++ +??
??
?From the definition, we know that theinformation gain is the weighted average of themutual information ( , )iI t c and ( , )iI t c  wherethe weights are the joint probabilities ( , )iP t c and( , )iP t c :1 1( ) ( , ) ( , ) ( , ) ( , )m mi i i ii iG t P t c I t c P t c I t c= == +?
?Since ( , )iP t c is closely related to thedocument frequency iA  and the mutualinformation ( , )iI t c  is shown to be based on thesecond measurement, we can say that the IGscore is influenced by the two basicmeasurements.3.4 2?
Statistic (CHI)The CHI measurement (Yang and Pedersen,1997) is defined as2( )( ) ( ) ( ) ( )all i i i ii i i i i i i iN A D C BCHIA C B D A B C D?
?=+ ?
+ ?
+ ?
+In order to get the relationship between CHIand the two measurements, the above formula isrewritten as follows2[ ( ) ( ) ]( ) ( ) [ ( )]all i all i i i i ii all i i i all i iN A N N B N A BCHIN N N A B N A B?
?
?
?
?=?
?
?
+ ?
?
+For simplicity, we assume that there are twocategories and the numbers of the trainingdocuments in the two categories are the same( 2all iN N= ).
The CHI score then can be writtenas222 ( )( ) [2 ( )]2 ( / 1)2( / 1) [ / ( / 1)]i i ii i i i ii i iii i i i i iiN A BCHIA B N A BN A BNA B A B A BA?=+ ?
?
+?=+ ?
?
?
+From the above formula, we see that the CHIscore is related to both the frequencymeasurement iAand ratio measurement/i iA B .
Also, when keeping the same ratio value,the terms with higher document frequencies willyield higher CHI scores.3.5 Bi-Normal Separation (BNS)BNS method is originally proposed by Forman(2003) and it is defined as1 1( , ) ( ( | )) ( ( | )i i iBNS t c F P t c F P t c?
?= ?It is calculated using the following formula1 1( ) ( )i ii all iA BBNS F FN N N?
?= ?
?where ( )F x  is the cumulative probabilityfunction of standard normal distribution.For simplicity, we assume that there are twocategories and the numbers of the trainingdocuments in the two categories are the same,i.e., 2all iN N=  and we also assume that i iA B> .It should be noted that this assumption is only toallow easier analysis but will not be applied inour experiment implementation.
In addition, weonly consider the case when / 0.5i iA N ?
.
Infact, most terms take the document frequencyiA which is less than half of iN .Under these conditions, the BNS score can beshown in Figure 1 where the area of the shadowpart represents ( / / )i i i iA N B N?
and the lengthof the projection to the x  axis represents theBNS score.695From Figure 1, we can easily draw the twofollowing conclusions:1) Given the same value of iA , the BNS scoreincreases with the increase of i iA B?
.2) Given the same value of i iA B?
, BNS scoreincrease with the decrease of iA .Figure 1.
View of BNS using the normal probabilitydistribution.
Both the left and right graphs haveshadowed areas of the same size.And the value of i iA B?
can be rewritten asthe following1(1 )/i ii i i ii i iA BA B A AA A B??
= ?
= ?
?The above analysis gives the followingconclusions regarding the relationship betweenBNS and the two basic measurements:1) Given the same iA , the BNS score increaseswith the increase of /i iA B .2) Given the same /i iA B , when iA  increases,i iA B?
also increase.
It seems that the BNSscore does not show a clear relationship withiA .In summary, the BNS FS method is biasedtowards the terms with the high category ratiobut cannot be said to be sensitive to documentfrequency.3.6 Weighted Log Likelihood Ratio(WLLR)WLLR method (Nigam et al, 2000) is defined as( | )( , ) ( | ) log ( | )ii iiP t cWLLR t c P t cP t c=And it is estimated as( )logi i all ii i iA A N NWLLRN B N?
?=?The formula shows WLLR is proportional tothe frequency measurement and the logarithm ofthe ratio measurement.
Clearly, WLLR is biasedtowards the terms with both high category ratioand high document frequency and the frequencymeasurement seems to take a more importantplace than the ratio measurement.3.7 Weighed Frequency and Odds (WFO)So far in this section, we have shown that thetwo basic measurements constitute the six FSmethods.
The class prior probabilities,( ),  1,2,...,iP c i m= , are also related to theselection methods except for the two basicmeasurements.
Since they are often estimatedaccording to the distribution of the documents inthe training data and are identical for all theterms in a class, we ignore the discussion of theirinfluence on the selection measurements.
In theexperiment, we consider the case when trainingdata have equal class prior probabilities.
Whentraining data are unbalanced, we need to changethe forms of the two basic measurements to/i iA N  and ( ) / ( )i all i i iA N N B N?
?
?
.Because some methods are expressed incomplex forms, it is difficult to explain theirrelationship with the two basic measurements,for example, which one prefers the category ratiomost.
Instead, we will give the preferenceanalysis in the experiment by analyzing thefeatures in real applications.
But the followingtwo conclusions are drawn without doubtaccording to the theoretical analysis given above.1) Good features are features with highdocument frequency;2) Good features are features with highcategory ratio.These two conclusions are consistent with theoriginal intuition.
However, using any single onedoes not provide competence in selecting thebest set of features.
For example, stop words,such as ?a?, ?the?
and ?as?, have very highdocument frequency but are useless for theclassification.
In real applications, we need tomix these two measurements to select goodfeatures.
Because of different distribution offeatures in different domains, the importance ofeach measurement may differ a lot in differentapplications.
Moreover, even in a given domain,when different numbers of features are to beselected, different combinations of the twomeasurements are required to provide the bestperformance.Although a great number of FS methods isavailable, none of them can appropriately changethe preference of the two measurements.
A betterway is to tune the importance according to theapplication rather than to use a predeterminedcombination.
Therefore, we propose a new FSmethod called Weighed Frequency and Odds(WFO), which is defined as696( | ) / ( | ) 1i iwhen P t c P t c >1( | )( , ) ( | ) [log ]( | )ii iiP t cWFO t c P t cP t c?
?
?=( , ) 0ielseWFO t c =And it is estimated as1( )( ) (log )i i all ii i iA A N NWFON B N?
???
?=?where ?is the parameter for tuning the weightbetween frequency and odds.
The value of ?varies from 0 to 1.
By assigning different valueto ?
we can adjust the preference of eachmeasurement.
Specially, when 0?
= , thealgorithm prefers the category ratio that isequivalent to the MI method; when 1?
= , thealgorithm is similar to DF; when 0.5?
= , thealgorithm is exactly the WLLR method.
In realapplications, a suitable parameter ?
needs to belearned by using training data.4 Experimental Studies4.1 Experimental SetupData Set:  The experiments are carried out onboth topic-based and sentiment text classificationdatasets.
In topic-based text classification, weuse two popular data sets: one subset ofReuters-21578 referred to as R2 and the 20Newsgroup dataset referred to as 20NG.
In detail,R2 consist of about 2,000 2-category documentsfrom standard corpus of Reuters-21578.
And20NG is a collection of approximately 20,00020-category documents 1 .
In sentiment textclassification, we also use two data sets: one isthe widely used Cornell movie-review dataset2(Pang and Lee, 2004) and one dataset fromproduct reviews of domain DVD3 (Blitzer et al,2007).
Both of them are 2-category tasks andeach consists of 2,000 reviews.
In ourexperiments, the document numbers of all datasets are (nearly) equally distributed cross allcategories.Classification Algorithm: Manyclassification algorithms are available for textclassification, such as Na?ve Bayes, MaximumEntropy, k-NN, and SVM.
Among these methods,SVM is shown to perform better than othermethods (Yang and Pedersen, 1997; Pang et al,1http://people.csail.mit.edu/~jrennie/20Newsgroups/2 http://www.cs.cornell.edu/People/pabo/movie-review-data/3http://www.seas.upenn.edu/~mdredze/datasets/sentiment/2002).
Hence we apply SVM algorithm with thehelp of the LIBSVM 4  tool.
Almost allparameters are set to their default values exceptthe kernel function which is changed from apolynomial kernel function to a linear onebecause the linear one usually performs better fortext classification tasks.Experiment Implementation: In theexperiments, each dataset is randomly andevenly split into two subsets: 90% documents asthe training data and the remaining 10% astesting data.
The training data are used fortraining SVM classifiers, learning parameters inWFO method and selecting "good" features foreach FS method.
The features are single wordswith a bool weight (0 or 1), representing thepresence or absence of a feature.
In addition tothe ?principled?
FS methods, terms occurring inless than three documents ( 3DF ? )
in thetraining set are removed.4.2 Relationship between FS Methods andthe Two Basic MeasurementsTo help understand the relationship between FSmethods and the two basic measurements, theempirical study is presented as follows.Since the methods of DF and MI only utilizethe document frequency and categoryinformation respectively, we use the DF scoresand MI scores to represent the information of thetwo basic measurements.
Thus we would selectthe top-2% terms with each method and theninvestigate the distribution of their DF and MIscores.First of all, for clear comparison, wenormalize the scores coming from all themethods using Min-Max normalization methodwhich is designed to map a score s  to 's  inthe range [0, 1] by computing's MinsMax Min?=?whereMinand Maxdenote the minimumand maximum values respectively in all terms?scores using one FS method.Table 1 shows the mean values of all top-2%terms?
MI scores and DF scores of all the six FSmethods in each domain.
From this table, we canapparently see the relationship between eachmethod and the two basic measurements.
Forinstance, BNS most distinctly prefers the termswith high MI scores and low DF scores.According to the degree of this preference, we4http://www.csie.ntu.edu.tw/~cjlin/libsvm/697FSMethodsDomain20NG R2 Movie DVDDF score MI score DF score MI score DF score MI score DF score MI scoreMI 0.004 0.870 0.047 0.959 0.003 0.888 0.004 0.881BNS 0.005 0.864 0.117 0.922 0.008 0.881 0.006 0.880CHI 0.015 0.814 0.211 0.748 0.092 0.572 0.055 0.676IG 0.087 0.525 0.209 0.792 0.095 0.559 0.066 0.669WLLR 0.026 0.764 0.206 0.805 0.168 0.414 0.127 0.481DF 0.122 0.252 0.268 0.562 0.419 0.09 0.321 0.111Table 1.
The mean values of all top-2% terms?
MI and DF scores using six FS methods in each domaincan rank these six methods asMI, BNS IG, CHI, WLLR DFf f , where x yfmeans method xprefers the terms withhigher MI scores (higher category information)and lower DF scores (lower document frequency)than method y.
This empirical discovery is inagreement with the finding that WLLR is biasedtowards the high frequency terms and also withthe finding that BNS is biased towards highcategory information (cf.
Section 3 theoreticalanalysis).
Also, we can find that CHI and IGshare a similar preference of these twomeasurements in 2-category domains, i.e., R2,movie, and DVD.
This gives a good explanationthat CHI and IG are two similar-performedmethods for 2-category tasks, which have beenfound by Forman (2003) in their experimentalstudies.According to the preference, we roughlycluster FS methods into three groups.
The firstgroup includes the methods which dramaticallyprefer the category information, e.g., MI andBNS; the second one includes those which preferboth kinds of information, e.g., CHI, IG, andWLLR; and the third one includes those whichstrongly prefer frequency information, e.g., DF.4.3 Performances of Different FS MethodsIt is worth noting that learning parameters inWFO is very important for its good performance.We use 9-fold cross validation to help learningthe parameter ?
so as to avoid over-fitting.Specifically, we run nine times by using every 8fold documents as a new training data set and theremaining one fold documents as a developmentdata set.
In each running with one fixed featurenumber m, we get the best,i m best?
?
(i=1,..., 9)value through varying,i m?
from 0 to 1 with thestep of 0.1 to get the best performance in thedevelopment data set.
The average valuem best?
?
,i.e.,9,1( ) / 9m best i m besti?
??
?== ?is used for further testing.Figure 2 shows the experimental results whenusing all FS methods with different selectedfeature numbers.
The red line with star tagsrepresents the results of WFO.
At the first glance,in R2 domain, the differences of performancesacross all are very noisy when the featurenumber is larger than 1,000, which makes thecomparison meaningless.
We think that this isbecause the performances themselves in this taskare very high (nearly 98%) and the differencesbetween two FS methods cannot be very large(less than one percent).
Even this, WFO methoddo never get the worst performance and can alsoachieve the top performance in about half times,e.g., when feature numbers are 20, 50, 100, 500,3000.Let us pay more attention to the other threedomains and discuss the results in the followingtwo cases.In the first case when the feature number islow (about less than 1,000), the FS methods inthe second group including IG, CHI, WLLR,always perform better than those in the other twogroups.
WFO can also perform well because itsparametersm best?
?
are successfully learned to bearound 0.5, which makes it consistently belongto the second group.
Take 500 feature numberfor instance, the parameters 500 best?
?
are 0.42,0.50, and 0.34 in these three domainsrespectively.In the second case when the feature number islarge, among the six traditional methods, MI andBNS take the leads in the domains of 20NG andMovie while IG and CHI seem to be better andmore stable than others in the domain of DVD.As for WFO, its performances are excellent crossall these three domains and different featurenumbers.
In each domain, it performs similarlyas or better than the top methods due to itswell-learned parameters.
For example, in 20NG,the parametersm best?
?
are 0.28, 0.20, 0.08, and0.01 when feature numbers are 10,000, 15,000,20,000, and 30,000.
These values are close to 0698(WFO equals MI when 0?
= ) while MI is thetop one in this domain.10 20 50 100 200 500 1000 2000 3000 42270.880.90.920.940.960.981feature numberaccuracyTopic - R2DFMIIGBNSCHIWLLRWFO200 500 1000 2000 5000 10000 15000 20000 30000 320910.50.550.60.650.70.750.80.850.9feature numberaccuracyTopic - 20NGDFMIIGBNSCHIWLLRWFO50 200 500 1000 4000 7000 10000 13000 151760.550.60.650.70.750.80.85feature numberaccuracySentiment - MovieDFMIIGBNSCHIWLLRWFO20 50 100 500 1000 1500 2000 3000 4000 58240.50.550.60.650.70.750.8feature numberaccuracySentiment - DVDDFMIIGBNSCHIWLLRWFOFigure 2.
The classification accuracies of the four domainsusing seven different FS methods while increasing thenumber of selected features.From Figure 2, we can also find that FS doeshelp sentiment classification.
At least, it candramatically decrease the feature numberswithout losing classification accuracies (seeMovie domain, using only 500-4000 features isas good as using all 15176 features).5 Conclusion and Future WorkIn this paper, we propose a framework with twobasic measurements and use it to theoreticallyanalyze six FS methods.
The differences amongthem mainly lie in how they use these twomeasurements.
Moreover, with the guidance ofthe analysis, a novel method called WFO isproposed, which combine these twomeasurements with trained weights.
Theexperimental results show that our frameworkhelps us to better understand and comparedifferent FS methods.
Furthermore, the novelmethod WFO generated from the framework, canperform robustly across different domains andfeature numbers.In our study, we use four data sets to test ournew method.
There are much more data sets ontext categorization which can be used.
Inadditional, we only focus on using balancedsamples in each category to do the experiments.It is also necessary to compare the FS methodson some unbalanced data sets, which arecommon in real-life applications (Forman, 2003;Mladeni and Marko, 1999).
These matters willbe dealt with in the future work.AcknowledgmentsThe research work described in this paper hasbeen partially supported by Start-up Grant forNewly Appointed Professors, No.
1-BBZM in theHong Kong Polytechnic University.ReferencesJ.
Blitzer, M. Dredze, and F. Pereira.
2007.Biographies, Bollywood, Boom-boxes andBlenders: Domain adaptation for sentimentclassification.
In Proceedings of ACL-07, the 45thMeeting of the Association for ComputationalLinguistics.J.
Brank, M. Grobelnik, N. Milic-Frayling, and D.Mladenic.
2002.
Interaction of feature selectionmethods and linear classification models.
InWorkshop on Text Learning held at ICML.H.
Cui, V. Mittal, and M. Datar.
2006.
Comparativeexperiments on sentiment classification for onlineproduct reviews.
In Proceedings of AAAI-06, the21st National Conference on Artificial Intelligence.G.
Forman.
2003.
An extensive empirical study offeature selection metrics for text classification.
TheJournal of Machine Learning Research, 3(1):1289-1305.699E.
Gabrilovich and S. Markovitch.
2004.
Textcategorization with many redundant features: usingaggressive feature selection to make SVMscompetitive with C4.5.
In Proceedings of the ICML,the 21st International Conference on MachineLearning.G.
John, K. Ron, and K. Pfleger.
1994.
Irrelevantfeatures and the subset selection problem.
InProceedings of ICML-94, the 11st InternationalConference on Machine Learning.S.
Li and C. Zong.
2005.
A new approach to featureselection for text categorization.
In Proceedings ofthe IEEE International Conference on NaturalLanguage Processing and Knowledge Engineering(NLP-KE).D.
Mladeni and G. Marko.
1999.
Feature selection forunbalanced class distribution and naive bayes.
InProceedings of ICML-99, the 16th InternationalConference on Machine Learning.A.
Moschitti.
2003.
A study on optimal parametertuning for Rocchio text classifier.
In Proceedingsof ECIR, Lecture Notes in Computer Science,vol.
2633, pp.
420-435.E.
Moyotl-Hernandez and H. Jimenez-Salazar.
2005.Enhancement of DTP feature selection method fortext categorization.
In Proceedings of CICLing,Lecture Notes in Computer Science, vol.3406,pp.719-722.V.
Ng, S. Dasgupta, and S. M. Niaz Arifin.
2006.Examining the role of linguistic knowledge sourcesin the automatic identification and classification ofreviews.
In Proceedings of the COLING/ACL MainConference Poster Sessions.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.2000.
Text classification from labeled andunlabeled documents using EM.
Machine Learning,39(2/3): 103-134.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machinelearning techniques.
In Proceedings of EMNLP-02,the Conference on Empirical Methods in NaturalLanguage Processing.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivitysummarization based on minimum cuts.
InProceedings of ACL-04, the 42nd Meeting of theAssociation for Computational Linguistics.E.
Riloff, S. Patwardhan, and J. Wiebe.
2006.
Featuresubsumption for opinion analysis.
In Proceedingsof EMNLP-06, the Conference on EmpiricalMethods in Natural Language Processing,.F.
Sebastiani.
2002.
Machine learning in automatedtext categorization.
ACM Computing Surveys,34(1): 1-47.W.
Shang, H. Huang, H. Zhu, Y. Lin, Y. Qu, and Z.Wang.
2007.
A novel feature selection algorithmfor text categorization.
The Journal of ExpertSystem with Applications, 33:1-5.Y.
Yang and J. Pedersen.
1997.
A comparative studyon feature selection in text categorization.
InProceedings of ICML-97, the 14th InternationalConference on Machine Learning.Y.
Yang and X. Liu.
1999.
A re-examination of textcategorization methods.
In Proceedings ofSIGIR-99, the 22nd annual international ACMConference on Research and Development inInformation Retrieval.700
