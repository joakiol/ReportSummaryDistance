Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 163?169,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAppropriately Incorporating Statistical Significance in PMIOm P. Damani and Shweta GhongeIIT BombayIndia{damani,shwetaghonge}@cse.iitb.ac.inAbstractTwo recent measures incorporate the notion ofstatistical significance in basic PMI formula-tion.
In some tasks, we find that the new mea-sures perform worse than the PMI.
Our anal-ysis shows that while the basic ideas in incor-porating statistical significance in PMI are rea-sonable, they have been applied slightly inap-propriately.
By fixing this, we get new mea-sures that improve performance over not justPMI but on other popular co-occurrence mea-sures as well.
In fact, the revised measuresperform reasonably well compared with moreresource intensive non co-occurrence basedmethods also.1 IntroductionThe notion of word association is used in many lan-guage processing and information retrieval appli-cations and it is important to have low-cost, high-quality association measures.
Lexical co-occurrencebased word association measures are popular be-cause they are computationally efficient and they canbe applied to any language easily.
One of the mostpopular co-occurrence measure is Pointwise MutualInformation (PMI) (Church and Hanks, 1989).One of the limitations of PMI is that it only workswith relative probabilities and ignores the absoluteamount of evidence.
To overcome this, recently twonew measures have been proposed that incorporatethe notion of statistical significance in basic PMIformulation.
In (Washtell and Markert, 2009), sta-tistical significance is introduced in PMIsig by mul-tiplying PMI value with the square root of the ev-idence.
In contrast, in (Damani, 2013), cPMId isintroduced by bounding the probability of observinga given deviation between a given word pair?s co-occurrence count and its expected value under a nullmodel where with each word a global unigram gen-eration probability is associated.
In Table 1, we givethe definitions of PMI, PMIsig, and cPMId.While these new measures perform better thanPMI on some of the tasks, on many other tasks,we find that the new measures perform worse thanthe PMI.
In Table 3, we show how these measuresperform compared to PMI on four different tasks.We find that PMIsig degrades performance in threeout of these four tasks while cPMId degrades per-formance in two out of these four tasks.
The ex-perimental details and discussion are given in Sec-tion 4.2.Our analysis shows that while the basic ideas inincorporating statistical significance are reasonable,they have been applied slightly inappropriately.
Byfixing this, we get new measures that improve per-formance over not just PMI, but also on other pop-ular co-occurrence measures on most of these tasks.In fact, the revised measures perform reasonablywell compared with more resource intensive non co-occurrence based methods also.2 Adapting PMI for Statistical SignificanceIn (Washtell and Markert, 2009), it is assumed thatthe statistical significance of a word pair associationis proportional to the square root of the evidence.The question of what constitutes the evidence is an-swered by taking the lesser of the frequencies of thetwo words in the word pair, since at most that manypairings are possible.
Hence the PMI value is multi-163Method Formula Revised FormulaPMI (Church andHanks, 1989)log f(x,y)f(x)?f(y)/WPMIsig (Washtelland Markert, 2009)log f(x,y)f(x)?f(y)/W ?
?min(f(x),f(y)) PMIs: log f(x,y)f(x)?f(y)/W ?
?max(f(x),f(y))cPMId (Damani,2013)log d(x,y)d(x)*d(y)/D+?d(x)?
?ln ?
(?2.0)sPMId: log d(x,y)max(d(x),d(y))*min(d(x),d(y))/D+?max(d(x),d(y))?
?ln ?
(?2.0)Terminology:W Total number of words in the corpus D Total number of documents in the corpusf(x), f(y) unigram frequencies of x, y respectively in the corpus d(x), d(y) Total number of documents in the corpus containingat least one occurrence of x and y respectivelyf(x, y) Span-constrained (x, y) word pair frequency in the corpus d(x, y) Total number of documents in the corpus having at-leastone span-constrained occurrence of the word pair (x, y)?
a parameter varying between 0 and 1Table 1: Definitions of PMI and its statistically significant adaptations.
The sub-parts in bold represent the changesbetween the original formulas and the revised formulas.
The product max(d(x), d(y)) ?min(d(x), d(y)) in sPMIdformula can be simplified to f(x) ?
f(y), however, we left it this way to emphasize the transformation from cPMId.plied by?min(f(x), f(y)) to get PMIsig.In (Damani, 2013), statistical significance isintroduced by bounding the probability of observinga given number of word-pair occurrences in thecorpus, just by chance, under a null model of inde-pendent unigram occurrences.
For this computation,one needs to decide what constitutes a random trialwhen looking for a word-pair occurrence.
Is it theoccurrence of the first word (say x) in the pair, orthe second (say y).
In (Damani, 2013), occurrencesof x are arbitrarily chosen to represent the sites ofthe random trial.
Using Hoeffdings Inequality:P [f(x, y) ?
f(x) ?
f(y)/W + f(x) ?
t]?
exp(?2 ?
f(x) ?
t2)By setting t =?ln ?/(?2 ?
f(x)), we get ?
as anupper bound on probability of observing more thanf(x)?f(y)/W +f(x)?
t bigram occurrences in thecorpus, just by chance.
Based on this Corpus LevelSignificant PMI(cPMI) is defined as:cPMI(x, y) = logf(x, y)f(x) ?
f(y)/W + f(x) ?
t= logf(x, y)f(x) ?
f(y)/W +?f(x) ?
?ln ?/(?2)In (Damani, 2013), several variants of cPMI are in-troduced that incorporate different notions of sta-tistical significance.
Of these Corpus Level Signif-icant PMI based on Document count(cPMId - de-fined in Table 1) is found to be the best performing,and hence we consider this variant only in this work.2.1 Choice of Random TrialWhile considering statistical significance, one hasto decide what constitutes a random trial.
Whenlooking for a word-pair (x, y)?s occurrences, y canpotentially occur near each occurrence of x, or xcan potentially occur near each occurrence of y.Which of these two set of occurrences should beconsidered the sites of random trial.
We believethat the occurrences of the more frequent of x and yshould be considered, since near each of these occur-rences the other word could have occurred.
Hencef(x) and f(y) in cPMI definition should be re-placed with max(f(x), f(y)) and min(f(x), f(y))respectively.
Similarly, d(x) and d(y) in cPMId for-mula should be replaced with max(d(x), d(y)) andmin(d(x), d(y)) respectively to give a new measureSignificant PMI based on Document count(sPMId).Using the same logic,?min(f(x), f(y))in PMIsig formula should be replaced with?max(f(x), f(y)) to give the formula for a newmeasure PMI-significant(PMIs).
The definitions ofsPMId and PMIs are also given in Table 1.3 Related WorkThere are three main types of word association mea-sures: Knowledge based, Distributional Similaritybased, and Lexical Co-occurrence based.Based on Firth?s You shall know a word by thecompany it keeps (Firth, 1957), distributional sim-ilarity based measures characterize a word by thedistribution of other words around it and compare164Method FormulaChiSquare (?2)?i,j(f(i,j)?Ef(i,j))2Ef(i,j)Dice (Dice, 1945) f(x,y)f(x)+f(y)GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))?log d(x,y)logD?min(log d(x),log d(y))Jaccard (Jaccard, 1912) f(x,y)f(x)+f(y)?f(x,y)LLR (Dunning, 1993)?x?
?
{x,?x}y?
?
{y,?y}f(x?, y?
)log f(x?,y?)f(x?)f(y?
)nPMI (Bouma, 2009)log f(x,y)f(x)?f(y)/Wlog 1f(x,y)/WOchiai (Janson and Vegelius, 1981) f(x,y)?f(x)f(y)PMI2 (Daille, 1994) logf(x,y)f(x)?f(y)/W1f(x,y)/W= log f(x,y)2f(x)?f(y)Simpson (Simpson, 1943) f(x,y)min(f(x),f(y))SCI (Washtell and Markert, 2009) f(x,y)f(x)?f(y)T-test f(x,y)?Ef(x,y)?f(x,y)(1?
f(x,y)W )Table 2: Definition of other co-occurrence measures being compared in this work.
The terminology used here is sameas that in Table 1, except that E in front of a variable name means the expected value of that variable.TaskSemantic Sentence SynonymRelatedness Similarity SelectionDataset WordSim Li ESL TOEFLMetric Spearman RankCorrelationPearson Cor-relationFractionCorrectFractionCorrectPMI 0.68 0.69 0.62 0.59PMIsig 0.67 0.85 0.58 0.56cPMId 0.72 0.67 0.56 0.59PMIs 0.66 0.85 0.66 0.61sPMId 0.72 0.75 0.70 0.61ChiSquare (?2) 0.62 0.80 0.62 0.58Dice 0.58 0.76 0.56 0.57GoogleDistance 0.53 0.75 0.09 0.19Jaccard 0.58 0.76 0.56 0.57LLR 0.50 0.18 0.18 0.27nPMI 0.72 0.35 0.54 0.54Ochiai/ PMI2 0.62 0.77 0.62 0.60SCI 0.65 0.85 0.62 0.60Simpson 0.59 0.78 0.58 0.57TTest 0.44 0.63 0.44 0.52Semantic Net (Li et al 2006) 0.82ESA (Gabrilovich and Markovitch, 2007) 0.74(reimplemented in (Yeh et al 2009)) 0.71Distributional Similarity (on web corpus) (Agirre etal., 2009))0.65Context Window based Distributional Similar-ity (Agirre et al 2009))0.60Latent Semantic Analysis (on web corpus) (Finkel-stein et al 2002)0.56WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87PMI-IR3 (using context) (Turney, 2001) 0.73Table 3: 5-fold cross-validation results for different co-occurrence measures.
The results for the best, and second bestco-occurrence measures for each data-set is shown in bold and underline respectively.
Except GoogleDistance andLLR, all results for all co-occurrence measures are statistically significant at p = .05.
For each task, the best knownresult for different non co-occurrence based methods is also shown.165two words for distributional similarity (Agirre etal., 2009; Wandmacher et al 2008; Bollegala etal., 2007; Chen et al 2006).
They are also usedfor modeling the meaning of a phrase or a sen-tence (Grefenstette and Sadrzadeh, 2011; Wartena,2013; Mitchell, 2011; G. Dinu and Baroni, 2013;Kartsaklis et al 2013).Knowledge-based measures use knowledge-sources like thesauri, semantic networks, ortaxonomies (Milne and Witten, 2008; Hughesand Ramage, 2007; Gabrilovich and Markovitch,2007; Yeh et al 2009; Strube and Ponzetto, 2006;Finkelstein et al 2002; Liberman and Markovitch,2009).Co-occurrence based measures (Pecina andSchlesinger, 2006) simply rely on unigram and bi-gram frequencies of the words in a pair.
In this work,our focus is on the co-occurrence based measures,since they are resource-light and can easily be usedfor resource-scarce languages.3.1 Co-occurrence Measures being ComparedCo-occurrence based measures of association be-tween two entities are used in several domains likeecology, psychology, medicine, language process-ing, etc.
To compare the performance of our newlyintroduced measures with other co-occurrence mea-sures, we have selected a number of popu-lar co-occurrence measures like ChiSquare (?2),Dice (Dice, 1945), GoogleDistance (L.Cilibrasi andVitany, 2007), Jaccard (Jaccard, 1912), LLR (Dun-ning, 1993), Simpson (Simpson, 1943), and T-testfrom these domains.In addition to these popular measures, wealso experiment with other known variations ofPMI like nPMI (Bouma, 2009), PMI2 (Daille,1994), Ochiai (Janson and Vegelius, 1981), andSCI (Washtell and Markert, 2009).
Since PMI2 isa monotonic transformation of Ochiai, we presenttheir results together.
In Table 2, we present the def-initions of these measures.
While the motivationgiven for SCI in (Washtell and Markert, 2009) isslightly different, in light of the discussion in Sec-tion 2.1, we can assume that SCI is PMI adapted forstatistical significance (multiplied by?f(y)), wherethe site of random trial is taken to be the occurrencesof the second word y, instead of the less frequentword, as in the case of PMIsig.When counting co-occurrences, we only con-sider the non-overlapping span-constrained occur-rences.
The span of a word-pair?s occurrence is thedirection-independent distance between the occur-rences of the members of the pair.
We consider onlythose co-occurrences where span is less than a giventhreshold.
Therefore, span threshold is a parameterfor all the co-occurrence measures being considered.4 Performance EvaluationHaving introduced the revised measures PMIs andsPMId, we need to evaluate the performance of thesemeasures compared to PMI and the original mea-sures introducing significance.
In addition, we alsowish to compare the performance of these measureswith other co-occurrence measures.
To compare theperformance of these measures with more resourceheavy non co-occurrence based measures, we havechosen those tasks and datasets on which publishedresults exist for distributional similarity and knowl-edge based word association measures.4.1 Task DetailsWe evaluate these measures on three tasks: Sen-tence Similarity(65 sentence-pairs from (Li et al2006)), Synonym Selection(50 questions ESL (Tur-ney, 2001) and 80 questions TOEFL (Landauer andDutnais, 1997) datasets), and, Semantic Related-ness(353 words Wordsim (Finkelstein et al 2002)dataset).For each of these tasks, gold standard humanjudgment results exist.
For sentence similarity, fol-lowing (Li et al 2006), we evaluate a measure bythe Pearsons correlation between the ranking pro-duced by the measure and the human ranking.
Forsynonym selection, we compute the percentage ofcorrect answers, since there is a unique answer foreach challenge word in the datasets.
Semantic relat-edness has been evaluated by Spearman?s rank cor-relation with human judgment instead of Pearsonscorrelation in literature and we follow the same prac-tice to make results comparable.For sentence similarity detection, the algorithmused by us (Li et al 2006) assumes that the asso-ciation scores are between 0 and 1.
Hence we nor-malize the value produced by each measure using166ChallengexOption y(correct)Option z(incorrect)f(x) f(y) f(z) f(x, y) f(x, z) PMIsig(x, y)PMIsig(x, z)PMIs(x, y)PMIs(x, z)brass metal plastic 15923 125088 24985 228 75 14 24 40 30twist intertwine curl 11407 153 2047 1 9 7 17 61 41saucer dish frisbee 2091 12453 1186 5 1 9 14 21 18mass lump element 90398 1595 43321 14 189 4 10 29 15applause approval friends 1998 19673 11689 8 6 9 11 29 28confession statement plea 7687 47299 5232 76 12 18 22 45 26swing sway bounce 33580 2994 4462 13 17 7 8 24 21sheet leaf book 20470 20979 586581 20 194 7 2 7 12Table 4: Details of ESL word-pairs, correctness of whose answers changes between PMIsig and PMIs.
Except for thegray-row, for all other questions, incorrect answers becomes correct on using PMIs instead of PMIsig , and vice-versafor the gray-row.
The association values have been suitably scaled for readability.
To save space, of the four choices,options not selected by either of the methods have been omitted.
These results are for a 10 word span.max-min normalization:v?
=v ?minmax?minwhere max and min are computed over all associa-tion scores for the entire task for a given measure.4.2 Experimental ResultsWe use a 1.24 Gigawords Wikipedia corpus for get-ting co-occurrence statistics.
Since co-occurrencemethods have span-threshold as a parameter, we fol-low the standard methodology of five-fold cross val-idation.
Note that, in addition to span-threshold, cP-MId and sPMId have an additional parameter ?.In Table 3, we present the performance of all theco-occurrence measures considered on all the tasks.Note that, except GoogleDistance and LLR, all re-sults for all co-occurrence measures are statisticallysignificant at p = .05.
For completeness of compari-son, we also include the best known results from lit-erature for different non co-occurrence based wordassociation measures on these tasks.4.3 Performance Analysis and ConclusionsWe find that on average, PMIsig and cPMId, the re-cently introduced measures that incorporate signif-icance in PMI, do not perform better than PMI onthe given datasets.
Both of them perform worsethan PMI on three out of four datasets.
By ap-propriately incorporating significance, we get newmeasures PMIs and sPMId that perform better thanPMI(also PMIsig and cPMId respectively) on mostdatasets.
PMIs improves performance over PMI onthree out of four datasets, while sPMId improvesperformance on all four datasets.The performance improvement of PMIs overPMIsig and of sPMId over cPMId, is not random.For example, on the ESL dataset, while the percent-age of correct answers increases from 58 to 66 fromPMIsig to PMIs, it is not the case that on movingfrom PMIsig to PMIs, several correct answers be-come incorrect and an even larger number of in-correct answers become correct.
As shown in Ta-ble 4, only one correct answers become incorrectwhile seven incorrect answers get corrected.
Thesame trend holds for most parameters values, andfor moving from cPMId to sPMId.
This substanti-ates the claim that the improvement is not random,but due to the appropriate incorporation of signifi-cance, as discussed in Section 2.1.PMIs and sPMId perform better than not justPMI, but they perform better than all popular co-occurrence measures on most of these tasks.
Whencompared with any other co-occurrence measure,on three out of four datasets each, both PMIs andsPMId perform better than that measure.
In fact,PMIs and sPMId perform reasonably well comparedwith more resource intensive non co-occurrencebased methods as well.
Note that different non co-occurrence based measures perform well on differ-ent tasks.
We are comparing the performance of asingle measure (say sPMId or PMIs) against the bestmeasure for each task.AcknowledgementsWe thank Dipak Chaudhari for his help with the im-plementation.167ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In NAACL-HLT 2009,Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies.Danushka Bollegala, Yutaka Matsuo, and MitsuruIshizuka.
2007.
Measuring semantic similarity be-tween words using web search engines.
In WWW2007, The World Wide Web Conference, pages 757?766.Gerlof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction, from form tomeaning: Processing texts automatically.
In GSCL2009, Proceedings of the Biennial International Con-ference of the German Society for Computational Lin-guistics and Language Technology.Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.2006.
Novel association measures using web searchwith double checking.
In COLING/ACL 2006, Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics.Kenneth Ward Church and Patrick Hanks.
1989.
Wordassociation norms, mutual information and lexicogra-phy.
In ACL 1989, Proceedings of the Annual Meet-ing of the Association for Computational Linguistics,pages 76?83.B.
Daille.
1994.
Approche mixte pour l?extraction au-tomatique de terminologie: statistiques lexicales etl-tres linguistiques.
Ph.D. thesis, Universitie Paris 7.Om P. Damani.
2013.
Improving pointwise mutualinformation (pmi) by incorporating significant co-occurrence.
In CoNLL 2013, Conference on Compu-tational Natural Language Learning.L.
R. Dice.
1945.
Measures of the amount of ecologicalassociation between species.
Ecology, 26:297?302.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61?74.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: the conceptrevisited.
ACM Transactions on Information Systems,20(1):116?131.J.
R. Firth.
1957.
A synopsis of linguistics theory.
Stud-ies in Linguistic Analysis, pages 1930?1955.N.
Pham G. Dinu and M. Baroni.
2013.
General esti-mation and evaluation of compositional distributionalsemantic models.
In CVSC 2013, Proceedings of theACL Workshop on Continuous Vector Space Modelsand their Compositionality.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using wikipedia-based ex-plicit semantic analysis.
In IJCAI 2007, InternationalJoint Conference on Artificial Intelligence.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical compositionaldistributional model of meaning.
In EMNLP 2011,Conference on Empirical Methods on Natural Lan-guage Processing, pages 1394?1404.T Hughes and D Ramage.
2007.
Lexical semantic relat-edness with random graph walks.
In EMNLP 2007,Conference on Empirical Methods on Natural Lan-guage Processing.P.
Jaccard.
1912.
The distribution of the flora of thealpine zone.
New Phytologist, 11:37?50.Svante Janson and Jan Vegelius.
1981.
Measures of eco-logical association.
Oecologia, 49:371?376.Dimitrios Kartsaklis, Mehrnoosh Sadrzadeh, and StephenPulman.
2013.
Separating disambiguation fromcomposition in distributional semantics.
In CoNLL2013, Conference on Computational Natural Lan-guage Learning.Thomas K Landauer and Susan T. Dutnais.
1997.
A so-lution to platos problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological review, 104(2):211?240.Rudi L.Cilibrasi and Paul M.B.
Vitany.
2007.
The googlesimilarity distance.
Psychological review, 19(3).Yuhua Li, David McLean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2006.
Sentence sim-ilarity based on semantic nets and corpus statistics.IEEE Transactions on Knowledge and Data Engineer-ing, 18(8):1138?1150, August.Sonya Liberman and Shaul Markovitch.
2009.
Com-pact hierarchical explicit semantic representation.
InWikiAI 2009, Proceedings of the IJCAI Workshopon User-Contributed Knowledge and Artificial Intel-ligence: An Evolving Synergy, Pasadena, CA, July.David Milne and Ian H. Witten.
2008.
An effective, low-cost measure of semantic relatedness obtained fromwikipedia links.
In ACL 2008, Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics.Jeffrey Mitchell.
2011.
Composition in DistributionalModels of Semantics.
Ph.D. thesis, The University ofEdinburgh.Pavel Pecina and Pavel Schlesinger.
2006.
Combin-ing association measures for collocation extraction.
InACL 2006, Proceedings of the Annual Meeting of theAssociation for Computational Linguistics.168Gabriel Recchia and Michael N. Jones.
2009.
More datatrumps smarter algorithms: Comparing pointwise mu-tual information with latent semantic analysis.
Behav-ior Research Methods, 3(41):647?656.George G. Simpson.
1943.
Mammals and the nature ofcontinents.
American Journal of Science, pages 1?31.Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
computing semantic relatedness usingwikipedia.
In AAAI 2006, Conference on Artificial In-telligence, pages 1419?1424.P.
Turney.
2001.
Mining the web for synonyms: PMI-IR versus LSA on TOEFL.
In ECML 2001, EuropeanConference on Machine Learning.T.
Wandmacher, E. Ovchinnikova, and T. Alexandrov.2008.
Does latent semantic analysis reflect humanassociations?
In ESSLLI 2008, European SummerSchool in Logic, Language and Information.Christian Wartena.
2013.
Hsh: Estimating semantic sim-ilarity of words and short phrases with frequency nor-malized distance measures.
In SemEval 2013, Inter-national Workshop on Semantic Evaluation.Justin Washtell and Katja Markert.
2009.
A comparisonof windowless and window-based computational asso-ciation measures as predictors of syntagmatic humanassociations.
In EMNLP 2009, Conference on Empir-ical Methods on Natural Language Processing, pages628?637.Eric Yeh, Daniel Ramage, Chris Manning, Eneko Agirre,and Aitor Soroa.
2009.
Wikiwalk: Random walkson wikipedia for semantic relatedness.
In TextGraphs2009, Proceedings of the ACL workshop on Graph-based Methods for Natural Language Processing.169
