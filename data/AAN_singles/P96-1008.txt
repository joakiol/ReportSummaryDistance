A FULLY  STAT IST ICAL  APPROACH TO NATURAL LANGUAGEINTERFACESScott Miller, David Stallard, Robert Bobrow, Richard SchwartzBBN Systems and Technologies70 Fawcett StreetCambridge, MA 02138szmiller@bbn.com, stallard@bbn.com, rusty@bbn.com, schwartz@bbn.comAbstractWe present a natural anguage interface system which isbased entirely on trained statistical models.
The systemconsists of three stages of processing: parsing, semanticinterpretation, and discourse.
Each of these stages ismodeled as a statistical process.
The models are fullyintegrated, resulting in an end-to-end system that maps inpututterances into meaning representation frames.1.
IntroductionA recent trend in natural anguage processing has beentoward a greater emphasis on statistical approaches,beginning with the success of statistical part-of-speechtagging programs (Church 1988), and continuing with otherwork using statistical part-of-speech tagging programs, suchas BBN PLUM (Weischedel et al 1993) and NYU Proteus(Grishman and Sterling 1993).
More recently, statisticalmethods have been applied to domain-specific semanticparsing (Miller et al 1994), and to the more difficult problemof wide-coverage syntactic parsing (Magerman 1995).Nevertheless, most natural language systems remainprimarily rule based, and even systems that do use statisticaltechniques, such as AT&T Chronus (Levin and Pieraccini1995), continue to require a significant rule basedcomponent.
Development of a complete end-to-endstatistical understanding system has been the focus of severalongoing research efforts, including (Miller et al 1995) and(Koppelman et al 1995).
In this paper, we present such asystem.
The overall structure of our approach isconventional, consisting of a parser, a semantic interpreter,and a discourse module.
The implementation and integrationof these elements is far less conventional.
Within eachmodule, every processing step is assigned a probability value,and very large numbers of alternative theories are pursued inparallel.
The individual modules are integrated through ann-best paradigm, in which many theories are passed from onestage to the next, together with their associated probabilityscores.
The meaning of a sentence is determined by takingthe highest scoring theory from among the n-best possibilitiesproduced by the final stage in the model.Some key advantages tostatistical modeling techniques are:?
All knowledge required by the system is acquiredthrough training examples, thereby eliminating the needfor hand-written rules.
In parsing for example, it issufficient to provide the system with examplesspecifying the correct parses for a set of trainingexamples.
There is no need to specify an exact set ofrules or a detailed procedure for producing such parses.?
All decisions made by the system are graded, and thereare principled techniques for estimating the gradations.The system is thus free to pursue unusual theories, whileremaining aware of the fact that they are unlikely.
In theevent that a more likely theory exists, then the morelikely theory is selected, but if no more likelyinterpretation can be found, the unlikely interpretation isaccepted.The focus of this work is primarily to extract sufficientinformation from each utterance to give an appropriateresponse to a user's request.
A variety of problems regardedas standard in computational inguistics, such asquantification, reference and the like, are thus ignored.To evaluate our approach, we trained an experimental systemusing data from the Air Travel Information (ATIS) domain(Bates et al 1990; Price 1990).
The selection of ATIS wasmotivated by three concerns.
First, a large corpus of ATISsentences already exists and is readily available.
Second,ATIS provides an existing evaluation methodology, completewith independent training and test corpora, and scoringprograms.
Finally, evaluating on a common corpus makes iteasy to compare the performance of the system with thosebased on different approaches.We have evaluated our system on the same blind test setsused in the ARPA e.valuations (Pallett et al 1995), andpresent apreliminary esult at the conclusion of this paper.The remainder of the paper is divided into four sections, onedescribing the overall structure of our models, and one foreach of the three major components of parsing, semanticinterpretation a d discourse.552.
Model StructureGiven a string of input words W and a discourse history H ,the task of a statistical language understanding system is tosearch among the many possible discourse-dependentmeanings Mo for the most likely meaning M0:M 0 = argmax P(M o I W, H).MoDirectly modeling P(Mo I W,/-/) is difficult because the gapthat the model must span is large.
A common approach innon-statistical natural language systems is to bridge this gapby introducing intermediate r presentations such as parsestructure and pre-discourse sentence meaning.
Introducingthese intermediate levels into the statistical framework gives:M 0 =argmax EP(MD IW, H, Ms,T)P(Ms,TIW, H)MD M s,Twhere T denotes a semantic parse tree, and Ms denotes pre-discourse sentence meaning.
This expression can besimplified by introducing two independence assumptions:1.
Neither the parse tree T, nor the pre-discourse meaningMs, depends on the discourse history H.2.
The post-discourse meaning Mo does not depend on thewords W or the parse structure T, once the pre-discoursemeaning Ms is determined.Under these assumptions,M 0 = argmax EP(MD IH'Ms) P(Ms'TIW) "Mo M s ,TNext, the probability P(Ms,TIW) can be rewritten usingBayes rule as:P(M s,T I W) =leading to:P( M s ,T) P(W I M S ,T)P(W)M 0 = argmax E P(MD IH'Ms) P(Ms'T) P(WI Ms,T)MD Ms,r P(W)Now, since P(W) is constant for any given word string, theproblem of finding meaning 34o that maximizesP(M S,T) P(WI M S,T)E P(M D IH, M s) P(W)M s ,Tis equivalent tofinding Mo that maximizesE P(M D I H, ,T) P(WI M S,T).
Ms) P(MsM s ,TM 0 = argmax EP(MD IH, M s) P(Ms,T) P(WI Ms,T).Mo M s ,TWe now introduce a third independence assumption:3.
The probability of words W does not depend on meaningMs, given that parse Tis known.This assumption is justified because the word tags in ourparse representation specify both semantic and syntactic classinformation.
Under this assumption:M 0 = argmax EP(Mo IH, M s) P(Ms,T) P(WIT)MD M s ,TFinally, we assume that most of the probability mass for eachdiscourse-dependent meaning is focused on a single parsetree and on a single pre-discourse meaning.
Under this(Viterbi) assumption, the summation operator can bereplaced by the maximization perator, yielding:Mo = arg max( max (P( M o l H, M s ) P( M s,T) P(W I T) ) \]M D ~.Ms,TThis expression corresponds to the computation actuallyperformed by our system which is shown in Figure 1.Processing proceeds in three stages:1.
Word string W arrives at the parsing model.
The fullspace of possible parses T is searched for n-bestcandidates according to the measure P(T)P(WIT).These parses, together with their probability scores, arepassed to the semantic nterpretation model.2.
The constrained space of candidate parses T (receivedfrom the parsing model), combined with the full spaceof possible pre-discourse meanings Ms, is searched forn-best candidates according to the measureP(M s,T) P(W I T).
These pre-discourse meanings,together with their associated probability scores, arepassed to the discourse model.Thus,___  Parsing ~ lnterpretati?n I f\[ Model Model j \ Model y \/ / /P(T)P(WIT) P(Ms,T)P(WIT) P(MolMs,H)P(Ms,T)P(WIT)Figure 1: Overview of statistical processing.563.
The constrained space of candidate pre-discoursemeanings Ms (received from the semantic interpretationmodel), combined with the full space of possible post-discourse meanings Mo, is searched for the singlecandidate that maximizesP( M o I H, M s) P( M s,T) P(W I T) ,  conditioned on thecurrent history H. The discourse history is then updatedand the post-discourse meaning is returned.We now proceed to a detailed iscussion of each of thesethree stages, beginning with parsing.3.
Pars ingOur parse representation is essentially syntactic in form,patterned on a simplified head-centered theory of phrasestructure.
In content, however, the parse trees are as muchsemantic as syntactic.
Specifically, each parse node indicatesboth a semantic and a syntactic lass (excepting a few typesthat serve purely syntactic functions).
Figure 2 shows asample parse of a typical ATIS sentence.
Thesemantic/syntactic character of this representation offersseveral advantages:1.
Annotation: Well-founded syntactic principles providea framework for designing an organized and consistentannotation schema.2.
Decoding: Semantic and syntactic constraints aresimultaneously available during the decoding process;the decoder searches for parses that are bothsyntactically and semantically coherent.3.
Semantic Interpretation: Semantic/syntactic parse treesare immediately useful to the semantic interpretationprocess: semantic labels identify the basic units ofmeaning, while syntactic structures help identifyrelationships between those units.3.1 Statistical Parsing ModelThe parsing model is a probabilistic recursive transitionnetwork similar to those described in (Miller et ai.
1994) and(Seneff 1992).
The probability of a parse tree T given a wordstring Wis rewritten using Bayes role as:P(T) P(W I T) P(TIW) =P(W)Since P(W) is constant for any given word string, candidateparses can be ranked by considering only the product P(T)P(W I 7").
The probability P(T) is modeled by state transitionprobabilities in the recursive transition etwork, and P(W I T)is modeled by word transition probabilities.
* State transition probabilities have the formP(state n I staten_l, stateup) .
For example,P(location/pp I arrival/vp-head, arrival/vp) is theprobability of a location/pp following an arrival/vp-head within an arrival/vp constituent.?
Word transition probabilities have the formP(word n I wordn_ l,tag) .
For example,P("class" I "first", class-of-service/npr) is the probabilityof the word sequence "first class" given the tagclass-of-service/npr.Each parse tree T corresponds directly with a path throughthe recursive transition network.
The probabilityP(T) P(W I 1") is simply the product of each transition/wh-question//// /// / 1 / / / / ~v~P a~re/ I //wh-head /aux /det /np-head /comp /vp-head /prep /aptI I I I I I I IWhen do the flights that leave from Boston/vp /vpationpQarrival location city/vp-head /prep /nprJ J Iarrive in AtlantaFigure 2: A sample parse tree.57probability along the path corresponding to T.3.2 Training the Parsing ModelTransition probabilities are estimated irectly by observingoccurrence and transition frequencies in a training corpus ofannotated parse trees.
These estimates are then smoothed toovercome sparse data limitations.
The semantic/syntacticparse labels, described above, provide a further advantage interms of smoothing: for cases of undertrained probabilityestimates, the model backs off to independent syntactic andsemantic probabilities a  follows:Ps(semlsyn n I semlsynn_ 1 ,semlsyn up) =~.
( semlsyn n I semlsynn_ l ,seral syn up)x P(semlsyn n I semlsynn_ 1 ,sem/syn up)+ (1 - ,\].
(semlsyn n Isemlsynn_ !
,semlsyn up)X P(sem n I semup) P(syn n I synn_l,synup)where Z is estimated as in (Placeway et al 1993).
Backingoff to independent semantic and syntactic probabilitiespotentially provides more precise estimates than the usualstrategy of backing off directly form bigram to unigrammodels.3.3 Searching the Parsing ModelIn order to explore the space of possible parses efficiently,the parsing model is searched using a decoder based on anadaptation of the Earley parsing algorithm (Earley 1970).This adaptation, related to that of (Stolcke 1995), involvesreformulating the Earley algorithm to work with probabilisticrecursive transition etworks rather than with deterministicproduction rules.
For details of the decoder, see (Miller1996).4.
Semantic InterpretationBoth pre-discourse and post-discourse meanings in ourcurrent system are represented using a simple framerepresentation.
Figure 3 shows a sample semantic framecorresponding to the parse in Figure 2.Air-TransportationShow: (Arrival-Time)Origin: (City "Boston")Destination: (City "Atlanta")Figure 3: A sample semantic frame.Recall that the semantic interpreter is required to computeP(Ms,T)  P(WIT ).
The conditional word probabilityP(WIT) has already been computed uring the parsingphase and need not be recomputed.
The current problem,then, is to compute the prior probability of meaning Ms andparse T occurring together.
Our strategy is to embed theinstructions for constructing Ms directly into parse T oresulting in an augmented tree structure.
For example, theinstructions needed to create the frame shown in Figure 3 are:1.
Create an Air-Transportation frame.2.
Fill the Show slot with Arrival-Time.3.
Fill the Origin slot with (City "Boston")4.
Fill the Destination slot with (City "Atlanta")These instructions are attached to the parse tree at the pointsindicated by the circled numbers (seeFigure 2).
The probability P(Ms ,T  ) is then simply theprior probability of producing the augmented tree structure.4.1 Statistical Interpretation ModelMeanings Ms are decomposed into two parts: the frame typeFT, and the slot fillers S. The frame type is always attachedto the topmost node in the augmented parse tree, while theslot filling instructions are attached to nodes lower down inthe tree.
Except for the topmost node, all parse nodes arerequired to have some slot filling operation.
For nodes thatdo not directly trigger any slot fill operation, the specialoperation ull is attached.
The probability P(Ms, T) is then:P( Ms,T) = P( FT, S,T)= P( FT) P(T I FT) P(S I FT, T).Obviously, the prior probabilities P(FT) can be obtaineddirectly from the training data.
To compute P(T I FT), eachof the state transitions from the previous parsing model aresimply rescored conditioned on the frame type.
The newstate transition probabilities are:P(state n I staten_ t,stateup, FT) .To compute P(S I FT, T) , we make the independenceassumption that slot filling operations depend only on theframe type, the slot operations already performed, and on thelocal parse structure around the operation.
This localneighborhood consists of the parse node itself, its two leftsiblings, its two right siblings, and its four immediateancestors.
Further, the syntactic and semantic omponents ofthese nodes are considered independently.
Under theseassumptions, the probability of a slot fill operation is:P(slot n I FT, Sn_l,semn_ 2 ..... sem n..... semn+2,Synn-2 ..... synn ..... Synn+2,semupl ..... semup4, Synupl ..... synup4 )and the probability P(S I FT, T) is simply the product of allsuch slot fill operations in the augmented tree.4.2 Training the Semantic InterpretationModelTransition probabilities are estimated from a training corpusof augmented trees.
Unlike probabilities in the parsingmodel, there obviously is not sufficient training data toestimate slot fill probabilities directly.
Instead, theseprobabilities are estimated by statistical decision trees similar58to those used in the Spatter parser (Magerman 1995).
Unlikemore common decision tree classifiers, which simply classifysets of conditions, tatistical decision trees give a probabilitydistribution over all possible outcomes.
Statistical decisiontrees are constructed in a two phase process.
In the firstphase, a decision tree is constructed in the standard fashionusing entropy reduction to guide the construction process.This phase is the same as for classifier models, and thedistributions at the leaves are often extremely sharp,sometimes consisting of one outcome with probability I, andall others with probability 0.
In the second phase, thesedistributions are smoothed by mixing together distributionsof various nodes in the decision tree.
As in (Magerman1995), mixture weights are determined by deletedinterpolation  a separate block of training data.4.3 Searching the Semantic InterpretationModelSearching the interpretation model proceeds in two phases.In the first phase, every parse T received from the parsingmodel is rescored for every possible frame type, computingP(T I FT) (our current model includes only a half dozendifferent types, so this computation is tractable).
Each ofthese theories is combined with the corresponding priorprobability P(FT) yielding P(FT) P(T I FT).
The n-best ofthese theories are then passed to the second phase of theinterpretation process.
This phase searches the space of slotfilling operations using a simple beam search procedure.
Foreach combination of FT and T, the beam search procedureconsiders all possible combinations of fill operations, whilepruning partial theories that fall beneath the thresholdimposed by the beam limit.
The surviving theories are thencombined with the conditional word probabilities P(W I T),computed uring the parsing model.
The final result of thesesteps is the n-best set of candidate pre-discourse meanings,scored according to the measure P(M s,T) P(WIT).5.
Discourse ProcessingThe discourse module computes the most probable post-discourse meaning of an utterance from its pre-discoursemeaning and the discourse history, according to the measure:P(M o I H, M S) P(M S , T) P(W I T).Because pronouns can usually be ignored in the ATISdomain, our work does not treat he problem of pronominalreference.
Our probability model is instead shaped by thekey discourse problem of the ATIS domain, which is theinheritance of constraints from context.
This inheritancephenomenon, similar in spirit to one-anaphora, is illustratedby the following dialog::USER 1:SYSTEM 1:USER2:I want o fly from Boston to Denver.<displays Boston to Denver flights>Which flights are available on Tuesday?SYSTEM2: <displays Boston to Denver flights forTuesday>In USER2, it is obvious from context that the user is askingabout flights whose ORIGIN is BOSTON and whoseDESTINATION is DENVER, and not all flights between anytwo cities.
Constraints are not always inherited, however.For example, in the following continuation f this dialogue:USER3: Show me return flights from Denver to Boston,it is intuitively much less likely that the user means the "onTuesday" constraint tocontinue to apply.The discourse history H simply consists of the list of all post-discourse frame representations for all previous utterances inthe current session with the system.
These frames are thesource of candidate constraints o be inherited.
For mostutterances, we make the simplifying assumption that we needonly look at the last (i.e.
most recent) frame in this list, whichwe call Me.5.1 Statistical Discourse ModelThe statistical discourse model maps a 23 element inputvector X onto a 23 element output vector Y.
These vectorshave the following interpretations:?
X represents the combination of previous meaning Meand the pre-discourse meaning Ms.?
Y represents he post-discourse meaning Mo.Thus,P( M D I H, Ms) = P(YI X) .The 23 elements in vectors X and Y correspond to the 23possible slots in the frame schema.
Each element in X canhave one of five values, specifying the relationship betweenthe filler of the corresponding slot in Me and Ms:INITIAL - slot filled in Ms but not in MeTACIT - slot filled in Me but not in MsREITERATE - slot filled in both Me and Ms; value thesameCHANGE - slot filled in both Me and Ms; valuedifferentIRRELEVANT - slot not filled in either Me or MsOutput vector Y is constructed by directly copying all fieldsfrom input vector X except those labeled TACIT.
Thesedirect copying operations are assigned probability 1.
Forfields labeled TACIT, the corresponding field in Y is filledwith either INHERITED or NOT-INHERITED.
Theprobability of each of these operations i  determined by astatistical decision tree model.
The discourse model contains23 such statistical decision trees, one for each slot position.An ordering is imposed on the set of frame slots, such thatinheritance decisions for slots higher in the order areconditioned on the decisions for slots lower in the order.59The probability P(YIX) is then the product of all 23decision probabilities:P(Y I X)  = P(YllX) P(Y2 1X,yl)... P(Y23 1X,Yl,y 2 ..... Y22) ?5.2 Training the Discourse ModelThe discourse model is trained from a corpus annotated withboth pre-discourse and post-discourse semantic frames.Corresponding pairs of input and output (X, I,') vectors arecomputed from these annotations, which are then used totrain the 23 statistical decision trees.
The training procedurefor estimating these decision tree models is similar to thatused for training the semantic interpretation model.5.3 Searching The Discourse ModelSearching the discourse model begins by selecting a meaningframe Me from the history stack H, and combining it witheach pre-discourse meaning Ms received from the semanticinterpretation model.
This process yields a set of candidateinput vectors X.
Then, for each vector X, a search processexhaustively constructs and scores all possible output vectorsY according to the measure P(Y I X) (this computation isfeasible because the number of TACIT fields is normallysmall).
These scores are combined with the pre-discoursescores P(M s,T) P(W I T),  already computed by thesemantic interpretation process.
This computation yields:P(YI X) P(M S,r) P(WIT),which is equivalent to:P(M D I H, Ms) P(Ms,T) P(W IT).The highest scoring theory is then selected, and astraightforward computation derives the final meaning frameMo from output vector Y.6.
Experimental ResultsWe have trained and evaluated the system on a commoncorpus of utterances collected from naive users in the ATISdomain.
In this test, the system was trained on approximately4000 ATIS 2 and ATIS 3 sentences, and then evaluated onthe December 1994 test material (which was held aside as ablind test set).
The combined system produced an error rateof 21.6%.
Work on the system is ongoing, however, andinterested parties are encouraged to contact he authors formore recent results.7.
ConclusionWe have presented a fully trained statistical natural languageinterface system, with separate models corresponding to theclassical processing steps of parsing, semantic interpretationand discourse.
Much work remains to be done in order torefine the statistical modeling techniques, and to extend thestatistical models to additional linguistic phenomena such asquantification a d anaphora resolution.8.
AcknowledgmentsWe wish to thank Robert Ingria for his effort in supervisingthe annotation of the training corpus, and for his helpfultechnical suggestions.This work was supported by the Advanced Research ProjectsAgency and monitored by the Office of Naval Researchunder Contract No.
N00014-91-C-0115, and by Ft. Huachucaunder Contract Nos.
DABT63-94-C-0061 and DABT63-94-C-0063.
The content of the information does not necessarilyreflect he position or the policy of the Government and noofficial endorsement should be inferred.9.
ReferencesBates, M., Boisen, S., and Makhoul, J.
"Developing anEvaluation Methodology for Spoken Language Systems.
"Speech and Natural Language Workshop, Hidden Valley,Pennsylvania, 102-108.Church, K. "A Stochastic Parts Program and Noun PhraseParser for Unrestricted Text."
Second Conference on AppliedNatural Language Processing, Austin, Texas.Earley, J.
(1970).
"An ,Efficient Context-Free ParsingAlgorithm."
Communications of the ACM, 6, 451-455.Grishman, R., and Sterling, J.
"Description of the ProteusSystem as Used for MUC-5."
Fifth Message UnderstandingConference, Baltimore, Maryland, 181-194.Koppelman, J., Pietra, S. D., Epstein, M., Roukos, S., andWard, T. "A statistical pproach to language modeling for theATIS task."
Eurospeech 1995, Madrid.Levin, E., and Pieraccini, R. "CHRONUS: The NextGeneration."
Spoken Language Systems TechnologyWorkshop, Austin, Texas, 269-271.Magerman, D. "Statistical Decision Tree Models forParsing."
33rd Annual Meeting of the Association forComputational Linguistics, Cambridge, Massachusetts, 276-283.Miller, S. (1996).
"Hidden Understanding Models,"Northeastern University, Boston, MA.Miller, S., Bates, M., Bobrow, R., Ingria, R., Makhoul, J.,and Schwartz, R. "Recent Progress in Hidden UnderstandingModels."
Spoken Language Systems Technology Workshop,Austin, Texas, 276-280.Miller, S., Bobrow, R., Ingria, R., and Schwartz, R. "HiddenUnderstanding Models of Natural Language."
32nd AnnualMeeting of the Association 'ibr Computational Linguistics,Las Cruces, New Mexico, 25-32.60Pallett, D., Fiscus, J., Fisher, W., Garofolo, J., Lund, B.,Martin, A., and Przybocki, M. "1994 Benchmark Tests forthe ARPA Spoken Language Program."
Spoken LanguageSystems Technology Workshop, Austin, Texas.Placeway, P., Schwartz, R., Fung, P., and Nguyen, L. "TheEstimation of Powerful Language Models from Small andLarge Corpora."
IEEE ICASSP, 33-36.Price, P. "Evaluation of Spoken Language Systems: the ATISDomain."
Speech and Natural Language Workshop, HiddenValley, Pennsylvania, 91-95.Seneff, S. (1992).
'?FINA: A Natural Language System forSpoken Language Applications."
Computational Linguistics,18,1, 61-86.Stolcke, A.
(1995).
"An Efficient Probabilistic Context-FreeParsing Algorithm that Computes Prefix Probabilites.
"Computational Linguistics, 21 (2), 165-201.Weischedel, R., Ayuso, D., Boisen, S., Fox, H., Ingfia, R.,Matsukawa, T., Papageorgiou, C., MacLaughlin, D.,Kitagawa, M., Sakai, T., Abe, J., Hosihi, H., Miyamoto, Y.,and Miller, S. "Description of the PLUM System as Used forMUC-5."
Fifth Message Understanding Conference,Baltimore, Maryland, 93-107.61
