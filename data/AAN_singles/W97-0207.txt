mmm\[\]mmmmmmmmmmnmmmImmmmmMeasuring Semantic EntropyDept.I.
Dan Me lamedof Computer  and Information ScienceUnivers i ty of PennsylvaniaPhi ladelphia,  PA, 19104, U.S.A.melamed@unagi, cis.
upenn, eduAbst ractSemantic entropy is a measure of se-mantic -mbigu i ty  and uninformat ive-hess.
It is a graded lexical featurewhich may play a role anywhere lex-ical semantics plays a role.
This pa-per  presents a method for measur ingsemantic entropy using translat ionaldistributions of words in parallel textcorpora.
The measurement  methodis well-defined for all words, includingfunct ion words, and even for punctu-ation.1 In t roduct ionSemantic entropy is a measure of semantic ambi-gnity and uninformativeness.
This paper presents amethod for measuring semantic entropy using trans-lational distributions of words in parallel text cor-pora.
The measurement method is well-defined forall words, including function words, and even forpunctuation.
The hypothesis behind the measure-ment method is that semantically heavy words aremore likely to have unique counterparts in other lan-guages, so they tend to be translated more consis-tently than semantically ighter words.
The consis-tency with which words are translated can be calcu-lated from the translational distributions of wordsin parallel texts in two languages (bitexts).
Thetranslational distributions can be estimated usingany reasonably good word translation model, such asthose described in (BD+93; Che96) or in (Me196b).Semantic entropy is a graded lexical feature whichmay play a role anywhere lexical semantics plays arole.
For example, semantic entropy can be inter-preted as semantic ambiguity.
On this interpreta-tion, it can predict the difficulty of disambiguatingthe sense of a given word.
Brown et al (BD+91)present a word-sense disambiguation algorithm in-volving minimization of semantic entropy weightedby word frequency.
Yarowsky (Yar93) compares thesemantic entropy of homographs conditioned on dif-ferent contexts.
Another way to use semantic en-tropy for word-sense disambiguation is to allow dis-ambiguation algorithms that favor precision over re-call to ignore words with high semantic entropy.
Inthe same vein, developers of interlinguas for machinetranslation can use semantic entropy to predict herequired complexity of lexical elements of the repre-sentation.Another interpretation ofentropy is as the inverseof reliability.
Machine learning algorithms may ben-efit from discounting the importance ofdata that hashigh entropy.
For example, an algorithm learning se-lectional preferences may not want to generalize thestatistical characteristics of "take into account" toother objects of "take," if it knows that "take" hashigh semantic entropy.
I.e.
the selectional prefer-ences of "take" are hard to predict because it usu-ally functions as a support verb.
Resnik has usedsemantic entropy to explore selectional preferences,although e measured it in a different way (Res93).Semantic entropy can help researchers decide notonly how to work with words, but also which wordsto work with.
Several applications in computationallinguistics use stop-lists of unusual words.
Thecanonical example is information retrieval systems,which routinely remove function words from queries.Another example is algorithms for mapping bitextcorrespondence at the word level.
Such algorithmswork better given a stop-list of words that are notlikely to have cognates in other languages (Me196a).For both of these applications, stop lists are typi-cally constructed by rule of thumb and trial and er-ror, uninformed by any theoretical underpinning.
Acommon first approximation is the set of closed-classwords.
As will be illustrated in Section 3, semanticentropy may be a better indicator of function-word-hood than syntactic lass.The function/content word distinction also has along history in psycholingnistics.
For example, earlyresearch in the cognitive neuroscience of languagesuggested that function words and content wordselicit qualitatively different event-related brain po-tentials (K&H83).
Later work by the same re-searchers revealed that the differences were onlyquantitative and closely tied to word frequency41(Kim97).
Section 4 explores the relationship be-tween frequency and semantic entropy.
It may beas useful or more useful to control semantic entropyin psycholinguistic experiments, the way that wordfrequency is usually controlled.2 Method2.1 Translational DistributionsThe first step in measuring semantic entropy is tocompute the translational distribution Pr(T\[s) ofeach source word s in a bitext.
A relatively simplemethod for estimating this distribution is describedin (Me196b).
Briefly, the method works as follows:1.
Extract a set of aligned text segment pairs froma parallel corpus, e.g.
using the techniques in(G&Cgla) or in (Me196a).2.
Construct an initial translation lexicon withlikelihood scores attached to each entry, e.g.
us-ing the method in (Mel95) or in (G&Cgl).3.
Assume that words always translate one-to-one.4.
Armed with the current lexicon, greedily "link"each word token with its most likely translationin each pair of aligned segments.5.
Discard lexicon entries representing word pairsthat are never linked.6.
Estimate the parameters of a maximum-likelihood word translation model.7.
Re-estimate the likelihood of each lexicon en-try, using the number of times n its componentsco-occur, the number of times k that they arelinked, and the probability Pr(kln, model).8.
Repeat from Step 4 until the lexicon converges.After the lexicon converges, Step 4 is repeatedone last time, keeping track of how many times eachEnglish (source) word is linked to each French (tar-get) word.
Using the link frequencies F(s, t) andthe frequencies F(s) of each English source words, the maximum likelihood estimates of Pr(t\[s), theprobability that s translates to the French targetword t, can be computed in the usual way: Pr(tls ) =F(8,0/F(s).2.2 Translational EntropyThe above method constructs translation lexiconscontaining only word-to-word correspondences.
Thebest it can do for compound words like "au chau-range" and "right away" is to link their transla-tion to the most representative part of the com-pound.
For example, a typical translation lexiconmay contain the entries "unemployed/chaumage"and "right/imm~liatement."
Thisbehavior is quitesuitable for our purposes, because we are interestedonly in the degree to which the translational proba-bility mass is scattered over different arget words,42not in the particular target words over which it isscattered.The translational inconsistency of words can becomputed following the principles of informationtheory z.
In information theory, inconsistency iscalled entropy.
Entropy is a functional of probabilitydistribution functions (pdf's).
If P is a pdf over therandom variable X, then the entropy of P is definedas2g(P) = - E P(z)logP(z).zEXSince probabilities are always between zero and one,their logarithms are always negative; the minus signin the formula ensures that entropies are always pos-itive.The translational inconsistency of a source words is proportional to the entropy H(T\]s) of its trans-lational pdf P(TIs):H(TIs  ) = - ~_~ e(t ls  ) log P(tls ).
(1)tETNote that H(T\[s) is not the same as the conditionalentropy H(TIS ).
The latter is a functional of theentire pdf of source words, whereas the former is afunction of the particular source word s. The con-ditional entropy is actually a weighted sum of theindividual translational entropies:H(TIS) = ~ P(s)H(Tls).aE$2.3 Null LinksAll languages have words that don't translate asilyinto other languages, and paraphrases are commonin translation.
Most bitexts contain a number ofword tokens in each text for which there is no obvi-ous counterpart in the other text.
Semantically ightwords are more likely to be paraphrased or trans-lated non-literally.
So, the frequency with which aparticular word gets linked to nothing is an impor-tant factor in estimating its semantic entropy.Ideally, a measure of translational inconsistencyshould be sensitive to which null links represent thesame sense of a given source word and which onesrepresent different senses.
Given that algorithmsfor making this distinction are currently beyond thestate of the art, the simplest way to account for"null" links is to invent a special NULL  word, andto pretend that all null links are actually links toNULL  (BD-{-93).
This heuristic produces undesiredresults, however, since it implies that the transla-tion of a word which is never linked to anything isperfectly consistent.
A better solution lies at theopposite extreme, in the assumption that each nulllink represents a different sense of the source wordi See (C&T91) for a good introduction.21t is standard to use the shorthand notation P(x)for Prp(X = x).Table 1: Parts of speech sorted by mean semanticentropy.
Verbs include participles.Part of Speech(P)" prepositionsdeterminerspronounsconjunctionspunctuationinterjectionsadverbsverbsnumeralsadjectivescommon ounsproper nounsnumber of variancetypes Ep of Ep70 5.84 4.9931 4.59 3.2337 4.14 2.8611 2.77 1.4011 2.59 11.2410 2.35 3.82972 2.21 2.367133 1.70 1.9595 1.35 3.595700 1.18 1.5610371 1.15 1.339280 0.34 0.53Table 2: Semantic entropy of punctuation has highvariance.punctuation()i?frequency count E230810 12.278519 4.881922 2.73105 2.36763 1.9111271 1.4411264 1.4270 1.381270 0.0220231 0.00278559 0.00in question.
Under this assumption, the contribu-tion to the semantic entropy of s made by each nulllink is --F--~ log F--~" If F(NULLIs) represents thenumber of times that s is linked to nothing, thenthe total contribution of all these null links to thesemantic entropy of s isN(s) = -F (NULL Is  ) log F(s)= P(NULL\]s)IogF(s) (2)The semantic entropy E(s) of each word s ac-counts for both the null links and the non-null linksof 8.
"E(s) = H(TIs ) + N(s).
(3)3 Resu l tsTo estimate the semantic entropy of English words,roughly thirteen million words were used from therecord of proceedings of the Canadian parliament("Hansards"), which is available in English and inFrench.
Before induction of the translation lexi-con, both halves of the bitext were tagged for partof speech (POS) using Brill's transformation-basedTable 3: Adjectivesadjectiveothersamesuchablefewmuchcertainleastfarfreeunemployedcorporatehardeasternactingnowcoastsuccessfullatestrongreactionaryexplanatorypsychiatricbiologicalstrategicmusicalintrinsicaugustcavalierorted by semantic entropy.~equency count E9984 8.244913 8.045630 7.943217 7.392490 7.332402 7.222109 7.221846 7.221760 7.043845 7.01319 5.50475 5.50721 5.50279 5.49282 5.49286 5.48277 5.47448 5.47588 5.471161 5.4217 0.6617 0.6617 0.6617 0.6671 0.66I0 0.64I0 0.64I0 0.64I0 0.64tagger (Bri92).
The POS information was not usedin the lexicon induction process but, after estimat-ing the semantic entropies for all the English wordsin the corpus, the words were grouped into roughpart-of-speech categories.First, mean semantic entropy was comparedacross parts of speech.
Table I lists the mean seman-tic entropies Ep  for each part of speech P, sorted by\]~p, and the variance of each Ep.
The table providesempirical evidence for the intuition that functionwords are translated less consistently than contentwords: The mean semantic entropy of each function-word POS is higher than that of any content-wordPOS.
The table also shows that punctuation and in-terjections rank between the function words at thetop and the content words at the bottom.
This rank-ing is consistent with the intuition that punctuationand interjections have more semantic weight thanfunction words, but less than content words.43Table 4: Pronouns sorted by semantic entropy.pronoun'SthereitthemselvesourselveswhatshemeIhimhistheirtheethoufrequency count E11459 9.031636 7.2824040 6.5276036 6.431252 5.73615 5.6020180 5.603281 3.325324 3.31809(}1 3.244265 3.238 1.916 1.794 1.396 1.24After analyzing the aggregated results, it was timeto peek into the semantic entropy rankings withineach POS.
Several of these were particularly inter-esting.
Table 2 explains the atypically high varianceof the semantic entropy of punctuation.End-of-sentence punctuation is used very con-sistently and almost identically in English and inFrench.
So, the question mark, the exclamationmark and the period have almost no semantic en-tropy.
In contrast, the two languages have differentrules for comas and colons, especially around quota-tions.
Comas and dashes are often used for similarpurposes, so one is often translated as the other.Moreover, English comas are often lost in transla-tion.
For these reasons, the short Table 2 includesboth the lowest and the highest semantic entropyvalues for English words in the Hansards.Table 3 shows some of the adjectives, ranked bysemantic Entropy.
The top eight adjectives in thetable say very little about the nouns that they mightmodify.
They seem like thinly disguised functionwords that happen to appear in syntactic positionsnormally reserved for adjectives.
Adjectives in themiddle of the table are more typical, but they areless specific than the adjectives in the bottom thirdof the table.Table 4 displays a sorted sample of the pronouns.Topping the list are the English possessive suffixes,which have no equivalent in French or in most otherlanguages.
Existential "there" is next.
"It" is highon the list because of its frequent pleonastic func-tion ("It is necessary to....").
These four pronounsare atypically functional.
The most frequent of thethirty seven pronouns in the corpus, "I," is eleventhfrom the bottom of the list.
The most consistently44translated pronouns are the archaic forms "thee"and "thou.
"Table 5: Verbs with the highest semantic entropy.verb participle?
frequency E \[do !
- 37113 8.44 1 being present 4166 7.75going i present 1954 7.37get - 6888 7.14be - 245324 7.07having present 1989 7.02made past 4865 7.01come - 7088 6.99!concerned past 2213 6.94go - 10079 6.87involved past 1784 6.77making present 1100 6.65take - 9249 6.59put - 4692 6.57according present 985 6.53done past 2580 6.52doing present 1192 6.49taking ~ present 848 6.47trying present 837 6.44stand - 1939 6.44given past 2593 6.36let - 2975 6.36given past 2593 6.36concerning present 716 6.36getting present 649 6.36dealing present 870 6.30saying present 1262 6.28may 5264 6.26happen 3134 6.25giving present 653 6.23make 11493 6.22might 2755 6.20told past 663 6.11taken past 2061 6.10clear 720 6.10coming present 899 6.09become 2361 6.09talking present 526 6.08directed past 995 6.08shall 796 6.08brought past 1159 6.03bringing present 515 6.03putting present 433 5.99looking present 480 5.99been ; past 3274 5.96regarding I present 491 5.96living present 655 5.94occur 674 5.92agree 2770 5.89bring 3075 5.84fail 704 5.84called past 563 5.83providing present 489 5.81using present 477 5.81The most interesting ranking of semantic en-tropies is among the verbs, including present andpast participles.
As shown in Table 5, verbs can havehigh entropies for several reasons.
The verb withthe highest semantic entropy by far is the functionalverb place-holder "do."
Very high on the list are var-ious forms of the functional auxiliaries "be, .... have,"and "(be) going (to)," as well as the modals "may,""might," and "shall."
The past participles "con-cerning, .... involving," "according," "dealing," and"regarding" are near the top of the list because theyoccur most often as the heads of adjectival phrasesmodifying noun phrases, as in "the world accord-ing to NP", an English construction that is usuallyparaphrased in translation.
"Try" and "let" axe upthere because they often serve as mere modal mod-ifiers of a sentential argument.
Most of the otherverbs at the top of the list are light verbs.
Verbs like"get," "make," "come," "take," "put, .... stand," and"give" are often used as syntactic filler while mostof the semantic content of the phrase is conveyed bytheir argument.4 D iscuss ionThe most in-depth study of semantic entropy andits applications to date was carried out by Resnik(Res93; Res95).
Resnik's approach differs from thepresent one in three major ways.
First, he definessemantic entropy over concepts, rather than overwords.
This definition is more useful for his par-ticular applications, namely evaluating concept sim-ilarity and estimating selectional preferences.
Sec-ond, in order to measure semantic similarity overconcepts, his method requires a concept taxonomy,such as the Princeton WordNet (Milg0), which isgrounded in the lexical ontology of a particular lan-guage.
In contrast, the method presented in this pa-per requires a large bitext.
Both kinds of resourcesare still available only for a limited number of lan-guages, so only one of the two methods may be aviable option in any given situation.
Third, Resnik'smeasure of information content is defined in termsof the logarithm of each concept's frequency in text,where the frequency of a concept is defined as thesum of the frequencies of words representing thatconcept in the taxonomy.Given only monolingual data, log-frequency is arelatively good estimator of semantic entropy.
Look-ing through the various tables in this paper, you mayhave noticed that words with higher entropy tend tohave higher frequency.
Semantic entropy, as mea-sured here, actually correlates quite well with thelogarithm of word frequency (p = 0.79).
This corre-lation is to be expected, since the max imum possibleentropy of a word with frequency f is log(f), whichis what Equation (3) evaluates to when a word isalways linked to nothing.
Yet the correlation is notperfect; simply sorting the words by frequency wouldproduce a suboptimal result.
For instance, the mostfrequent pronoun in Table 4 is eleventh from thebottom of the list of thirty seven, because 'T' hasa very consistent meaning.
Likewise, "going" has ahigher entropy than "go" in Table 5, even though itis less than one fifth as frequent, because "going" canbe used as a near-future tense marker whereas "go"has no such function.
The best counter-example tothe correlation between semantic entropy and log-frequency is the period, which is the most frequenttoken in the English Hansards and has a semanticentropy of zero.The method presented here for measuring seman-tic entropy is sensitive to ontological and syntacticdifferences between languages.
It is partly motivatedby the observation that translators must paraphrasewhen the target language has no obvious equivalentfor some word or syntactic construct in the sourcetext.
There are many more ways to paraphrasesomething than to translate it literally, and trans-lators usually strive for variety in order to improvereadability.
That's why, for example, English lightverbs have such high entropies even though thereare many English verbs that are more frequent.
Theentropy of English light verbs would likely remainrelatively high if English/Chinese bitexts were usedinstead of English/French, because the lexicalizationpatterns involving light verbs in English are partic-ular to English.
Reliance on this property of trans-lated texts is a double-edged sword, however, dueto the converse possibility that two languages sharean unusual syntactic construct or an unusual bit ofontology.
In that case, the relevant semantic en-tropies may be estimated too low.
Ideally, seman-tic entropy should be estimated by averaging eachsource language of interest over several different tar-get languages.A more serious drawback of translational entropyas an estimate of semantic entropy is that wordsmay be inconsistently ranslated either because theydon't mean very much or because they mean severaldifferent things, or both.
For example, WordNet 1.5lists twenty six senses for the English verb "run."
Wewould expect he different senses to have differenttranslations in other languages, and we would expectseveral of these senses to occur in any sufficientlylarge bitext, resulting in a high estimate of semanticentropy for "run" (5.65 in the Hansards).
Mean-while, Table 5 shows that the English verb "be" istranslated much less consistently s than "run," eventhough only nine senses are listed for it in WordNet.This is because "be" rarely conveys much informa-tion.
It is useful to know about both of these com-ponents of semantic entropy, but it would be moreuseful to know about hem separately (Ros97).
Thisknowledge is contingent on knowledge of the elusive3The semantic entropy metric is logarithmic.
A dif-ference of I represents a factor of 2.45Pr(senselword), which is currently the subject ofmuch research (see, e.g.
(NS~L96) and referencestherein).
Knowing Pr(senselword) would also im-prove Resnik's method, which st) far has been forcedto assume that this distribution is uniform (Res95).5 Conc lus ionThe semantic entropy of a word can be interpreted asits semantic ambiguity and is inversely proportionalto the word's informatio n content, semantic weight,and consistency in translation.
This paper presentedan information-theoretic method for measuring thesemantic entropy of any word in text, using transla-tional distributions estimated from parallel text cor-pora.
This measurement technique has produced en-tropy rankings that correspond well with intuitionsabout the relative semantic import of various wordsand word classes.
The method can be implementedfor any language for which a reasonably large bitextis available.AcknowledgmentsMany thanks to Jason Eisner, Martha Palmer,Joseph Rosenzweig, and three anonymous review-era for helpful comments on earlier versions ofthis paper.
This research was supported by SunMicrosystems Laboratories Inc. and by ARPA Con-tract #N66001-94C-6043.References(Bri92) E. Brill, "A Simple Rule-Based Part ofSpeech Tagger," 3rd Conference on Applied Nat-ural Language Processing, pp.
152-155, 1992.
(BD+91) P. F. Brown, S. Della Pietra, V. DellaPietra, R. Mercer, "Word Sense Disarnbiguationusing Statistical Methods", Proceedings of the~9th Annual Meeting of the Association for Com-putational Linguistics, Berkeley, Ca., 1991.
(BD+93) P. F. Brown, V. J. Della Pietra, S. A. DellaPietra & R. L. Mercer, "The Mathematics ofSta-tistical Machine Translation: Parameter Estima-tion," Computational Linguistics 19(2), 1993.
(Che96) S. Chen, Building Probabilistic Models forNatural Language, Ph.D. Thesis, Harvard Univer-sity, May 1996.
(C&T91) T. M. Cover & J.
A. Thomas, Elementsof Information Theory, John Wiley & Sons, NewYork, NY, 1991.
(G&C91a) W. Gale, & K. W. Church, "A Programfor Aligning Sentences inBilingual Corpora" Pro-ceedings of the P9th Annual Meeting of the Asso-ciation for Computational Linguistics, Berkeley,Ca., 1991.46(G&C91) W. Gale & K. W. Church, "Identify-ing Word Correspondences in Parallel Texts,"DARPA SNL Workshop, 1991.
(Kim97) A. Kim, personal communication, 1997.
(K&H83) M. Kutas & S. A. Hillyard, "Event-relatedbrain potentials to grammatical errors and seman-tic anomalies," Memory ~ Cognition II(5), 1983.
(Me195) I. D. Melamed "Automatic Evaluation andUniform Filter Cascades for Inducing N-bestTranslation Lexicons," Third Workshop on VeryLarge Corpora, Boston, MA, 1995.
(Me196a) I. D. Melamed, "A Geometric Approach toMapping Bitext Correspondence," Conference onEmpirical Methods in Natural Language Process-ing, Philadelphia, U.S.A, 1996.
(Me196b) I. D. Melarned, "Automatic Constructionof Clean Broad-Coverage Translation Lexicons,"~nd Conference of the Association for MachineTranslation in the Americas, Montreal, Canada,1996.
(Mil90) G. A. Miller (ed.
), WordNet: An On-LineLezical Database.
Special issue of InternationalJournal of Lexicography 4(3), 1990.
(N&L96) H. T. Ng & H. B. Lee, '~ntegrating Mul-tiple Knowledge Sources to Disarnbiguate WordSense: An Exemplar-Based Approach," Proceed-ings of the 34th Annual Meeting of the Associa-tion for Computational Linguistics, Santa Cruz,CA, 1996.
(Res93) P. Resnik, Selection and Information: AClass-Based Approach to Lezical Relationships,PhD Thesis, University of Pennsylvania, Philadel-phia, U.S.A, 1993.
(Res95) P. Resnik, "Using Information Content oEvaluate Semantic Similarity in a Taxonomy,"Proceedings of the Fourteenth International JointConference on Artilicial Intelligence, Montreal,Canada, 1995.
(Ros97) J. Rosenzweig, personal communication,1997.
(Yar95) D. Yarowsky, "One Sense Per Collocation,"DARPA Workshop on Human Language Technol-ogy, Princeton, NJ, 1993.
