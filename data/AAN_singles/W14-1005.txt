Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 21?29,Gothenburg, Sweden, April 27, 2014.c?2014 Association for Computational LinguisticsUsing Feature Structures to Improve Verb Translation inEnglish-to-German Statistical MTPhilip Williams?p.j.williams-2@sms.ed.ac.ukSchool of Informatics?University of EdinburghPhilipp Koehn?
?pkoehn@inf.ed.ac.ukCenter for Speech and Language Processing?The Johns Hopkins UniversityAbstractSCFG-based statistical MT models haveproven effective for modelling syntacticaspects of translation, but still suffer prob-lems of overgeneration.
The productionof German verbal complexes is particu-larly challenging since highly discontigu-ous constructions must be formed con-sistently, often from multiple independentrules.
We extend a strong SCFG-basedstring-to-tree model to incorporate a richfeature-structure based representation ofGerman verbal complex types and com-pare verbal complex production againstthat of the reference translations, finding ahigh baseline rate of error.
By developingmodel features that use source-side infor-mation to influence the production of ver-bal complexes we are able to substantiallyimprove the type accuracy as compared tothe reference.1 IntroductionSyntax-based models of statistical machine trans-lation (SMT) are becoming increasingly compet-itive against state-of-the-art phrase-based mod-els, even surpassing them for some languagepairs.
The incorporation of syntactic structurehas proven effective for modelling reordering phe-nomena and improving the fluency of target out-put, but these models still suffer from problems ofovergeneration.One example is the production of German ver-bal constructions.
This is particularly challengingfor SMT models since highly discontiguous con-structions must be formed consistently, often frommultiple independent rules.
Whilst the model?s.failedhaspolicythisyetTOP-SPUNC..S-TOPVP-OCVVPPfehlgeschlagenNP-SBNNStrategiePDATdieseVAFINistKONdochFigure 1: Alignment graph for a sentence pairfrom the training data.
The boxes indicate thecomponents of the target-side verbal complex: amain verb, fehlgeschlagen, and an auxiliary, ist.grammar may contain rules in which a completemulti-word verb translation is captured in a singlediscontiguous rule, in practice many verb transla-tions are incompletely or inconsistently produced.There are many routes by which ill-formed con-structions come to be licensed by the model, noneof which is easy to address.
For instance, Figure 1shows an example from our training data in whicha missing alignment link (between has and ist) al-lows the extraction of rules that translate has failedto the incomplete fehlgeschlagen.Even with perfect word alignments, the ex-tracted rules may not include sufficient context toensure the overall grammaticality of a derivation.The extent of this problem will depend partly onthe original treebank annotation style, which typi-cally will not have been designed with translationin mind.
The problem may be further exacerbatedby errors during automatic parsing.In this paper, we address the problem by fo-cusing on the derivation process.
We extend astrong SCFG-based string-to-tree model to incor-porate a rich feature-structure based representation21of German verbal complex types.
During decod-ing, our model composes type values for everyclause.
When we compare these values againstthose of the reference translations, we find a highbaseline rate of error (either incomplete or mis-matching values).
By developing model featuresthat use source-side information to influence theproduction of verbal complexes we are able to sub-stantially improve the type accuracy as comparedto the reference.2 Verbal Complex StructuresAdopting the terminology of Gojun andFraser (2012), we use the term ?verbal com-plex?
to mean a main verb and any associatedauxiliaries within a single clause.2.1 Feature StructuresWe use feature structures to represent the under-lying grammatical properties of German verbalcomplexes.
The feature structures serve two mainfunctions: the first is to specify a type for theverbal complex.
The types describe clause-levelproperties and are defined along four dimensions:1. tense (present, past, perfect, pluperfect, future,future perfect), 2. voice (active, werden-passive,sein-passive), 3. mood (indicative, subjunctive I,subjunctive II), and 4. auxiliary modality (modal,non-modal).The second function is to restrict the choice ofindividual word forms that are allowed to com-bine within a given type.
For example, a fea-ture structure value for the verbal complex hat.
.
.
gespielt belongs to the perfect, active, indica-tive, non-modal type.
Additionally, it specifiesthat for this type, the verbal complex comprisesexactly two verbs: one is a finite, indicative formof the auxiliary haben or sein, the other is a past-participle.2.2 The LexiconOur model uses a lexicon that maps each Germanverb in the target-side terminal vocabulary to a setof features structures.
Each feature structure con-tains two top-level features: POS, a part-of-speechfeature, and VC, a verbal complex feature of theform described above.Since a verbal complex can comprise multipleindividual verbs, the lexicon entries include partialVC structures.
The full feature structure values arecomposed through unification during decoding.VP-OC ?
?
rebuilt , wieder aufgebaut ??
VP-OC VC ?
= ?
aufgebaut VC ??
aufgebaut POS ?
= VVPPS-TOP ?
?
X1have X2been X3,PP-MO1wurde NP-SB2VP-OC3??
S-TOP VC ?
= ?
wurde VC ??
S-TOP VC ?
= ?
VP-OC VC ??
wurde POS ?
= VAFINFigure 2: SCFG rules with constraintsThe lexicon?s POS values are derived from theparse trees on the target-side of the training data.The VC values are assigned according to POS valuefrom a small set of hand-written feature struc-tures.
Every main verb is assigned VC values fromone of three possible groups, selected according towhether the verb is finite, a past-participle, or aninfinitive.
For the closed class of modal and non-modal auxiliary verbs, VC values were manuallyassigned.3 The GrammarOur baseline translation model is learned from aparallel corpus with automatically-derived wordalignments.
In the literature, string-to-tree trans-lation models are typically based on either syn-chronous context-free grammars (SCFGs) (as inChiang et al.
(2007)) or tree transducers (as in Gal-ley et al.
(2004)).
In this work, we use an SCFG-based model but our extensions are applicable inboth cases.Following Williams and Koehn (2011), eachrule of our grammar is supplemented witha (possibly-empty) set of PATR-II-style identi-ties (Shieber, 1984).
Figure 2 shows two examplerules with identities.
The identities should be in-terpreted as constraints that the feature structuresof the corresponding rule elements are compatibleunder unification.
During decoding, this imposesa hard constraint on rule application.3.1 Identity ExtractionThe identities are learned using the following pro-cedure:1.
The syntax of the German parse trees is usedto identify verbal complexes and label theparticipating verb and clause nodes.22rebuiltbeenthesehaverecentlyonlyS-TOPVP-OCVVPPaufgebautADVwiederNP-SBPDSdieseVAFINwurdePP-MONNZeitADJAj?ungsterAPPRinADVerstFigure 3: Alignment graph for a sentence pairfrom the training data.
The target sentence hasa single verbal complex.
Participating nodes areindicated by the boxes.2.
Grammar rule extraction is extended to gen-erate identities between VC values when anSCFG rule contains two or more nodes froma common verbal complex.3.
POS identities are added for terminals that ap-pear in VC identities.Figure 3 shows a sentence-pair from the train-ing data with the verbal complex highlighted.The rules in Figure 2 were extracted from thissentence-pair.Crucially, in step 2 of the extraction procedurethe identities can be added to SCFG rules thatcover only part of a verbal complex.
For example,the first rule of Figure 2 includes the main verb butnot the auxiliary.
On application of this rule, thepartial VC value is propagated from the main verbto the root.
The second rule in Figure 2 identifiesthe VC value of an auxiliary with the VC value ofa VP-OC subderivation (such as the subderivationproduced by applying the first rule).4 Source-side FeaturesSince Och and Ney (2002), most SMT modelshave been defined as a log-linear sum of weightedfeature functions.
In this section, we define twoverbal-complex-specific feature functions.
In or-der to do so, we first describe ?clause projection,?a simple source-syntactic restriction on decoding.We then describe our heuristic method of obtain-ing probability estimates for a target verbal com-plex value given the source clause.4.1 Clause ProjectionOur feature functions assume that we have analignment from source-side clauses to targetclauses.
In order to satisfy this requirement, weadopt a simple restriction that declarative clauses(both main and embedded) on the source-sidemust be translated as clauses on the target-side.This is clearly an over-simplification from a lin-guistic perspective but it appears not to harm trans-lation quality in practice.
Table 1 shows smallgains in BLEU score over our baseline systemwith this restriction.Test Set Baseline Clause Proj.newstest2008 15.7 15.8 (+0.1)newstest2009 14.9 15.0 (+0.1)newstest2010 16.5 16.8 (+0.3)newstest2011 15.4 15.5 (+0.1)Table 1: Results with and without clause projec-tion (baseline tuning weights are used for clauseprojection)Clause projection is implemented as follows:1.
The input sentence is parsed and a setof clause spans is extracted according tothe 1-best parse.
We use the Berkeleyparser (Petrov and Klein, 2007), which istrained on the Penn Treebank and so we baseour definition of a declarative clause on thetreebank annotation guidelines.2.
We modify the decoder to produce deriva-tions in chart cells only if the cell span isconsistent with the set of clause spans (i.e.if source span [i,j] is a clause span then noderivation is built over span [m,n] where i <m ?
j and n > j, etc.)3.
We modify the decoder so that grammar rulescan only be applied over clause spans if theyhave a clause label (?S?
or ?CS?, since theparser we use is trained on the Tiger tree-bank).4.2 Verbal Complex ProbabilitiesWhen translating a clause, the source-side verbalcomplex will often provide sufficient informationto select a reasonable type for the target verbalcomplex, or to give preferences to a few candi-dates.
By matching up source-side and target-sideverbal complexes we estimate co-occurrence fre-quencies in the training data.
To do this for allpairs in the training data, we would need to alignclauses between the source and target training sen-tences.
However, it is not crucial that we identify23every last verbal complex and so we simplify thetask by restricting training data to sentence pairs inwhich both source and target sentences are declar-ative sentences, making the assumption that themain clause of the source sentence aligns with themain clause of the target.We represent source-side verbal complexeswith a label that is the string of verbs andparticles and their POS tags in the order thatthey occur in the clause, e.g.
plays VBZ,is addressing VBZ VBG.
The target-sidefeature structures are generated by identifyingverbal complex nodes in the training data parsetrees (as in Section 3.1) and then unifying thecorresponding feature structures from the lexicon.Many source verbal complex labels exhibit astrong co-occurrence preference for a particulartarget type.
For example, Table 2 shows thethree most frequent feature structure values forthe target-side clause when the source label isis closed VBZ VBN.
The most frequent valuecorresponds to a non-modal, sein-passive con-struction in the present tense and indicative mood.RF F-Structure0.841?????
?FIN[AUX[LEMMA seinMOOD indicativeTENSE present]]NON-FIN[PP/SP[PP[LEMMA*]]]?????
?0.045[FIN[FULL[LEMMA sein]]NON-FIN none]0.034????????
?FIN[AUX[LEMMA werdenMOOD indicativeTENSE present]]NON-FIN???WPP??
?PP[LEMMA*]WERDEN noneWORDEN noneSEIN none???????????????.
.
.
.
.
.Table 2: Observed values and relative frequencies(RF) for is closed, which was observed 44 times inthe training data.4.3 Feature FunctionsAs with the baseline features, our verbal complex-specific feature functions are evaluated for everyrule application riof the synchronous derivation.Like the language model feature, they are non-local features and so cannot be pre-computed.
Un-like the baseline features, their value depends onwhether the source span that the rule is applied tois a declarative clause or not.Both features are defined in terms of X , theverbal complex feature structure value of the sub-derivation at rule application ri.The first feature function, f(ri), uses the sourceverb label, l, and the probability estimate, P (X|l),learned from the training data:f(ri) =????????
?P (X|l) if ricovers a clause spanwith verb label land cl?
cmin1 otherwiseThe probability estimates are not used for scoringif the number of training observations falls belowa threshold, cmin.
We use a threshold of 10 in ex-periments.The second feature function, g(ri), is simpler:it penalizes the absence of a target-side finite verbwhen translating a source declarative clause:g(ri) =????
?exp(1) if ricovers a clause spanand X has no finite verb1 otherwiseUnlike f , which requires the verb label to havebeen observed a number of times during training,g is applied to all source spans that cover a declar-ative clause.Dropped finite verbs are a frequent problem inour baseline model and this feature was motivatedby an early version of the analysis presented inSection 5.3.5 Experiments and AnalysisIn preliminary experiments, we found that changesin translation quality resulting from our verb trans-lation features were difficult to measure usingBLEU.
In the following experiments, we mea-sure accuracy by comparing verbal complex val-ues against feature structures derived from the ref-erence sentences.5.1 SetupOur experiments use the GHKM-based string-to-tree pipeline implemented in Moses (Koehn et al.,2007; Williams and Koehn, 2012).
We extend aconventional baseline model using the constraintsand feature functions described earlier.24Data Set Reference Baseline Hard Constraint(MC count) F E Total F E Total F E TotalDev 95.6% 4.4% 100.0% 86.1% 13.9% 100.0% 87.6% 12.4% 100.0%(633) 637 29 666 545 88 633 559 79 638Test 92.2% 7.8% 100.0% 83.5% 16.5% 100.0% 85.4% 14.6% 100.0%(2445) 2439 206 2645 2034 403 2437 2096 359 2455Table 3: Counts of main clause VC structures that are present and contain at least a finite verb (F) versusthose that are empty or absent (E).
Declarative main clause counts (MC count) are given for each inputset.
Counts for the three test sets are aggregated.We extracted a translation grammar using allEnglish-German parallel data from the WMT2012 translation task (Callison-Burch et al., 2012),a total of 2.0M sentence pairs.
We used all of theWMT 2012 monolingual German data to train a5-gram language model.The baseline system uses the feature functionsdescribed in Williams and Koehn (2012).
Thefeature weights were tuned on the WMT new-stest2008 development set using MERT (Och,2003).
We use the newstest2009, newstest2010,and newstest2011 test sets for evaluation.
The de-velopment and test sets all use a single reference.5.2 Main Clause Verb ErrorsWhen translating a declarative main clause, thetranslation should usually also be a declarativemain clause ?
that is, it should usually contain atleast a finite verb.
From manually inspecting theoutput it is clear that verb dropping is a commonsource of translation error in our baseline system.By making the assumption that a declarative mainclause should always be translated to a declara-tive main clause, we can use the absence of a finiteverb as a test for translation error.By evaluating identities, our decoder now gen-erates a trace of verbal complex feature structures.We obtain a reference trace by applying the sameprocess of verbal complex identification and fea-ture structure unification to a parse of our refer-ence data.
Given these two traces, we compare thepresence or absence of main clause finite-verbs inthe baseline and reference.Since we do not have alignments between theclause nodes of the test and reference trees, we re-strict our analysis to a simpler version of this task:the translation of declarative input sentences thatcontain only a single clause.
To select test sen-tences, we first parse the source-side of the tuningand test sets.
Filtering out sentences that are notdeclarative or that contain multiple clauses leaves633, 699, 793, and 953 input sentences for new-stest2008, 2009, 2010, and 2011, respectively.Our baseline system evaluates constraints in or-der to generate a trace of feature structures butconstraint failures are allowed and hypotheses areretained.
Our hard constraint system discards allhypotheses for which the constraints fail.
The fand g feature functions are not used in these ex-periments.For all main clause nodes in the output tree,we count the number of feature structure valuesthat contain finite verbs and are complete versusthe number that are either incomplete or absent.Since constraint failure results in the productionof empty feature structures, incompatible verbalcombinations do not contribute to the finite verbtotal even if a finite verb is produced.
We com-pare the counts of clause nodes with empty fea-ture structures for these two systems against thoseof the reference set.Table 3 shows total clause counts for the ref-erence, baseline, and hard constraint system (the?total?
columns).
For each system, we record howfrequently a complete feature structure containingat least a finite verb is present (the F columns) ornot (E).As expected, the finite verb counts for the refer-ence translations closely match the counts for thesource sentences.
The reference sets also containverb-less clauses (accounting for 4.4% and 7.8%of the total clause counts for the dev and test sets).Verb-less clauses are common in the training dataand so it is not surprising to find them in the refer-ence sets.Our baseline and hard constraint systems bothfail to produce complete feature structures for ahigh proportion of test sentences.
Table 4 showsthe proportion of single-clause declarative sourcesentences for which the translation trace does not25include a complete feature structure.
As well assuggesting a high level of baseline failure, theseresults suggest that using constraints alone is in-sufficient.Test set Ref.
Baseline HCnewstest2008 0.0% 13.9% 11.7%newstest2009 0.6% 18.6% 16.0%newstest2010 0.0% 14.5% 12.5%newstest2011 1.4% 17.4% 14.4%Table 4: Proportion of declarative single-clausesentences for which there is not a complete featurestructure for the translation.
Ref.
is the referenceand HC is our hard constraint system.5.3 Error ClassificationIn order to verify that the incomplete feature struc-tures indicate genuine translation errors and to un-derstand the types of errors that occur, we manu-ally check 100 sentences from our baseline systemand classify the errors.
We check the verb con-structions of the sentences containing the first 50failures in newstest2009 and the first 50 failures innewstest2011.Invalid Combination (27) An ungrammaticalcombination of auxiliary and main verbs.Example: im Jahr 2007 hatte es bereits umzwei Drittel reduziert worden .Perfect missing aux (25) There is a past-participle in sentence-final position, but noauxiliary verb.Example: der Dow Jones etwas sp?aterwieder bereitgestellt .False positive (14) Output is OK.
In the samplethis happens either because the output stringis well-formed in terms of verb structure, butthe tree is wrong, or because the parse of thesource is wrong and the input does not actu-ally contain a verb.No verb (13) The input contains at least one verbthat should be translated but the output con-tains none.Example: der universelle Charakter derHandy auch Nachteile .Invalid sentence structure (13) Verbs arepresent and make sense, but sentence struc-ture is wrongExample: die rund hunderttausend Men-schen in Besitz von ihren eigenen ChipcardOpencard in dieser Zeit , diese Kuponbekommen kann .Inf missing aux (5) There is an infinitive insentence-final position, but no auxiliaryverb or the main verb is erroneously in finalposition (the output is likely to be ambiguousfor this error type).Example: die Preislisten dieser Un-ternehmen in der Regel nur ausgew?ahltePersonen erreichen .Unknown verb (2) The input verb is untrans-lated.Example: dann scurried ich auf meinemPlatz .Werden-passive missing aux (1) There is awerden-passive non-finite part, but no finiteauxiliary verb.Example: die meisten ger?aumigen undluxuri?osesten Wohnung im ersten Stock f?urdie?Offentlichkeit ge?offnet worden .In our classification, the most common individ-ual error type in the baseline is the ungrammaticalcombination of verbs, at 27 out of 100.
However,there are multiple categories that can be character-ized as the absence of a required verb and com-bined these total 44 out of 100 errors.
There arealso some false positives and potentially mislead-ing results in which wider syntactic errors resultin the failure to produce a feature structure, but themajority are genuine errors.
However, this methodfails to identify instances where the verbal com-plex is grammatical but has the wrong features.For that, we compare accuracy against referencevalues.5.4 Feature Structure AccuracyIf we had gold-standard feature structures for ourreference sets and alignments between test and ref-erence clauses then we could evaluate accuracy bycounting the number of matches and reporting pre-cision, recall, and F-measure values for this task.In the absence of gold reference values, we relyon values generated automatically from our refer-ence sets.
This requires accepting some level oferror from parsing and verb labelling (we performa manual analysis to estimate the degree of thisproblem).
We also require alignments between26Data Set Experiment F E g m Prec.
Recall F1Dev Baseline 545 88 637 253 46.4 39.7 42.8f 610 48 637 312 51.1 49.0 50.0g 600 58 637 289 48.2 45.4 46.7f + g 627 29 637 317 50.6 49.8 50.2Test Baseline 2034 403 2439 993 48.8 40.7 44.4f 2370 224 2439 1214 51.2 49.8 50.5g 2307 278 2439 1072 46.5 44.0 45.2f + g 2437 145 2439 1225 50.3 50.2 50.2Table 5: Feature structure accuracy for the development and test sets.
As in Table 3, counts are given formain clause VC structures that are present and contain at least a finite verb (F) versus those that are absentor empty (E).
The VC values of the output are compared against the reference values giving the numberof matches (m).
The counts F, m, and g, (the number of gold reference values) are used to computeprecision, recall, and F1 values.Input Bangladesh ex-PM is denied bailReference Ehemaliger Premierministerin von Bangladesch wird Kaution verwehrtBaseline Bangladesch ex-PM ist keine Kautionf + g Bangladesch ex-PM wird die Kaution verweigertInput the stock exchange in Taiwan dropped by 3.6 percent according to the local index .Reference Die B?orse in Taiwan sank nach dem dortigen Index um 3,6 Prozent .Baseline die B?orse in Taiwan die lokalen Index entsprechend um 3,6 Prozent gesunken .f + g die B?orse in Taiwan fiel nach Angaben der ?ortlichen Index um 3,6 Prozent .Input the commission had been assembled at the request of Minister of Sport Miroslav Drzeviecki .Reference Die Kommission war auf Anfrage von Sportminister Miroslaw Drzewiecki zusammengekommen.Baseline die Kommission hatte auf Antrag der Minister f?ur Sport Miroslav Drzeviecki montiert worden .f + g die Kommission war auf Antrag der Minister f?ur Sport Miroslav Drzeviecki versammelt .Figure 4: Example translations where the baseline verbal complex type does not match the reference butthe f + g system does.test and reference clauses.
Here we make the samesimplification as in Section 5.2 and restrict evalu-ation to single-clause declarative sentences.We test the effect of the f and g features onfeature structure accuracy.
Their log-linear modelweights were tuned by running a line search tooptimize the F1 score on a subset of the new-stest2008 dev set containing sentences up to 30tokens in length (all baseline weights were fixed).For the experiments in which both features areused, we first tune the weight for f and then tuneg with the f weight fixed.Table 5 reports feature structure accuracy for thedevelopment and test sets.
On the test set, the indi-vidual f and g features both improve the F1 score.f is effective in terms of both precision and recall,but the g feature degrades precision compared tothe baseline.
Using both features appears to offerlittle benefit beyond using f alone.Compared with the baseline or using hard con-straints alone (Table 3), the proportion of sen-tences with incomplete or inconsistent verbalcomplex values (column E) is substantially re-duced by the f and g feature functions.To estimate the false match rate, we manuallychecked the first 50 sentences from the 2009 testset in which one system was reported to agree withreference and the other not:37/50 Verb constructions are grammatical.
Weagree with comparisons against the referencevalue.9/50 Verb constructions are grammatical.
Weagree with the comparison for the test system butnot the baseline.4/50 Verb constructions are ungrammatical ordifficult to interpret in both baseline and test.Figure 4 shows some example translations fromour system.275.5 BLEUFinally, we report BLEU scores for two versionsof our dev and test sets: in addition to the fulldata sets (Table 6), we use sub-sets that containall source sentences up to 30 tokens in length (Ta-ble 7).
There are two reasons for this: first, weexpect shorter sentences to use simpler sentencestructure with less coordination and fewer relativeand subordinate clauses.
All else being equal, weexpect to see a greater degree of high-level struc-tural divergence between complex source and tar-get sentence structures than between simple ones.We therefore anticipate that our naive clause pro-jection strategy is more likely to break down onlong sentences.
Second, we expect the effects onBLEU score to become diluted as sentence lengthincreases, for the simple reason that verbs arelikely to account for a smaller proportion of thetotal number of words (though this effect seems tobe small: in a parse of the newstest2009-30 subset,verbs account for 14.2% of tokens; in the full setthey account for 13.1%).
We find that the changein BLEU is larger for the constrained test sets, butonly slightly.Experiment 2008 2009 2010 2011baseline 15.7 14.9 16.5 15.4f 15.8 15.0 16.9 15.5g 15.9 15.1 16.9 15.6f + g 15.8 15.0 16.9 15.6Table 6: BLEU scores for full dev/test setsExperiment 2008 2009 2010 2011baseline 16.1 15.7 16.3 15.1f 16.2 15.8 16.9 15.3g 16.4 15.9 16.9 15.4f + g 16.3 15.9 16.9 15.4Table 7: BLEU scores for constrained dev/test sets(max.
30 tokens)6 Related WorkThe problem of verbal complex translation inEnglish-to-German is tackled by Gojun andFraser (2012) in the context of phrase-basedSMT.
They overcome the reordering limitation ofphrase-based SMT by preprocessing the source-side of the training and test data to move En-glish verbs within clauses into more ?German-like?positions.
In contrast, our SCFG-based baselinemodel does not place any restriction on reorderingdistance.Arora and Mahesh (2012) address a similarproblem in English-Hindi translation.
They im-prove a phrase-based model by merging verbs andassociated particles into single tokens, thus simpli-fying the task of word alignment and phrase-pairextraction.
Their approach relies upon the mostly-contiguous nature of English and Hindi verbalcomplexes.
The discontiguity of verbal complexesrules out this approach for translation into Ger-man.Our model adopts a similar constraint-based ex-tension of SCFG to that described in Williams andKoehn (2011).
In that work, constraints are used toenforce target-side agreement between nouns andmodifiers and between subjects and verbs.
Whilstthat constraint model operates purely on the target-side, our verbal complex feature functions alsotake source-side information into account.7 ConclusionWe have presented a model in which a conven-tional SCFG-based string-to-tree system is ex-tended with a rich feature-structure based repre-sentation of German verbal complexes, a gram-matical construction that is difficult for an SMTmodel to produce correctly.
Our feature struc-ture representation enabled us to easily identifywhere our baseline model made errors and pro-vided a means to measure accuracy against the ref-erence translations.
By developing feature func-tions that use source-side information to influenceverbal complex formation we were able to im-prove translation quality, measured both in termsof BLEU score where there were small, consis-tent gains across the test sets, and in terms of task-specific accuracy.In future work we intend to explore the useof richer models for predicting target-side verbalcomplex types.
For example, discriminative mod-els that include non-verbal source features.AcknowledgementsWe would like to thank the anonymous reviewersfor their helpful feedback and suggestions.
The re-search leading to these results has received fund-ing from the European Union Seventh FrameworkProgramme (FP7/2007-2013) under grant agree-ment 287658 (EU-BRIDGE).28ReferencesKarunesh Kumar Arora and R.Mahesh K. Sinha.
2012.Improving statistical machine translation throughco-joining parts of verbal constructs in english-hinditranslation.
In Proceedings of the Sixth Workshop onSyntax, Semantics and Structure in Statistical Trans-lation, pages 95?101, Jeju, Republic of Korea, July.Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Comput.
Linguist., 33(2):201?228.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In HLT-NAACL ?04.Anita Gojun and Alexander Fraser.
2012.
Determin-ing the placement of german verbs in english?to?german smt.
In Proceedings of the 13th Confer-ence of the European Chapter of the Associationfor Computational Linguistics, pages 726?735, Avi-gnon, France, April.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Morristown, NJ, USA.Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 295?302, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Association for Computational Linguistics.Stuart M. Shieber.
1984.
The design of a computer lan-guage for linguistic information.
In Proceedings ofthe 10th international conference on Computationallinguistics, COLING ?84, pages 362?366, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Philip Williams and Philipp Koehn.
2011.
Agree-ment constraints for statistical machine translationinto german.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 217?226,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Philip Williams and Philipp Koehn.
2012.
Ghkmrule extraction and scope-3 parsing in moses.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 388?394, Montr?eal,Canada, June.
Association for Computational Lin-guistics.29
