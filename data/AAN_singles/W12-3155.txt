Proceedings of the 7th Workshop on Statistical Machine Translation, pages 433?441,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsEvaluating the Learning Curve of Domain AdaptiveStatistical Machine Translation SystemsNicola Bertoldi Mauro Cettolo Marcello FedericoFondazione Bruno Kesslervia Sommarive 1838123 Trento, Italy<surename>@fbk.euChristian BuckUniversity of Edinburgh10 Crichton StreetEH8 9AB Edinburgh, UKchristian.buck@ed.ac.ukAbstractThe new frontier of computer assisted transla-tion technology is the effective integration ofstatistical MT within the translation workflow.In this respect, the SMT ability of incremen-tally learning from the translations producedby users plays a central role.
A still openproblem is the evaluation of SMT systems thatevolve over time.
In this paper, we proposea new metric for assessing the quality of anadaptive MT component that is derived fromthe theory of learning curves: the percentageslope.1 IntroductionTranslation memories and computer assisted trans-lation (CAT) tools are currently the dominant tech-nologies in the translation and localization market,but recent achievements in statistical MT have raisednew expectations in the translation industry.
So far,statistical MT has focused on providing ready-to-usetranslations, rather than outputs that minimize theeffort of a human translator.
The MateCAT project1aims at pushing what can be considered the newfrontier of CAT technology: how to effectively inte-grate statistical MT within the translation workflow.One pursued research direction is developing do-main adaptive SMT models, i.e.
models that dynam-ically adapt to the translations that are continuouslyadded to the translation memory by the user dur-ing her/his work.
The ideal goal is to progressivelyreduce the mismatch between training and testing1http://www.matecat.com/data, in such a way that the adapted SMT engine willbe able to provide the user with useful suggestions?
i.e.
perfect or worth being post-edited ?
when thetranslation memory fails to retrieve perfect or almostperfect matches.
Among the well known machinelearning paradigms that fit with this scenario are on-line learning and incremental learning, which basi-cally differ in the amount of data that is employedto dynamically adapt the system: a single piece ofdata in the first case and a batch of data in the lat-ter.
Notice that in both cases one assumes that do-main adaptation is performed efficiently, i.e.
by onlyprocessing the newly received data.
Moreover, al-though the quantity of acquired in-domain data isgenerally limited, their high quality and relevance tothe translation task justify their exploitation by allmeans possible.Domain adaptive SMT embeds two challenges:(1) the design of effective adaptation algorithms, and(2) the evaluation of MT systems evolving over time.Since the ultimate goal of our efforts is to increasethe productivity of human translators, the most ac-curate assessment methodology would be of courseto run a field test.
This way, we could compare pro-ductivity of human translators receiving suggestionsfrom an MT engine featuring dynamic domain adap-tation against the productivity of human translatorsworking with a static MT engine.
As this evaluationis infeasible during daily MT development, we canresort to the several automatic MT metrics, whichhowever, as we will see later, are unsuitable to trackthe dynamic behaviors we are interested to inves-tigate.
Metrics for measuring performance in thecase of interactive MT, see for example (Khadivi,4332008), like Key-Stroke Ratio (KSR), Mouse-ActionRatio (MAR), Key-Stroke and Mouse-Action Ratio(KSMR) are known to correlate well with the pro-ductivity of human translators, but their computationrequires the actual use of an interactive MT system,i.e.
a field test.In the SMART project,2 the evaluation of adap-tive interactive MT is explored (Cesa-Bianchi et al,2008).
While no specific metric is proposed, theanalysis is based on a plot of cumulative differencesof BLEU scores between a baseline and an adaptivesystem.
These differences are computed sentence bysentence and present an interesting view of the dy-namic change of the MT system.
We are going tofurther elaborate on this idea.Other metrics like Character Error Rate (CER)and Translation Edit Rate (TER) would accuratelypredict the translators?
productivity if referenceswere generated by using the CAT system; on thecontrary, references are usually, as in this paper, gen-erated from scratch based only on the source textand can thus be quite far from CAT-based transla-tions, both lexically and syntactically.
The Human-targeted variant of TER, HTER (Snover et al, 2006),needs human intervention and is therefore unfit tomeet our requirements.The main goal of this paper is to design an objec-tive automatic evaluation methodology for an MTsystem adapting over time.
We propose to use thepercentage slope from the theory on learning curvesto measure the learning ability of adaptive MT sys-tems.To assess the proposed metric, we have imple-mented a simple but effective adaptation strategysuitable for an MT system integrated in a CAT tool.We show that the percentage slope is able to exposedifferent dynamic behaviors, such as learning, nolearning, and forgetting.2 Dynamic Adaptation FrameworkIn the MateCAT project scenario, the MT system,which is embedded in the CAT tool to increase thetranslators?
productivity, adapts over time by ex-ploiting translations generated by the user.
Theadapted system is then used to provide the userwith translation suggestions for the next sentences.2http://www.smart-project.euWe refer to this process as dynamic (or incremen-tal) adaptation to emphasize that adaptation hap-pens continuously based on a stream of data.2.1 Abstract View of the Adaptation ProcessFrom an abstract point of view, the framework of in-cremental adaptation can be summarized as follows:i) before the process starts, an initial system isbuilt on available data including a parallel cor-pus;ii) a stream of parallel data becomes available thatis split into blocks of (not necessarily) similarsize;iii) the first/next block is considered, but only thesource is available yet;iv) the latest instance of the adapting system trans-lates the source text of the current block;v) the target part of the current block becomesavailable for use;3vi) the system is adapted using the current parallelblock and possibly all the previous ones;vii) the loop continues from step iii) until all blocksare processed.In each adaptation step, all of the data availableso far can be used, but no look ahead is possible.Note that, in principle, each block is translated witha different instance of the adapting system; hence,the same text occurring in two different blocks canbe translated differently.2.2 Evaluation Goals and RequirementsAlthough dynamic adaptation is closely related tostatic domain adaptation (Foster and Kuhn, 2007),in this scenario we are not interested in the qualityof the final model.
In fact, this model is only avail-able once the stream is depleted and therefore is notused anymore.What we are interested in, and what we want tocompare among different approaches, is the systemsevolvement over time.Consider a translator who uses such an incremen-tally adapting system and performs post-editing onits suggested translations.
The highest productivity3In the CAT framework, the target part of a block is thetranslation post-edited by the user.434gain is achieved when the adaptation is quick andpersistent.Even though in this paper we are concerned withan automatic metric, it is important to keep the usecase of CAT in mind, in particular the presence ofa human translator.
The TransType2 project4 hasfound that repeated correction of the same error isstrongly disliked by editors (Macklovitch, 2006) andmay lead to rejection of the entire system.
Similarly,segments that were translated correctly by previous,less adapted systems, should not be negatively af-fected by updates.
We will refer to these particularaspects of adaptation as backward reliability.Automatic measures, which are aimed at staticMT modules, can not take the evolution of the sys-tem into account and are therefore unable to pinpointsuch problems.
Thus, they are not suitable for thedynamic adaptation scenario.A new evaluation methodology should satisfy thefollowing requirements:?
ability to compare different strategies?
show behavior over time and reward early im-provements and consistent adaptation?
expose possible overfitting, i.e.
check whethergeneralization is lost due to overly aggressiveadaptation?
strong correlation to human productivity?
estimate benefit over a static baseline modelwithout adaptation?
check backward reliability.2.3 Evaluation ProtocolThe performance of adaptive systems as sketchedin Section 2.1 is evaluated on different parts of thestream as opposed to the global evaluation used forstatic systems.
We distinguish between two proto-cols which differ in their use of historic data.For block-wise evaluation only the translations ofthe most recent block are evaluated with respect tothe correct translations once these become available.Any static automatic MT score, e.g.
TER (Snoveret al, 2006), BLEU (Papineni et al, 2001), can beused, provided that it is reliable on a block of usuallyrelatively small size.In contrast, in incremental evaluation the scoresare computed on all blocks available so far.
The4http://tt2.atosorigin.estranslations of previous blocks are kept fixed, i.e.blocks are not translated again once a newly adaptedsystem becomes available as this new system has al-ready seen this data.Both the block-wise and incremental protocolsyield a sequence of scores that reflects the adaptationbehavior over time.
The former is useful to exposepotential weaknesses as discussed above: we expectto see improvement at first and after a while, whenenough adaptation data is available, a level curve.
Ifthis is not the case, this indicates a problem:i) should the scores deteriorate over time wemight be facing overfitting, possibly due to un-expected heterogeneity in our corpus;ii) if the scores continue to improve, then the adap-tation method is not aggressive enough and thesystem underfits.The incremental evaluation on the other hand allowsfor easy comparison of different adaptation strate-gies.
While the performance on the most recentblock becomes less important over time, the perfor-mance on all the blocks processed so far nicely re-flects the utility of the system in the application set-ting.The metric we are going to propose in the nextsection processes such sequences of partial scores.It accumulates the trend into a single number andoffers an interpretation that relates adaptive behaviorto productivity gains.3 The Percentage SlopeLearning curves (see (Stump P.E., 2002) for a de-tailed introduction) are mathematical models usedto estimate the efficiency gain when an activity isrepeated.
The learning effect was noted in indus-trial environment: the underlying notion is that whenpeople repeat an activity, there tends to be a gain inefficiency.
That is exactly the expected behavior ofour dynamically adapting MT system: it should im-prove its performance on texts including terms andexpressions whose proper translation has been pre-viously provided.
Thus we decided to exploit ele-ments from learning theory to measure the evolutionof translation capability.Several learning curve models have been pro-posed, but only two are in widespread use, the unit435(U) model due to Crawford and the cumulative av-erage (CA) model due to Wright.
Both models arebased on a common mathematical form:y = axb (1)where:a represents the theoretical labor hours requiredto build the first unit produced (a positive num-ber)b represents the rate of learning (negative value,except for ?forgetting?
)x represents the number of an item in the produc-tion sequence (unit #1,#2,#3, .
.
.
)The models differ in the interpretation of y:U: y is the labor hours required to build unit #xCA: y is the average labor hours per unit requiredto build the first x unitsSince b is a mathematically appropriate butcounter-intuitive number for describing the slope,the percentage slope S is typically used:S = 10b log10(2)+2 (2)S provides the rate of learning on a scale of 0 to 100,as a percentage.
A 100% slope represents no learn-ing at all, zero percentage reflects a theoretically in-finite rate of learning.
In practice, human operationshardly ever achieve a rate of learning faster than 70%as measured on this scale.The correspondence between our block-wise eval-uation (Section 2.3) with the U model, and the incre-mental evaluation with the CA model is straightfor-ward.
In the first case, y is the number of errorsdone in the translation of the block #x; in the sec-ond case, y is the average number of errors (that isthe TER score or the 100-BLEU score) made on thefirst x blocks.From a practical point of view, the sequence ofscores can be provided while the adapting system isbeing used; the learning curve which best matchesthe sequence is then found5 and eventually the per-centage slope S is computed.5Notice that the best fitting learning curve can be estimatedin the log scale with a simple linear regression analysis.set #sent.
#src words #tgt wordstrain 1.2M 18.9M 19.4Mtest 3.4k 57.0k 61.4kTable 1: Overall statistics on parallel data of the ITdomain used for training and testing the SMT system.Counts of (English) source words and (Italian) targetwords refer to tokenized texts.4 ExperimentsIn order to test-drive the evaluation metric intro-duced in Section 3, several SMT systems showingeffective, weak, poor or absent adaptation capabil-ity have been developed.
Moreover, a preliminaryinvestigation on backward reliability has been car-ried out.
The next paragraphs detail and discuss theexperiments performed.4.1 DataThe task considered in this work involves the trans-lation from English into Italian of documents in theInformation Technology (IT) domain.The training set consists of a large TranslationMemory in the IT domain and several OPUS6 sub-corpora, namely KDE4, KDEdoc and PHP.
The testset includes the human generated translation of 6documents, disjoint from the training set.
Althoughin the same domain, the test set is quite differentfrom the training data as shown by comparing val-ues of perplexity (650 vs. 40) and OOV rate (2.4%vs.
0.4%) computed on the source side.7 Further-more, the 6 documents significantly differ amongeach other: perplexity and OOV rate range from 465to 880 and from 0.8 to 3.3, respectively.
Table 1 col-lects overall statistics on training and test sets.To simulate the stream of fresh data, the IT testset has been split into blocks of about a thousand8words each.
Before splitting, sentences have beenscrambled, with the rationale of generating a largenumber of homogeneous blocks, simulating a testset consisting of a single document.6http://opus.lingfil.uu.se7Figures for the training data were measured through across-validation technique.8Different sizes have been also considered (three and fivethousands) to test different adaptation rates, but results werequalitatively similar to those on shorter blocks and then are notreported.4364.2 Baseline SystemThe SMT baseline system is built upon the open-source MT toolkit Moses9 (Koehn et al, 2007).The translation and the lexicalized reordering mod-els are estimated on parallel training data with thedefault setting; a 5-gram LM smoothed through theimproved Kneser-Ney technique (Chen and Good-man, 1999) is estimated on monolingual texts viathe IRSTLM toolkit (Federico et al, 2008).
Here-inafter, these models are referred to as background(BG) models.
The log-linear interpolation weightsare optimized by means of the standard MERT pro-cedure provided within the Moses toolkit.4.3 Adaptive SystemThe adapting SMT system is built on Moses as well.Besides the BG models of the baseline system, trans-lation, reordering and language models estimated onthe stream of fresh data are employed as additionalfeatures.
Hereinafter, these models are referred toas foreground (FG) models.
Unless differently spec-ified, the FG models employed to translate a givenblock are trained on all preceding blocks.
Note thatthe first instance of the adapting system (i.e.
thattranslating the first block) is exactly the baseline sys-tem, because no adaptation data is available to trainFG models yet.
FG translation and reordering mod-els are trained in the same way as the BG models.Due to the limited amount of adaptation data, the FGLM is a 3-gram LM smoothed through the more ro-bust Witten-Bell technique (Witten and Bell, 1991).The interpolation weights are inherited from acompanion system trained and tuned on a differentdomain ?
official documents of the European Unionorganization ?
and are kept fixed.4.4 Experiments on Adaptive SMTFirst of all, the baseline and adapting systems wererun on the scrambled test set and compared at bothblock-wise and incremental mode (see Section 2.3).Figure 1 plots block-wise TER and BLEU scoresof the baseline and adapting systems as functions ofthe amount (number of words) of adaptation data.On one hand, it can be guessed that the adaptingsystem performs gradually better and better than thebaseline; on the other hand, it is evident that such9http://www.statmt.org/mosesplots are not the most effective way to show the evo-lution of the adapting system.
In fact, the transla-tion difficulty of contiguous blocks can differ a lot.Hence, scores computed on them are not comparableand the corresponding curves are jagged.The block-wise differences of TER and BLEUscores between the adapting and the baseline sys-tems are plotted in Figure 2: the plots are nowcleaner and more readable and vaguely suggest apositive trend, but still remain too jagged and do notprovide any information about the absolute perfor-mance of the systems.Figure 3 plots the incremental TER and BLEUscores of the baseline and adapting systems as func-tions of the amount of adaptation data.
First of all,it is worth noting that the right-most values are thescores computed on the whole test set.
In standardevaluation, those would be the only scores providedto show how the adapting system outperforms thebaseline system; in particular, the relative improve-ment is larger for TER (9.3%) than for BLEU (3.9%)supposedly because tuning was performed to opti-mize BLEU score which thus is harder to improve.However, the overall scores obscure the way theyare reached, that is the evolution over time of thesystems, which is especially important for adaptivesystems.Secondly, the incremental evaluation yields muchsmoother plots clearly showing that after initial fluc-tuations: (i) performance of the baseline stabilizesaround an average which does not change over time;(ii) scores of the adapting system tend to get increas-ingly better as more adaptation data is available forupdating FG models.The evaluation metric we are proposing, the per-centage slope introduced in Section 3, is indeed ableto spot such kind of paradigmatic behaviors as wewill see in the next section.
But before going onwith the assessment of the metric, some further com-ments on Figure 3:?
in early stages, the adaptation is not effective,likely because of the scarcity of data.
Thisraises two issues: design of more effectiveadaptation strategies and, in the CAT frame-work, identifying the appropriate time to re-place the baseline with the adapting system;?
the adaptive system outperforms the baseline in4374045505560650  10000  20000  30000  40000  50000  60000TER(%)# Number of Wordsadabsln16182022242628303234360  10000  20000  30000  40000  50000  60000BLEU(%)# Number of WordsadabslnFigure 1: Block-wise TER (on the left) and BLEU (right) scores of the baseline and the dynamically adapting systems.-12-10-8-6-4-20240  10000  20000  30000  40000  50000  60000TER(%)# Number of Wordsada -- bsln-8-6-4-2024680  10000  20000  30000  40000  50000  60000BLEU(%)# Number of Wordsada -- bslnFigure 2: Block-wise TER (left) and BLEU (right) differences between the baseline and the dynamically adaptingsystems.terms of TER very soon, while the overtakingwith regard to BLEU is observed much later.This is because the baseline SMT system wastuned with respect to the BLEU score on in-domain data, differently to the adapting system.Both these issues are out of the scope of this paperand will be subject of future investigations.4.5 Assessment of the Percentage SlopeTo assess its effectiveness, the percentage slope hasbeen computed on errors committed by the baselinesystem, the adapting system and an adapting systemfeaturing only FG models (that is without BG mod-els).
The FG-only system was used to translate eachblock either fairly and unfairly: the former mode fitsthe adaptation process sketched in Section 2.1; in thelatter mode, the FG model is adapted on the blockbefore its translation starts.Figure 4 shows the TER and BLEU scores of suchsystems in the incremental evaluation.
The four dif-ferent behaviors are expected to correspond to dif-ferent percentage slopes.
In fact, the S values col-lected in Table 2 confirm the expectations:?
the baseline, completely unable to learn, has infact an S of 100%?
the adapting system, that learns through a dy-namic adaptation of FG models and generalizesthanks to BG models, has an S of 96-98%?
the FG-only adapting system tested in unfairmode worsens its performance as the modelsbecome larger, i.e.
less focused on the block tobe translated: this is evidenced by an S greaterthan 100%438505152535455565758590  10000  20000  30000  40000  50000  60000TER(%)# Number of Wordsadabsln2121.52222.52323.52424.52525.50  10000  20000  30000  40000  50000  60000BLEU(%)# Number of WordsadabslnFigure 3: Incremental TER (left) and BLEU (right) scores of the baseline and the dynamically adapting systems.modelsystembaseline adaptingFG-only adaptingfair unfairU 100.4 96.9 96.2 107.2CA 100.3 97.7 96.5 107.4Table 2: S values of 4 SMT systems (see text) forthe block-wise TER evaluation, corresponding to the Umodel, and the incremental evaluation, corresponding tothe CA model.?
the FG-only adapting system tested in fairmode increases its performance as the modelsbecome larger, i.e.
more general, as evidencedby an S similar to that of our original adaptingsystem (96%).Therefore, we can state that S exposes commonbehaviors of evolving SMT systems; however, stan-dard metrics like TER and BLEU are still in chargeof providing absolute performance measures.In order to give a hint for properly interpret-ing the values reported, we summarize the discus-sion in (Stump P.E., 2002) about ?typical learningslopes?.
Operations that are fully automated tendto have slopes of 100%, 70% if entirely manual, anintermediate value if mixed.
In real industrial envi-ronments, the average slope depends on the type ofmanufacturing activity: for example, in aircraft in-dustry it is about 85%, it ranges in 90-95% in elec-tronics and in machining.
Hence, a 96-98% slopeas we measured in our experiments must be con-sidered a significant learning ability of a fully au-tomated system.4.6 Experiments on Backward ReliabilityA proper assessment of the backward reliability ofan evolving system as defined in Section 2.2 wouldrequire the identification of patterns translated dif-ferently by the system during its life.
We will inves-tigate this issue in the future.
For the moment, wetry to attack the problem from a global point of view:we simply check that the adaptive system does ?re-member?
its previous translation capabilities ?on av-erage?, while it learns to better translate novel texts.To this end, a cross-validation policy was fol-lowed: the first two thirds of each test set documentare used for dynamically training the FG models,while the remaining portions are used as held-outtest sets.Figure 5 reports the TER and BLEU scores onthe 6 test sets of three systems: the baseline sys-tem (bsln), the adapting system (ada) fed by in-crementally merging the available reduced adapta-tion sets, and the system adapted on all adaptationdata sets (final).The final system achieves performance closeto ada system on each held-out set; this reveals thatour adaptation process is effective both in learningand in remembering.We think that the monitoring of the backward re-liability of adapting systems is a good practice.
Across validation scheme like ours allows not only toreveal the backward reliability as shown before, butalso to discover the forgetting trend of, for example,an MT system featuring an overly aggressive learn-43920253035404550556065700  10000  20000  30000  40000  50000  60000TER(%)# Number of WordsbslnadaFGonly fair adaFGonly unfair ada102030405060700  10000  20000  30000  40000  50000  60000BLEU(%)# Number of WordsbslnadaFGonly fair adaFGonly unfair adaFigure 4: Incremental TER (left) and BLEU (right) of 4 systems showing different learning slopes.ing method.
On the other hand, it only provides cuesabout the average behavior and it is not as quicklyinformative as a single score could be.
Hence, thedesign of a proper metric for measuring the back-ward reliability of MT systems is a challenging taskthat should be faced by the research community.5 Summary and Future WorkThe evaluation of a dynamically adapting system isan open issue.
Metrics used in interactive MT suchas HTER or field tests, are infeasible in the daily de-velopment as they involve human translators/judges.On the other hand, standard MT evaluation met-rics either do not expose changes over time (BLEU,TER) or cannot be applied (CER).The main contribution of this paper is to proposethe use of the percentage slope for the evaluation ofadapting MT systems, a metric borrowed from thetheory on learning curves.
For assessing its effec-tiveness, we have developed a simple but effectiveadapting SMT system suitable to work in the contextof a CAT tool supported by MT.
We have comparedseveral ways to plot the change in error rate overtime for different systems and identified the mostsuitable for computing the percentage slope.
Finally,we have shown that the percentage slope well ex-poses the paradigmatic behaviors of evolving SMTsystems.The MateCAT project has scheduled field testsfor the near future which will allow for inclusionof human productivity in the assessment of the per-centage slope.
Moreover, efforts will be devoted tothe design of adaptation techniques which are moresophisticated than the simple approach used in thiswork.We have also identified the issue of backward re-liability of an adapting system, that is the ability tolearn without forgetting the past, and the importanceof monitoring it.
A best practice based on a crossvalidation scheme has been proposed.
Future inves-tigations will concern finding an effective metric tomeasure backward reliability.AcknowledgmentsThis work was supported by the MateCAT project,which is funded by the EC under the 7th FrameworkProgramme.ReferencesN.
Cesa-Bianchi, G. Reverberi, and S. Szedmak.
2008.Online learning algorithms for computer-assistedtranslation.
Deliverable 4.2, SMART project (FP6).http://www.smart-project.eu/files/D42.pdf.S.
F. Chen and J. Goodman.
1999.
An empirical study ofsmoothing techniques for language modeling.
Com-puter Speech and Language, 4(13):359?393.M.
Federico, N. Bertoldi, and M. Cettolo.
2008.IRSTLM: an Open Source Toolkit for Handling LargeScale Language Models.
In Proc.
of Interspeech, pp.1618?1621, Melbourne, Australia.G.
Foster and R. Kuhn.
2007.
Mixture-Model Adapta-tion for SMT.
In Proc.
of WMT, pp.
128?135, Prague,Czech Republic.S.
Khadivi.
2008.
Statistical Computer-Assisted Trans-lation.
Ph.D. thesis, RWTH Aachen University,440455055606570751  2  3  4  5  6TER(%)Documentbslnadafinal182022242628303234361  2  3  4  5  6BLEU(%)DocumentbslnadafinalFigure 5: TER (left) and BLEU (right) scores of the baseline system, the evolving system and the final adapted systemon the document-specific held-out test sets.Aachen, Germany.
Advisors: Hermann Ney and En-rique Vidal.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Proc.
ofACL: Demo and Poster Sessions, pp.
177?180, Prague,Czech Republic.E.
Macklovitch.
2006.
Transtype2: The last word.
InProc.
of LREC 2006, Genoa, Italy.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Research Report RC22176, IBM ResearchDivision, Thomas J. Watson Research Center.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.Makhoul.
2006.
A study of translation edit rate withtargeted human annotation.
In Proc.
of AMTA, Boston,US-MA.E.
Stump P.E.
2002.
All about learning curves.
In Proc.of SCEA.
http://www.galorath.com/images/uploads/LearningCurves1.pdf.I.
H. Witten and T. C. Bell.
1991.
The zero-frequencyproblem: Estimating the probabilities of novel eventsin adaptive text compression.
IEEE Trans.
Inform.Theory, IT-37(4):1085?1094.441
