Coling 2010: Poster Volume, pages 1176?1184,Beijing, August 2010Notes on the Evaluation of Dependency ParsersObtained Through Cross-Lingual ProjectionKathrin SpreyerDepartment of LinguisticsUniversity of Potsdamspreyer@uni-potsdam.deAbstractIn this paper we address methodologicalissues in the evaluation of a projection-based framework for dependency parsingin which annotations for a source lan-guage are transfered to a target languageusing word alignments in a parallel cor-pus.
The projected trees then constitutethe training data for a data-driven parser inthe target language.
We discuss two prob-lems that arise in the evaluation of suchcross-lingual approaches.
First, the anno-tation scheme underlying the source lan-guage annotations ?
and hence the pro-jected target annotations and predictionsof the parser derived from them ?
is likelyto differ from previously existing goldstandard test sets devised specifically forthe target language.
Second, the stan-dard procedure of cross-validation cannotbe performed in the absence of parallelgold standard annotations, so an alterna-tive method has to be used to assess thegeneralization capabilities of the projectedparsers.1 IntroductionThe manual annotation of treebanks for natu-ral language parsing is time-consuming and ex-pensive, but the availability of such resourcesis crucial for data-driven parsers, which requirelarge amounts of training examples.
A techniqueknown as annotation projection (Yarowsky andNgai, 2001) provides a means to relax this re-source bottleneck to some extent: In a word-aligned parallel corpus, the text of one language(source language, SL), say English, is annotatedwith an existing parser, and the word alignmentsare then used to transfer (or project) the result-ing annotations to the other language (target lan-guage, TL).
The projected trees, albeit noisy, canthen constitute the training data for data-drivenTL parsers (Hwa et al, 2005; Spreyer and Kuhn,2009).
Finally, in order to assess the quality of theprojected parser, its output needs to be comparedto held-out TL test data.Two problems arise in the evaluation of suchapproaches.
First, the annotations projected fromthe SL usually differ stylistically from those foundin the TL test data, rendering any immediate com-parison between the predictions of the projectedparser and the gold standard meaningless.
We dis-cuss the use of tree transformations for evaluationpurposes, namely to consolidate discrepancies be-tween the annotation schemes.
We then presentexperiments that investigate the influence of theannotation scheme used in training on the general-ization capabilities of the resulting parser.
We alsobriefly address the interaction between annotationstyle and parsing algorithm (transition-based vs.graph-based).The second problem addressed here is the as-sessment of variance in the training data, andhence in parser quality.
The standard proce-dure for this purpose would be cross-validation.However, the popular data sets used for bench-marking parsers, such as those that emerged1176from the CoNLL-X shared task on dependencyparsing (Buchholz and Marsi, 2006), are typi-cally based on monolingual text.
This meansthat cross-validation is unavailable for projection-based frameworks, because no projection can beperformed for the training splits in the absence ofa translation in the SL.
We therefore propose a val-idation scheme which accounts for training datavariance by training a parser multiple times, onrandom samples drawn from the projected train-ing data.
Each of the obtained parsers can subse-quently be evaluated against a fixed, held-out testset independent of the projection step, and the ar-ray of accuracy measurements thus obtained canbe further subjected to significance testing to ver-ify that observed performance differences are notmerely random effects.The paper is structured as follows.
Section 2describes the projection framework we are assum-ing.
Section 3 summarizes and contrasts the char-acteristics of four different annotation schemesunderlying our SL parsers (English, German) andTL test data (Dutch, Italian).
Experiments withdifferent annotation schemes and parsing algo-rithms are presented in Section 4.
In Section 5 wediscuss variance assessment in more detail.
Sec-tion 6 concludes.2 The Projection FrameworkThis section briefly describes how we obtain de-pendency parsers for new languages via annota-tion projection in a parallel corpus.
A detailed dis-cussion can be found in Spreyer and Kuhn (2009).We use the Europarl corpus (Koehn, 2005) asour parallel corpus.
It comprises parallel datafrom 11 languages; in this paper, we present ex-periments with English and German as SLs, andDutch and Italian as TLs.First, the bitexts for the language pairs un-der consideration (English-Dutch, English-Italian,German-Dutch, and German-Italian) are word-aligned using Giza++ (Och and Ney, 2003), andall texts are part-of-speech tagged with the Tree-Tagger (Schmid, 1994) according to pre-trainedmodels.11Available from http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html.the minutes of the sittingde notulen van de vergaderingFigure 1: Dependency tree projection from En-glish to Dutch.Second, we annotate the SL portions, i.e., theGerman and English texts, with MaltParser de-pendency parsers (Nivre et al, 2006) trained onstandard data sets for the two languages; specifi-cally, we are using the baseline parsers of ?vrelidet al (2010).
The English training data consists ofthe Wall Street Journal sections 2?24 of the PennTreebank (Marcus et al, 1993), converted to de-pendencies (Johansson and Nugues, 2007).
Thetreebank data used to train the German parser isthe Tiger Treebank (Brants et al, 2002), in theversion released with the CoNLL-X shared task(Buchholz and Marsi, 2006).Given the SL dependency trees, we project thedependencies to the corresponding (i.e., aligned)TL elements as shown in Figure 1.
The links be-tween the English and Dutch words indicate theword alignment.
We postulate edges between TLwords (e.g., de and notulen) if there is an edgebetween their respective SL counterparts (the andminutes).The projected dependencies are then used astraining data for TL (Dutch and Italian) depen-dency parsers.
In order to account for the factthat many of the projected dependency structuresare incomplete due to missing alignments or non-parallelism of the translation, we employ fMalt(Spreyer and Kuhn, 2009), a modified version ofthe MaltParser which handles fragmented trainingdata.
We restrict the admissible fragmentation tothree fragments per sentence, for sentences withfour or more words, based on early experimentswith automatically labeled Dutch data.
Sentencesthat receive more fragmented analyses are dis-carded.Finally, we evaluate the projected TL parsersagainst gold standard test sets by parsing theTL test data and comparing the parser output to1177PTB (en) Tiger (de) Alpino (nl) TUT (it)NP/PPPrep Det Noun Prep Det Noun Prep Det Noun Prep Det NounauxiliariesAux Verb Aux Verb Aux Verb Aux Verbsubord.
clausesComp Verb Comp Verb Comp Verb Comp Verbrelative clausesRel Verb Rel Verb Rel Verb Rel VerbcoordinationX1 Conj X2 X1 Conj X2 X1 Conj X2 X1 Conj X2Table 1: Different annotation schemes in dependency-converted treebanks.the reference annotations.
However, we discussbelow how differences in annotation style pro-hibit a direct comparison, and how the annotationschemes affect the learnability of the grammar andtherefore the accuracy of the derived parsers.3 Annotation SchemesIn a projection setting like the one describedabove, we deal with two sets of annotations: thoseprojected from the SL, and those marked up in theTL gold standard.
The four annotation schemeswe compare here are those used in the Penn Tree-bank (PTB; WSJ sections) (Marcus et al, 1993)for English, the Tiger Treebank (Brants et al,2002) for German, the Alpino Treebank (van derBeek et al, 2002) for Dutch, and the Turin Uni-versity Treebank2 (TUT) for Italian.Table 1 illustrates the most obvious differencesamong the annotation schemes.
Note that wecompare annotations in the dependency-convertedformat.
This restricts the comparison to attach-ment decisions and eliminates the bracket bias in-herent to constituent-based comparisons (Carrollet al, 1998; Rehbein and van Genabith, 2007).Again, we use the dependency-converted data setsof the CoNLL-X shared task.As shown in the table, both the English and the2http://www.di.unito.it/?tutreebDutch treebank annotate prepositional phrases hi-erarchically, with an embedded NP.
The flat an-notation scheme of the German treebank, on theother hand, makes every word in the PP a depen-dent of the preposition (with some exceptions).The Italian annotation scheme assumes a hierar-chical structure like English and Dutch, but de-clares the determiner rather than the noun as thehead of nominal phrases.
Another idiosyncrasyof the Italian annotation scheme is the treatmentof fused prepositions such as della which incor-porate the determiner of the embedded NP: In thedependency-converted TUT, these fused preposi-tions are represented as two separate tokens, onetagged as a preposition, the other as a determiner.Next, auxiliaries take the lexical verb as theirdependent in all treebanks except the Italian TUT,which inverts the dependency, resulting in a flatstructure with the lexical verb as its head.
Thestructure of subordinate clauses is hierarchical ac-cording to the English, Dutch and Italian anno-tation schemes, but flat in Tiger, with the com-plementizer as a dependent of the embedded verb.Relative clauses, on the other hand, are assigneda flat structure in all but the Dutch scheme, wherethe relativizer is the head of the embedded verb.Finally, coordination is annotated in three differ-ent ways: While the treebanks for English andItalian implement a strictly right-branching strat-1178egy, the German annotation scheme attaches boththe conjunction and the second conjunct to thefirst conjunct.
The Dutch treebank annotates coor-dinations as flat structures, with all conjuncts de-pending on the conjunction.In order to evaluate projected parsers, any dif-ferences in the source and target annotations needto be consolidated.
A straightforward way ofdoing so is by means of tree transformations.Naturally, this begs the question of where suchtransformations should take place: One couldtransform the projected annotations to conformto the reference annotations encountered in thetest set; alternatively, one can manipulate the testset to reflect the annotation decisions adopted inthe source annotations.
A variant of the formerapproach has been implemented by Hwa et al(2005).
They apply post-projection transforma-tions to Chinese training data projected from En-glish in order to infuse TL-specific informationwhich has no counterpart in the source language.We argue in favor of the alternative, since in apractical application scenario, where rapid, inex-pensive development plays a prominent role, it isconceivable that the SL annotation scheme wouldbe adopted unaltered for the TL parser.
Con-sider, for instance, an architecture for multilingualsyntax-based information retrieval which is basedon parsers for various TLs, all to be derived from asingle SL.
Devising a tailored annotation schemefor each of the TLs would require linguisticallytrained personnel with extensive knowledge of thelanguages at hand.
By contrast, adhering to the SLannotation scheme results in homogeneous parseroutput across the TLs and thus facilitates stream-lined higher-level processing.In Section 4 we present experiments thatinvolve the language pairs English?Dutch,German?Dutch, English?Italian, and German?Italian.
For each of the TLs Dutch and Italian,we therefore derive transformed test sets for eachSL: one version according to the English PTBannotation style to evaluate the parsers projectedfrom English, and another version according tothe German Tiger-style annotations to evaluateparsers projected from German.
As an example,Table 2 illustrates the transformations performedon the Italian test set for the parser projected fromTUT (it) ?
PTB (en)NP/PPPrep Det Noun ?
Prep Det NounauxiliariesAux Verb ?
Aux VerbfusedprepositionsPrepDetp PrepDetd ?
PrepDetTable 2: Transformations performed on the Italiantest set for the parser projected from English.a.
lang orig PTB Tigernl ?
69.21 67.38it ?
66.44 53.09b.
lang orig PTB Tigernl 79.23 80.79 79.19it 88.52 86.88 84.02Table 3: Unlabeled attachment scores obtained bytraining MaltParsers on (a) projected and (b) goldstandard dependencies according to different an-notation schemes.English.4 Annotation Scheme Experiments4.1 LearnabilityIf the annotation style is carried over from thesource language as we suggest above, we mayask: Is one annotation scheme more appropriatethan the other?
When more than one source lan-guage (annotation scheme) is available, will oneproduce more ?learnable?
TL annotations than theother?
We explore these questions experimentally.Table 3a shows the performance of Dutch (?nl?
)and Italian (?it?)
MaltParsers trained on annota-tions projected from English (?PTB?)
and German(?Tiger?
), as evaluated against the respective trans-formed Dutch and Italian gold standards.Looking at the results for Dutch, we find thatthere is indeed a significant difference betweenthe parser projected from English and the oneprojected from German.
The former, generatingPTB-style dependencies, achieves 69.21% unla-1179lang.
words/sent words/frag frags/senten?nl 27.83 1.95 14.25de?nl 27.55 1.98 13.92en?it 28.86 2.26 12.79de?it 28.79 1.66 17.33Table 4: Average fragmentation in the projecteddependencies.beled attachment score (UAS).
According to a t-test (cf.
Section 5), this is significantly (p<0.01)better than the parser projected from GermanTiger-style annotations, which achieves 67.38%.Turning to Italian, the parser projected fromthe English PTB-style annotations again performsbetter.
However, the huge difference of 13.35%UAS suggests a more fundamental underlyingproblem with the word alignment between theGerman and Italian sentences.
And indeed, in-spection of the degree of fragmentation in the Ital-ian projected dependencies (Table 4) confirms thatconsiderably more edges are missing in the de-pendencies projected from German than from En-glish.
Missing edges are an indication of missingword alignment links.In order to control such factors and focusonly on the learnability of the different anno-tation schemes, we report in Table 3b the re-sults of training on gold standard monolingualtreebank data (distinct from the test data), trans-formed ?
like the test sets ?
to conform with theEnglish and German annotation scheme, respec-tively.3 In addition, the column labeled ?orig?shows the performance obtained when the origi-nal (dependency-converted) Alpino/TUT annota-tion scheme is used.
For Italian, the results cor-roborate those obtained with the projected parsers:training on the PTB-transformed treebank is sig-nificantly4 (p<0.01) more effective than trainingon the Tiger-transformed treebank.
The origi-nal TUT scheme is even more effective (p<0.01),which comes as no surprise given that the TUTguidelines were tailored to the traits of the Italian3We did not attempt parameter optimization, so the fig-ures reported here do not represent the state-of-the-art in de-pendency parsing for either language.4According to Dan Bikel?s Randomized Parsing Eval-uation Comparator: http://www.cis.upenn.edu/?dbikel/software.html#comparatorparser orig PTB TigerMST 81.41 83.01 83.87Tiger ?
PTB > origMalt 79.23 80.79 79.19PTB > orig > TigerTable 5: UAS of the Dutch MST parsers trainedon gold standard dependencies.
(MaltParser re-sults repeated from Table 3b.
)language.The Dutch parser, too, responds better to thehierarchical PTB-based annotation scheme thanto the flat Tiger scheme (p<0.01).
In fact, italso outperforms the parser trained with the orig-inal Alpino annotations (p<0.01).
This demandsfor further investigation, reported in the followingsection.4.2 Interaction with Parsing AlgorithmsThe results in Table 3 affirm that the performanceof a parser hinges on the annotation scheme thatit is trained on.
However, the learnability of agiven scheme depends not only on the annotationdecisions, but also on the parsing algorithm im-plemented by the parser.
For instance, it has beennoted (Joakim Nivre, p.c.
2008) that flat coordina-tion structures like those in the Alpino Treebankgenerally pose a challenge to incremental, deter-ministic parsers like MaltParser.In order to see to what extent our results areinfluenced by characteristics of the MaltParser,we repeated the experiments with the MST parser(McDonald et al, 2005), focusing on Dutchparsers from gold standard training data.5The MST parser is a graph-based dependencyparser which considers all possible edges to findthe globally optimal tree.
The results of the MSTexperiments are given in Table 5, together withthe corresponding Malt results repeated from Ta-ble 3b.
We observe that the relative learnabilityranking among the three annotation schemes is in-deed different with MST.
While in the transition-based paradigm the original Alpino annotationsstill appeared more adequate for training than the5With projected training data for Dutch, and in all ex-periments with Italian, MST produced the same pattern ofrelative performance as Malt.1180trans Malt MSTnone 79.23 81.41coordinationen 80.91 83.01relativeen 79.21 81.81allen 80.79 83.01coordinationde 79.39 82.19relativede 79.21 81.81subordde 79.47 82.67np/ppde 80.73 83.83allde 79.19 83.87Table 6: Impact of individual transformations onDutch treebank parsers.
Significant improvements(p<0.01) over original Alpino annotation (?none?
)are in bold face.Tiger trees, it is now outperformed by both thePTB and the Tiger trees under the graph-based ap-proach.
There is no significant difference betweenthe Tiger-based and the PTB-based parser.To shed some light on the unexpected rank-ing of the Alpino annotation scheme, we look atthe impact of the individual transformations sep-arately in Table 6.
The upper part of the tableshows how the transformations of the Alpino datatowards PTB-style annotations affects learnabil-ity.
We find that both the MaltParser and the MSTparser benefit from the right-branching coordina-tion markup of the PTB scheme.
The attachmentof relativizers in relative clauses seems to playonly a minor role and makes no significant dif-ference.Turning to the Tiger-style transformations, firstnote that the semi-flat coordination adopted in theGerman treebank does not seem to be superior tothe flat annotations in Alpino: no significant im-provement is achieved for either parser by usingthe former (?coordinationde?).
Surprisingly, bothparsers benefit from the flat annotation of prepo-sitional phrases (?np/ppde?).
The MST parser, butnot the MaltParser, further takes advantage of theflat subordination structure annotated in Tiger.
Asmentioned earlier, this is in line with the funda-mentally different parsing paradigms representedby Malt and MST.We tentatively conclude that the MST parseris in fact better at exploiting the flat aspects ofthe Tiger annotations, while both parsers largelybenefit from the highly hierarchical coordinationstructure of the PTB annotation scheme.
A moredetailed exploration of these issues is clearly inorder, and subject to future research.4.3 DiscussionKu?bler et al (2008) present an extensive compar-ison of two German treebanks: the Tiger treebankwith its rather flat annotation scheme, and theTu?Ba/DZ treebank with more hierarchical struc-tures.
They find that the flat Tiger annotationscheme is more easily learned by constituent-based (PCFG) parsers when evaluated on a depen-dency level.
Our results suggest the opposite, butthis may well be due to the differences in the ex-perimental setup: Our training data represent de-pendency trees directly, and we learn incremen-tal, deterministic dependency parsers rather thanPCFGs.5 Variance AssessmentThe second question we address in this paper isthe assessment of variance in the training data,and hence in parser quality.
The standard proce-dure for this purpose would be cross-validation.To perform k-fold cross-validation, the data is par-titioned into k splits of equal size, and one of thesplits is used as test data, while the remaining k-1splits serve as training data.
The train?test cycle isrepeated until each of the k subsamples has beenused as test data exactly once.However, the popular data sets used for bench-marking parsers, such as the CoNLL-X sharedtask data used here, are typically based on mono-lingual text.
This means that cross-validation isunavailable for projection-based frameworks, be-cause no projection can be performed for the train-ing splits in the absence of a translation in the SL.Moreover, the expected noise level in the pro-jected dependencies requires that there be a con-siderable amount of training data for an evaluationto be meaningful.
So even if parallel test data isavailable, the data partitioning performed in cross-validation may compromise the results.We therefore propose a validation schemewhich (i) does not reduce the amount of test databy partitioning (this may be a problem when onlya small number of gold standard annotations is1181nlptb nltig itptb ittig68.51 67.25 66.56 54.0170.07 66.79 66.45 54.2169.21 68.13 66.07 53.3769.45 68.29 66.47 52.7768.47 67.31 66.74 52.5569.07 66.97 66.20 53.6669.99 67.87 66.56 52.7069.71 66.43 66.37 52.7068.77 67.11 66.05 52.0868.83 67.67 66.96 52.82mean 69.21 67.38 66.44 53.09sd 0.58 0.60 0.29 0.69Table 7: Intra-system variance assessment.available), (ii) does not require parallel test dataand is independent of the projection step, and (iii)takes advantage of the fact that training data ischeap and therefore abundant in projection-basedsettings.
Specifically, given that we have plentyof training data, we can train a particular parsermultiple (say, k) times, each time sampling afixed number of training examples from the poolof training data.
The k parsers can then eachparse the unseen test set, and subsequent compar-ison against the gold standard annotations yieldsk values of the performance metric at hand (here,UAS).
As in conventional cross-validation, thesek values are then averaged to provide an aggre-gated score, and they can be used to derive stan-dard deviations etc.
The arrays of measurementsfor different systems can further be subjected tosignificance tests such as the two-sample t-test toverify that observed performance differences arenot merely random effects.5.1 ExperimentsWe use the validation procedure just described(with k=10) to investigate the variance in the pro-jected parsers discussed in the previous section(Table 3a).
Table 7 lists the scores obtained bythe individual parsers, each trained on a differentrandom sample of 100,000 words, drawn from thepool of all projected annotations.
We also showthe standard deviation and repeat the mean UAS.We observe that, for a given language, standarddeviation seems to correlate negatively with meanUAS; in other words, the better parsers also seemto be more robust towards variance in the trainingdata.5.2 DiscussionClassical cross-validation and the validationmethod described here do measure slightly dif-ferent things.
First, in cross-validation it is notonly the training data that is varied, but the testdata as well.
Second, when two systems are com-pared under the cross-validation regime, the krounds can usually be considered paired samplesbecause both systems are trained and evaluatedon identical partitionings of the data.
In contrast,projection-based settings typically involve someform of filtering on the basis of the projected an-notations; in our case, the filter restricts the de-gree of fragmentation in the projected dependencytree.
This filtering makes it all but impossibleto pair the training samples without seriously di-minishing the pool from which the samples aredrawn.
For instance, when comparing the Italianparser projected from English (itptb) and the oneprojected from German (ittig), a training sentencemay receive a complete analysis from the Englishtranslation, and hence be included in the trainingpool for itptb; but the same (Italian) sentence mayreceive a highly fragmented analysis under projec-tion from German (e.g., due to missing alignmentlinks) and be discarded from the training pool forittig.With samples that cannot be paired, it is alsonot obvious how evaluation strategies like therandomized comparison mentioned above (fn.
4)could be employed in a sound way (by non-statisticians).6 ConclusionsWe have discussed two issues that arise in theevaluation of frameworks that involve cross-lingual projection of annotations.
We focused onthe projection of dependency trees from Germanand English to Dutch and Italian, and presentedexperiments that compare parsers trained on theprojected dependencies.
The parsers differ in theannotation scheme they follow: When they areprojected from German, they employ the flat Tigerannotation scheme of the source language; pro-1182jected from English, they learn the more hierar-chical PTB structures.
In order to evaluate theprojected parsers against target language (Dutch,Italian) gold standard annotations, we convert thetest sets to the annotation scheme employed in therespective source language.While our experiments with gold standard tree-bank data affirm that the annotation scheme thatis being learned has some influence on the perfor-mance of the parser, one should bear in mind thatin a projection scenario, the quality of the wordalignment plays at least an equally important rolewhen it comes to chosing a suitable source lan-guage and annotation scheme.We have further proposed a validation schemewhich unlike cross-validation does not requireparallel test data.
Instead, it exploits the fact thattraining data is usually available in abundance inprojection scenarios, so parsers can be trained onmultiple random samples and evaluated against asingle, independent test set which need not be fur-ther partitioned.AcknowledgmentsThe work reported in this paper was supportedby the Deutsche Forschungsgemeinschaft (DFG;German Research Foundation) in the SFB 632 onInformation Structure, project D4 (Methods forinteractive linguistic corpus analysis).ReferencesBrants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories, pages 24?41.Buchholz, Sabine and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.In Proceedings of CoNLL-X, pages 149?164, NewYork City, June.Carroll, John, Ted Briscoe, and Antonio Sanfilippo.1998.
Parser evaluation: a survey and a new pro-posal.
In Proceedings of LREC 1998, pages 447?454, Granada, Spain.Hwa, Rebecca, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(3):311?325.Johansson, Richard and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In Nivre, J., H.-J.
Kaalep, and M. Koit, ed-itors, Proceedings of NODALIDA 2007, pages 105?112.Koehn, Philipp.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofthe MT Summit 2005.Ku?bler, Sandra, Wolfgang Maier, Ines Rehbein, andYannick Versley.
2008.
How to Compare Tree-banks.
In Proceedings of LREC 2008, pages 2322?2329.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.McDonald, Ryan, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT-EMNLP 2005.Nivre, Joakim, Johan Hall, Jens Nilsson, Gu?ls?enEryig?it, and Svetoslav Marinov.
2006.
Labeledpseudo-projective dependency parsing with supportvector machines.
In Proceedings of CoNLL-X,pages 221?225, New York City, June.Och, Franz Josef and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.
?vrelid, Lilja, Jonas Kuhn, and Kathrin Spreyer.
2010.Cross-framework parser stacking for data-driven de-pendency parsing.
To appear in TAL 2010 specialissue on Machine Learning for NLP 50(3), eds.
Is-abelle Tellier and Mark Steedman.Rehbein, Ines and Josef van Genabith.
2007.
Tree-bank annotation schemes and parser evaluation forGerman.
In Proceedings of EMNLP-CoNLL 2007,pages 630?639, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Schmid, Helmut.
1994.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Language Processing,pages 44?49, Manchester, England.Spreyer, Kathrin and Jonas Kuhn.
2009.
Data-drivendependency parsing of new languages using incom-plete and noisy training data.
In Proceedings ofCoNLL 2009, pages 12?20, Boulder, CO, June.van der Beek, Leonoor, Gosse Bouma, Robert Malouf,and Gertjan van Noord.
2002.
The Alpino depen-dency treebank.
In Computational Linguistics in theNetherlands (CLIN).1183Yarowsky, David and Grace Ngai.
2001.
InducingMultilingual POS Taggers and NP Bracketers viaRobust Projection across Aligned Corpora.
In Pro-ceedings of NAACL 2001, pages 200?207.1184
