Two levels of evaluation in a complex NL systemJean-Baptiste BerthelinLIMSI-CNRSB?t.508 Universit?
Paris XI91403 Orsay, Francejbb@limsi.frBrigitte GrauLIMSI-CNRSB?t.508 Universit?
Paris XI91403 Orsay, Francebg@limsi.frMartine Hurault-PlantetLIMSI-CNRSB?t.508 Universit?
Paris XI91403 Orsay, Francemhp@limsi.frAbstractThe QALC question-answering system,developed at LIMSI, has been aparticipant for two years in the QAtrack of the TREC conference.
In thispaper, we present a quantitativeevaluation of various modules in oursystem, based on two criteria: first, thenumbers of documents containing thecorrect answer and selected by thesystem; secondly, the number ofanswers found.
The first criterion isused for evaluating locally the modulesin the system, which contribute inselecting documents that are likely tocontain the answer.
The second oneprovides a global evaluation of thesystem.
As such, it also serves for anindirect evaluation of various modules.1 IntroductionFor two years, the TREC EvaluationConference, (Text REtrieval Conference) hasbeen featuring a Question Answering track, inaddition to those already existing.
This trackinvolves searching for answers to a list ofquestions, within a collection of documentsprovided by NIST, the conference organizer.Questions are factual or encyclopaedic, whiledocuments are newspaper articles.
The TREC9-QA track, for instance, proposed 700 questionswhose answers should be retrieved in a corpusof about one million documents.In addition to the evaluation, by humanjudges, of their systems?
results (Voorhees andTice, 2000), TREC participants are alsoprovided with an automated evaluation tool,along with a database.
These data consist of alist of judgements of all results sent in by allparticipants.
The evaluation tool automaticallydelivers a score to a set of answers given by asystem to a set of questions.
This score isderived from the mean reciprocal rank of thefirst five answers.
For each question, the firstcorrect answers get a mark in reverse proportionto their rank.
Those evaluation tool and data arequite useful, since it gives us a way ofappreciating what happens when modifying oursystem to improve it.We have been taking part to TREC for twoyears, with the QALC question-answeringsystem (Ferret et al 2000), currently developedat LIMSI.
This system has followingarchitecture: parsing of the question to find theexpected type of the answer, selection of asubset of documents among the approximatelyone million TREC-provided items, tagging ofnamed entities within the documents, and,finally, search for possible answers.
Some of thecomponents serve to enrich both questions anddocuments, by adding system-readable data intothem.
Such is the case for the modules that parsequestions and tag documents.
Other componentsoperate a selection among documents, usingadded data.
One example of such modules arethose which select relevant documents, anotheris the one which extracts the answer from thedocuments.A global evaluation of the system is based onjudgement about its answers.
This criterionprovides only indirect evaluation of eachcomponent, via the evolution of the final scorewhen this component is modified.
To get acloser evaluation of our modules, we need othercriteria.
In particular, concerning the evaluationof components for document selection, weadopted an additional criterion about selectedrelevant documents, that is, those that yield thecorrect answer.This paper describes a quantitativeevaluation of various modules in our system,based on two criteria: first, the number ofselected relevant documents, and secondly, thenumber of found answers.
The first criterion isused for evaluating locally the modules in thesystem, which contribute in selecting documentsthat are likely to contain the answer.
The secondone provides a global evaluation of the system.It also serves for an indirect evaluation ofvarious modules.2 System architectureFigure 1 shows the architecture of the QALCsystem, made of five separate modules:Question analysis, Search engine, Re-indexingand selection of documents, Named entityrecognition, and Question/sentence pairing.Tagged Questions:Named entity tagsVocabulary &frequenciesNamed entityrecognitionCandidatetermsRetrieveddocumentsTagged sentences: named entitytags and term indexationOrdered sequences of 250 and50 charactersQuestion analysis Search engineQuestionsSubset of ranked documentsCorpusRe-indexing and selection ofdocuments (FASTR)Question/Sentence pairingFigure 1.
The QALC system2.1 Question analysisQuestion analysis is performed in order to assignfeatures to questions and use these features forthe matching measurement between a questionand potential answer sentences.
It relies on ashallow parser which spots discriminatingpatterns and assigns categories to a question.The categories correspond to the types of namedentities that are likely to constitute the answer tothis question.
Named entities receive one of thefollowing types: person, organisation, location(city or place), number (a time expression or anumber expression).
For example the patternhow far yields to the answer type length:Question: How far away is the moon?Answer type: LENGTHAnswer  within the document :With a <b_numex_TYPE="NUMBER"> 28<e_numex> -power telescope you can see it onthe  moon <b_numex_TYPE="LENGTH">250,000 miles <e_numex> away.2.2 Selection of relevant documentsThe second module is a classic searchengine, giving, for each question, a ranked list ofdocuments, each of which could contain theanswer.This set of documents is then processed by athird module, made of FASTR (Jacquemin,1999), a shallow transformational naturallanguage analyser and of a ranker.
This modulecan select, among documents found by thesearch engine, a subset that satisfies morerefined criteria.
FASTR improves thingsbecause it indexes documents with a set ofterms, including not only the (simple orcompound) words of the initial question, butalso their morphological, syntactic and semanticvariants.
Each index is given a weight all thehigher as it is close to the original word in thequestion, or as it is significant.
For instance,original terms are considered more reliable thansemantic variants, and proper names areconsidered more significant than nouns.
Then,documents are ordered according to the numberand the quality of the terms they contain.
Ananalysis of the weight graph of the indexeddocuments enables the system to select arelevant subpart of those documents, whose sizevaries along the questions.
Thus, when the curvepresents a high negative slope, the system onlyselect documents before the fall, otherwise afixed threshold is used.2.3 Named entity recognitionThe fourth module tags named entities indocuments selected by the third one.
Namedentities are recognized through a combination oflexico-syntactic patterns and significantly largelexical data.
The three lists used for lexicallookup are CELEX (1998), a lexicon of 160,595inflected words with associated lemma andsyntactic category, a list of 8,070 first names(6,763 of which are from the CLR (1998)archive) and a list of 211,587 family names alsofrom the CLR archive.2.4 Question-sentence pairingThe fifth module evaluates each sentence inthe ranker-selected documents, using asimilarity measure between, on one side, termsand named entities in the sentence, and on theother side, words in the questions and expectedanswer type.
To do so, it uses the results of thequestion parser, and the named entity tagger,along with a frequency-weighted vocabulary ofthe TREC corpus.The QALC system proposes long and shortanswers.
Concerning the short ones, the systemfocuses on parts of sentences that contain theexpected named entity tags, when they areavailable, or on the larger subpart without anyterms.3 Search engine evaluationThe second module of the QALC system dealswith the selection, through a search engine, ofdocuments that may contain an answer to agiven question from the whole TREC corpus(whose size is about 3 gigabytes).We tested three search engines with the 200questions that were proposed at the TREC8 QAtrack.
The first one is Zprise, a vectorial searchengine developed by NIST.
The second isIndexal (de Loupy et al1998), a pseudo-booleansearch engine developed by BertinTechnologies1.
The third search engine is ATTwhose results to the TREC questions areprovided by NIST in the form of ranked lists ofthe top 1000 documents retrieved for eachquestion.
We based our search engine tests on1 We are grateful to Bertin Technologies for providing uswith the outputs of Indexal on the TREC collection for theTREC8-QA and TREC9-QA question set.the list of relevant documents extracted from thelist of correct answers provided by TRECorganizers.Since a search engine produces a largeranked list of relevant documents, we had todefine the number of documents to retain forfurther processing.
Indeed, having too manydocuments leads to a question processing timethat is too long, but conversely, having too fewdocuments reduces the possibility of obtainingthe correct answer.
The other goal of the testsobviously was to determine the best searchengine, that is to say the one that gives thehighest number of relevant documents.3.1 Document selection thresholdIn order to determine the best selectionthreshold, we carried out four different testswith the Zprise search engine.
We ran Zprise forthe 200 questions and then compared thenumber of relevant documents respectively inthe top 50, 100, 200, and 500 retrieveddocuments.
Table 1 shows the test results.SelectionThresholdQuestions withrelevantdocumentsQuestions withno relevantdocuments50 181 19100 184 16200 193 7500 194 6Table 1.
Number of questions with andwithout relevant documents retrieved fordifferent thresholdsAccording to Table 1, the improvement ofthe search engine results tends to decreasebeyond the threshold of 200 documents.
The top200 ranked documents thus seem to offer thebest trade-off between the number of documentsin which the answer may be found and thequestion processing time.3.2 EvaluationWe compared the results given by the threesearch engines for a threshold of 200documents.
Table 2 gives the tests results.Search Engine Indexal Zprise ATTNumber of questionswith relevantdocuments retrieved182 193 194Number of questionswithout relevantdocuments retrieved18 7 6Total number ofrelevant documentsthat were retrieved814 931 1021Table 2.
Compared performances of theIndexal, Zprise and ATT search enginesAll three search engines perform quite well.Nevertheless, the ATT search engine revealeditself the most efficient according to thefollowing two criteria: the lowest number ofquestions for which no relevant document wasretrieved, and the most relevant documentsretrieved for all the 200 questions.
Both criteriaare important.
First, it is most essential to obtainrelevant documents for as many questions aspossible.
But the number of relevant documentsfor each question also counts, since having moresentences containing the answer implies agreater probability to actually find it.4 Document ranking evaluationAs the processing of 200 documents by thefollowing Natural Language Processing (NLP)modules still was too time-consuming, weneeded an additional stronger selection.
Theselection of relevant documents performed bythe re-indexing and selection module relies onan NLP-based indexing composed of bothsingle-word and phrase indices, and linguisticlinks between the occurrences and the originalterms.
The original terms are extracted from thequestions.
The tool used for extracting textsequences that correspond to occurrences orvariants of these terms is FASTR (Jacquemin,1999).
The ranking of the documents relies on aweighted combination of the terms and variantsextracted from the documents.
The use of multi-words and variants for document weightingmakes a finer ranking possible.The principle of the selection is thefollowing: when there is a sharp drop of thedocuments weight curve after a given rank, wekeep only those documents which occur beforethe drop.
Otherwise, we arbitrarily keep the first100.In order to evaluate the efficiency of theranking process, we proceeded to severalmeasures.
First, we apply our system on thematerial given for the TREC8 evaluation, onetime with the ranking process, and another timewithout this process.
200 documents wereretained for each of the 200 questions.
Thesystem was scored by 0.463 in the first case, andby 0.452 in the second case.
These results showthat document selection slightly improves thefinal score while much reducing the amount oftext to process.However, a second measurement gave usmore details about how things are improved.Indeed, when we compare the list of relevantdocuments selected by the search engine withthe list of ranker-selected ones, we find that theranker loses relevant documents.
For thirteenquestions among the 200 in the test, the rankerdid not consider relevant documents selected bythe search engine.
What happens is: the globalscore improves, because found answers rankhigher, but the number of found answersremains the same.The interest to perform such a selection isalso illustrated by the results given Table 3,computed on the TREC9 results.Number of documentsselected by ranking100 <<100Distribution among thequestions342(50%)340(50%)Number of correctanswers175(51%)200(59%)Number of correct answerat rank 188(50%)128(64%)Table 3.
Evaluation of the ranking processWe see that the selection process discardsdocuments for 50% of the questions: 340questions are processed from less than 100documents.
For those 340 questions, the averagenumber of selected documents is 37.
Thedocument set retrieved for those questions has aweight curve with a sharp drop.
QALC findsmore often the correct answer and in a betterposition for these 340 questions than for the 342remaining ones.
These results are veryinteresting when applying such time-consumingprocesses as named-entities recognition andquestion/sentence matching.
Document selectionwill also enable us to apply further sentencesyntactic analysis.5 Question-sentence pairing evaluationWe sent to TREC9 two runs which gaveanswers of 250 characters length, and one runwhich gave answers of 50 characters length.
Thefirst and the last runs used ATT as searchengine, and the second one, Indexal.
Results areconsistent with our previous analysis (seeSection 3.2).
Indeed, the run with ATT searchengine gives slightly better results (0.407 strict)2than those obtained with the Indexal searchengine (0.375 strict).
Table 4 sums up thenumber of answers found by our two runs.Rank of the correctanswer retrievedRun usingATTRun usingIndexal1 216 1872 to 5 159 185Total of correctanswers retrieved375 372No correct answerretrieved307 310Table 4.
Number of correct answers retrieved,by rank, for the two runs at 250 charactersThe score of the run with answers of 50characters length was not encouraging,amounting only 0.178, with 183 correct answersretrieved3.5.1 Long answersFrom results of the evaluation concerningdocument ranking, we see that the performancelevel of the question-sentence matcher dependspartly on the set of sentences it has parsed, andnot only on the presence, or absence, of theanswer within these sentences.
In other words,we do not find the answer each time it is in theset of selected sentences, but we find it easily ifthere are few documents (and then fewsentences) selected.
That is because similarity2 With this score, the QALC system was ranked 6th among25 participants at TREC9 QA task for answers with 250characters length.3 With this score, the QALC system was ranked 19thamong 24 participants at TREC9 QA task for answers with50 characters length.assessment relies upon a small number ofcriteria, which are found to be insufficientlydiscriminant.
Therefore, several sentencesobtain the same mark, in which case, the rank ofthe correct answer depends on the order inwhich sentences are encountered.This is something we cannot yet manage, sowe evaluated the matcher?s performance,without any regard to the side effect induced bydocument processing order.
As remarked in 3.2,search engines perform well.
In particular, ATTretains relevant documents, namely, those thatyield good answers, for 97 percent of thequestions.
The ranker, while improving the finalscore, loses some questions.
After it stepped in,the system retains relevant documents for 90%of the questions.
The matcher finds a relevantdocument in the first five answers for 74% ofthe questions, but answers only 62% of themcorrectly.
Finding the right document is but onestep, knowing where to look inside it is noobvious task.5.2 Short answersA short answer is selectively extracted froma long one.
We submitted this short answerselector (under 50 characters) to evaluationlooking for the impact of the expected answertype.
Among TREC questions, some expect ananswer consisting of a named entity: for instancea date, a personal or business name.
In suchcases, assigning a type to the answer is rathersimple, although it implies the need of a goodnamed entity recognizer.
Answers to otherquestions (why questions for instance, or somesort of what questions), however, will consist ofa noun or sentence.
Finding its type is morecomplex, and is not done very often.Some systems, like FALCON (Harabagiu etal 2000) use Wordnet word class hierarchies toassign types to answers.
Among 682 answers inTREC9, 57.5% were analysed by our system asnamed-entity questions, while others receivedno type assignment.
Among answers from ourbest 250-character run, 62.7% were aboutnamed entities.
However, our run for shorteranswers, yielding a more modest score, gives84% of named-entities answers.
In our systemanswer type assignment is of surprisingly smallimport, where longer answers are concerned.However, it does modify the selecting process,when the answer is extracted from a longersentence.Such evaluations help us to see more clearlywhere our next efforts should be directed.Having more criteria in the similaritymeasurement would, in particular, be a source ofimprovement.6 DiscussionWe presented quantitative evaluations.
But sincewe feel that evaluations should contribute toimprovements of the system, more qualitativeand local ones also appear interesting.TREC organizers send us, along with runresults, statistics about how many runs found thecorrect answer, and at which rank.
Suchstatistics are useful in many ways.
Particularly,they provide a characterisation of a posterioridifficult questions.
Knowing that a question is adifficult one is certainly relevant when trying toanswer it.
Concerning this problem, de Loupyand Bellot (2000) proposed an interesting set ofcriteria to recognize a priori difficult questions.They use word frequency, multi-words,polysemy (a source of noise) and synonymy (asource of silence).
They argue that an?intelligent?
system could even insist that aquestion be rephrased when it is too difficult.While their approach is indeed quite promising,we consider that their notion of a prioridifficulty should be complemented by the notionof a posteriori difficulty we mentioned: the twoupcoming examples of queries show that aquestion may seem harmless at first sight, evenusing de Loupy and Bellot?s criteria, and stillcreate problems for most systems.From these statistics, we also founddisparities between our system and others forcertain questions.
At times, it finds a goodanswer where most others fail and obviously thereverse also happens.
This is the case in the twofollowing examples.
The first one concerns aninteresting issue in a QA system that is thedetermination of which terms from the questionare to be selected for the question-answerpairing.
This is particularly important when thequestion has few words.
For instance, to thequestion  How far away is the moon?, our termextractor kept not only moon?
(NN), but alsoaway?
(RB) .
Moreover, our question parserknows that how far is an interrogative phraseyielding a LENGTH type for the answer.
Thisleads our system to retrieve the correct answer:With a 28-power telescope, you can see it on themoon 250,000 miles away4.The second example concerns the relativeweight of the terms within the question.
When aproper noun is present, it must be found in theanswer, hence an important weight for it.
Lookat the question Who manufactures the software,?
?PhotoShop???
.
The term extractor kepts o f t w a r e ( N N ) , PhotoShop(NP),  a n dmanufacture(VBZ) as terms to be matched, butthe matcher assigns equal weights to them, sowe could not find the answer5.
Later, wemodified these weights, and the problem wassolved.Indeed, evaluation corpus seems to bedifficult to build.
Apart from the problem of thequestion difficulty level, question typedistribution may also vary from a corpus toanother.
For instance, we note that TREC8proposed much more questions with namedentity answer type (about 80%) than TREC9(about 60%).
Thus, some participants who trainstheir systems on the TREC8 corpus weresomehow disapointed by their results at TREC9with regards with their training results (Scottand Gaizauskas, 2000).However, it is generally hard to predict whatwill happen if we modify the system.
A localimprovement can result in a loss of performancefor other contexts.
Although the system?scomplexity cannot be reduced to just two levels(a local one and a global one), this can be anefficient step in the design of improvements tothe whole system via local adjustments.
But thisis a very frequent situation in engineering tasks.7 Conclusion and perspectivesEach evaluation reflects a viewpoint, underlyingthe criterion we use.
In our case, the choice ofcriteria was guided by the existence of two mainstages in the QA process, namely the selectionof relevant documents and the selection of theanswer among the selected documentssentences.
Sometimes, such criteria concur in4 Among the 42 runs using 250 byte limit, submitted atTREC9-QA, only seven found the correct answer at rank 1,and 27 do not found it.5 22 runs, out of 42 found the right answer at rank 1.
Only9 were unable to find it.revealing the same positive or negative featureof the system.
They can also yield a moreprecise assessment of the reasons behind thesefeatures, as was the case in our evaluation of theranker.
Moreover, when a system consists ofseveral modules, their specific evaluationsshould imply different criteria.This is particularly true in dialogue systems,where different kinds of processes are co-operating.
Since information retrieval is aninteractive task, it seems natural to associate adialogue component to it.
Indeed, users tend toask a question, evaluate the answer, andreformulate their question to make it morespecific (or, contrariwise, more general, or quitedifferent).
A QA system is, therefore, a goodapplicative setting for a dialogue module.Quantitative assessment of the QA systemwould be useful in assessing the dialogue systemin this particular context.
Such a globalassessment would provide an objectivejudgement about whether the task (finding theanswer) was achieved, or not.
Successfulness ina task is a necessary component of theevaluation, nevertheless it is just a part of it.Obviously, dialogue evaluation is also a matterof cost (time, number of exchanges) and of user-friendliness (cognitive ergonomy).However, objectivity is almost impossible toattain in these domains.
In a recent debate(LREC 2000), serious objections about naturallanguage tools evaluation and validation weredeveloped e.g.
by Sabah (2000).
The main issuehe raises is about the great complexity of suchsystems.
However, we consider that by going asfar as possible in the experimental search forevaluation criteria, we also make a meaningfulcontribution to this debate.
While it is true thatcomplexity should never be ignored, weconsider that, by successive approximatemodelisation and evaluation cycles, we cancapture some of it at each step of our system?sdevelopement.ReferencesCELEX.
1998.http://www.ldc.upenn.edu/readme_files/celex.readme.html.
Consortium for Lexical Resources,UPenns, Eds.CLR.
1998. http://crl.nmsu.edu/cgi-bin/Tools/CLR/clrcat#D3.
Consortium for LexicalResources, NMSUs, Eds., New Mexico.Fabre C., Jacquemin C, 2000.
Boosting variantrecognition with light semantics.
ProceedingsCOLING?2000, pp.
264-270, Luxemburg.Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
Cambridge, MA, MIT Press.Ferret O., Grau B., Hurault-Plantet M., Illouz G.,Jacquemin C. (2000), QALC ?
the Question-Answering system of LIMSI-CNRS.
Pre-proceedings of TREC9, NIST, Gaithersburg, CA.Harabagiu S., Pasca M., Maiorano J.
2000.Experiments with Open-Domain Textual QuestionAnswering.
Proceedings of Coling'2000,Saarbrucken, Germany.Jacquemin C. 1999.
Syntagmatic and paradigmaticrepresentations of term variation.
Proceedings ofACL'99.
341-348.de Loupy C., Bellot P., El-B?ze M., Marteau P.-F..Query Expansion and Classification of RetrievedDocuments, TREC7 (1998), 382-389.de Loupy C., Bellot P. 2000.
Evaluation ofDocument Retrieval Systems and QueryDifficulty.
Proceedings of the SecondInternational Conference on Language Resourcesand Evaluation (LREC 2000) Workshop, Athens,Greece.
32-39.Sabah G. 2000 To validate or not to validate?
Sometheoretical difficulties for a scientific evaluation ofnatural language processing systems.
Proceedingsof the Second International Conference onLanguage Resources and Evaluation (LREC 2000)Workshop, Athens, Greece.
58-61.Scott S., Gaizauskas R. 2000.
University of SheffieldTREC-9 Q & A System.
Pre-proceedings ofTREC9, NIST, Gaithersburg, CA.
548-557.Voorhees E., Tice D. 2000.
Implementing a QuestionAnswering Evaluation.
Proceedings of the SecondInternational Conference on Language Resourcesand Evaluation (LREC 2000).
Athens, Greece.
40-45.
