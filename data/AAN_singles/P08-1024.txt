Proceedings of ACL-08: HLT, pages 200?208,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Discriminative Latent Variable Modelfor Statistical Machine TranslationPhil Blunsom, Trevor Cohn and Miles OsborneSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh, EH8 9LW, UK{pblunsom,tcohn,miles}@inf.ed.ac.ukAbstractLarge-scale discriminative machine transla-tion promises to further the state-of-the-art,but has failed to deliver convincing gains overcurrent heuristic frequency count systems.
Weargue that a principle reason for this failure isnot dealing with multiple, equivalent transla-tions.
We present a translation model whichmodels derivations as a latent variable, in bothtraining and decoding, and is fully discrimina-tive and globally optimised.
Results show thataccounting for multiple derivations does in-deed improve performance.
Additionally, weshow that regularisation is essential for max-imum conditional likelihood models in orderto avoid degenerate solutions.1 IntroductionStatistical machine translation (SMT) has seena resurgence in popularity in recent years, withprogress being driven by a move to phrase-based andsyntax-inspired approaches.
Progress within theseapproaches however has been less dramatic.
We be-lieve this is because these frequency count based1models cannot easily incorporate non-independentand overlapping features, which are extremely use-ful in describing the translation process.
Discrimi-native models of translation can include such fea-tures without making assumptions of independenceor explicitly modelling their interdependence.
How-ever, while discriminative models promise much,they have not been shown to deliver significant gains1We class approaches using minimum error rate training(Och, 2003) frequency count based as these systems re-scale ahandful of generative features estimated from frequency countsand do not support large sets of non-independent features.over their simpler cousins.
We argue that this is dueto a number of inherent problems that discrimina-tive models for SMT must address, in particular theproblems of spurious ambiguity and degenerate so-lutions.
These occur when there are many ways totranslate a source sentence to the same target sen-tence by applying a sequence of steps (a derivation)of either phrase translations or synchronous gram-mar rules, depending on the type of system.
Exist-ing discriminative models require a reference deriva-tion to optimise against, however no parallel cor-pora annotated for derivations exist.
Ideally, a modelwould account for this ambiguity by marginalisingout the derivations, thus predicting the best transla-tion rather than the best derivation.
However, doingso exactly is NP-complete.
For this reason, to ourknowledge, all discriminative models proposed todate either side-step the problem by choosing simplemodel and feature structures, such that spurious am-biguity is lessened or removed entirely (Ittycheriahand Roukos, 2007; Watanabe et al, 2007), or else ig-nore the problem and treat derivations as translations(Liang et al, 2006; Tillmann and Zhang, 2007).In this paper we directly address the problem ofspurious ambiguity in discriminative models.
Weuse a synchronous context free grammar (SCFG)translation system (Chiang, 2007), a model whichhas yielded state-of-the-art results on many transla-tion tasks.
We present two main contributions.
First,we develop a log-linear model of translation whichis globally trained on a significant number of paral-lel sentences.
This model maximises the conditionallikelihood of the data, p(e|f), where e and f are theEnglish and foreign sentences, respectively.
Our es-timation method is theoretically sound, avoiding thebiases of the heuristic relative frequency estimates200lllll llllllsentence lengthderivations5 7 9 11 13 151e+031e+051e+08Figure 1.
Exponential relationship between sentencelength and the average number of derivations (on a logscale) for each reference sentence in our training corpus.
(Koehn et al, 2003).
Second, within this frame-work, we model the derivation, d, as a latent vari-able, p(e,d|f), which is marginalised out in train-ing and decoding.
We show empirically that thistreatment results in significant improvements over amaximum-derivation model.The paper is structured as follows.
In Section 2we list the challenges that discriminative SMT mustface above and beyond the current systems.
We sit-uate our work, and previous work, on discrimina-tive systems in this context.
We present our modelin Section 3, including our means of training and de-coding.
Section 4 reports our experimental setup andresults, and finally we conclude in Section 5.2 Challenges for Discriminative SMTDiscriminative models allow for the use of expres-sive features, in the order of thousands or millions,which can reference arbitrary aspects of the sourcesentence.
Given most successful SMT models havea highly lexicalised grammar (or grammar equiva-lent), these features can be used to smuggle in lin-guistic information, such as syntax and documentcontext.
With this undoubted advantage come fourmajor challenges when compared to standard fre-quency count SMT models:1.
There is no one reference derivation.
Oftenthere are thousands of ways of translating asource sentence into the reference translation.Figure 1 illustrates the exponential relationshipbetween sentence length and the number ofderivations.
Training is difficult without a cleartarget, and predicting only one derivation at testtime is fraught with danger.2.
Parallel translation data is often very noisy,with such problems as non-literal translations,poor sentence- and word-alignments.
A modelwhich exactly translates the training data willinevitably perform poorly on held-out data.This problem of over-fitting is exacerbatedin discriminative models with large, expres-sive, feature sets.
Regularisation is essential formodels with more than a handful of features.3.
Learning with a large feature set requires manytraining examples and typically many iterationsof a solver during training.
While current mod-els focus solely on efficient decoding, discrim-inative models must also allow for efficienttraining.Past work on discriminative SMT only addresssome of these problems.
To our knowledge no sys-tems directly address Problem 1, instead choosing toignore the problem by using one or a small handfulof reference derivations in an n-best list (Liang et al,2006; Watanabe et al, 2007), or else making localindependence assumptions which side-step the issue(Ittycheriah and Roukos, 2007; Tillmann and Zhang,2007; Wellington et al, 2006).
These systems all in-clude regularisation, thereby addressing Problem 2.An interesting counterpoint is the work of DeNero etal.
(2006), who show that their unregularised modelfinds degenerate solutions.
Some of these discrim-inative systems have been trained on large trainingsets (Problem 3); these systems are the local models,for which training is much simpler.
Both the globalmodels (Liang et al, 2006; Watanabe et al, 2007)use fairly small training sets, and there is no evi-dence that their techniques will scale to larger datasets.Our model addresses all three of the above prob-lems within a global model, without resorting to n-best lists or local independence assumptions.
Fur-thermore, our model explicitly accounts for spuriousambiguity without altering the model structure or ar-bitrarily selecting one derivation.
Instead we modelthe translation distribution with a latent variable forthe derivation, which we marginalise out in trainingand decoding.201the hatle chapeauredthe hatle chapeauredFigure 2.
The dropping of an adjective in this examplemeans that there is no one segmentation that we couldchoose that would allow a system to learn le ?
the andchapeau?
hat.?S?
?
?S 1 X 2 , S 1 X 2 ??S?
?
?X 1 , X 1 ??X?
?
?ne X 1 pas, does not X 1 ??X?
?
?va, go??X?
?
?il, he?Figure 3.
A simple SCFG, with non-terminal symbols Sand X, which performs the transduction: il ne vas pas ?he does not goThis itself provides robustness to noisy data, inaddition to the explicit regularisation from a priorover the model parameters.
For example, in manycases there is no one perfect derivation, but rathermany imperfect ones which each include some goodtranslation fragments.
The model can learn frommany of these derivations and thereby learn fromall these translation fragments.
This situation is il-lustrated in Figure 2 where the non-translated ad-jective red means neither segmentation is ?correct?,although both together present positive evidence forthe two lexical translations.We present efficient methods for training and pre-diction, demonstrating their scaling properties bytraining on more than a hundred thousand train-ing sentences.
Finally, we stress that our main find-ings are general ones.
These results could ?
andshould ?
be applied to other models, discriminativeand generative, phrase- and syntax-based, to furtherprogress the state-of-the-art in machine translation.3 Discriminative SynchronousTransductionA synchronous context free grammar (SCFG) con-sists of paired CFG rules with co-indexed non-terminals (Lewis II and Stearns, 1968).
By assign-ing the source and target languages to the respectivesides of a SCFG it is possible to describe translationas the process of parsing the source sentence usinga CFG, while generating the target translation fromthe other (Chiang, 2007).
All the models we presentuse the grammar extraction technique described inChiang (2007), and are bench-marked against ourown implementation of this hierarchical model (Hi-ero).
Figure 3 shows a simple instance of a hierar-chical grammar with two non-terminals.
Note thatour approach is general and could be used with othersynchronous grammar transducers (e.g., Galley et al(2006)).3.1 A global log-linear modelOur log-linear translation model defines a condi-tional probability distribution over the target trans-lations of a given source sentence.
A particular se-quence of SCFG rule applications which produces atranslation from a source sentence is referred to as aderivation, and each translation may be produced bymany different derivations.
As the training data onlyprovides source and target sentences, the derivationsare modelled as a latent variable.The conditional probability of a derivation, d, fora target translation, e, conditioned on the source, f ,is given by:p?
(d, e|f) =exp?k ?kHk(d, e, f)Z?
(f)(1)where Hk(d, e, f) =?r?dhk(f , r) (2)Here k ranges over the model?s features, and?
= {?k} are the model parameters (weights fortheir corresponding features).
The feature functionsHk are predefined real-valued functions over thesource and target sentences, and can include over-lapping and non-independent features of the data.The features must decompose with the derivation,as shown in (2).
The features can reference the en-tire source sentence coupled with each rule, r, in aderivation.
The distribution is globally normalisedby the partition function, Z?
(f), which sums out thenumerator in (1) for every derivation (and thereforeevery translation) of f :Z?
(f) =?e?d??
(e,f)exp?k?kHk(d, e, f)Given (1), the conditional probability of a targettranslation given the source is the sum over all ofits derivations:p?
(e|f) =?d??(e,f)p?
(d, e|f) (3)202where ?
(e, f) is the set of all derivations of the tar-get sentence e from the source f.Most prior work in SMT, both generative and dis-criminative, has approximated the sum over deriva-tions by choosing a single ?best?
derivation using aViterbi or beam search algorithm.
In this work weshow that it is both tractable and desirable to directlyaccount for derivational ambiguity.
Our findingsecho those observed for latent variable log-linearmodels successfully used in monolingual parsing(Clark and Curran, 2007; Petrov et al, 2007).
Thesemodels marginalise over derivations leading to a de-pendency structure and splits of non-terminal cate-gories in a PCFG, respectively.3.2 TrainingThe parameters of our model are estimatedfrom our training sample using a maximum aposteriori (MAP) estimator.
This maximisesthe likelihood of the parallel training sen-tences, D = {(e, f)}, penalised using a prior,i.e., ?MAP = arg max?
p?(D)p(?).
We use azero-mean Gaussian prior with the probabilitydensity function p0(?k) ?
exp(?
?2k/2?2).2 Thisresults in the following log-likelihood objective andcorresponding gradient:L =?
(e,f)?Dlog p?
(e|f) +?klog p0(?k) (4)?L?
?k= Ep?(d|e,f)[hk]?
Ep?(e|f)[hk]?
?k?2(5)In order to train the model, we maximise equation(4) using L-BFGS (Malouf, 2002; Sha and Pereira,2003).
This method has been demonstrated to be ef-fective for (non-convex) log-linear models with la-tent variables (Clark and Curran, 2004; Petrov et al,2007).
Each L-BFGS iteration requires the objectivevalue and its gradient with respect to the model pa-rameters.
These are calculated using inside-outsideinference over the feature forest defined by theSCFG parse chart of f yielding the partition func-tion, Z?
(f), required for the log-likelihood, and themarginals, required for its derivatives.Efficiently calculating the objective and its gradi-ent requires two separate packed charts, each rep-resenting a derivation forest.
The first one is the fullchart over the space of possible derivations given the2In general, any conjugate prior could be used instead of asimple Gaussian.source sentence.
The inside-outside algorithm overthis chart gives the marginal probabilities for eachchart cell, from which we can find the feature ex-pectations.
The second chart contains the space ofderivations which produce the reference translationfrom the source.
The derivations in this chart are asubset of those in the full derivation chart.
Again,we use the inside-outside algorithm to find the ?ref-erence?
feature expectations from this chart.
Theseexpectations are analogous to the empirical observa-tion of maximum entropy classifiers.Given these two charts we can calculate the log-likelihood of the reference translation as the inside-score from the sentence spanning cell of the ref-erence chart, normalised by the inside-score of thespanning cell from the full chart.
The gradient is cal-culated as the difference of the feature expectationsof the two charts.
Clark and Curran (2004) providesa more complete discussion of parsing with a log-linear model and latent variables.The full derivation chart is produced using a CYKparser in the same manner as Chiang (2005), and hascomplexity O(|e|3).
We produce the reference chartby synchronously parsing the source and referencesentences using a variant of CYK algorithm over twodimensions, with a time complexity of O(|e|3|f |3).This is an instance of the ITG alignment algorithm(Wu, 1997).
This step requires the reference transla-tion for each training instance to be contained in themodel?s hypothesis space.
Achieving full coverageimplies inducing a grammar which generates all ob-served source-target pairs, which is difficult in prac-tise.
Instead we discard the unreachable portion ofthe training sample (24% in our experiments).
Theproportion of discarded sentences is a function ofthe grammar used.
Extraction heuristics other thanthe method used herein (Chiang, 2007) could allowcomplete coverage (e.g., Galley et al (2004)).3.3 DecodingAccounting for all derivations of a given transla-tion should benefit not only training, but also decod-ing.
Unfortunately marginalising over derivations indecoding is NP-complete.
The standard solution isto approximate the maximum probability translationusing a single derivation (Koehn et al, 2003).Here we approximate the sum over derivations di-rectly using a beam search in which we produce abeam of high probability translation sub-strings foreach cell in the parse chart.
This algorithm is sim-203X[1,2]onX[2,3]theX[3,4]tableX[1,3]on theX[2,4]the tableX[1,3]on the tableX[3,4]chartX[2,4]the chartX[1,3]on the charts1sur2la3table4Figure 4.
Hypergraph representation of max translationdecoding.
Each chart cell must store the entire targetstring generated.ilar to the methods for decoding with a SCFG in-tersected with an n-gram language model, which re-quire language model contexts to be stored in eachchart cell.
However, while Chiang (2005) stores anabbreviated context composed of the n ?
1 targetwords on the left and right edge of the target sub-string, here we store the entire target string.
Addi-tionally, instead of maximising scores in each beamcell, we sum the inside scores for each derivationthat produces a given string for that cell.
When thebeam search is complete we have a list of trans-lations in the top beam cell spanning the entiresource sentence along with their approximated in-side derivation scores.
Thus we can assign eachtranslation string a probability by normalising its in-side score by the sum of the inside scores of all thetranslations spanning the entire sentence.Figure 4 illustrates the search process for the sim-ple grammar from Table 2.
Each graph node repre-sents a hypothesis translation substring covering asub-span of the source string.
The space of trans-lation sub-strings is exponential in each cell?s span,and our algorithm can only sum over a small fractionof the possible strings.
Therefore the resulting prob-abilities are only estimates.
However, as demon-strated in Section 4, this algorithm is considerablymore effective than maximum derivation (Viterbi)decoding.4 EvaluationOur model evaluation was motivated by the follow-ing questions: (1) the effect of maximising transla-tions rather than derivations in training and decod-ing; (2) whether a regularised model performs betterthan a maximum likelihood model; (3) how the per-formance of our model compares with a frequencycount based hierarchical system; and (4) how trans-lation performance scales with the number of train-ing examples.We performed all of our experiments on theEuroparl V2 French-English parallel corpus.3 Thetraining data was created by filtering the full cor-pus for all the French sentences between five andfifteen words in length, resulting in 170K sentencepairs.
These limits were chosen as a compromisebetween experiment turnaround time and leavinga large enough corpus to obtain indicative results.The development and test data was taken from the2006 NAACL and 2007 ACL workshops on ma-chine translation, also filtered for sentence length.4Tuning of the regularisation parameter and MERTtraining of the benchmark models was performed ondev2006, while the test set was the concatenationof devtest2006, test2006 and test2007, amounting to315 development and 1164 test sentences.Here we focus on evaluating our model?s basicability to learn a conditional distribution from sim-ple binary features, directly comparable to thosecurrently employed in frequency count models.
Assuch, our base model includes a single binary iden-tity feature per-rule, equivalent to the p(e|f) param-eters defined on each rule in standard models.As previously noted, our model must be able toderive the reference sentence from the source for itto be included in training.
For both our discrimina-tive and benchmark (Hiero) we extracted our gram-mar on the 170K sentence corpus using the approachdescribed in Chiang (2007), resulting in 7.8 millionrules.
The discriminative model was then trained onthe training partition, however only 130K of the sen-tences were used as the model could not producea derivation of the reference for the remaining sen-tences.
There were many grammar rules that the dis-criminative model did not observe in a referencederivation, and thus could not assign their feature apositive weight.
While the benchmark model has a3http://www.statmt.org/europarl/4http://www.statmt.org/wmt0{6,7}204DecodingTraining derivation translationAll Derivations 28.71 31.23Single Derivation 26.70 27.32ML (?2 =?)
25.57 25.97Table 1.
A comparison on the impact of accounting for allderivations in training and decoding (development set).positive count for every rule (7.8M), the discrimina-tive model only observes 1.7M rules in actual refer-ence derivations.
Figure 1 illustrates the massive am-biguity present in the training data, with fifteen wordsentences averaging over 70M reference derivations.Performance is evaluated using cased BLEU4score on the test set.
Although there is no direct rela-tionship between BLEU and likelihood, it providesa rough measure for comparing performance.Derivational ambiguity Table 1 shows the im-pact of accounting for derivational ambiguity intraining and decoding.5 There are two options fortraining, we could use our latent variable model andoptimise the probability of all derivations of thereference translation, or choose a single derivationthat yields the reference and optimise its probabilityalone.
The second option raises the difficult questionof which one, of the thousands available, we shouldchoose?
We use the derivation which contains themost rules.
The intuition is that small rules are likelyto appear more frequently, and thus generalise bet-ter to a test set.
In decoding we can search for themaximum probability derivation, which is the stan-dard practice in SMT, or for the maximum probabil-ity translation which is what we actually want fromour model, i.e.
the best translation.The results clearly indicate the value in opti-mising translations, rather than derivations.
Max-translation decoding for the model trained on singlederivations has only a small positive effect, while forthe latent variable model the impact is much larger.6For example, our max-derivation model trainedon the Europarl data translates carte sur la table ason the table card.
This error in the reordering of card(which is an acceptable translation of carte) is dueto the rule ?X?
?
?carte X 1 , X 1 card?
being thehighest scoring rule for carte.
This is reasonable, as5When not explicitly stated, both here and in subsequent re-sults, the regularisation parameter was set to one, ?2 = 1.6We also experimented with using max-translation decodingfor standard MER trained translation models, finding that it hada small negative impact on BLEU score.llll l llbeam widthdevelopment BLEU(%)29.029.530.030.531.031.5100 1k 10kFigure 5.
The effect of the beam width (log-scale) on max-translation decoding (development set).carte is a noun, which in the training data, is oftenobserved with a trailing adjective which needs to bereordered when translating into English.
In the ex-ample there is no adjective, but the simple hierarchi-cal grammar cannot detect this.
The max-translationmodel finds a good translation card on the table.This is due to the many rules that enforce monotoneordering around sur la, ?X?
?
?X 1 sur, X 1 in??X?
?
?X 1 sur la X 2 , X 1 in the X 2 ?
etc.The scores of these many monotone rules sum to begreater than the reordering rule, thus allowing themodel to use the weight of evidence to settle on thecorrect ordering.Having established that the search for the besttranslation is effective, the question remains as tohow the beam width over partial translations affectsperformance.
Figure 5 shows the relationship be-tween beam width and development BLEU.
Evenwith a very tight beam of 100, max-translation de-coding outperforms maximum-derivation decoding,and performance is increasing even at a width of10k.
In subsequent experiments we use a beam of5k which provides a good trade-off between perfor-mance and speed.Regularisation Table 1 shows that the per-formance of an unregularised maximum likeli-hood model lags well behind the regularised max-translation model.
From this we can conclude thatthe maximum likelihood model is overfitting thetraining set.
We suggest that is a result of the degen-erate solutions of the conditional maximum likeli-hood estimate, as described in DeNero et al (2006).Here we assert that our regularised maximum a pos-205Grammar Rules ML MAP(?2 =?)
(?2 = 1)?X??
?carte, map?
1.0 0.5?X??
?carte, notice?
0.0 0.5?X??
?sur, on?
1.0 1.0?X??
?la, the?
1.0 1.0?X??
?table, table?
1.0 0.5?X??
?table, chart?
0.0 0.5?X??
?carte sur, notice on?
1.0 0.5?X??
?carte sur, map on?
0.0 0.5?X??
?sur la, on the?
1.0 1.0?X??
?la table, the table?
0.0 0.5?X??
?la table, the chart?
1.0 0.5Training data:carte sur la table?
map on the tablecarte sur la table?
notice on the chartTable 2.
Comparison of the susceptibility to degeneratesolutions for a ML and MAP optimised model, using a sim-ple grammar with one parameter per rule and a monotoneglue rule: ?X?
?
?X 1 X 2 , X 1X 2 ?teriori model avoids such solutions.This is illustrated in Table 2, which shows theconditional probabilities for rules, obtained by lo-cally normalising the rule feature weights for a sim-ple grammar extracted from the ambiguous pair ofsentences presented in DeNero et al (2006).
Thefirst column of conditional probabilities correspondsto a maximum likelihood estimate, i.e., without reg-ularisation.
As expected, the model finds a degener-ate solution in which overlapping rules are exploitedin order to minimise the entropy of the rule trans-lation distributions.
The second column shows thesolution found by our model when regularised by aGaussian prior with unit variance.
Here we see thatthe model finds the desired solution in which the trueambiguity of the translation rules is preserved.
Theintuition is that in order to find a degenerate solu-tion, dispreferred rules must be given large negativeweights.
However the prior penalises large weights,and therefore the best strategy for the regularisedmodel is to evenly distribute probability mass.Translation comparison Having demonstratedthat accounting for derivational ambiguity leads toimprovements for our discriminative model, we nowplace the performance of our system in the contextof the standard approach to hierarchical translation.To do this we use our own implementation of Hiero(Chiang, 2007), with the same grammar but with thetraditional generative feature set trained in a linearmodel with minimum BLEU training.
The featureset includes: a trigram language model (lm) trainedSystem Test (BLEU)Discriminative max-derivation 25.78Hiero (pd, gr, rc, wc) 26.48Discriminative max-translation 27.72Hiero (pd, pr, plexd , plexr , gr, rc, wc) 28.14Hiero (pd, pr, plexd , plexr , gr, rc, wc, lm) 32.00Table 3.
Test set performance compared with a standardHiero systemon the English side of the unfiltered Europarl corpus;direct and reverse translation scores estimated as rel-ative frequencies (pd, pr); lexical translation scores(plexd , plexr ), a binary flag for the glue rule which al-lows the model to (dis)favour monotone translation(gr); and rule and target word counts (rc, wc).Table 3 shows the results of our system on thetest set.
Firstly we show the relative scores of ourmodel against Hiero without using reverse transla-tion or lexical features.7 This allows us to directlystudy the differences between the two translationmodels without the added complication of the otherfeatures.
As well as both modelling the same dis-tribution, when our model is trained with a singleparameter per-rule these systems have the same pa-rameter space, differing only in the manner of esti-mation.Additionally we show the scores achieved byMERT training the full set of features for Hiero, withand without a language model.8 We provide theseresults for reference.
To compare our model directlywith these systems we would need to incorporate ad-ditional features and a language model, work whichwe have left for a later date.The relative scores confirm that our model, withits minimalist feature set, achieves comparable per-formance to the standard feature set without the lan-guage model.
This is encouraging as our model wastrained to optimise likelihood rather than BLEU, yetit is still competitive on that metric.
As expected,the language model makes a significant difference toBLEU, however we believe that this effect is orthog-onal to the choice of base translation model, thus wewould expect a similar gain when integrating a lan-guage model into the discriminative system.An informal comparison of the outputs on the de-velopment set, presented in Table 4, suggests that the7Although the most direct comparison for the discriminativemodel would be with pd model alone, omitting the gr, rc andwc features and MERT training produces poor translations.8Hiero (pd, pr, plexd , plexr , gr, rc, wc, lm) represents state-of-the-art performance on this training/testing set.206S: C?est pourquoi nous souhaitons que l?affaire nous soit ren-voye?e.R: We therefore want the matter re-referred to ourselves.D: That is why we want the that matters we to be referredback.T: That is why we would like the matter to be referred back.H: That is why we wish that the matter we be referred back.S: Par contre, la transposition dans les E?tats membres restetrop lente.R: But implementation by the Member States has still beentoo slow.D: However, it is implemented in the Member States is stilltoo slow.T: However, the implementation measures in Member Statesremains too slow.H: In against, transposition in the Member States remains tooslow.S: Aussi, je conside`re qu?il reste e?norme?ment a` faire dans cedomaine.R: I therefore consider that there is an incredible amount stillto do in this area.D: So I think remains a lot to be done in this field.T: So I think there is still much to be done in this area.H: Therefore, I think it remains a vast amount to do in thisarea.Table 4.
Example output produced by the max-derivation (D), max-translation (T) decoding algorithmsand Hiero(pd, pr, plexd , plexr , gr, rc, wc) (H) models, relativeto the source (S) and reference (R).translation optimising discriminative model moreoften produces quite fluent translations, yet not inways that would lead to an increase in BLEU score.9This could be considered a side-effect of optimisinglikelihood rather than BLEU.Scaling In Figure 6 we plot the scaling charac-teristics of our models.
The systems shown in thegraph use the full grammar extracted on the 170ksentence corpus.
The number of sentences uponwhich the iterative training algorithm is used to esti-mate the parameters is varied from 10k to the max-imum 130K for which our model can reproduce thereference translation.
As expected, the more dataused to train the system, the better the performance.However, as the performance is still increasing sig-nificantly when all the parseable sentences are used,it is clear that the system?s performance is sufferingfrom the large number (40k) of sentences that arediscarded before training.5 Discussion and Further WorkWe have shown that explicitly accounting for com-peting derivations yields translation improvements.9Hiero was MERT trained on this set and has a 2% higherBLEU score compared to the discriminative model.lllllltraining sentencesdevelopmentBLEU (%)26272829303110k 25k 50k 75k 100k 130kFigure 6.
Learning curve showing that the model contin-ues to improve as we increase the number of training sen-tences (development set)Our model avoids the estimation biases associatedwith heuristic frequency count approaches and usesstandard regularisation techniques to avoid degener-ate maximum likelihood solutions.Having demonstrated the efficacy of our modelwith very simple features, the logical next step isto investigate more expressive features.
Promisingfeatures might include those over source side re-ordering rules (Wang et al, 2007) or source con-text features (Carpuat and Wu, 2007).
Rule fre-quency features extracted from large training cor-pora would help the model to overcome the issue ofunreachable reference sentences.
Such approacheshave been shown to be effective in log-linear word-alignment models where only a small supervisedcorpus is available (Blunsom and Cohn, 2006).Finally, while in this paper we have focussed onthe science of discriminative machine translation,we believe that with suitable engineering this modelwill advance the state-of-the-art.
To do so wouldrequire integrating a language model feature intothe max-translation decoding algorithm.
The use ofricher, more linguistic grammars (e.g., Galley et al(2004)) may also improve the system.AcknowledgementsThe authors acknowledge the support of the EPSRC(Blunsom & Osborne, grant EP/D074959/1; Cohn,grant GR/T04557/01).207ReferencesPhil Blunsom and Trevor Cohn.
2006.
Discriminativeword alignment with conditional random fields.
InProc.
of the 44th Annual Meeting of the ACL and 21stInternational Conference on Computational Linguis-tics (COLING/ACL-2006), pages 65?72, Sydney, Aus-tralia, July.Marine Carpuat and Dekai Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In Proc.
of the 2007 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2007), pages 61?72, Prague, Czech Republic.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of the 43rdAnnual Meeting of the ACL (ACL-2005), pages 263?270, Ann Arbor, Michigan, June.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In Proc.
of the42nd Annual Meeting of the ACL (ACL-2004), pages103?110, Barcelona, Spain.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics, 33(4).John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperformsurface heuristics.
In Proc.
of the HLT-NAACL 2006Workshop on Statistical Machine Translation, pages31?38, New York City, June.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.
ofthe 4th International Conference on Human LanguageTechnology Research and 5th Annual Meeting of theNAACL (HLT-NAACL 2004), Boston, USA, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of the 44th Annual Meeting of the ACL and 21st In-ternational Conference on Computational Linguistics(COLING/ACL-2006), pages 961?968, Sydney, Aus-tralia, July.Abraham Ittycheriah and Salim Roukos.
2007.
Directtranslation model 2.
In Proc.
of the 7th InternationalConference on Human Language Technology Researchand 8th Annual Meeting of the NAACL (HLT-NAACL2007), pages 57?64, Rochester, USA.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.
ofthe 3rd International Conference on Human LanguageTechnology Research and 4th Annual Meeting of theNAACL (HLT-NAACL 2003), pages 81?88, Edmonton,Canada, May.Philip M. Lewis II and Richard E. Stearns.
1968.
Syntax-directed transduction.
J. ACM, 15(3):465?488.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proc.
of the 44th An-nual Meeting of the ACL and 21st International Con-ference on Computational Linguistics (COLING/ACL-2006), pages 761?768, Sydney, Australia, July.Robert Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
In Proc.
ofthe 6th Conference on Natural Language Learning(CoNLL-2002), pages 49?55, Taipei, Taiwan, August.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of the 41st An-nual Meeting of the ACL (ACL-2003), pages 160?167,Sapporo, Japan.Slav Petrov, Adam Pauls, and Dan Klein.
2007.
Discrim-inative log-linear grammars with latent variables.
InAdvances in Neural Information Processing Systems20 (NIPS), Vancouver, Canada.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proc.
of the3rd International Conference on Human LanguageTechnology Research and 4th Annual Meeting of theNAACL (HLT-NAACL 2003), pages 134?141, Edmon-ton, Canada.Christoph Tillmann and Tong Zhang.
2007.
A block bi-gram prediction model for statistical machine transla-tion.
ACM Transactions Speech Language Processing,4(3):6.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proc.
of the 2007 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2007), pages 737?745, Prague, Czech Re-public.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for statis-tical machine translation.
In Proc.
of the 2007 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2007), pages 764?773, Prague,Czech Republic.Benjamin Wellington, Joseph Turian, Chris Pike, andI.
Dan Melamed.
2006.
Scalable purely-discriminative training for word and tree transducers.In Proc.
of the 7th Biennial Conference of the Associa-tion for Machine Translation in the Americas (AMTA),Boston, USA.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.208
