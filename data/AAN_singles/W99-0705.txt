The #-TBL System:Logic Programming Tools forTransformation-Based LearningTorbjSrn LagerDepartment ofLinguisticsUppsala UniversityTorbj orn.
Lager~ling.
uu.
seAbstractThe #u-TBL system represents an attempt o use thesearch and database capabilities of the Prolog pro-gramming language to implement a generalized formof transformation-based learning.
In the true spiritof logic-programming, the implementation is 'derived'from a declarative, logical interpretation f transforma-tion rules.
The #-TBL system recognizes four kindsof rules, that can be used to implement various kindsof disambiguators, including Constraint Grammar dis-ambiguators a  well as more traditional 'Brill-taggers'.Results from a number of experiments and benchmarksare presented which show that the system is both flex-" ible and efficient.IntroductionSince Eric Brill first introduced the method of Trans-formation-Based Learning (TBL) it has been usedto learn rules for many natural anguage processingtasks, such as part-of-speech tagging \[Brill, 1995\], PP-attachment disambiguation \[Brill and Resnik, 1994\],text chunking \[Ramshaw and Marcus, 1995\], spell-ing correction \[Mangu and Brill, 1997\], dialogue acttagging \[Samuel et al, 1998\] and ellipsis resolution\[Hardt, 1998\].
Thus, TBL has proved very useful, inmany different ways, and is likely to continue to do soin the future.Moreover, since Brill generously made his own TBLimplementation publicly available, l many researchers inneed of all off-the-shelf retrainable part-of-speech tag-ger have found what they were looking for.
However,although very useful, Brill's original implementationis somewhat opaque, templates are not compositional,IThroughout this paper, when referring to Brill's TBLimplementation, it is always his contextual-rule-learner -implemented in C - that I have in mind.
"It is available fromhttp://www, cs.
jhu.
edu/~bri l l / ,  along with ~veral otherlearners and utility programs.and they are hard-wired into the program.
Therefore,the program is difficult o modify and extend.
What ismore, it is fairly slow.This paper is dedicated to the design and implemen-tation of an alternative transformation-based learnersystem, called "the #-TBL system" (pronounced "mu-table").
The p-TBL system is designed to be the-oretically transparent, flexible and efficient.
Trans-parency is achieved by performing a 'logical reconstruc-tion' of TBL, and by deriving the system from there.Flexibility is achieved through the use of a composi-tional rule and template formalism, and 'pluggable' "al-gorithms.
As for the implementation, it turns out thattransformation-based learning can be implemented verystraightforwardly in a logic programming language suchas Prolog.
Efficient indexing of data, unification andbacktracking search, as well as established Prolog pro-gramming techniques for building rule compilers andmeta-interpreters, contribute to the making of a logi-cally transparent, easily extendible, and fairly efficientsystem.
2The content of the paper is presented in a bottom-upfashion, starting from the semantics of transformationrules.
First, I show that, contrary to what is often as-sumed, transformation rules can be given a declarative,logical interpretation.
I then introduce the IL-TBL sys-tem, which in a manner of speaking is derived from thisinterpretation f rules.
The template compiler, a partof the system which translates templates into efficientProlog programs, is described, and by w~" of examplesit is shown how a particular combination of trainingdata and templates may be 'queried' from the Prologprompt.
Next, a number of variants of all-solutionspredicates are specified, that deal with notions uch asscores, rankings and thresholds.
Since they appear tobe independently useful - even useful outside TBL -"The ~-TBL system is available fromhttp://~w, ling.
gu.
se/-~lager/mutbl, html.33IIIIIIIIIIIIIIIIIIIthey belong in a separate library.
By combining pred-icates from these code libraries, a number of TBL-likealgorithms are assembled, and benchmarks are run thatshow the/~-TBL system to be quite efficient.
Finally, asmall experiment using transformation-based l arningto induce Constraint Grammars from corpora is per-formed.The Semantics of Transformation RulesThe object of TBL is to learn an ordered sequence oftransformation rules.
The p-TBL system supports fourkinds of transformation rules.Rep lacement  ru les  dictate when - based on thecontext - one feature value for a word should be re-placed with another feature value.
An example wouldbe "replace tag vb with nn if the Word immediatelyto the left has a tag dt" .
Here is how this ruleis represented in the #-TBL system's compositionalrule/template formalism:tag:vb>nn <- tag:dr@\[-1\].This is of course the exact counterpart of the transfor-mation rule in Brill's original framework.Add i t ion  rules specify when a feature value shouldbe added to a word.
An example would be "add tag nnto a word if the word immediately to the left has a tagtit":tag:0>nn <- tag:dr@\[-1\].Note that a feature value is actually added to a wordonly if it not already there.De le t ion  rules dictate when a feature value shouldbe removed from a word.
An example would be "removetag vb from a word if the word immediately to the lefthas a tag dt":tag:vb>0 <- tag:dt@\[-1\].Reduct ion  rules reduce the set of feature valuesfor a word with a certain value.
An example wouldbe "reduce a word's tag values with tag vb if the wordimmediately to the left has a tag dr":tag:vb>l <- tag:dr@I-I\].An important difference between deletion rules and re-duction rules is that the latter will only remove a featurevalue from a word if it is not the last value for that fea-ture.
If vb is the last value the above rule is not applica-ble and the reduction will not take place.
This shouldremind us of the kind of constraints that are centralto the so called reductionistic approach to disambigua-tion.
as represented by for example Constraint Gram-34mar \[Karlsson et al, 1995\]).
Constraint grammars mayindeed be possible to learn in the IL-TBL system, as Iwill show towards the end of this paper.In the p-TBL system's rule formalism, conditionsmay refer to different symbol features, and complexconditions may be composed from simpler ones.
Forexample, here is a rule saying "replace the tag for ad-verb with the tag for adjective, if the current word is"only", and if the previous tag, or the tag before that,is a determiner tag.
":tag:ab>jj <- wd:only@\[O\] k tag:dt@\[-1,-2\] .Ill this paper, I will break with the tradition to thinkabout transformation rules in exclusively proceduralterms, and instead try to think about them in declara-tive and logical terms.
Transformation rules (partially)describe an ordered sequence of pairs of symbols, whichI will refer to as a relation.
Such a relation form trainingdata for a TBL system.
Here is a simple (and unrealis-tically small) example:dt vb nn dt vb kn dt vb ab dt vbdt nn vb dt nn kn dt jj kn dt nnThe sequence formed by the upper elements of the pairswill be referred to as Sl, and the sequence formed by thelower elements as Sn.
Such sequences can be.
modelledby means of two sets of clauses, which relate positionsin the sequences to symbol feature values:$1 (1, dr) Sl (2, vb) $1 (3, nn) ... S1 (11, vb)S. (I, dr) S~ (2, nn) S.(3, vb) ... S. (11, nn)A central point in this paper is the suggestion thatthe declarative semantics of transformation rules canbe captured by rule formulas in the form of univer-sally quantified implications, and that, for example, themeanings of the four very simple rules shown previouslyare captured by the following formulas:ReplacementVpo,p, \[S, (po,vb) A (Pl = 1)o - 1) A St (p, .dr) --r S ,  (po,nn)\]AdditionVpo,p, \[-,St (po,nn) A (p, = Po - 1) A St (p, ,at ) ---r S,  (po ,nn ) \]Delet ionVpo.p, \[St (po,vb ) A (p, = po - 1) A St (p, ,dr) --~ -~S.
(po,vb )\]ReductionVpo,p, \[Sl (po,vb) A Bxo\[S, (po,xo) A (x0 # vb)\]A(P, = P0 -- 1) A S,(pl,dt) --~ -.S.
(po,vb)\]Rule formulas as such will not be put to any directcomputational use, but the notion of a rule formulaprovides a starting point, from which computationaltools can be derived.A rule instance is a rule formula in which every vari-able has been replaced with a constant.
Now, we maydefine the notions of positive and negative instancesof rule formulas (and thus indirectly of transforma-tion rules).
A positive rule instance is a rule instancewhere the mltecedent and the consequent are both true.Thus, the following formula is a positive instance of theformula corresponding to the simple replacement ruleabove:Sl(2, vb) A (1 = 2 - 1) A S,(I,dt) --+ S,(2, nn)A negative instance of a rule is a rule instance where theantecedent is true but where the consequent is false, forexample:Sl(8,vb) A (7 = 8 - 1) A S~(7,dt) -~ S,,(8, nn)Note that Brilrs notion of a neutral instance of a rule,i.e.
an instance of a rule that replaces an incorrect agwith another incorrect ag, is a negative instance in myterminology.
(In practice, this does not seem to mattermuch, as I will show later.
)We now define two important rule evaluation mea-sures.
The score of a rule is the number of its positiveinstances minus the number of its negative instances:sco~e(R) =1 pos(R) 1 - I neg(R) IThe accuracy of a rule is its number of positive instancesdivided by the total number of instances of the rule:accuracy(R) = I pos(R) II P?S(R) I + I neg(R) IThe notion of rule accuracy is well-known in rule in-duction and inductive logic programming, and towardsthe end of this paper we will see that it may have a roleto play in the context of transformation-based l arningtoo.An Overv iew o f  the  #-TBL  SystemThrough the use of unification and a particular searchstrategy (backtracking), a logic programming environ-ment such as Prolog implements a constructive kindof inference which allows us to define predicates thatare able to recognize, generate and search for positiveand negative instances of transformation rules.
Fur-thermore, a layer of recta-logical predicates providesa way to collect and count such instances, and thusa way to calculate the score and accuracy for anyrule.
Therefore, in a logic programming framework,transformation-based l arning can be implemented in avery clear and simple way.However, for such an implementation to become use-ful.
we have to think about efficiency.
Among otherthings, we need to think about how we index our train-ing data.
Assuming the part-of-speech tagging task.corpus data can be represented by" means of three "kindsof clauses:35wd (P,N) is true iff the word W is at position P in thecorpustag(P,A)  is true iff the word at position P in the cor-pus is tagged Atag(A,B,P)  is true iff the word at, P is tagged A andthe correct ag for the word at P is BAlthough this representation may seem a bit redundant,it provides exactly the kind of indexing into the datathat is needed.
3 A decent Prolog system can deal withmillions of such clauses.Rules that can be learned in TBL  are instances oftemplates, such as "replace tag A with B if tho symbol(e.g.
the word) immediately to the left has tag C, whereA, B and C are variables.
Here is how we write thistemplate in the p-TBL system:t3(A,B,C) # tag:A>B <- tag:C@\[-l\].The term to the left of # is a unique identifier for thetemplate.
A template instance is a template in whichevery variable in the identifier has been replaced by aconstant.
If we strip the identifier we end up with atransformation rule again.
The instantiated identifieruniquely identifies that rule.Positive instances of rules that are instancbs of theabove template can be efficiently recognized, generatedand searched for, by means of the following clause:positive (t3(A,B,C)) :-tag(A,B,PO), Pl is PO-I, tag(Pl,C).Negative instances are handled as follows:negative (t3 (A,B, C) ) :-tag(A,X,PO), dif(X,B), P1 is PO-I,taE(PI,C).It should be clear how these clauses use the representa-tion described above, and that they respect the seman-tics exemplified in the previous section.
Clauses cor-responding to other templates and other types of rulescan be defined accordingly.Tied to each template is also an update proce.dure thatwill apply rules that are instances of this template, andthus update sequences, by replacing feature values withother feature values, adding to the feature values, orremoving from them.
For example:apply(t3 (A,B,C)) :-(tag(A,X,P), P1 is P-l, tag(PI,C),retract(tag(A,X,P)),  re t rac t ( tag(P , l ) ) ,assert(tag(B,X,P)), asser~(tag(P,B)),fail ; true).3Assuming a Prolog with first argument indexing.IIIIIIIIIIIIIIIIIIITo write clauses uch as these by hand for large setsof templates would be tedious and prone to errors andomissions.
Fortunately, since the formalism is composi-tional, it is easy to write a template compiler that gener-ates them automatically.
The #u-TBL system uses well-known Prolog compiler writing techniques to expandtemplates written in the compositional high-level nota-tion into clauses that can be run as programs.
Thus, theconvenience and flexibility of a high-level notation fortemplates and rules does not compromise performance.A template grammar defines the exact relation betweena template and a set of clauses.
As an illustration, thefollowing grammar rules are used to expand a templateinto a Prolog clause defining pos i t ive / l ,  negative/1and apply/ l ,  for that template:term_expansion((ID # A<-Cs),\ [ (pos i t ive( ID)  : -  G1),(negative(ID) :- G2),(apply(ID) :-(G3.fail;true))\]) :-pos ((A<-Cs) ,LI, \[\] ), list2goal(L1 .G1),ne E ((A<-Cs) ,L2, \[\] ).
list2goal (Li, G2),app((A<-Cs) ,L3, \[\] ), list2goal(L3,G3).pos((F:A>B<-Cs)) -->{G =.. \[F,A,B,P\]},\[G\], cond(Cs,P).neg((F:A>B<-Cs)) -->{G =.. \[F,A,X,P\]}, \[dif(X,B),G\], cond(Cs,P).app((F:A>B<-Cs)) -->{GI =.. \[F,A,X,P\], G2 =.. \[F,P,A\],G3 =.. \[F,B,X,P\], G4 =.. \[F.P,B\]},\[GI\], cond(Cs,P), \[retract(Gl),retract(G2), assert(G3), assert(G4)\].cond((C&Cs),P) --> cond(C,P), cond(Cs,P).cond(FA@Pos,PO) --> pos(Pos,PO,P), feat(FA,P).pos(Pos,PO,P) -->\[member(Offset,Pos), P is PO+Offset\].feat(F:A,P)--> {G =.. \[F,P,A\]}, \[G\].A modern Prolog system will compile the resultingclauses all the way down to machine code.
Thus.
aTBL-system implemented in Prolog can be quite effi-cient.The p -TBL  Template  Compi le rWhen a file containing transformation rules is consultedor compiled, each transformation rule is expanded intoseveral Prolog clauses) As a result of this, a largenumber of predicates becomes available, some of whichare documented in Figure 1.Using the predicates generated by the template com-piler, the training data in combination with the tem-4Also.
if the user does not provide them, template idea-tlfiers m'e constructed automatically.pair(?a,?B)pair(?h,?B,?P)A ~ aligned with B at a position P in the current data.positive (?RuleID)positive (?RuleID, ?a, ?B)positive (?RuleID, ?A, ?B, ?P)RuleID names a rule which has a positive instance inthe current data at a position P, whero A is alignedwith B.
The rule is an instance of a template, whichis identified by the functor of RuleID.
A call to thispredicate usually has many solutions, az~,l tile order inwhich solutions are returned on backtracking is deter-mined by the order in which templates are presentedto the system, and the order of symbols in the trainingdata.sample(?RuleID)sample(?RuleID,?A,?B)sample(?RuleID,?A,?B,?P)As positive/{l,2,3}, except that rules are randomlysampled.sample_R(+R,?A,?B,-Rules)Binds Rules to al ist  with R rmldomly samplcd rules.negative(?RuleID)negative(?RuleID,?h,?B)negative(?RuleID.
?A,?B,?P)RuleID names a rule which hasa negative instance mthe data a ta  position P, where h is aligned with B.apply (+RuleID)The rule RuleID is applied to the current data.Figure 1: Extract fi'om the manualplates may be queried.
By backtracking through thesolutions to a call to pos i t ive /1  we may for exampleverify that there are ten ways to instantiate our exam-ple template in our example data (for space reasons, Ishow only the first three solutions):\[ 7- positive(RuleID), RuleID # Rule?Rule = tag:vb>nn <- tag:dt@\[-l\] 7 ;Rule = tag:nn>vb <- tag:vb@\[-1\] ?
;Rule = tag:dr>dr <- tag:nn@\[-l\] ?
;?
.
.Alternatively, we might be interested only in instanceswhere the aligned feature values (h and B) are different,and there are six of those: aSdif/2 is a built-in predicate in SICStus Prolog.
A callto d i f  (X,Y) constrains X and Y to represent different erms.Calls to d i f /2  either succeed~ fail.
or are blocked dependingoil whether X and Y are sufficiently instantiated.36l 7- dif(A,B), positive(ID,A,B), ID # Rule.Or, we might be interested only in template instanceswhere the aligned symbols have feature values nn andvb, respectively.
There is only one such rule:i ?- positive(RulelD,nn,vb), RuleID # Rule.Rule = tag:nn>vb <- tag:vb@\[-l\] ?
;Sometimes, a random sample of a positive rule mightbe more useful:7-  sample(Ru le ID ,nn ,vb) ,  RuleID # Rule .Rule = tag :nn>vb <- tag :vb@\[ -1 \ ]As for negative instances, we may want to know if therule tag :vb>nn <-  tag :dr@\[ -1 \ ]  has any negative in-stances in the training data, and indeed there is one atposition 8, where vb is aligned with j j  rather than nn:?- RuleID # (tag:vb>nn <- tag:dt0\[-l\]),negative(RuleID,A,B,P).h = vb, B = j j ,  P = 8 ?L ib rary  rankingLibrary ranking is a package for scoring and rankingrules.
It was written for the specific purpose of scoringtransformation rules in the context of TBL, but is likelyto be more generally useful, hence deserving its statusas a libra~'y.
The basic notions are defined as follows:A score is an integer > 0A ranking entry is a pair S-R such that S is a scoreand R is a ruleA ranking is an ordered sequence of ranking entrieswhere each rule occurs only once.The score of a rule is determined by counting the so-lutions returned by goals containing the rule (or ratherits ID).
Thus.
many predicates in library ranking aremeta-predicates that work much the same way as theso called all-solutions predicates that are built into Pro-log.
Figure 2 lists some of the predicates available inlibrary ranking.The library encapsulates some of Brill's own w~'s ofoptimizing transformation-based l arning - optimiza-tions which are possible to perform for the ranking ofrules in general.The predicates in library ranking interact in astraightforward way with the predicates generated bythe template compiler, as the following examples willshow.
Here.
for instance, is how we compute (and print)a ranking on the basis of the goal invoh'ing a call toposit ive/3:37count (?R, +Goal , -N)Binds N to the number of solutions for Goal.
un\]ess itfails for lack of solutions.
If there are uninsta~itiatedvariables in Goal, then a call to countl3 may back-track, generating alternative values for N correspondingto different instantiations of the free variables of Goal.Defined as:count(R,Goal,N) :-bagof(R,Goal,Solutions),length(Solutions,N).rank (?R, +Goal, +ST, -Ranking)Computes the score for each instance of R and ranksthe instances.
However, instances with scores less thanthe score threshold (ST) are not ranked.
Defined as:rank (R, Goal, ST, Rnkng) :-setof (N-R, (count (., Goal, N), N>ST), Rnkng0),reverse  (RnkngO, Rnkng).penalize (?R, +Goal, +Rnkng, +ST, +AT, -NewRnkng)Re-ranks the rules ill Rnkng by subtracting from theirscores, giving a new ranking NewRnkng.
However.
anyrule with a score < ST or all accuracy < AT Is justdropped.at_position (+N, +Rnkng, -Rule, -Score)Retrieves the Nth rule in the ranking, and its score.highscore (?R, +PGoal, +NGoal, +ST, +AT, ?WR, ?WRS)Among the different instances of R, NR is the rule withthe highest score (i.e.
the 'winning rule'), and NRS isits score, defined as the number of solutions to thegoal PGoal minus the number of solutions to NGoal.However.
if NRS < ST, or if no rule clears the accuracythreshold (AT), highscore/7 fails.
Works a~ if definedby:highscore (R,PGoal,NGoal, ST, AT, WR, ~IRS) --rank (R, PGoal, ST, Rnkng),penalize (R, NGoal, Rnkng, ST, AT, NewRnkng),at _pos it ion ( 1, NewRnkng, WR, WRS).Ill fact, highscore/7 is implemented in a more efficientway.
It keeps track of a leading rule and its score, andthus only has to generate and count solutions to NGoalfor rules for which the number of positive instances Isgreater than the score for the leading lule.
Moreover,the score threshold (ST) for the counting of solutions ofNGoal can be set to the nmnber of solutions to PGoalminus the score for the leading rule.Figure 2: Extract fl'om the manual?- rank(R,A-B'(di f (A,B),posit ive(R,A,B)) , l ,L) ,print_ranking(L).3 tag:vb>nn <- tag:dt@\[-1\]1 tag:vb>jj <- tag:dtO\[-l\]IIIIIIIIIIIiIIIIIIIAnd here is how we find the highest scoring rule:I 7- highscore(R,A'B'(dif(A,B),positive(R,A,B)),negat i re  ( R), 1, WR, WRS ).WR = tag:vb>nn <- tag:tit@\[-1\], WRS = 3 ?
;This concludes the demonstration f how the templatecompiler and the ranking library allows a particularcombination of templates and training data to be in-teractively explored from the Prolog prompt.S imple  TBLFull transformation-based l arning is just a small snip-pet of code away.
Given corpus data, templates and val-ues for the thresholds (ST and AT), the predicate tb l /3implements learning of a sequence of rules:Program 1tb l  (ST,AT,WRs) : -( highscore (Rule,A'B" (dif (A,B) ,positive (Rule,A, B)),negative (Rule),ST,AT,WR,WRS)-> apply(~),tbl  (ST,AT,WRsl),was = \[~n~lWas13; WRs = \[\]).This predicate, defined entirely in terms of predicatesgenerated by the template compiler and predicates fromlibrary ranking, combines all the important principles ofTBL into a complete learning program, that repeatedlyinstantiates rule templates in training data, scores ruleson the basis of counts of positive and negative instancesof them, selects the highest scoring rule on the basis ofthis ranking, and applies it to the training data.Consider our small example once again.
Here are thethree rules learned (with the score threshold set to 1)tag:vb>nn <- tag:dt~\[-1\] .tag:ab>kn <- tag:nn@\[-1\].tag:nn>vb <- tag:nn@\[-l\].and here are the transformations that the upper se-quence of the training data goes through, when the rulesare applied in the given order:dt vb nn dt vb kn dt vb ab dt vbdt nn nn dt nn kn dt nn ab dt nndt nn nn dt nn kn dt nn kn dt nnd~ nn vb dt nn kn dt nn kn dt nnIt is interesting to regard what is happening hei-e as adecomposition of a relation S1-S,, into a number of re-38lations SI-S.>_ o. .
.
o S , - I -S , ,  corresponding to a numberof rules R1 o .
.
.
o R ,_ , .In general, for such a decomposition S,-S,  = Si-S,+l o S,+I-Sn, it holds that if a rule R, has P pos-itive and N negative instances in S,-S, ,  then (i) R,will have P + N positive and no negative instances inSs-Si+l, and (ii) Ri will have no positive nor negativeinstances in S,+I-S,.
Clearly, (i) follows from the factthat the update procedure associated with R, changedthe negative instances of Ri in S,-S,  into positive ones,and (ii) from the fact that the antecedent of R, mustbe false in S,+l'Sn.
As a corollary to (ii) it follows thatRi will not be selected next.For each step, as long as P > ST + N, then S, willbecome more similar to Sn.
Note.
in our example, thatthere is one rule, tag :nn>j j  <- tag:dr@\[ -1 \ ] ,  thatwould remove the only remaining difference between$4 and S,.
However, this rule also has three nega-tive instances, and thus the rule gets a score below timthreshold.Sca l ing  UpProgram 1is indeed small, simple and transparent.
Butwhat about efficiency?
How well does it scale up to han-dle real world tasks, such as part-of-speech tagging?
Inone small test the learner was operating on annotatedSwedish corpora 6 of three different sizes, with 23 dif-ferent tags, and the 26 templates that BriU uses in hisdistribution:tag:A>B <- tag:C@\[-l\].tag:A>B <- tag:C?\[l\].tag:A>B <- tag:C?\[-2\].tag:A>B <- tag:C@\[2\].tag:A>B <- tag:C@\[-l,-2\].tag:A>B <- tag:C@\[l,2\].tag:A>B <- tag:C@\[-l,-2,-3\].tag:A>B <- tag:C@\[l,2,3\].tag:A>B <- tag:C@\[-1\] & tag:D@\[1\].tag:A>B <- tag:C@\[-l\] ~ tag:D~\[-2\].tag:A>B <- tag:C@\[1\] & tag:D@\[2\].tag:A>B <- ,d:C@\[O\] ~ tag:D@\[-2\].tag:A>B <- wd:C@\[O\] & wd:D@\[-2\].tag:A>B <- wd:C@\[-1\].tag:A>B <- wd:C@\[l\].tag:A>B <- wd:C@\[-2\].tag:A>B <- wd:C@\[2\].tag:A>B <- wd:C@\[-l,-2\].tag:A>B <- ,d:C@\[1,2\].tag:A>B <- wd:C@\[O\] & ,d:D@\[-1\].6I have used selected parts of the Stockholm-Umea Cor-pus (SUC).
Here is a key to the part-of-speech tags appear-ing in the present paper: ma = noun, vb = verb, pp = prepo-sition, pra = proper name.
d t= determiner, pn = pronoun,ie = infinitive marker, sn = subjunction, j j  = adjective,ab = adverb, hp = relative pronoun, kn ---- conjunction.
See\[Ejerhed et al~ 1992\] for further details of this corpus.tag:h>B <- wd:C@\[O\] & wd:D@\[l\].tag:A>B <- wd:C@\[0\] & tag : D@ \[-l\] .tag:h>B <- wd:C@\[O\] & tag:D@\[1\].tag:A> B <- wd:C@\[O\].tag:A>B <- wd:C@\[O\] & tag:D@\[2\].tag:A>B <- wd:C@\[O\] & wd:D@\[2\].Below, I show the first thirteen rules, as they are re-ported by the p-TBL system (during training on the30kw corpus).
Each rule is preceded by its score (firstcolumn), and by its accuracy (second column).130 1.00 tag:dt>pn <- tag:vb~\[1 \ ] .114 0.82 tag : ie>sn <- tag:vb@\[2\].58 0.85 tag:pn>dt <- vd:det@\[O\] & tag:jj%\[l\].37 1.00 tag:ie>sn <- tag:dt@\[l\].37 0.97 tag:ie>sn <- tag:nn@\[1\].34 0.95 tag:dt>pn <- tag:pp@\[l\].29 1.00 tag:ie>sn <- tag:pn@\[1\].21 1.00 tag:ie>sn <- tag:pro@Ill.19 1.00 tag:jj>pn <- wd:bland@\[-1\].18 1.00 tag:dt>pn <- tag:hp@\[l\].17 0.84 tag:pp>sn <- ,d:om@\[O\] & tag:pn@\[l\].15 0.94 tag:sn>ie <- tag:vb@\[1\].14 0.63 tag:hp>kn <- wd:som@\[O\] & tag:nn@\[1\].Note that the actual accuracy of a learned rule cansometimes be well below 1.00.
(The accuracy thresh-old was set to 0.5 in this experiment.)
The sequenceof rules works well anyway, since the damage done byan incorrect rule can be repaired by rules later in thesequence.
(In fact, a small experiment confirmed thatthe setting of the accuracy threshold to 1.00 generatesa tagger which performs less well.
)For each corpus, the accuracy of the learned sequenceof rules was measured on a test corpus consisting of40,000 words, with an initial-state accuracy of 93.3%.The system was running on a Sun Ultra Enterprise 3000with a 250Mhz processor.
Table 1 summarizes the re-sults of the tests:Size ST30k 260k 4120k 6Runtime Mem.req.15 min 23M24 rain 38M54 rain 71M#(Its) Acc.99 95.5%81 95.7%88 95.8%Table 1: #-TBL performance - the simple algorithmThe performance of the p-TBL system was comparedwith Brill's learner unning on the same machine, withthe same templates, core thresholds and training data.Table 2 gives the figures.These tests verify that the program works as ex-pected, and also that it is quite efficient, despite itssmall size and simple design.
In fact.
the tests showthat p-TBL learner is an order of magnitude faster thanBrill's original learner for this particular task.39Size ST30k 260k 4120k 6Runtime Mem.req.90 min 24M185 min 43M560 min 82M#(Rs)1048998ACC.95.5%95.7%95.8%Table 2: Performance of Brill's learnerTBL  h la Er ic  Br i l lThis algorithm is perhaps tile one that resembles Brill's"own algorithm the most.
It differs fi'om the siml)le algo-rithm in that to learn one rule, it ranks the error typesthat occur in the trairting data (using rank /4  from li-brary ranking to do so), and then it searches top-to-bottom in this ranking, entry by entry, for a rule whichfixes the type of error recorded by the entry, alwayskeeping track of a leading rule and its score.
When thescore for a ranking entry drops below the leading rule'sscore, the search is abandoned, and the leader is de-clared winner.
This effectively prunes the search spacewithout losing completeness, and it also saves a lot ofmemory, since only rules for one kind of error at a timehave to be held in memory.Program 2tbl (ST, AT, WRs) :-( rank((h,B), (dif CA,B) ,pair(A,B)), ST,Rnkng),le arn_one (Rnkng, dummy, O, AT, I/R, NRS),NRS >= ST-> apply (WR),tb l  (ST,AT,NRsl),NRs = \[WRtWRsl\]WRs = \[\]).learn_one (RnkngO,LR,LRS, AT,NR, NRS) :-( RnkngO -- \[N-CA,B) lEnkng\],N > LRS-> ( highscore (R,positive (R,A,B),negative (R, A, A),LRS,AT,LR1,LRSI)-> learn_one(Rnkng,LR1,LRS1 ,AT,NR,NRS); learn_one (Rnkng,LR, LRS, AT,NR, WRS)); NR = LR, NRS = LI~).The benchmark results, using the same setup as withthe simple algorithm, are shown in Table 3.As can be seen from Table 3, the optimized algo-rithm is significantly faster than the simple one, andit uses less memory.
However, as pointed out in\[Ramshaw and Marcus, 1995\], the effect of this partic-ular optimization method depends on the size of the!Size ST30k 260k 4120k I.
6Runtime Mem.req.10 min 17M20 min 22M50 min 39M#(Rs) Acc.99 95.5%85 95.7%92 95.8%Table 3: I~-TBL performance - the optimized algorithmtag set.
The larger the tag set, the more benefit wecan expect.
Thus, we can expect o see even greaterimprovements for many learning tasks.Note also that in contrast with the simple algorithm,this algorithm uses Brill's notion of negative rule in-stance.
The  call negat ive (g, A, A) ensures that neutralinstances are not counted as negative.
However, it ap-pears that the way negative instances are counted doesnot matter much, at least not for this application.
Therules look pretty much the same as the rules generatedby Brill's learner, and in fact, the first ten rules areidentical.Monte  Car lo  TBLThe original TBL algorithm suffers from the fact thatthe number of candidate rules to consider grows veryfast with the number of rule templates, and in prac-tice only a small number of templates can be handled.\[Samuel t al., 1998\] presents a novel twist to the al-gorithm, in order to solve this problem.
The idea isto randomly sample from the space of possible rules,rather than generating them all.
The better the rule is,the greater the chance that it is included in the sample.Thus, the system is likely to find the best rules first.
Animplementation f this algorithm can be assembled byreplacing the definition of learn_one/6 in Program 2with the following definition:Program 3learn_one(Rnkng0,LR,LRS,AT,NR,NRS) :-( Rnkng0 = \[N-(A,B) IRnkng\],N > LRS-> samp1e_R(16,A,B,Rs),( highscore(R,(member(R,Rs),positive(R,A,B)),negative(R,A,A),LRS,AT,LRI,LRS1)learn_one(Rnlmg,LRI,LRSl,AT,NR,NRS)Iearn_one(Rnkng,LR,LRS,AT,NR,NRS)->)).NR = LR, NRS = LRS40That is, h ighscore/7 picks tile rules that it evalu-ates from the (here) 16 rules that are sampled.
Theamount of work that h ighscore/7 has to perform, andthe memory requirements, no longer depends on howmany templates there are.To test the algorithm, the system was run with 260templates with the 60,000 word corpus, and a compar-ison was made with the optimized algorithm.
The out-come of this experiment is reported in Table 4.Algorithm Runtime Mem.req.
#(Rs) Acc.Lazy 48 min 21M 202 95.7%Brill 427 min 84M 126 95.7%Table 4: #-TBL performance - the lazy algorithm vs.the '~ la Brill algorithm.As can be seen from the table, although the other al-gorithm did not perform too bad with 260 templates,the 'lazy' algorithm was an order of magllitude faster.Accuracy was not compromised, although the numberof rules grew.As a sidenote, let me describe a convenient /L-TBLsystem feature which makes it possible to train withvery many templates without actually writing them 'alldown.
Instead of loading a set of templates into thesystem, the user may load a couple of template decla-rations, which, in terms of 'window' sizes and ranges ofrelative positions over which windows 'slide', constrainthe relation between templates and clauses, defined bythe template grammar.
Constrained in this way, thegrammar Call be used to generate templates.
Withoutgoing into any further details, let me just show the dec-larations which causes the system to generate the 260templates used above:?
- head(tag:A>B).
:- window_size(tag,3).
"- window_size(wd,2).
:- range(tag,\[-3,-2,-l,l,2,3\]).
:- range(wd,\[-2,-1,1,2\]).
"- anchors(\[-t,O,1\]).Learn ing  Const ra in t  GrammarsIn another experiment, the #-TBL system was run witha number of templates for reduction rules, in order to seeif something resembling a Constraint Grammar couldbe induced from training data.
Each word token in atraining corpus of 30,000 words was assigned the setof part-of-speech tags that it can have according to alexicon.
The training data also indicated which memberof this set was the correct one.The system was run with the following four tem-plates:tag:A>l <- unique(tag:C@\[-l\]).tag:A>l <- unique (tag : C@ \[l\] ) .tag:A>l <- wd:C@\[O\] & unique(tag:D@\[-l\]).tag:h>l <- wd:C@\[O\] k unique(tag:DO\[l\]).The use of the un ique/1 wrapper in the conditions ofthe rules has the effect hat a rule will trigger only if theassignments of tags to words in the relevant surround-ings ar e non-mnbiguous.
(As Karlsson et al (1995) putit, the rules are run in "careful application mode".
)As mentioned earlier, replacement rules do not haveto be very accurate: if a rule early in a sequence of re-placement rules makes ome errors, the errors can oftenbe 'fixed' by rules later in the sequence.
By contrast,in a sequence of reduction rules there are no rules thatcan add tags once they have been removed.
There-fore, in order to maximize the accuracy of the wholesequence of rules, it must be induced under a valida-tion bias which sees to it that each rule is as accurateas possible.
In the #-TBL system, this is taken care ofby" setting the accuracy threshold to a very high value.However, a sequence of rules induced in this way willtypically leave many words with more than one tag.
Ifwe want instead to minimize the tags per words ratio,the accuracy threshold can be set to a lower value, butthen a lower tagging accuracy will naturally result.
Inthe experiment, he accuracy threshold was set to 0.99(which still allows for a bit of noise in the data) and to0.85.
Program 2 was used.Below, I show the first ten rules that were learned bythe system (with the accuracy threshold set to 0.99):451 1.00 tag:rg>l <-451 1.00 tag:pl>l <-274 1.00 tag:pn>l <-274 0.99 tag:ab>l <-230 1.00 tag:pl>l <-222 1.00 tag:ab>l <-221 1.00 tag : rg>l  <-219 1.00 tag:p1>1 <-200 1.00 tag:p1>1 <-166 0.99 tag:rg>l  <-wd:i@\[O\] ~ unique(nn@\[l\]).wd:i@\[O\] & unique(nn@\[1\]).wd: en@ \[0\] & unique (nn@ \[i\] ).wd: en@ \[0\] & unique (nn@ \[i\] ).unique(dl@ \[-I\] ).wd:av~\[O\] & unique(nn@\[-l\]).wd:?
@\[O\] & unique(nn@\[-l\])..d:i@\[O\] & unique(nn@\[-1\]).wd:p@\[O\] k unique(nn@\[-t\]).wd:en@\[O\] & unique(jj@\[l\]).The induced sequences of rules were tested on a cor-pus of 11,000 words.
Both the accuracy and the tagsper word ratio in the test corpus were measured7 Theinitial tags per word ratio in the test corpus was 1.35.The results of the tests are given in Table 5.Size ST AT #(Rs) Runtime Acc.
T /W30k 6 0.99 410 75 min.
99.6% 1.1730k 6 0.85 215 20 min.
98.1% 1.04Table 5: Result of Constraint Grammar inductionTA word is deemed to be accurately tagged if the correcttag is an element in the set of tags that the word has beenassigned.These results are promising.
But before it would befair to compare with other methods for inducing Con-straint Grammars from annotated corpora, e.g.
themethods described ill \[Samuelsson etal., 1996\] or in\[Lindberg and Eineborg, 1998\], it remains to determinethe optimal set of templates and the optimal settingsof the accuracy threshold.
Very likely, the learning pro-cess (applied to the learning of reduction rules) can alsobe optimized for speed.
In short, a lot more has to bedone, but at least this section has shown how easily anexperiment like this can be set up in the #-TBL envi-ronment.Summary and ConclusionsThe #-TBL system is not just a re-implementationof original TBL in another programming language.Rather it should be seen as all attempt o use tile rea-soning and database capabilities of Prolog to do TBLill a more high-level way.
The #-TBL system is:General  - The system supports four types of rules bymeans of which not only traditional 'Brill-taggers',but also Constraint Grammar disambiguators, arepossible to train.Easi ly extend ib le  - Through its support of a compo-sitional rule/template formalism and 'pluggable' al-gorithms, the system can easily be tailored to differ-ent learning tasks.T ransparent  - Rules have a declarative, logical se-mantics which, among other things, has proved to beof great value during the implementation work.Eff ic ient - A number of benchmarks have been runwhich show that the system is fairly efficient - anorder of magnitude faster than Brill's contextual-rulelearner.In teract ive  - Prolog is all interactive language andthis is something that the #-TBL system inherits.Smal l  - Thanks to the choice of implementation lan-guage, the system's code base can be kept quite small.Indeed, a 'light' version of the #-TBL system., con-sisting of just one page of Prolog code, has been im-plemented \[Lager, 1999\].In short, the #-TBL system is a powerful environ-ment in which to experiment with transformation-basedlearning.AcknowledgementsThanks to Lars Borin, Mats DahllSf and Natalia Zi-novjeva at Uppsala University for comments and sug-gestions.
Joakim Nivre in GSteborg "also provided mewith valuable insights and advice.41IIIIIIIIIIIIIIIIIIIReferences\[Brill and Resnik, 1994\] Brill, E. and Resnik, P., 1994.A transformation-based approach to prepositionalphrase attachment disambiguation.
I  Proceedingsof COLING'94, Kyoto, Japan.\[Brill, 1995\] Brill, E., 1995, Transformation-BasedError-Driven Learning and Natural Language Pro-cessing: A Case Study in Part of Speech Tagging.Computational Linguistics, December 1995.\[Ejerhed et al, 1992\] Ejerhed, E., K~illgren, G..\Vennstedt, O. and .
?.str5m, M., 1992, The Lin-guistic Annotation System of the Stockhohn-UmeaProject.
Department of Linguistics, University bfUme.\[Hardt, 1998\] Hardt, D. 1998, Improving Ellipsis Res-olution w'ith Transformation-Based L arning.
Toappear, AAAI Fall Symposium.\[Karlsson et al, 1995\] Karlsson, F., Voutilainen, A.,Heikkil~i, J., Anttila, A.
(eds.).
1995, ConstraintGrammar.
A Language-Independent System forParsing Unrestricted Text.
Mouton de Gruyter.\[Lager, 1999\] Lager, T. 1999, /~-TBL Lite: A Small,Extendible Transformation-Based L arner, In Pro-ceedings of the Ninth Conference o\] the EuropeanChapter of the Association for Computational Lin-guistics (EACL'99), Bergen, June 8 - 12, 1999.\[Lindberg and Eineborg, 1998\] Lindberg, N. andEineborg M., Learning Constraint Grammar-styledisambiguation rules using Inductive Logic Pro-gramming.
In Proceedings of COLING/ACL "98.\[Mangu and Brill, 1997\] Mangu, L. and Brill, E., 1997,Automatic Rule Acquisition for Spelling Correc-tion, In Proceedings of The Fourteenth Interna-tional Conference on Machine Learning, ICML 97,.
'Morgan Kaufmann.\[Ramshaw mad Marcus, 1995\] Ramshaw, L. A. andMarcus, M., P., 1995, Text Chunking usingTransformation-Based L arning, In Proceedings ofthe A CL Third Workshop on Very Large Corpora,June 1995, pp.
82-94.\[Samuel t al., 1998\] Samuel, K., Carberry, S. andVij~'-Shanker, K., 1998.
Dialogue Act Taggingwith Transformation-Based L arning.
In Proceed-ings of COLING/ACL'98, pp.
1150-1156.\[Samuelsson et al, 1996\] Samuelsson, C., Tapanainen.P.
and Voutilainen.
A., 1996, Inducing ConstraintGrammars.
In: Laurent.
M. and de la Higuera.
C.42(eds.)
Grammatical bfference: Learning Syntax: fi'om Sentences, Springer Verlag.Appendix: The #-TBL User InterfaceAlthough it certainly helps to be familiar with the Pro-log programming language, the p-TBL system is actu-ally designed to be usable also by those who lack Prologexperience.The system has a simple command line interface, de-picted in Figure 3, from which commands ('an be givenand queries be made.
Furthermore.
there are severalflags which control the way in which the system carriesout its tasks.
***************************************************The NUTBL System, vers ion 0.7(c) Torbjoern Lager, 1999Dept.
of L ingu is t i cs ,  Uppsala Un ivers i ty ,  SwedenThe MUTBL System comes v i th  abso lu te ly  no warranty.FLAGS:t ra in ing_data='data/ testcorpus 'test_data='data/10kw_test 'a lgor i tbm='a lgor i thms/br i l l 'templates=~templates/brill_dist_templates '.score_thresho ld=3accuracy_thresho ld=0.5verbos i ty=2COMMANDS:loadtraintestset F=Vhelpf lagscommands- loads  data ,  compi les  .
templates ,  e tc .- s ta r ts  t ra in ing  process- tests result  of t ra in ing- sets f lag F to the va lue V- shows this menu- sho.s sett ings of flags- lists available commandspredicates - lists available pred icatesI ?-Figure 3: The #-TBL User Interface
