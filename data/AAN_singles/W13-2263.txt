Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 503?511,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsHidden Markov Tree Model for Word AlignmentShuhei Kondo Kevin Duh Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916-5, Takayama, Ikoma, Nara, 630-0192, Japan{shuhei-k,kevinduh,matsu}@is.naist.jpAbstractWe propose a novel unsupervised wordalignment model based on the HiddenMarkov Tree (HMT) model.
Our modelassumes that the alignment variables havea tree structure which is isomorphic to thetarget dependency tree and models the dis-tortion probability based on the source de-pendency tree, thereby incorporating thesyntactic structure from both sides of theparallel sentences.
In English-Japaneseword alignment experiments, our modeloutperformed an IBM Model 4 baselineby over 3 points alignment error rate.While our model was sensitive to poste-rior thresholds, it also showed a perfor-mance comparable to that of HMM align-ment models.1 IntroductionAutomatic word alignment is the first step in thepipeline of statistical machine translation.
Trans-lation models are usually extracted from word-aligned bilingual corpora, and lexical translationprobabilities based on word alignment models arealso used for translation.The most widely used models are the IBMModel 4 (Brown et al 1993) and Hidden MarkovModels (HMM) (Vogel et al 1996).
These mod-els assume that alignments are largely monotonic,possibly with a few jumps.
While such assump-tion might be adequate for alignment between sim-ilar languages, it does not necessarily hold be-tween a pair of distant languages like English andJapanese.Recently, several models have focused on in-corporating syntactic structures into word align-ment.
As an extension to the HMM alignment,Lopez and Resnik (2005) present a distortionmodel conditioned on the source-side dependencytree, and DeNero and Klein (2007) propose adistortion model based on the path through thesource-side phrase-structure tree.
Some super-vised models receive syntax trees as their inputand use them to generate features and to guide thesearch (Riesa and Marcu, 2010; Riesa et al 2011),and other models learn a joint model for pars-ing and word alignment from word-aligned par-allel trees (Burkett et al 2010).
In the context ofphrase-to-phrase alignment, Nakazawa and Kuro-hashi (2011) propose a Bayesian subtree align-ment model trained with parallel sampling.
Noneof these models, however, can incorporate syntac-tic structures from both sides of the language pairand can be trained computationally efficiently inan unsupervised manner at the same time.The Hidden Markov Tree (HMT) model(Crouse et al 1998) is one such model that sat-isfies the above-mentioned properties.
The HMTmodel assumes a tree structure of the hidden vari-ables, which fits well with the notion of word-to-word dependency, and it can be trained from un-labeled data via the EM algorithm with the sameorder of time complexity as HMMs.In this paper, we propose a novel word align-ment model based on the HMT model and showthat it naturally enables unsupervised trainingbased on both source and target dependency treesin a tractable manner.
We also compare our HMTword alignment model with the IBM Model 4 andthe HMM alignment models in terms of the stan-dard alignment error rates on a publicly availableEnglish-Japanese dataset.2 IBM Model 1 and HMM AlignmentWe briefly review the IBM Model 1 (Brown etal., 1993) and the Hidden Markov Model (HMM)word alignment (Vogel et al 1996) in this section.Both are probabilistic generative models that fac-503tor asp(f |e) =?ap(a, f |e)p(a, f |e) =J?j=1pd(aj |aj )pt(fj |eaj )where e = {e1, ..., eI} is an English (source) sen-tence and f = {f1, ..., fJ} is a foreign (target)sentence.
a = {a1, ..., aJ} is an alignment vec-tor such that aj = i indicates the j-th target wordaligns to the i-th source word and aj = 0 meansthe j-th target word is null-aligned.
j is the indexof the last non null-aligned target word before theindex j.In both models, pt(fj |eaj ) is the lexical transla-tion probability and can be defined as conditionalprobability distributions.
As for the distortionprobability pd(aj |aj ), pd(aj = 0|aj = i?)
= p0where p0 is NULL probability in both models.pd(aj = i|aj = i?)
is uniform in the Model 1and proportional to the relative count c(i ?
i?)
inthe HMM for i 6= 0.
DeNero and Klein (2007)proposed a syntax-sensitive distortion model forthe HMM alignment, in which the distortion prob-ability depends on the path from the i-th word tothe i?-th word on the source-side phrase-structuretree, instead of the linear distance between the twowords.These models can be trained efficiently usingthe EM algorithm.
In practice, models in two di-rections (source to target and target to source) aretrained and then symmetrized by taking their in-tersection, union or using other heuristics.
Lianget al(2006) proposed a joint objective of align-ment models in both directions and the probabilityof agreement between them, and an EM-like algo-rithm for training.They also proposed posterior thresholding fordecoding and symmetrization, which takea = {(i, j) : p(aj = i|f , e) > ?
}with a threshold ?
.
DeNero and Klein (2007) sum-marized some criteria for posterior thresholding,which are?
Soft-Union?pf (aj = i|f , e) ?
pr(ai = j|f , e)?
Soft-Intersectionpf (aj = i|f , e) + pr(ai = j|f , e)2?
Hard-Unionmax(pf (aj = i|f , e), pr(ai = j|f , e))?
Hard-Intersectionmin(pf (aj = i|f , e), pr(ai = j|f , e))where pf (aj = i|f , e) is the alignment probabil-ity under the source-to-target model and pr(ai =j|f , e) is the one under the target-to-source model.They also propose a posterior decoding heuris-tic called competitive thresholding.
Given a j ?
imatrix of combined weights c and a threshold ?
, itchoose a link (j, i) only if its weight cji ?
?
and itis connected to the link with the maximum weightboth in row j and column i.3 Hidden Markov Tree ModelThe Hidden Markov Tree (HMT) model was firstintroduced by Crouse et al(1998).
Though it hasbeen applied successfully to various applicationssuch as image segmentation (Choi and Baraniuk,2001), denoising (Portilla et al 2003) and biol-ogy (Durand et al 2005), it is largely unnoticedin the field of natural language processing.
Tothe best of our knowledge, the only exception isZ?abokrtsky` and Popel (2009) who used a variantof the Viterbi algorithm for HMTs in the transferphase of a deep-syntax based machine translationsystem.An HMT model consists of an observed randomtree X = {x1, ..., xN} and a hidden random treeS = {s1, ..., sN}, which is isomorphic to the ob-served tree.The parameters of the model are?
P (s1 = j), the initial hidden state prior?
P (st = j|s?
(t) = i), transition probabilities?
P (xt = h|st = j), emission probabilities,where ?
() is a function that maps the index of ahidden node to the index of its parent node.
Theseparameters can be trained via the EM algorithm.The ?upward-downward?
algorithm proposedin Crouse et al(1998), an HMT analogue of theforward-backward algorithm for HMMs, can beused in the E-step.
However, it is based on the de-composition of joint probabilities and suffers fromnumerical underflow problems.Durand et al(2004) proposed a smoothed vari-ant of the upward-downward algorithm, which is504based on the decomposition of smoothed probabil-ities and immune to underflow.
In the next section,we will explain this variant in the context of wordalignment.4 Hidden Markov Tree Word AlignmentWe present a novel word alignment model basedon the HMT model.
Given a target sentence f ={f1, ..., fJ}with a dependency treeF and a sourcesentence e = {e1, ..., eI} with a dependency treeE, an HMT word alignment model factors asp(f |e) =?ap(a, f |e)p(a, f |e) =J?j=1pd(aj |aj )pt(fj |eaj ).While these equations appear identical to the onesfor the HMM alignment, they are different in that1) e, f and a are not chain-structured but tree-structured, and 2) j is the index of the non null-aligned lowest ancestor of the j-th target word1,rather than that of the last non null-aligned wordpreceding the j-th word as in the HMM alignment.Note that A, the tree composed of alignment vari-ables a = {a1, ..., aJ}, is isomorphic to the targetdependency tree F.Figure 1 shows an example of a target depen-dency tree with an alignment tree, and a sourcedependency tree.
Note that English is the target(or foreign) language and Japanese is the source(or English) language here.
We introduce the fol-lowing notations following Durand et al(2004),slightly modified to better match the context ofword alignment.?
?
(j) denotes the index of the head of the j-thtarget word.?
c(j) denotes the set of indices of the depen-dents of the j-th target word.?
Fj = f j denotes the target dependency sub-tree rooted at the j-th word.As for the parameters of the model, the initialhidden state prior described in Section 3 can bedefined by assuming an artificial ROOT node forboth dependency trees, forcing the target ROOTnode to be aligned only to the source ROOT1This dependence on aj can be implemented as a first-order HMT, analogously to the case of the HMM alignment(Och and Ney, 2003).node and prohibiting other target nodes from be-ing aligned to the source ROOT node.
The lexi-cal translation probability pt(fj |eaj ), which corre-sponds to the emission probability, can be definedas conditional probability distributions just like inthe IBM Model 1 and the HMM alignment.The distortion probability pd(aj = i|aj = i?
),which corresponds to the transition probability,depends on the distance between the i-th sourceword and the i?-th source word on the source de-pendency tree E, which we denote d(i, i?)
here-after.
We model the dependence of pd(aj =i|aj = i?)
on d(i, i?)
with the counts c(d(i, i?
)).In our model, d(i, i?)
is represented by a pairof non-negative distances (up, down), where upis the distance between the i-th word and thelowest common ancestor (lca) of the two words,down is the one between the i?-th word and thelca.
For example in Figure 1b, d(0, 2) = (0, 4),d(2, 5) = (2, 2) and d(4, 7) = (3, 0).
In practice,we clip the distance by a fixed window size w andstore c(d(i, i?))
in a two-dimensional (w + 1 ) ?
(w + 1 ) matrix.
When w = 3, for example, thedistance d(0, 2) = (0, 3) after clipping.We can use the smoothed variant of upward-downward algorithm (Durand et al 2004) for theE-step of the EM algorithm.
We briefly explainthe smoothed upward-downward algorithm in thecontext of tree-to-tree word alignment below.
Forthe detailed derivation, see Durand et al(2004).In the smoothed upward-downward algorithm,we first compute the state marginal probabilitiesp(aj = i)=?i?p(a?
(j) = i?
)pd(aj = i|a?
(j) = i?
)for each target node and each state, wherepd(aj = i|a?
(j) = i?)
= p0if the j-th word is null-aligned, andpd(aj = i|a?
(j) = i?
)= (1?
p0) ?c(d(i?, i))?i??
6=0 c(d(i?, i??
))if the j-th word is aligned.
Note that we must ar-tificially normalize pd(aj = i|a?
(j) = i?
), becauseunlike in the case of the linear distance, multiplewords can have the same distance from the j-thword on a dependency tree.505a0 a1 a2 a3 a4 a5?
?
?
?
?
?
?ROOT?f0 Thatf1 wasf2 Mountf3 Kuramaf4 .f5(a) Target sentence with its dependency/alignment tree.
Target words {f0, ..., f5} are emitted fromalignment variables {a0, ..., a5}.
Ideally, a0 = 0, a1 = 1, a2 = 7, a3 = 5, a4 = 4 and a5 = 9.?ROOT?e0 ?
?e1 ?e2 ?e3 ?
?e4 ?e5 ?e6 ?
?e7 ?e8 ?e9that mountain Kurama mountain be .
(b) Source sentence with its dependency tree.
None of the target words are aligned to e2, e3, e6 and e8.Figure 1: An example of sentence pair under the Hidden Markov Tree word alignment model.
If weignore the source words to which no target words are aligned, the dependency structures look similar toeach other.In the next phase, the upward recursion, wecompute p(aj = i|Fj = f j) in a bottom-up man-ner.
First, we initialize the upward recursion foreach leaf by?j(i) = p(aj = i|Fj = fj)= pt(fj |ei)p(aj = i)Nj,whereNj = p(Fj = fj) =?ipt(fj |ei)p(aj = i).Then, we proceed from the leaf to the root with thefollowing recursion,?j(i) = p(aj = i|Fj = f j)={?j?
?c(j) ?j,j?
(i)}pt(fj |ei)p(aj = i)Nj,whereNj =p(Fj = f j)?j?
?c(j) p(Fj?
= f j?)=?i{?j??c(j)?j,j?
(i)}pt(fj |ei)p(aj = i)and??
(j),j(i) =p(Fj = f j |a?
(j) = i)p(Fj = f j)=?i??j(i?
)pd(aj = i?|a?
(j) = i)p(aj = i?
).After the upward recursion is completed, wecompute p(aj = i|F0 = f0) in the downwardrecursion.
It is initialized at the root node by?0(i) = p(a0 = i|F0 = f0).Then we proceed in a top-down manner, comput-ing?j(i) = p(aj = i|F0 = f0)= ?j(i)p(aj = i)?
?i?pd(aj = i|a?
(j) = i?)??(j)(i?)??(j),j(i?
).for each node and each state.The conditional probabilitiesp(aj = i, a?
(j) = i?|F0 = f0)=?j(i)pd(aj = i|a?
(j) = i?)??(j)(i?
)p(aj = i)??(j),j(i?
),506which is used for the estimation of distortion prob-abilities, can be extracted during the downward re-cursion.In the M-step, the lexical translation model canbe updated withpt(f |e) =c(f, e)c(e) ,just like the IBM Models and HMM alignments,where c(f, e) and c(e) are the count of the wordpair (f, e) and the source word e. However, theupdate for the distortion model is a bit compli-cated, because the matrix that stores c(d(i, i?
))does not represent a probability distribution.
Toapproximate the maximum likelihood estimation,we divide the counts c(d(i, i?))
calculated duringthe E-step by the number of distortions that havethe distance d(i, i?)
in the training data.
Then wenormalize the matrix byc(d(i, i?))
= c(d(i, i?
))?wi=0?wi?=0 c(d(i, i?
)).Given initial parameters for the lexical trans-lation model and the distortion counts, an HMTaligner collects the expected counts c(f, e), c(e)and c(d(i, i?))
with the upward-downward algo-rithm in the E-step and re-estimate the parametersin the M-Step.
Dependency trees for the sentencepairs in the training data remain unchanged duringthe training procedure.5 ExperimentWe evaluate the performance of our HMT align-ment model in terms of the standard alignment er-ror rate2 (AER) on a publicly available English-Japanese dataset, and compare it with the IBMModel 4 (Brown et al 1993) and HMM alignmentwith distance-based (HMM) and syntax-based (S-HMM) distortion models (Vogel et al 1996;Liang et al 2006; DeNero and Klein, 2007).We use the data from the Kyoto Free Transla-tion Task (KFTT) version 1.3 (Neubig, 2011).
Ta-ble 1 shows the corpus statistics.
Note that thesenumbers are slightly different from the ones ob-served under the dataset?s default training proce-dure because of the difference in the preprocessingscheme, which is explained below.2Given sure alignments S and possible alignments P , thealignment error rate of alignments A is 1 ?
|A?S|+|A?P ||A|+|S|(Och and Ney, 2003).The tuning set of the KFTT has manual align-ments.
As the KFTT doesn?t distinguish betweensure and possible alignments, F-measure equals1?AER on this dataset.5.1 PreprocessingWe tokenize the English side of the data using theStanford Tokenizer3 and parse it with the BerkeleyParser4 (Petrov et al 2006).
We use the phrase-structure trees for the Berkeley Aligner?s syntacticdistortion model, and convert them to dependencytrees for our dependency-based distortion model5.As the Berkeley Parser couldn?t parse 7 (out ofabout 330K) sentences in the training data, we re-moved those lines from both sides of the data.
Allthe sentences in the other sets were parsed suc-cessfully.For the Japanese side of the data, we first con-catenate the function words in the tokenized sen-tences using a script6 published by the authorof the dataset.
Then we re-segment and POS-tag them using MeCab7 version 0.996 and parsethem using CaboCha8 version 0.66 (Kudo andMatsumoto, 2002), both with UniDic.
Finally,we modify the CoNLL-format output of CaboChawhere some kind of symbols such as punctuationmarks and parentheses have dependent words.
Wechose this procedure for a reasonable compromisebetween the dataset?s default tokenization and thedependency parser we use.As we cannot use the default gold alignment dueto the difference in preprocessing, we use a script9published by the author of the dataset to modifythe gold alignment so that it better matches thenew tokenization.5.2 TrainingWe initialize our models in two directions withjointly trained IBM Model 1 parameters (5 itera-tions) and train them independently for 5 iterations3http://nlp.stanford.edu/software/4We use the model trained on the WSJ portion ofOntonotes (Hovy et al 2006) with the default setting.5We use Stanford?s tool (de Marneffe et al 2006)with options -conllx -basic -makeCopulaHead-keepPunct for conversion.6https://github.com/neubig/util-scripts/blob/master/combine-predicate.pl7http://code.google.com/p/mecab/8http://code.google.com/p/cabocha/9https://github.com/neubig/util-scripts/blob/master/adjust-alignments.pl507Sentences English Tokens Japanese TokensTrain 329,974 5,912,543 5,893,334Dev 1,166 24,354 26,068Tune 1,235 30,839 33,180Test 1,160 26,730 27,693Table 1: Corpus statistics of the KFTT.Precision Recall AERHMT (Proposed) 71.77 55.23 37.58IBM Model 4 60.58 57.71 40.89HMM 69.59 56.15 37.85S-HMM 71.60 56.14 37.07Table 2: Alignment error rates (AER) based oneach model?s peak performance.with window size w = 4 for the distortion model.The entire training procedure takes around 4 hourson a 3.3 GHz Xeon CPU.We train the IBM Model 4 using GIZA++ (Ochand Ney, 2003) with the training script of theMoses toolkit (Koehn et al 2007).The HMM and S-HMM alignment models areinitialized with jointly trained IBM Model 1 pa-rameters (5 iterations) and trained independentlyfor 5 iterations using the Berkeley Aligner.
Wefind that though initialization with jointly trainedIBM Model 1 parameters is effective, joint train-ing of HMM alignment models harms the perfor-mance on this dataset (results not shown).5.3 ResultWe use posterior thresholding for the HMT andHMM alignment models, and the grow-diag-final-and heuristic for the IBM Model 4.Table 2 and Figure 2 show the result.
Asthe Soft-Union criterion performed best, we don?tshow the results based on other criteria.
On theother hand, as the peak performance of the HMTmodel is better with competitive thresholding andthose of HMM models are better without it, wecompare Precision/Recall curves and AER curvesboth between the same strategy and the best per-forming strategy for each model.As shown in Table 2, the peak performance ofthe HMT alignment model is better than that ofthe IBM Model 4 by over 3 point AER, and it wassomewhere between the HMM and the S-HMM.Taking into account that our distortion model issimpler than that of S-HMM, these results seemnatural, and it would be reasonable to expect thatreplacing our distortion model with more sophisti-cated one might improve the performance.When we look at Precision/Recall curves andAER curves in Figures 2a and 2d, the HMT modelis performing slightly better in the range of 50 to60 % precision and 0.15 to 0.35 posterior thresh-old with the Soft-Union strategy.
Results in Fig-ures 2b and 2e show that the HMT model performsbetter around the range around 60 to 70 precisionand it corresponds to 0.2 to 0.4 posterior thresh-old with the competitive thresholding heuristic.
Inaddition, results on both strategies show that per-formance curve of the HMT model is more peakedthan those of HMM alignment models.We suspect that a part of the reason behind suchbehavior can be attributed to the fact that the HMTmodel?s distortion model is more uniform than thatof HMM models.
For example, in our model, allsibling nodes have the same distortion probabilityfrom their parent node.
This is in contrast with thesituation in HMM models, where nodes within afixed distance have different distortion probabili-ties.
With more uniform distortion probabilities,many links for a target word may have a consider-able amount of posterior probability.
If that is true,too many links will be above the threshold when itis set low, and too few links can exceed the thresh-old when it is set high.
More sophisticated distor-tion model may help mitigate such sensitivity tothe posterior threshold.6 Related WorksLopez and Resnik (2005) consider an HMMmodel with distortions based on the distance independency trees, which is quite similar to ourmodel?s distance.
DeNero and Klein (2007) pro-pose another HMM model with syntax-based dis-tortions based on the path through constituencytrees, which improves translation rule extractionfor tree-to-string transducers.
Both models as-508(a) Precision/Recall Curve with Soft-Union.
(b) Precision/Recall Curve with Soft-Union + Competi-tive Thresholding.
(c) Precision/Recall Curve with the Best Strategy.
(d) Alignment Error Rate with Soft-Union.
(e) Precision/Recall Curve with Soft-Union + Competi-tive Thresholding.
(f) Alignment Error Rate with with the Best Strategy.Figure 2: Precision/Recall Curve and Alignment Error Rate with Different Models and Strategies.509sume a chain structure for hidden variables (align-ment) as opposed to a tree structure as in ourmodel, and condition distortions on the syntacticstructure only in one direction.Nakazawa and Kurohashi (2011) proposea dependency-based phrase-to-phrase alignmentmodel with a sophisticated generative story, whichleads to an increase in computational complexityand requires parallel sampling for training.Several supervised, discriminative models usesyntax structures to generate features and to guidethe search (Burkett et al 2010; Riesa and Marcu,2010; Riesa et al 2011).
Such efforts are orthog-onal to ours in the sense that discriminative align-ment models generally use statistics obtained byunsupervised, generative models as features andcan benefit from their improvement.
It would beinteresting to incorporate statistics of the HMTword alignment model into such discriminativemodels.Z?abokrtsky` and Popel (2009) use HMT mod-els for the transfer phase in a tree-based MT sys-tem.
While our model assumes that the tree struc-ture of alignment variables is isomorphic to tar-get side?s dependency tree, they assume that thedeep-syntactic tree of the target side is isomorphicto that of the source side.
The parameters of theHMT model is given and not learned by the modelitself.7 ConclusionWe have proposed a novel word alignment modelbased on the Hidden Markov Tree (HMT) model,which can incorporate the syntactic structures ofboth sides of the language into unsupervised wordalignment in a tractable manner.
Experiments onan English-Japanese dataset show that our modelperforms better than the IBM Model 4 and com-parably to the HMM alignment models in termsof alignment error rates.
It is also shown that theHMT model with a simple tree-based distortionis sensitive to posterior thresholds, perhaps due tothe flat distortion probabilities.As the next step, we plan to improve the dis-tortion component of our HMT alignment model.Something similar to the syntax-sensitive distor-tion model of DeNero and Klein (2007) might bea good candidate.It is also important to see the effect of ourmodel on the downstream translation.
Apply-ing our model to recently proposed models thatdirectly incorporate dependency structures, suchas string-to-dependency (Shen et al 2008) anddependency-to-string (Xie et al 2011) models,would be especially interesting.Last but not least, though the dependency struc-tures don?t pose a hard restriction on the align-ment in our model, it is highly likely that parseerrors have negative effects on the alignment ac-curacy.
One way to estimate the effect of parseerrors on the accuracy is to parse the input sen-tences with inferior models, for example trainedon a limited amount of training data.
Moreover,preserving some ambiguities using k-best trees orshared forests might help mitigate the effect of 1-best parse errors.AcknowledgmentsWe thank anonymous reviewers for insightful sug-gestions and comments.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Computational linguistics,19(2):263?311.David Burkett, John Blitzer, and Dan Klein.
2010.Joint Parsing and Alignment with Weakly Synchro-nized Grammars.
In Proceedings of NAACL HLT2010, pages 127?135.Hyeokho Choi and Richard G. Baraniuk.
2001.
Mul-tiscale Image Segmentation Using Wavelet-DomainHidden Markov Models.
IEEE Transactions on Im-age Processing, 10(9):1309?1321.Matthew S. Crouse, Robert D. Nowak, and Richard G.Baraniuk.
1998.
Wavelet-Based Statistical SignalProcessing Using Hidden Markov Models.
IEEETransactions on Signal Processing, 46(4):886?902.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of LREC?06, pages 449?454.John DeNero and Dan Klein.
2007.
Tailoring WordAlignments to Syntactic Machine Translation.
InProceedings of ACL 2007, pages 17?24.Jean-Baptiste Durand, Paulo Gonc?alve`s, and YannGue?don.
2004.
Computational Methods for Hid-den Markov Tree Models-An Application to WaveletTrees.
IEEE Transactions on Signal Processing,52(9):2551?2560.510J.-B.
Durand, Y. Gue?don, Y. Caraglio, and E. Costes.2005.
Analysis of the plant architecture via tree-structured statistical models: the hidden Markov treemodels.
New Phytologist, 166(3):813?825.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: The 90% Solution.
In Proceedings ofHLT-NAACL 2006, Short Papers, pages 57?60.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of ACL 2007, Demonstration Ses-sion, pages 177?180.Taku Kudo and Yuji Matsumoto.
2002.
Japanese De-pendency Analysis using Cascaded Chunking.
InProceedings of CoNLL-2002, pages 63?69.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by Agreement.
In Proceedings of HLT-NAACL2006, pages 104?111.Adam Lopez and Philip Resnik.
2005.
Im-proved HMM Alignment Models for Languageswith Scarce Resources.
In Proceedings of the ACLWorkshop on Building and Using Parallel Texts,pages 83?86.Toshiaki Nakazawa and Sadao Kurohashi.
2011.Bayesian Subtree Alignment Model based on De-pendency Trees.
In Proceedings of IJCNLP 2011,pages 794?802.Graham Neubig.
2011.
The Kyoto Free TranslationTask.
http://www.phontron.com/kftt.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational linguistics, 29(1):19?51.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, andInterpretable Tree Annotation.
In Proceedings ofCOLING/ACL 2006, pages 433?440.Javier Portilla, Vasily Strela, Martin J. Wainwright,and Eero P. Simoncelli.
2003.
Image DenoisingUsing Scale Mixtures of Gaussians in the WaveletDomain.
IEEE Transactions on Image Processing,12(11):1338?1351.Jason Riesa and Daniel Marcu.
2010.
HierarchicalSearch for Word Alignment.
In Proceedings of ACL2010, pages 157?166.Jason Riesa, Ann Irvine, and Daniel Marcu.
2011.Feature-Rich Language-Independent Syntax-BasedAlignment for Statistical Machine Translation.
InProceedings of EMNLP 2011, pages 497?507.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT, pages 577?585.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-Based Word Alignment in Statisti-cal Translation.
In Proceedings of COLING 1996,pages 836?841.Jun Xie, Haitao Mi, and Qun Liu.
2011.
ANovel Dependency-to-String Model for StatisticalMachine Translation.
In Proceedings of EMNLP2011, pages 216?226.Zdene?k Z?abokrtsky` and Martin Popel.
2009.
HiddenMarkov Tree Model in Dependency-based MachineTranslation.
In Proceedings of ACL-IJCNLP 2009,Short Papers, pages 145?148.511
