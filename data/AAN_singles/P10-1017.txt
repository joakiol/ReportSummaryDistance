Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157?166,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsHierarchical Search for Word AlignmentJason Riesa and Daniel MarcuInformation Sciences InstituteViterbi School of EngineeringUniversity of Southern California{riesa, marcu}@isi.eduAbstractWe present a simple yet powerful hier-archical search algorithm for automaticword alignment.
Our algorithm inducesa forest of alignments from which wecan efficiently extract a ranked k-best list.We score a given alignment within theforest with a flexible, linear discrimina-tive model incorporating hundreds of fea-tures, and trained on a relatively smallamount of annotated data.
We report re-sults on Arabic-English word alignmentand translation tasks.
Our model out-performs a GIZA++ Model-4 baseline by6.3 points in F-measure, yielding a 1.1BLEU score increase over a state-of-the-artsyntax-based machine translation system.1 IntroductionAutomatic word alignment is generally acceptedas a first step in training any statistical machinetranslation system.
It is a vital prerequisite forgenerating translation tables, phrase tables, or syn-tactic transformation rules.
Generative alignmentmodels like IBM Model-4 (Brown et al, 1993)have been in wide use for over 15 years, and whilenot perfect (see Figure 1), they are completely un-supervised, requiring no annotated training data tolearn alignments that have powered many currentstate-of-the-art translation system.Today, there exist human-annotated alignmentsand an abundance of other information for manylanguage pairs potentially useful for inducing ac-curate alignments.
How can we take advantageof all of this data at our fingertips?
Using fea-ture functions that encode extra information is onegood way.
Unfortunately, as Moore (2005) pointsout, it is usually difficult to extend a given genera-tive model with feature functions without chang-ing the entire generative story.
This difficultyVisualization generated by riesa: Feb 12, 2010 20:06:24683.g (a1)683.union.a (a2)683.e (e)683.f (f)Sentence 1thefiveprevioustestshavebeenlimitedtothetargetmissileandoneotherbody.!"#$%!&!'
()"*+,-*!&.
(/01234(!5!67*,8.(9:;<)+,=.(1>?
@!A8BC(DEFG*)#1G(?H()*1Figure 1: Model-4 alignment vs. a gold stan-dard.
Circles represent links in a human-annotatedalignment, and black boxes represent links in theModel-4 alignment.
Bold gray boxes show linksgained after fully connecting the alignment.has motivated much recent work in discriminativemodeling for word alignment (Moore, 2005; Itty-cheriah and Roukos, 2005; Liu et al, 2005; Taskaret al, 2005; Blunsom and Cohn, 2006; Lacoste-Julien et al, 2006; Moore et al, 2006).We present in this paper a discriminative align-ment model trained on relatively little data, witha simple, yet powerful hierarchical search proce-dure.
We borrow ideas from both k-best pars-ing (Klein and Manning, 2001; Huang and Chi-ang, 2005; Huang, 2008) and forest-based, andhierarchical phrase-based translation (Huang andChiang, 2007; Chiang, 2007), and apply them toword alignment.Using a foreign string and an English parsetree as input, we formulate a bottom-up searchon the parse tree, with the structure of the treeas a backbone for building a hypergraph of pos-sible alignments.
Our algorithm yields a forest of157the manate theNPVPSNPthe????????the????????the?????????man????????themanatethebread??????????
??????????breadbread????????????
?Figure 2: Example of approximate search through a hypergraph with beam size = 5.
Each black squareimplies a partial alignment.
Each partial alignment at each node is ranked according to its model score.In this figure, we see that the partial alignment implied by the 1-best hypothesis at the leftmost NPnode is constructed by composing the best hypothesis at the terminal node labeled ?the?
and the 2nd-best hypothesis at the terminal node labeled ?man?.
(We ignore terminal nodes in this toy example.
)Hypotheses at the root node imply full alignment structures.word alignments, from which we can efficientlyextract the k-best.
We handle an arbitrary numberof features, compute them efficiently, and scorealignments using a linear model.
We train theparameters of the model using averaged percep-tron (Collins, 2002) modified for structured out-puts, but can easily fit into a max-margin or relatedframework.
Finally, we use relatively little train-ing data to achieve accurate word alignments.
Ourmodel can generate arbitrary alignments and learnfrom arbitrary gold alignments.2 Word Alignment as a HypergraphAlgorithm input The input to our alignment al-gorithm is a sentence-pair (en1, fm1 ) and a parse treeover one of the input sentences.
In this work,we parse our English data, and for each sentenceE = en1, let T be its syntactic parse.
To gener-ate parse trees, we use the Berkeley parser (Petrovet al, 2006), and use Collins head rules (Collins,2003) to head-out binarize each tree.Overview We present a brief overview here anddelve deeper in Section 2.1.
Word alignments arebuilt bottom-up on the parse tree.
Each node v inthe tree holds partial alignments sorted by score.158u11u12u13u212.2 4.1 5.5u222.4 3.5 7.2u233.2 4.5 11.4u11u12u13u212.2 4.1 5.5u222.4 3.5 7.2u233.2 4.5 11.4u11u12u13u212.2 4.1 5.5u222.4 3.5 7.2u233.2 4.5 11.4(a) Score the left corner align-ment first.
Assume it is the 1-best.
Numbers in the rest of theboxes are hidden at this point.u1u12u13u212.2 4.1 5.5u22.4 3.5 7.2u233.2 4.5 1 .4u1u12u13u212.2 4.1 5.5u22.4 3.5 7.2u233.2 4.5 1 .4u1u12u13u212.2 4.1 5.5u22.4 3.5 7.2u233.2 4.5 1 .4(b) Expand the frontier of align-ments.
We are now looking forthe 2nd best.u1u12u13u212.
4.1 5.u22.4 3.5 7.2u233.2 4.5 1 .4u1u12u13u212.
4.1 5.u22.4 3.5 7.2u233.2 4.5 1 .4u1u12u13u212.
4.1 5.u22.4 3.5 7.2u233.2 4.5 1 .4(c) Expand the frontier further.After this step we have our topk alignments.Figure 3: Cube pruning with alignment hypotheses to select the top-k alignments at node v with children?u1, u2?.
In this example, k = 3.
Each box represents the combination of two partial alignments to createa larger one.
The score in each box is the sum of the scores of the child alignments plus a combinationcost.Each partial alignment comprises the columns ofthe alignment matrix for the e-words spanned byv, and each is scored by a linear combination offeature functions.
See Figure 2 for a small exam-ple.Initial partial alignments are enumerated andscored at preterminal nodes, each spanning a sin-gle column of the word alignment matrix.
Tospeed up search, we can prune at each node, keep-ing a beam of size k. In the diagram depicted inFigure 2, the beam is size k = 5.From here, we traverse the tree nodes bottom-up, combining partial alignments from child nodesuntil we have constructed a single full alignment atthe root node of the tree.
If we are interested in thek-best, we continue to populate the root node untilwe have k alignments.1We use one set of feature functions for preter-minal nodes, and another set for nonterminalnodes.
This is analogous to local and nonlo-cal feature functions for parse-reranking used byHuang (2008).
Using nonlocal features at a non-terminal node emits a combination cost for com-posing a set of child partial alignments.Because combination costs come into play, weuse cube pruning (Chiang, 2007) to approxi-mate the k-best combinations at some nonterminalnode v. Inference is exact when only local featuresare used.Assumptions There are certain assumptions re-lated to our search algorithm that we must make:1We use approximate dynamic programming to storealignments, keeping only scored lists of pointers to initialsingle-column spans.
Each item in the list is a derivation thatimplies a partial alignment.
(1) that using the structure of 1-best English syn-tactic parse trees is a reasonable way to frame anddrive our search, and (2) that F-measure approxi-mately decomposes over hyperedges.We perform an oracle experiment to validatethese assumptions.
We find the oracle for a given(T ,e, f ) triple by proceeding through our search al-gorithm, forcing ourselves to always select correctlinks with respect to the gold alignment when pos-sible, breaking ties arbitrarily.
The the F1 score ofour oracle alignment is 98.8%, given this ?perfect?model.2.1 Hierarchical searchInitial alignments We can construct a wordalignment hierarchically, bottom-up, by makinguse of the structure inherent in syntactic parsetrees.
We can think of building a word alignmentas filling in an M?N matrix (Figure 1), and we be-gin by visiting each preterminal node in the tree.Each of these nodes spans a single e word.
(Line2 in Algorithm 1).From here we can assign links from each e wordto zero or more f words (Lines 6?14).
At thislevel of the tree the span size is 1, and the par-tial alignment we have made spans a single col-umn of the matrix.
We can make many such partialalignments depending on the links selected.
Lines5 through 9 of Algorithm 1 enumerate either thenull alignment, single-link alignments, or two-linkalignments.
Each partial alignment is scored andstored in a sorted heap (Lines 9 and 13).In practice enumerating all two-link alignmentscan be prohibitive for long sentence pairs; we seta practical limit and score only pairwise combina-159Algorithm 1: Hypergraph AlignmentInput:Source sentence en1Target sentence fm1Parse tree T over en1Set of feature functions hWeight vector wBeam size kOutput:A k-best list of alignments over en1 and fm11 function A????
(en1, fm1 ,T )2 for v ?
T in bottom-up order do3 ?v ?
?4 if ??-P??????????N???
(v) then5 i?
index-of(v)6 for j = 0 to m do7 links?
(i, j)8 score?
w ?
h(links, v, en1, fm1 )9 P???
(?v, ?score, links?, k )10 for k = j + 1 to m do11 links?
(i, j), (i, k)12 score?
w ?
h(links, v, en1, fm1 )13 P???
(?v, ?score, links?, k )14 end15 end16 else17 ?v ?
G???S???
(children(v), k)18 end19 end20 end21 function G???S???
(?u1, u2?, k)22 return C???P??????(?
?u1 , ?u2?, k,w,h)23 endtions of the top n = max{| f |2 , 10}scoring single-link alignments.We limit the number of total partial alignments?v kept at each node to k. If at any time we wish topush onto the heap a new partial alignment whenthe heap is full, we pop the current worst off theheap and replace it with our new partial alignmentif its score is better than the current worst.Building the hypergraph We now visit internalnodes (Line 16) in the tree in bottom-up order.
Ateach nonterminal node v we wish to combine thepartial alignments of its children u1, .
.
.
, uc.
Weuse cube pruning (Chiang, 2007; Huang and Chi-ang, 2007) to select the k-best combinations of thepartial alignments of u1, .
.
.
, uc (Line 19).
NoteSentence 1TOP1S2NP-C1NPB2DTNPB-BAR2CDNPB-BAR2JJ NNSS-BAR1VP1VBPVP-C1VBNVP-C1VBNPP1INNP-C1NP-C-BAR1NP1NPB2DTNPB-BAR2NN NN CCNP1NPB2CDNPB-BAR2JJ NN.thefiveprevioustestshavebeenlimitedtothetargetmissileandoneotherbody.!"#$%!&!'
()"*+,-*!&.
(/01234(!5!67*,8.(9:;<)+,=.(1>?
@!A8BC(DEFG*)#1G(?H()*Figure 4: Correct version of Figure 1 after hyper-graph alignment.
Subscripts on the nonterminallabels denote the branch containing the head wordfor that span.that Algorithm 1 assumes a binary tree2, but is notnecessary.
In the general case, cube pruning willoperate on a d-dimensional hypercube, where d isthe branching factor of node v.We cannot enumerate and score every possibil-ity; without the cube pruning approximation, wewill have kc possible combinations at each node,exploding the search space exponentially.
Figure 3depicts how we select the top-k alignments at anode v from its children ?
u1, u2 ?.3 Discriminative trainingWe incorporate all our new features into a linearmodel and learn weights for each using the on-line averaged perceptron algorithm (Collins, 2002)with a few modifications for structured outputs in-spired by Chiang et al (2008).
We define:2We find empirically that using binarized trees reducessearch errors in cube pruning.160inin??.
.
.......Figure 5: A common problem with GIZA++Model 4 alignments is a weak distortion model.The second English ?in?
is aligned to the wrongArabic token.
Circles show the gold alignment.?
(y) = `(yi, y) + w ?
(h(yi) ?
h(y)) (1)where `(yi,y) is a loss function describing how badit is to guess y when the correct answer is yi.
In ourcase, we define `(yi,y) as 1?F1(yi,y).
We select theoracle alignment according to:y+ = arg miny?????(x)?
(y) (2)where ????
(x) is a set of hypothesis alignmentsgenerated from input x.
Instead of the traditionaloracle, which is calculated solely with respect tothe loss `(yi,y), we choose the oracle that jointlyminimizes the loss and the difference in modelscore to the true alignment.
Note that Equation 2is equivalent to maximizing the sum of the F-measure and model score of y:y+ = arg maxy?????
(x)(F1(yi, y) + w ?
h(y)) (3)Let y?
be the 1-best alignment according to ourmodel:y?
= arg maxy?????
(x)w ?
h(y) (4)Then, at each iteration our weight update is:w?
w + ?
(h(y+) ?
h(y?))
(5)where ?
is a learning rate parameter.3 We findthat this more conservative update gives rise to amuch more stable search.
After each iteration, weexpect y+ to get closer and closer to the true yi.4 FeaturesOur simple, flexible linear model makes it easy tothrow in many features, mapping a given complex3We set ?
to 0.05 in our experiments.alignment structure into a single high-dimensionalfeature vector.
Our hierarchical search frameworkallows us to compute these features when needed,and affords us extra useful syntactic information.We use two classes of features: local and non-local.
Huang (2008) defines a feature h to be lo-cal if and only if it can be factored among the lo-cal productions in a tree, and non-local otherwise.Analogously for alignments, our class of local fea-tures are those that can be factored among the localpartial alignments competing to comprise a largerspan of the matrix, and non-local otherwise.
Thesefeatures score a set of links and the words con-nected by them.Feature development Our features are inspiredby analysis of patterns contained among our goldalignment data and automatically generated parsetrees.
We use both local lexical and nonlocal struc-tural features as described below.4.1 Local featuresThese features fire on single-column spans.?
From the output of GIZA++ Model 4, wecompute lexical probabilities p(e | f ) andp( f | e), as well as a fertility table ?
(e).From the fertility table, we fire features ?0(e),?1(e), and ?2+(e) when a word e is alignedto zero, one, or two or more words, respec-tively.
Lexical probability features p(e | f )and p( f | e) fire when a word e is aligned toa word f .?
Based on these features, we include a binarylexical-zero feature that fires if both p(e | f )and p( f | e) are equal to zero for a given wordpair (e, f ).
Negative weights essentially pe-nalize alignments with links never seen be-fore in the Model 4 alignment, and positiveweights encourage such links.
We employ aseparate instance of this feature for each En-glish part-of-speech tag: p( f | e, t).We learn a different feature weight for each.Critically, this feature tells us how much totrust alignments involving nouns, verbs, ad-jectives, function words, punctuation, etc.from the Model 4 alignments from which ourp(e | f ) and p( f | e) tables are built.
Ta-ble 1 shows a sample of learned weights.
In-tuitively, alignments involving English parts-of-speech more likely to be content words(e.g.
NNPS, NNS, NN) are more trustworthy161PPINNPeprepehead...fNPDTNPedetehead...fVPVBDVPeverbehead...fFigure 6: Features PP-NP-head, NP-DT-head, and VP-VP-head fire on these tree-alignment patterns.
Forexample, PP-NP-head fires exactly when the head of the PP is aligned to exactly the same f words as thehead of it?s sister NP.PenaltyNNPS ?1.11NNS ?1.03NN ?0.80NNP ?0.62VB ?0.54VBG ?0.52JJ ?0.50JJS ?0.46VBN ?0.45... ...POS ?0.0093EX ?0.0056RP ?0.0037WP$ ?0.0011TO 0.037RewardTable 1: A sampling of learned weights for the lex-ical zero feature.
Negative weights penalize linksnever seen before in a baseline alignment used toinitialize lexical p(e | f ) and p( f | e) tables.
Posi-tive weights outright reward such links.than those likely to be function words (e.g.TO, RP, EX), where the use of such words isoften radically different across languages.?
We also include a measure of distortion.This feature returns the distance to the diag-onal of the matrix for any link in a partialalignment.
If there is more than one link, wereturn the distance of the link farthest fromthe diagonal.?
As a lexical backoff, we include a tag prob-ability feature, p(t | f ) that fires for somelink (e, f ) if the part-of-speech tag of e is t.The conditional probabilities in this table arecomputed from our parse trees and the base-line Model 4 alignments.?
In cases where the lexical probabilities aretoo strong for the distortion feature toovercome (see Figure 5), we develop themultiple-distortion feature.
Although localfeatures do not know the partial alignments atother spans, they do have access to the entireEnglish sentence at every step because our in-put is constant.
If some e exists more thanonce in en1 we fire this feature on all links con-taining word e, returning again the distance tothe diagonal for that link.
We learn a strongnegative weight for this feature.?
We find that binary identity andpunctuation-mismatch features are im-portant.
The binary identity feature fires ife = f , and proves useful for untranslatednumbers, symbols, names, and punctuationin the data.
Punctuation-mismatch fires onany link that causes nonpunctuation to bealigned to punctuation.Additionally, we include fine-grained versions ofthe lexical probability, fertility, and distortion fea-tures.
These fire for for each link (e, f ) and part-of-speech tag.
That is, we learn a separate weightfor each feature for each part-of-speech tag in ourdata.
Given the tag of e, this affords the model theability to pay more or less attention to the featuresdescribed above depending on the tag given to e.Arabic-English specific features We describehere language specific features we implement toexploit shallow Arabic morphology.162PPINNPfrom...?...Figure 7: This figure depicts the tree/alignmentstructure for which the feature PP-from-prepfires.
The English preposition ?from?
is alignedto Arabic word 	??.
Any aligned words in the spanof the sister NP are aligned to words following 	?
?.English preposition structure commonly matchesthat of Arabic in our gold data.
This family of fea-tures captures these observations.?
We observe the Arabic prefix ?, transliteratedw- and generally meaning and, to prepend tomost any word in the lexicon, so we definefeatures p?w(e | f ) and p?w( f | e).
If f be-gins with w-, we strip off the prefix and returnthe values of p(e | f ) and p( f | e).
Otherwise,these features return 0.?
We also include analogous feature functionsfor several functional and pronominal pre-fixes and suffixes.44.2 Nonlocal featuresThese features comprise the combination costcomponent of a partial alignment score and mayfire when concatenating two partial alignmentsto create a larger span.
Because these featurescan look into any two arbitrary subtrees, theyare considered nonlocal features as defined byHuang (2008).?
Features PP-NP-head, NP-DT-head, andVP-VP-head (Figure 6) all exploit head-words on the parse tree.
We observe Englishprepositions and determiners to often align tothe headword of their sister.
Likewise, we ob-serve the head of a VP to align to the head ofan immediate sister VP.4Affixes used by our model are currently: K., ?, ?
@, ?AK.,?, ?
?, A?
?, ?
?, A??.
Others either we did not experimentwith, or seemed to provide no significant benefit, and are notincluded.In Figure 4, when the search arrives at theleft-most NPB node, the NP-DT-head fea-ture will fire given this structure and linksover the span [the ... tests].
Whensearch arrives at the second NPB node, itwill fire given the structure and links over thespan [the ... missle], but will not fire atthe right-most NPB node.?
Local lexical preference features competewith the headword features described above.However, we also introduce nonlocal lexical-ized features for the most common types ofEnglish and foreign prepositions to also com-pete with these general headword features.PP features PP-of-prep, PP-from-prep, PP-to-prep, PP-on-prep, and PP-in-prep fire atany PP whose left child is a preposition andright child is an NP.
The head of the PP is oneof the enumerated English prepositions and isaligned to any of the three most common for-eign words to which it has also been observedaligned in the gold alignments.
The last con-straint on this pattern is that all words un-der the span of the sister NP, if aligned, mustalign to words following the foreign preposi-tion.
Figure 7 illustrates this pattern.?
Finally, we have a tree-distance feature toavoid making too many many-to-one (frommany English words to a single foreign word)links.
This is a simplified version of and sim-ilar in spirit to the tree distance metric usedin (DeNero and Klein, 2007).
For any pair oflinks (ei, f ) and (e j, f ) in which the e wordsdiffer but the f word is the same token ineach, return the tree height of first commonancestor of ei and e j.This feature captures the intuition that it ismuch worse to align two English words atdifferent ends of the tree to the same foreignword, than it is to align two English wordsunder the same NP to the same foreign word.To see why a string distance feature thatcounts only the flat horizontal distance fromei to e j is not the best strategy, consider thefollowing.
We wish to align a determinerto the same f word as its sister head noununder the same NP.
Now suppose there areseveral intermediate adjectives separating thedeterminer and noun.
A string distance met-163ric, with no knowledge of the relationship be-tween determiner and noun will levy a muchheavier penalty than its tree distance analog.5 Related WorkRecent work has shown the potential for syntac-tic information encoded in various ways to sup-port inference of superior word alignments.
Veryrecent work in word alignment has also started toreport downstream effects on BLEU score.Cherry and Lin (2006) introduce soft syntac-tic ITG (Wu, 1997) constraints into a discrimi-native model, and use an ITG parser to constrainthe search for a Viterbi alignment.
Haghighi etal.
(2009) confirm and extend these results, show-ing BLEU improvement for a hierarchical phrase-based MT system on a small Chinese corpus.As opposed to ITG, we use a linguistically mo-tivated phrase-structure tree to drive our searchand inform our model.
And, unlike ITG-style ap-proaches, our model can generate arbitrary align-ments and learn from arbitrary gold alignments.DeNero and Klein (2007) refine the distor-tion model of an HMM aligner to reflect treedistance instead of string distance.
Fossum etal.
(2008) start with the output from GIZA++Model-4 union, and focus on increasing precisionby deleting links based on a linear discriminativemodel exposed to syntactic and lexical informa-tion.Fraser and Marcu (2007) take a semi-supervisedapproach to word alignment, using a small amountof gold data to further tune parameters of aheadword-aware generative model.
They showa significant improvement over a Model-4 unionbaseline on a very large corpus.6 ExperimentsWe evaluate our model and and resulting align-ments on Arabic-English data against those in-duced by IBM Model-4 using GIZA++ (Och andNey, 2003) with both the union and grow-diag-final heuristics.
We use 1,000 sentence pairs andgold alignments from LDC2006E86 to train modelparameters: 800 sentences for training, 100 fortesting, and 100 as a second held-out developmentset to decide when to stop perceptron training.
Wealso align the test data using GIZA++5 along with50 million words of English.5We use a standard training procedure: 5 iterations ofModel-1, 5 iterations of HMM, 3 iterations of Model-3, and 3iterations of Model-4.0 5 10 15 20 25 30 35 400.730.7350.740.7450.750.7550.760.7650.770.775Training epochTraining F?measureFigure 8: Learning curves for 10 random restartsover time for parallel averaged perceptron train-ing.
These plots show the current F-measure onthe training set as time passes.
Perceptron traininghere is quite stable, converging to the same generalneighborhood each time.0.670.680.690.700.710.720.730.740.750.76Model 1 HMM Model 4F-measureInitial alignmentsFigure 9: Model robustness to the initial align-ments from which the p(e | f ) and p( f | e) featuresare derived.
The dotted line indicates the baselineaccuracy of GIZA++ Model 4 alone.6.1 Alignment QualityWe empirically choose our beam size k from theresults of a series of experiments, setting k=1, 2,4, 8, 16, 32, and 64.
We find setting k = 16 to yieldthe highest accuracy on our held-out test data.
Us-ing wider beams results in higher F-measure ontraining data, but those gains do not translate intohigher accuracy on held-out data.The first three columns of Table 2 show thebalanced F-measure, Precision, and Recall of ouralignments versus the two GIZA++ Model-4 base-lines.
We report an F-measure 8.6 points overModel-4 union, and 6.3 points over Model-4 grow-diag-final.164F P R Arabic/English # UnknownBLEU WordsM4 (union) .665 .636 .696 45.1 2,538M4 (grow-diag-final) .688 .702 .674 46.4 2,262Hypergraph alignment .751 .780 .724 47.5 1,610Table 2: F-measure, Precision, Recall, the resulting BLEU score, and number of unknown words on aheld-out test corpus for three types of alignments.
BLEU scores are case-insensitive IBM BLEU.
Weshow a 1.1 BLEU increase over the strongest baseline, Model-4 grow-diag-final.
This is statisticallysignificant at the p < 0.01 level.Figure 8 shows the stability of the search proce-dure over ten random restarts of parallel averagedperceptron training with 40 CPUs.
Training ex-amples are randomized at each epoch, leading toslight variations in learning curves over time butall converge into the same general neighborhood.Figure 9 shows the robustness of the model toinitial alignments used to derive lexical featuresp(e | f ) and p( f | e).
In addition to IBM Model 4,we experiment with alignments from Model 1 andthe HMM model.
In each case, we significantlyoutperform the baseline GIZA++ Model 4 align-ments on a heldout test set.6.2 MT ExperimentsWe align a corpus of 50 million words withGIZA++ Model-4, and extract translation rulesfrom a 5.4 million word core subset.
We alignthe same core subset with our trained hypergraphalignment model, and extract a second set of trans-lation rules.
For each set of translation rules, wetrain a machine translation system and decode aheld-out test corpus for which we report results be-low.We use a syntax-based translation system forthese experiments.
This system transforms Arabicstrings into target English syntax trees Translationrules are extracted from (e-tree, f -string, align-ment) triples as in (Galley et al, 2004; Galley etal., 2006).We use a randomized language model (similarto that of Talbot and Brants (2008)) of 472 mil-lion English words.
We tune the the parametersof the MT system on a held-out development cor-pus of 1,172 parallel sentences, and test on a held-out parallel corpus of 746 parallel sentences.
Bothcorpora are drawn from the NIST 2004 and 2006evaluation data, with no overlap at the documentor segment level with our training data.Columns 4 and 5 in Table 2 show the resultsof our MT experiments.
Our hypergraph align-ment algorithm allows us a 1.1 BLEU increase overthe best baseline system, Model-4 grow-diag-final.This is statistically significant at the p < 0.01level.
We also report a 2.4 BLEU increase overa system trained with alignments from Model-4union.7 ConclusionWe have opened up the word alignment task toadvances in hypergraph algorithms currently usedin parsing and machine translation decoding.
Wetreat word alignment as a parsing problem, andby taking advantage of English syntax and the hy-pergraph structure of our search algorithm, we re-port significant increases in both F-measure andBLEU score over standard baselines in use by moststate-of-the-art MT systems today.AcknowledgementsWe would like to thank our colleagues in the Nat-ural Language Group at ISI for many meaningfuldiscussions and the anonymous reviewers for theirthoughtful suggestions.
This research was sup-ported by DARPA contract HR0011-06-C-0022under subcontract to BBN Technologies, and aUSC CREATE Fellowship to the first author.ReferencesPhil Blunsom and Trevor Cohn.
2006.
DiscriminativeWord Alignment with Conditional Random Fields.In Proceedings of the 44th Annual Meeting of theACL.
Sydney, Australia.Peter F. Brown, Stephen A. Della Pietra, Vincent DellaJ.
Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?312.
MIT Press.
Camrbidge, MA.
USA.165Colin Cherry and Dekang Lin.
2006.
Soft SyntacticConstraints for Word Alignment through Discrimi-native Training.
In Proceedings of the 44th AnnualMeeting of the ACL.
Sydney, Australia.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics.
33(2):201?228.MIT Press.
Cambridge, MA.
USA.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online Large-Margin Training of Syntactic andStructural Translation Features.
In Proceedings ofEMNLP.
Honolulu, HI.
USA.Michael Collins.
2003.
Head-Driven Statistical Mod-els for Natural Language Parsing.
ComputationalLinguistics.
29(4):589?637.
MIT Press.
Cam-bridge, MA.
USA.Michael Collins 2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.John DeNero and Dan Klein.
2007.
Tailoring WordAlignments to Syntactic Machine Translation.
InProceedings of the 45th Annual Meeting of the ACL.Prague, Czech Republic.Alexander Fraser and Daniel Marcu.
2007.
Gettingthe Structure Right for Word Alignment: LEAF.
InProceedings of EMNLP-CoNLL.
Prague, Czech Re-public.Victoria Fossum, Kevin Knight, and Steven Abney.2008.
Using Syntax to Improve Word AlignmentPrecision for Syntax-Based Machine Translation.
InProceedings of the Third Workshop on StatisticalMachine Translation.
Columbus, Ohio.Dan Klein and Christopher D. Manning.
2001.
Parsingand Hypergraphs.
In Proceedings of the 7th Interna-tional Workshop on Parsing Technologies.
Beijing,China.Aria Haghighi, John Blitzer, and Dan Klein.
2009.Better Word Alignments with Supervised ITG Mod-els.
In Proceedings of ACL-IJCNLP 2009.
Singa-pore.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
In Proceedings of the 9th InternationalWorkshop on Parsing Technologies.
Vancouver, BC.Canada.Liang Huang and David Chiang.
2007.
Forest Rescor-ing: Faster Decoding with Integrated LanguageModels.
In Proceedings of the 45th Annual Meet-ing of the ACL.
Prague, Czech Republic.Liang Huang.
2008.
Forest Reranking: DiscriminativeParsing with Non-Local Features.
In Proceedingsof the 46th Annual Meeting of the ACL.
Columbus,OH.
USA.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a Translation Rule?In Proceedings of NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Training ofContext-Rich Syntactic Models In Proceedings ofthe 44th Annual Meeting of the ACL.
Sydney, Aus-tralia.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for Arabic-English ma-chine translation.
In Proceedings of HLT-EMNLP.Vancouver, BC.
Canada.Simon Lacoste-Julien, Ben Taskar, Dan Klein, andMichael I. Jordan.
2006.
Word alignment viaQuadratic Assignment.
In Proceedings of HLT-EMNLP.
New York, NY.
USA.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear Models for Word Alignment In Proceedingsof the 43rd Annual Meeting of the ACL.
Ann Arbor,Michigan.
USA.Robert C. Moore.
2005.
A Discriminative Frameworkfor Word Alignment.
In Proceedings of EMNLP.Vancouver, BC.
Canada.Robert C. Moore, Wen-tau Yih, and Andreas Bode.2006.
Improved Discriminative Bilingual WordAlignment In Proceedings of the 44th Annual Meet-ing of the ACL.
Sydney, Australia.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics.
29(1):19?52.MIT Press.
Cambridge, MA.
USA.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein 2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation In Proceedings of the44th Annual Meeting of the ACL.
Sydney, Australia.Kishore Papineni, Salim Roukos, T. Ward, and W-J.Zhu.
2002.
BLEU: A Method for Automatic Evalu-ation of Machine Translation In Proceedings of the40th Annual Meeting of the ACL.
Philadelphia, PA.USA.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.2005.
A Discriminative Matching Approach toWord Alignment.
In Proceedings of HLT-EMNLP.Vancouver, BC.
Canada.David Talbot and Thorsten Brants.
2008.
Random-ized Language Models via Perfect Hash Functions.In Proceedings of ACL-08: HLT.
Columbus, OH.USA.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics.
23(3):377?404.
MITPress.
Cambridge, MA.
USA.166
