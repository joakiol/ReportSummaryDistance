Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 605?610,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsUnsupervised Alignment of Privacy Policies using Hidden Markov ModelsRohan Ramanath Fei Liu Norman Sadeh Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{rrohan,feiliu,sadeh,nasmith}@cs.cmu.eduAbstractTo support empirical study of online pri-vacy policies, as well as tools for userswith privacy concerns, we consider theproblem of aligning sections of a thousandpolicy documents, based on the issues theyaddress.
We apply an unsupervised HMM;in two new (and reusable) evaluations, wefind the approach more effective than clus-tering and topic models.1 IntroductionPrivacy policy documents are verbose, often eso-teric legal documents that many people encounteras clients of companies that provide services onthe web.
McDonald and Cranor (2008) showedthat, if users were to read the privacy policies ofevery website they access during the course of ayear, they would end up spending a substantialamount of their time doing just that and wouldoften still not be able to answer basic questionsabout what these policies really say.
Unsurpris-ingly, many people do not read them (FederalTrade Commission, 2012).Such policies therefore offer an excellent op-portunity for NLP tools that summarize or ex-tract key information that (i) helps users under-stand the implications of agreeing to these poli-cies and (ii) helps legal analysts understand thecontents of these policies and make recommenda-tions on how they can be improved or made moreclear.
Past applications of NLP have sought toparse privacy policies into machine-readable rep-resentations (Brodie et al, 2006) or extract sub-policies from larger documents (Xiao et al, 2012).Machine learning has been applied to assess cer-tain attributes of policies (Costante et al, 2012;Ammar et al, 2012; Costante et al, 2013; Zim-meck and Bellovin, 2013).This paper instead analyzes policies in aggre-gate, seeking to align sections of policies.
Thistask is motivated by an expectation that many poli-cies will address similar issues,1such as collec-tion of a user?s contact, location, health, and fi-nancial information, sharing with third parties, anddeletion of data.
This expectation is supportedby recommendation by privacy experts (Gellman,2014) and policymakers (Federal Trade Commis-sion, 2012); in the financial services sector, theGramm-Leach-Bliley Act requires these institu-tions to address a specific set of issues.
Aligningpolicy sections is a first step toward our aforemen-tioned summarization and extraction goals.We present the following contributions:?
A new corpus of over 1,000 privacy policiesgathered from widely used websites, manuallysegmented into subtitled sections by crowdwork-ers (?2).?
An unsupervised approach to aligning the policysections based on the issues they discuss.
Forexample, sections that discuss ?user data on thecompany?s server?
should be grouped together.The approach is inspired by the application ofhidden Markov models to sequence alignment incomputational biology (Durbin et al, 1998; ?3).?
Two reusable evaluation benchmarks for the re-sulting alignment of policy sections (?4).
Wedemonstrate that our approach outperforms na?
?vemethods (?5).Our corpus and benchmarks are available athttp://usableprivacy.org/data.2 Data CollectionWe collected 1,010 unique privacy policydocuments from the top websites ranked byAlexa.com.2These policies were collected duringa period of six weeks during December 2013 andJanuary 2014.
They are a snapshot of privacypolicies of mainstream websites covering fifteen1Personal communication, Joel Reidenberg.2http://www.alexa.com605Business Computers Games HealthHome News Recreation ShoppingArts Kids and Teens Reference RegionalScience Society SportsTable 1: Fifteen website categories provided by Alexa.com.We collect privacy policies from the top 100 websites in each.of Alexa.com?s seventeen categories (Table 1).3Finding a website?s policy is not trivial.
Thoughmany well-regulated commercial websites providea ?privacy?
link on their homepages, not all do.We found university websites to be exceptionallyunlikely to provide such a link.
Even once the pol-icy?s URL is identified, extracting the text presentsthe usual challenges associated with scraping doc-uments from the web.
Since every site is differ-ent in its placement of the document (e.g., burieddeep within the website, distributed across severalpages, or mingled together with Terms of Service)and format (e.g., HTML, PDF, etc.
), and since wewish to preserve as much document structure aspossible (e.g., section labels), full automation wasnot a viable solution.We therefore crowdsourced the privacy policydocument collection using Amazon MechanicalTurk.
For each website, we created a HIT inwhich a worker was asked to copy and paste thefollowing privacy policy-related information intotext boxes: (i) privacy policy URL; (ii) last up-dated date (or effective date) of the current privacypolicy; (iii) privacy policy full text; and (iv) thesection subtitles in the top-most layer of the pri-vacy policy.
To identify the privacy policy URL,workers were encouraged to go to the website andsearch for the privacy link.
Alternatively, theycould form a search query using the website nameand ?privacy policy?
(e.g., ?Amazon.com privacypolicy?)
and search in the returned results for themost appropriate privacy policy URL.
Given theprivacy policy full text and the section subtitles,we partition the full privacy document into differ-ent sections, delimited by the section subtitles.
Aprivacy policy is then converted into XML.Each HIT was completed by three workers, paid$0.05, for a total cost of $380 (including Ama-zon?s surcharge).3The ?Adult?
category was excluded; the ?World?
cate-gory was excluded since it contains mainly popular websitesin different languages, and we opted to focus on policies inEnglish in this first stage of research, though mulitlingual pol-icy analysis presents interesting challenges for future work.3 ApproachGiven the corpus of privacy policies described in?2, we designed a model to efficiently infer analignment of policy sections.
While we expect thatdifferent kinds of websites will likely address dif-ferent privacy issues, we believe that many poli-cies will discuss roughly the same set of issues.Aligning the policies is a first step in a larger effortto (i) automatically analyze policies to make themless opaque to users and (ii) support legal expertswho wish to characterize the state of privacy on-line and make recommendations (Costante et al,2012; Ammar et al, 2012; Costante et al, 2013).We are inspired by multiple sequence alignmentmethods in computational biology (Durbin et al,1998) and by Barzilay and Lee (2004), who de-scribed a hidden Markov model (HMM) for doc-ument content where each state corresponds to adistinct topic and generates sentences relevant tothat topic according to a language model.
Weestimate an HMM-like model on our corpus, ex-ploiting similarity across privacy policies to theextent it is evident in the data.
In our formula-tion, each hidden state corresponds to an issue ortopic, characterized by a distribution over wordsand bigrams appearing in privacy policy sectionsaddressing that issue.
The transition distributioncaptures tendencies of privacy policy authors toorganize these sections in similar orders, thoughwith some variation.The generative story for our model is as follows.Let S denote the set of hidden states.1.
Choose a start state y1from S according to thestart-state distribution.2.
For t = 1, 2, .
.
., until ytis the stopping state:(a) Sample the tth section of the document bydrawing a bag of terms, ot, according to theemission multinomial distribution for state yt.Note the difference from traditional HMMs, inwhich a single observation symbol is drawnat each time step.
otis generated by repeat-edly sampling from a distribution over termsthat includes all unigrams and bigrams exceptthose that occur in fewer than 5% of the doc-uments and in more than 98% of the docu-ments.
This filtering rule was designed toeliminate uninformative stopwords as well ascompany-specific terms (e.g., the name of thecompany).44The emission distributions are not a proper language606Websites with Unique privacy Unique privacy Ave. sections Ave. tokensCategory privacy URL policies policies w/ date per policy per policyArts 94 80 72 11.1 (?
3.8) 2894 (?
1815)Business 100 95 75 10.1 (?
4.9) 2531 (?
1562)Computers 100 78 62 10.7 (?
4.9) 2535 (?
1763)Games 92 80 51 10.2 (?
4.9) 2662 (?
2267)Health 92 86 57 10.0 (?
4.4) 2325 (?
1891)Home 100 84 68 11.5 (?
3.8) 2493 (?
1405)Kids and Teens 96 86 62 10.3 (?
4.5) 2683 (?
1979)News 96 91 68 10.7 (?
3.9) 2588 (?
2493)Recreation 98 97 67 11.9 (?
4.5) 2678 (?
1421)Reference 84 86 55 9.9 (?
4.1) 2002 (?
1454)Regional 98 91 72 11.2 (?
4.2) 2557 (?
1359)Science 71 75 49 9.2 (?
4.1) 1705 (?
1136)Shopping 100 99 84 12.0 (?
4.1) 2683 (?
1154)Society 96 94 65 10.2 (?
4.6) 2505 (?
1587)Sports 96 62 38 10.9 (?
4.0) 2222 (?
1241)Average 94.2 85.6 63.0 10.7 (?
4.3) 2471 (?
1635)Table 2: Statistics of each website category, including (i) the number of websites with an identified privacy policy link; (ii)number of unique privacy policies in each category (note that in rare cases, multiple unique privacy policies were identifiedfor the same website, e.g., a website that contains links to both new and old versions of its privacy policy); (iii) number ofwebsites with an identified privacy modification date; (iv) average number of sections per policy; (v) average number of tokensper policy.
(b) Sample the next state, yt+1, according to thetransition distribution over S.This model can nearly be understood as a hid-den semi-Markov model (Baum and Petrie, 1966),though we treat the section lengths as observable.Indeed, our model does not even generate theselengths, since doing so would force the states to?explain?
the length of each section, not just itscontent.
The likelihood function for the model isshown in Figure 1.The parameters of the model are almost iden-tical to those of a classic HMM (start state dis-tribution, emission distributions, and transitiondistributions), except that emissions are char-acterized by multinomial rather than a cate-gorical distributions.
These are learned us-ing Expectation-Maximization, with a forward-backward algorithm to calculate marginals (E-step) and smoothed maximum likelihood estima-tion for the M-step (Rabiner, 1989).
After learn-ing, the most probable assignment of a policy?ssections to states can be recovered using a variantof the Viterbi algorithm.We consider three HMM variants.
?Vanilla?
al-lows all transitions.
The other two posit an order-ing on the states S = {s1, s2, .
.
.
, sK}, and re-strict the set of transitions that are possible, impos-ing bias on the learner.
?All Forward?
only allowsmodels (e.g., a bigram may be generated by as many as threedraws from the emission distribution: once for each unigramit contains and once for the bigram).skto transition to {sk, sk+1, .
.
.
, sK}.
?Strict For-ward?
only allows skto transition to skor sk+1.4 EvaluationDeveloping a gold-standard alignment of privacypolicies would either require an interface that al-lows each annotator to interact with the entire cor-pus of previously aligned documents while read-ing the one she is annotating, or the definition (andlikely iterative refinement) of a set of categoriesfor manually labeling policy sections.
These weretoo costly for us to consider, so we instead pro-pose two generic methods to evaluate modelsfor sequence alignment of a collection of docu-ments with generally similar content.
Though ourmodel (particularly the restricted variants) treatsthe problem as one of alignment, our evaluationsconsider groupings of policy sections.
In the se-quel, a grouping on a set X is defined as a collec-tion of subsets Xi?
X; these may overlap (i.e.,there might be x ?
Xi?Xj) and need not be ex-haustive (i.e., there might be x ?
X \?iXi).4.1 Evaluation by Human QAThis study was carried out as part of a larger col-laboration with legal scholars who study privacy.In that work, we have formulated a set of nine mul-tiple choice questions about a single policy thatask about collection of contact, location, health,and financial information, sharing of each with607Ppi,?,?
(?yt,ot?nt=1| ?`t?nt=1) = pi(y1)n?t=1(`t?i=1?
(ot,i| yi))?
(yt+1| yt)Figure 1: The likelihood function for the alignment model (one privacy policy).
ytis the hidden state for the tth section, otisthe bag of unigram and bigram terms observed in that section, and `tis the size of the bag.
Start-state, emission, and transitiondistributions are denoted respectively by pi, ?, and ?.
yn+1is the silent stopping state.third parties, and deletion of data.5The questionswere inspired primarily by the substantive interestof these domain experts?not by this particular al-gorithmic study.For thirty policies, we obtained answers fromeach of six domain experts who were not involvedin designing the questions.
For the purposes of thisstudy, the experts?
answers are not important.
Inaddition to answering each question for each pol-icy, we also asked each expert to copy and pastethe text of the policy that contains the answer.Experts were allowed to select as many sectionsfor each question as they saw fit, since answeringsome questions may require synthesizing informa-tion from different sections.For each of the nine questions, we take theunion of all policy sections that contain text se-lected by any annotator as support for her answer.This results in nine groups of policy sections,which we call answer-sets denoted A1, .
.
.
, A9.Our method allows these to overlap (63% of thesections in any Aioccurred in more than one Ai),and they are not exhaustive (since many sectionsof the policies were not deemed to contain answersto any of the nine questions by any expert).Together, these can be used as a gold standardgrouping of policy sections, against which we cancompare our system?s output.
To do this, we definethe set of section pairs that are grouped togetherin answer sets, G = |{?a, b?
| ?Ai3 a, b}|, anda similar set of pairs H from a model?s grouping.From these sets, we calculate estimates of preci-sion (|G ?H|/|H|) and recall (|G ?H|/|G|).One shortcoming of this approach, for whichthe second evaluation seeks to compensate, is thata very small, and likely biased, subset of the policysections is considered.4.2 Evaluation by Direct JudgmentWe created a separate gold standard of judgmentsof pairs of privacy policy sections.
The data se-lected for judgment was a sample of pairs stratified5The questions are available in an online appendix athttp://usableprivacy.org/data.by a simple measure of text similarity.
We derivedunigram tfidf vectors for each section in each of50 randomly sampled policies per category.
Wethen binned pairs of sections by cosine similarity(into four bins bounded by 0.25, 0.5, and 0.75).We sampled 994 section pairs uniformly across the15 categories?
four bins each.Crowdsourcing was used to determine, for eachpair, whether the two sections should be groupedtogether.
A HIT consisted of a pair of policy sec-tions and a multiple choice question, ?After read-ing the two sections given below, would you saythat they broadly discuss the same topic??
Thepossible answers were:1.
Yes, both the sections essentially convey thesame message in a privacy policy.2.
Although, the sections do not convey the samemessage, the broadly discuss the same topic.
(For ease of understanding, some examples ofcontent on ?the same topic?
were included.)3.
No, the sections discuss two different topics.The first two options were considered a ?yes?
forthe majority voting and for defining a gold stan-dard.
Every section-pair was annotated by at leastthree annotators (as many as 15, increased untilan absolute majority was reached).
Turkers withan acceptance rate greater than 95% with an ex-perience of at least 100 HITs were allowed andpaid $0.03 per annotation.
The total cost includ-ing some initial trials was $130.
535 out of the994 pairs were annotated to be similar in topic.
Anexample is shown in Figure 2.As in ?4.1, we calculate precision and recall onpairs.
This does not penalize the model for group-ing together a ?no?
pair; we chose it nonethelessbecause it is interpretable.5 ExperimentIn this section, we evaluate the three HMM vari-ants described in ?3, and two baselines, using themethods in ?4.
All of the methods require thespecification of the number of groups or hiddenstates, which we fix to ten, the average number ofsections per policy.608Section 5 of classmates.com:[46 words] .
.
.
You may also be required to use a password to access certain pages on the Services where certaintypes of your personal information can be changed or deleted.
.
.
.
[113 words]Section 2 of 192.com:[50 words] .
.
.
This Policy sets out the means by which You can have Your Personal Information removed fromthe Service.
192.com is also committed to keeping Personal Information of users of the Service secure and only touse it for the purposes set out in this Policy and as agreed by You.
.
.
.
[24 words]Figure 2: Selections from sections that discuss the issue of ?deletion of personal information?
and were labeled as discussingthe same issue by crowdworkers.
Both na?
?ve grouping and LDA put them in two different groups, but the Strict Forward variantof our model correctly groups them together.Precision Recall F1Mean S.D.
Mean S.D.
Mean S.D.Clust.
0.63 ?
0.30 ?
0.40 ?LDA 0.56 0.03 0.20 0.05 0.29 0.06Vanilla 0.62 0.04 0.41 0.04 0.49 0.03All F. 0.63 0.03 0.47 0.12 0.53 0.06Strict F. 0.62 0.05 0.46 0.18 0.51 0.07Clust.
0.62 ?
0.23 ?
0.34 ?LDA 0.57 0.03 0.18 0.01 0.28 0.02Vanilla 0.57 0.01 0.30 0.03 0.39 0.02All F. 0.58 0.02 0.32 0.06 0.41 0.04Strict F. 0.58 0.03 0.32 0.14 0.40 0.08Table 3: Evaluation by human QA (above) and direct judg-ment (below), aggregated across ten independent runs whereappropriate (see text).
Vanilla, All F(orward), and StrictF(orward) are three variants of our HMM.Baselines.
Our first baseline is a greedy divisiveclustering algorithm6to partition the policy sec-tions into ten clusters.
In this method, the de-sired K-way clustering solution is computed byperforming a sequence of bisections.
The imple-mentation uses unigram features and cosine simi-larity.
Our second baseline is latent Dirichlet alo-cation (LDA; Blei et al, 2003), with ten topics andonline variational Bayes for inference (Hoffman etal., 2010).7To more closely match our models,LDA is given access to the same unigram and bi-gram tokens.Results.
Table 3 shows the results.
For LDAand the HMM variants (which use random initial-ization), we report mean and standard deviationacross ten independent runs.
All three variantsof the HMM improve over the baselines on bothtasks, in terms of F1.
In the human QA evalu-ation, this is mostly due to recall improvements(i.e., more pairs of sections relevant to the samepolicy question were grouped together).The three variants of the model performed sim-ilarly on average, though Strict Forward had veryhigh variance.
Its maximum performance across6As implemented in CLUTO, http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview7As implemented in gensim (?Reh?u?rek and Sojka, 2010).ten runs was very high (67% and 53% F1on thetwo tasks), suggesting the potential benefits ofgood initialization or model selection.6 ConclusionWe considered the task of aligning sections ofa collection of roughly similarly-structured legaldocuments, based on the issues they address.
Weintroduced an unsupervised model for this taskalong with two new (and reusable) evaluations.Our experiments show the approach to be more ef-fective than clustering and topic models.
The cor-pus and evaluation data have been made availableat http://usableprivacy.org/data .
Infuture work, policy section alignments will beused in automated analysis to extract useful infor-mation for users and privacy scholars.AcknowledgmentsThe authors gratefully acknowledge helpful com-ments from Lorrie Cranor, Joel Reidenberg, Flo-rian Schaub, and several anonymous reviewers.This research was supported by NSF grant SaTC-1330596.ReferencesWaleed Ammar, Shomir Wilson, Norman Sadeh, andNoah A. Smith.
2012.
Automatic categorization ofprivacy policies: A pilot study.
Technical ReportCMU-LTI-12-019, Carnegie Mellon University.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proc.
of HLT-NAACL.Leonard E. Baum and Ted Petrie.
1966.
Statisticalinference for probabilistic functions of finite stateMarkov chains.
Annals of Mathematical Statistics,37:1554?1563.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet alocation.
the Journal ofmachine Learning research, 3:993?1022.609Carolyn A. Brodie, Clare-Marie Karat, and John Karat.2006.
An empirical study of natural language pars-ing of privacy policy rules using the SPARCLE pol-icy workbench.
In Proc.
of the Symposium on Us-able Privacy and Security.Elisa Costante, Yuanhao Sun, Milan Petkovi?c, andJerry den Hartog.
2012.
A machine learning solu-tion to assess privacy policy completeness.
In Proc.of the ACM Workshop on Privacy in the ElectronicSociety.Elisa Costante, Jerry Hartog, and Milan Petkovi.2013.
What websites know about you.
In RobertoPietro, Javier Herranz, Ernesto Damiani, and RaduState, editors, Data Privacy Management and Au-tonomous Spontaneous Security, volume 7731 ofLecture Notes in Computer Science, pages 146?159.Springer Berlin Heidelberg.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological SequenceAnalysis: Probabilistic Models of Proteins and Nu-cleic Acids.
Cambridge University Press.Federal Trade Commission.
2012.
Protecting con-sumer privacy in an era of rapid change: Recom-mendations for businesses and policymakers.Robert Gellman.
2014.
Fair information prac-tices: a basic history (v. 2.11).
Available athttp://www.bobgellman.com/rg-docs/rg-FIPShistory.pdf.Matthew D Hoffman, David M Blei, and Francis RBach.
2010.
Online learning for latent Dirichlet allocation.
In NIPS.Aleecia M. McDonald and Lorrie Faith Cranor.
2008.The cost of reading privacy policies.
I/S: A Journalof Law and Policy for the Information Society, 4(3).Lawrence Rabiner.
1989.
A tutorial on hidden Markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286.Radim?Reh?u?rek and Petr Sojka.
2010.
Software frame-work for topic modelling with large corpora.
InProc.
of the LREC Workshop on New Challenges forNLP Frameworks.Xusheng Xiao, Amit Paradkar, Suresh Thum-malapenta, and Tao Xie.
2012.
Automated ex-traction of security policies from natural-languagesoftware documents.
In Proc.
of the ACM SIGSOFTInternational Symposium on the Foundations ofSoftware Engineering.Sebastian Zimmeck and Steven M. Bellovin.
2013.Machine learning for privacy policy.610
