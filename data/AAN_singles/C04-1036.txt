Feature Vector Quality and Distributional SimilarityMaayan GeffetSchool of Computer Science and Engineering,Hebrew UniversityGivat Ram Campus,Jerusalem, Israel, 91904mary@cs.huji.ac.ilIdo DaganDepartment of Computer Science,Bar-Ilan UniversityRamat-Gan, Israel, 52900dagan@cs.biu.ac.ilAbstractWe suggest a new goal and evaluation criterion forword similarity measures.
The new criterion -meaning-entailing substitutability - fits the needsof semantic-oriented NLP applications and can beevaluated directly (independent of an application)at a good level of human agreement.
Motivated bythis semantic criterion we analyze the empiricalquality of distributional word feature vectors andits impact on word similarity results, proposing anobjective measure for evaluating feature vectorquality.
Finally, a novel feature weighting and se-lection function is presented, which yields superiorfeature vectors and better word similarity perform-ance.1 IntroductionDistributional Similarity has been an active re-search area for more than a decade (Hindle, 1990),(Ruge, 1992), (Grefenstette, 1994), (Lee, 1997),(Lin, 1998), (Dagan et al, 1999), (Weeds andWeir, 2003).
Inspired by Harris distributional hy-pothesis (Harris, 1968), similarity measures com-pare a pair of weighted feature vectors thatcharacterize two words.
Features typically corre-spond to other words that co-occur with the charac-terized word in the same context.
It is thenassumed that different words that occur withinsimilar contexts are semantically similar.As it turns out, distributional similarity capturesa somewhat loose notion of semantic similarity(see Table 1).
By construction, if two words aredistributionally similar then the occurrence of oneword in some contexts indicates that the otherword is also likely to occur in such contexts.
But itdoes not ensure that the meaning of the first wordis preserved when replacing it with the other one inthe given context.
For example, words of similarsemantic types, such as company ?
government,tend to come up as distributionally similar, eventhough they are not substitutable in a meaning pre-serving sense.On the other hand, many semantic-oriented appli-cations, such as Question Answering, Paraphrasingand Information Extraction, do need to recognizewhich words may substitute each other in a mean-ing preserving manner.
For example, a questionabout company may be answered by a sentenceabout firm, but not about government.
Such appli-cations usually utilize reliable taxonomies or on-tologies like WordNet, but cannot rely on the?loose?
type of output of distributional similaritymeasures.In recent work Dagan and Glickman (2004) ob-serve that applications usually do not require astrict meaning preserving criterion between textexpressions, but rather need to recognize that themeaning of one expression entails the other.
En-tailment modeling is thus proposed in their work asa generic (application-independent) framework forpractical semantic inference.
We suggest adoptingsuch (directional) entailment criterion at the lexicallevel for judging whether one word can be substi-tuted by another one.
For example, certain ques-tions about companies might be answered bysentences about automakers, since the meaning ofautomaker entails the meaning of company (thoughnot vice versa).
In this paper we adapt this newcriterion, termed meaning entailing substitutability,as a direct evaluation criterion for the "correctness"of the output of word similarity measures (as op-posed to indirect evaluations through WSD or dis-tance in WordNet).Our eventual research goal is improving wordsimilarity measures to predict better the more deli-cate meaning entailment relationship betweenwords.
As a first step it was necessary to analyzethe typical behavior of current similarity measuresand categorize their errors (Section 3).
Our mainobservation is that the quality of similarity scoresnationregionstate*worldislandprovince123456*cityterritoryarea*townrepublicafrican_country789101112*north*economy*neighbor*member*party*government131415161718*company*industrykingdomeuropean_countryplacecolony192025313641Table 1: The 20 top most similar words of country (and their ranks) in the similarity list by Lin98,followed by the next 4 words in the similarity list that are judged as correct.
Incorrect similarities,under the substitutability criterion, are marked with ?
*?.is often hurt by improper feature weights, whichyield rather noisy feature vectors.
We quantify thisproblem by a new measure for feature vector qual-ity, which is independent of any particular vectorsimilarity measure.To improve feature vector quality a novel fea-ture weighting function is introduced, called rela-tive feature focus (RFF) (Section 4).
While havinga simple (though non-standard) definition, thisfunction yields improved performance relative tothe two suggested evaluation criteria ?
for vectorquality and for word similarity.
The underlyingidea is that a good characteristic feature for a wordw should characterize also multiple words that arehighly similar to w. In other words, such featureshould have a substantial "focus" within the closesemantic vicinity of w.Applying RFF weighting achieved about 10%improvement in predicting meaning entailing sub-stitutability (Section 5).
Further analysis showsthat RFF also leads to "cleaner" characteristic fea-ture vectors, which may be useful for additionalfeature-based tasks like clustering.2 Background and DefinitionsIn the distributional similarity scheme each wordw is represented by a feature vector, where an entryin the vector corresponds to a feature f. Each fea-ture represents another word (or term) with whichw co-occurs, and possibly specifies also the syntac-tic relation between the two words.
The value ofeach entry is determined by some weight functionweight(w,f), which quantifies the degree of statisti-cal association between the feature and the corre-sponding word.Typical feature weighting functions include thelogarithm of the frequency of word-feature co-occurrence (Ruge, 1992), and the conditional prob-ability of the feature given the word (within prob-abilistic-based measures) (Pereira et al, 1993),(Lee, 1997), (Dagan et al, 1999).
Probably themost widely used association weight function is(point-wise) Mutual Information (MI) (Church etal., 1990), (Hindle, 1990), (Lin, 1998), (Dagan,2000), defined by:)()(),(log),( 2 fPwPfwPfwMI =A known weakness of MI is its tendency to assignhigh weights for rare features.
Yet, similaritymeasures that utilize MI showed good perform-ance.
In particular, a common practice is to filterout features by minimal frequency and weightthresholds.
A word's vector is then constructedfrom the remaining features, which we call hereactive features.Once feature vectors have been constructed, thesimilarity between two words is defined by somevector similarity metric.
Different metrics havebeen used in the above cited papers, such asWeighted Jaccard (Dagan, 2000), cosine (Ruge,1992), various information theoretic measures(Lee, 1997), and others.
We picked the widelycited and competitive (e.g.
(Weeds and Weir,2003)) measure of Lin (1998) as a representativecase, and utilized it for our analysis and as a start-ing point for improvement.2.1 Lin's (?98) Similarity MeasureLin's similarity measure between two words, wand v, is defined as follows:,),(),(),(),(),()()()()(?
?????
?++=fvweightfwweightfvweightfwweightvwsimvFfwFfvFwFfwhere F(w) and F(v) are the active features of thetwo words and the weight function is defined asMI.
A feature is defined as a pair <term, syntac-Country-StateRanks Country-EconomyRanksBroadcastGoodsCivil_servantBlocNonalignedNeighboringStatisticBorderNorthwest24140643055151651041501654776016543247174DevastateDevelopedDependentIndustrializedShatteredClubBlackMillionElectricity8136101491615512231130878268514138109245154Table 3: The top-10 common features for theword pairs country-state and country-economy,along with their corresponding ranks in thesorted feature lists of the two words.Feature  WeightCommercial-bank, gen ?Destination, pcomp ?Airspace, pcomp ?Landlocked, mod ?Trade_balance, gen ?Sovereignty, pcomp ?Ambition , nn ?Bourse, gen ?Politician, gen ?Border, pcomp ?8.087.9 77.837.797.787.787.777.727.547.53Table 2: The top-10 ranking features forcountry.tic_relation>.
For example, given the word ?com-pany?
the feature <earnings_report, gen?> (geni-tive) corresponds to the phrase ?company?searnings report?, and <profit, pcomp?> (preposi-tional complement) corresponds to ?the profit ofthe company?.
The syntactic relations are gener-ated by the Minipar dependency parser (Lin,1993).
The arrows indicate the direction of the syn-tactic dependency: a downward arrow indicatesthat the feature is the parent of the target word, andthe upward arrow stands for the opposite.In our implementation we filtered out featureswith overall frequency lower than 10 in the corpusand with MI weights lower than 4.
(In the tuningexperiments the filtered version showed 10% im-provement in precision over no feature filtering.
)From now on we refer to this implementation asLin98.3 Empirical Analysis of Lin98 andVector Quality MeasureTo gain better understanding of distributionalsimilarity we first analyzed the empirical behaviorof Lin98, as a representative case for state of theart (see Section 5.1 for corpus details).As mentioned in the Introduction, distributionalsimilarity may not correspond very tightly tomeaning entailing substitutability.
Under thisjudgment criterion two main types of errors occur:(1) word pairs that are of similar semantic types,but are not substitutable, like firm and government;and (2) word pairs that are of different semantictypes, like firm and contract, which might (ormight not) be related only at a topical level.
Table1 shows the top most similar words for the targetword country according to Lin98 .
The two errortypes are easily recognized, e.g.
world and city forthe first type, and economy for the second.A deeper look at the word feature vectors re-veals typical reasons for such errors.
In manycases, high ranking features in a word vector, whensorting the features by their weight, do not seemvery characteristic for the word meaning.
This isdemonstrated in Table 2, which shows the top-10features in the vector of country.
As can be seen,some of the top features are either too specific(landlocked, airspace), and so are less reliable, ortoo general (destination, ambition), and hence notindicative and may co-occur with many differenttypes of words.
On the other hand, more character-istic features, like population and governor, occurfurther down the list, at positions 461 and 832.Overall, features that characterize well the wordmeaning are scattered across the ranked list, whilemany non-indicative features receive high weights.This may yield high similarity scores for less simi-lar word pairs, while missing other correct similari-ties.An objective indication of the problematic fea-ture ranking is revealed by examining the commonfeatures that contribute mostly to the similarityscore of a pair of similar words.
We look at thecommon features of the two words and sort themby the sum of their weights in the two word vectors(which is the enumerator of Lin's sim formula inSection 2.1).
Table 3 shows the top-10 commonfeatures for a pair of substitutable words (country -state) and non-substitutable words (country - econ-omy).
In both cases the common features are scat-tered across each feature vector, making it difficultto distinguish between similar and non-similarword pairs.We suggest that the desired behavior of featureranking is that the common features of truly similarwords will be concentrated at the top ranks of theirvectors.
The common features for non-similarwords are expected to be scattered all across eachof the vectors.
More formally, given a pair of simi-lar words (judged as substitutable) w and v we de-fine the top joint feature rank criterion forevaluating feature vector quality:],),(),([211),,())()((?
+=????
fvrankfwranknnvwranktopvFwFntopfwhere rank(w,f) is the feature?s position in thesorted vector of the word w, and n is the number oftop joint features to consider (top-n), when sortedby the sum of their weights in the two word vec-tors.
We thus expect that a good weighting func-tion would yield (on average) a low top-rank scorefor truly similar words.4 Relative Feature Focus (RFF)Motivated by the observations above we proposea new feature weight function, called relative fea-ture focus (RFF).
The basic idea is to promote fea-tures which characterize many words that arehighly similar to w. These features are consideredas having a strong "focus" around w's meaning.Features which do not characterize sufficientlymany words that are sufficiently similar to w aredemoted.
Even if such features happen to have astrong direct association with w they are not con-sidered reliable, as they do not have sufficient sta-tistical mass in w's semantic vicinity.4.1 RFF DefinitionRFF is defined as follows.
First, a standardword similarity measure sim is computed to obtaininitial approximation of the similarity space (Lin98was used in this work).
Then, we define the wordset of a feature f, denoted by WS(f), as the set ofwords for which f is an active feature.
The seman-tic neighborhood of w, denoted by N(w), is definedas the set of all words v which are considered suf-ficiently similar to w, satisfying  sim(w,v)>s wheres is a threshold  (0.04 in our experiments).
RFF isthen defined by:?
?
?= ),(),( )()( vwsimfwRFF wNfWSv .That is, we identify all words v that are in the se-mantic neighborhood of w and are also character-ized by f and sum their similarities to w.Notice that RFF is a sum of word similarity val-ues rather than being a direct function of word-feature association values (which is the more com-mon approach).
It thus does not depend on the ex-act co-occurrence level between w and f. Instead, itdepends on a more global assessment of the asso-ciation between f and the semantic vicinity of w.Unlike the entropy measure, used in (Grefenstette,1994), our "focused" global view is relative toeach individual word w and is not a global inde-pendent function of the feature.We notice that summing the above similarityvalues captures simultaneously a desired balancebetween feature specificity and generality, address-ing the observations in Section 3.
Some featuresmight characterize just a single word that is verysimilar to w. But then the sum of similarities willinclude a single element, yielding a relatively lowweight.1 General features may characterize morewords within N(f), but then on average the similar-ity with w over multiple words is likely to becomelower, contributing smaller values to the sum.
Areliable feature has to characterize multiple words(not too specific) that are highly similar to w (nottoo general).4.2 Re-computing SimilaritiesOnce RFF weights have been computed they aresufficiently accurate to allow for aggressive featurereduction.
In our experiments it sufficed to useonly the top 100 features for each word in order toobtain optimal results, since the most informativefeatures now have the highest weights.
Similaritybetween words is then recomputed over the re-duced vectors using Lin's sim function (in Section2.1), with RFF replacing MI as the new weightfunction.1This is why the sum of similarities is used rather thanan average value, which might become too high bychance when computed over just a single element (orvery few elements).#Words Judge 1 (%) Judge 2 (%) Total (%)Top 10 63.4 / 54.1 62.6 / 53.4 63.0 / 53.7Top 20 57.0 / 48.3 56.4 / 45.8 56.8 / 47.0Top 30 55.3 / 45.1 53.3 / 43.4 54.2 / 44.2Top 40 53.5 / 44.6 51.6 / 42.0 52.6 / 43.3Table 4: Precision values for Top-N similarwords by the RFF / Lin98 methods.5 Evaluation5.1 Experimental SettingThe performance of the RFF-based similaritymeasure was evaluated for a sample of nouns andcompared with that of Lin98.
The experiment wasconducted using an 18 million tokens subset of theReuters RCV1 corpus,2 parsed by Lin?s Minipardependency parser (Lin, 1993).
We considered firstan evaluation based on WordNet data as a goldstandard, as in (Lin, 1998; Weeds and Weir, 2003).However, we found that many word pairs from theReuters Corpus that are clearly substitutable arenot linked appropriately in WordNet.We therefore conducted a manual evaluationbased on the judgments of two human subjects.The judgment criterion follows common evalua-tions of paraphrase acquisition (Lin and Pantel,2001), (Barzilay and McKeown, 2001), and corre-sponds to the meaning-entailing substitutabilitycriterion discussed in Section 1.
Two words arejudged as substitutable (correct similarity) if thereare some contexts in which one of the words canbe substituted by the other, such that the meaningof the original word can be inferred from the newone.Typically substitutability corresponds to certainontological relations.
Synonyms are substitutablein both directions.
For example, worker and em-ployee entail each other's meanings, as in the con-text ?high salaried worker/employee?.
Hyponymstypically entail their hypernyms.
For example, dogentails animal, as in ?I have a dog?
which entails?I have an animal?
(but not vice versa).
In somecases part-whole and member-set relations satisfythe meaning-entailing substitutability criterion.
Forexample, a discussion of division entails in manycontexts the meaning of company.
Similarly, theplural form of employee(s) often entails the mean-ing of staff.
On the other hand, non-synonymouswords that share a common hypernym (co-hyponyms) like company and government, orcountry and city, are not substitutable since theyalways refer to different meanings (such as differ-ent entities).Our test set included a sample of 30 randomlyselected nouns whose corpus frequency is above2Known as Reuters Corpus, Volume 1, English Lan-guage, 1996-08-20 to 1997-08-19.500.
For each noun we computed the top 40 mostsimilar words by both similarity measures, yieldinga total set of about 1600 (different) suggested wordsimilarity pairs.
Two independent assessors wereassigned, each judging half of the test set (800pairs).
The output pairs from both methods weremixed so the assessor could not relate a pair withthe method that suggested it.5.2 Similarity ResultsThe evaluation results are displayed in Table 4.As can be seen RFF outperformed Lin98 by 9-10percentage points of precision at all Top-N levels,by both judges.
Overall, RFF extracted 111 (21%)more correct similarity pairs than Lin98.
Theoverall relative recall3 of RFF is quite high (89%),exceeding Lin98 by 16% (73%).
These figures in-dicate that our method covers most of the correctsimilarities found by Lin98, while identifyingmany additional correct pairs.We note that the obtained precision values forboth judges are very close at all table rows.
To fur-ther assess human agreement level for this task thefirst author of this paper judged two samples of100 word pairs each, which were selected ran-domly from the two test sets of the original judges.The proportions of matching decisions between theauthor's judgments and the original ones were91.3% (with Judge 1) and 88.9% (with Judge 2).The corresponding Kappa values are 0.83 (?verygood agreement?)
and 0.75 (?good agreement?
).As for feature reduction, vector sizes were re-duced on average to about one third of their origi-nal size in the Lin98 method (recall that standardfeature reduction, tuned for the corpus, was alreadyapplied to the Lin98 vectors).3Relative recall shows the percentage of correct wordsimilarities found by each method relative to the jointset of similarities that were extracted by both methods.Feature WeightIndustry, gen ?Airport, gen  ?Neighboring, mod ?Law, gen ?Economy, gen ?Population, gen ?City, gen ?Impoverished, mod ?Governor, pcomp ?Parliament, gen ?1.211.161.061.041.021.020.930.920.920.91Table 5: Top-10 features of country byRFF.5.3 Empirical Observations for RFFWe now demonstrate the typical behavior ofRFF relative to the observations and motivationsof Section 3 (through the same example).Table 5 shows the top-10 features of country.We observe (subjectively) that the list now con-tains quite indicative and reliable features, wheretoo specific (anecdotal) and too general featureswere demoted (compare with Table 2).More objectively, Table 6 shows that most ofthe top-10 common features for country-state arenow ranked highly for both words.
On the otherhand, there are only two common features for theincorrect pair country-economy, both with quitelow ranks (compare with Table 3).
Overall, giventhe set of all the correct (judged as substitutable)word similarities produced by both methods, theaverage top joint feature rank of the top-10 com-mon features by RFF is 21, satisfying the desiredbehavior which was suggested in Section 3.
Thesame figure is much larger for the Lin98 vectors,which have an average top joint feature rank of105.Consequently, Table 7 shows a substantial im-provement in the similarity list for country, wheremost incorrect words, like economy and company,disappeared.
Instead, additional correct similari-ties, like kingdom and land, were promoted (com-pare with Table 1).
Some semantically related butnon-substitutable words, like ?world?
and ?city?,still remain in the list, but somewhat demoted.
Inthis case all errors correspond to quite close se-mantic relatedness, being geographic concepts.The remaining errors are mostly of the first typediscussed in Section 3 ?
pairs of words that areontologically or thematically related but are notsubstitutable.
Typical examples are co-hyponyms(country - city) or agent-patient and agent-actionpairs (industry ?
product, worker ?
job).
Usually,such word pairs also have highly ranked commonfeatures since they naturally appear with similarcharacteristic features.
It may therefore be difficultto filter out such non-substitutable similaritiessolely by the standard distributional similarityscheme, suggesting that additional mechanisms arerequired.6 Conclusions and Future WorkThis paper proposed the following contributions:1.
Considering meaning entailing substitutabilityas a target goal and evaluation criterion for wordsimilarity.
This criterion is useful for many seman-tic-oriented NLP applications, and can be evalu-ated directly by human subjects.2.
A thorough empirical error analysis of state ofthe art performance was conducted.
The main ob-servation was deficient quality of the feature vec-tors which reduces the quality of similaritymeasures.3.
Inspired by the qualitative observations we iden-tified a new qualitative condition for feature vectorevaluation ?
top joint feature rank.
Thus, featurevector quality can be measured independently ofthe final similarity output.4.
Finally, we presented a novel feature weightingfunction, relative feature focus.
This measure wasdesigned based on error analysis insights and im-Country-StateRanks Country-EconomyRanksNeighboringIndustryImpoverishedGovernorPopulationCityEconomyParliamentCitizenLaw3181061751014411189161815222533DevelopedLiberalization5010010079Table 6: RFF weighting: Top-10 commonfeatures for country-state and country-economy along with their correspondingranks in the two (sorted) feature vectors.proves performance over all the above criteria.We intend to further investigate the contributionof our measure to word sense disambiguation andto evaluate its performance for clustering methods.Error analysis suggests that it might be difficultto improve similarity output further within thecommon distributional similarity schemes.
Weneed to seek additional criteria and data types, suchas identifying evidence for non-similarity, or ana-lyzing more carefully disjoint features.Further research is suggested to extend thelearning framework towards richer notions of on-tology generation.
We would like to distinguishbetween different ontological relationships thatcorrespond to the substitutability criterion, such asidentifying the entailment direction, which wasignored till now.
Towards these goals we plan toinvestigate combining unsupervised distributionalsimilarity with supervised methods for learningontological relationships, and with paraphrase ac-quisition methods.ReferencesBarzilay, Regina and Kathleen McKeown.
2001.Extracting Paraphrases from a Parallel Corpus.In Proc.
of ACL/EACL, 2001.Church, Kenneth W. and Hanks Patrick.
1990.Word association norms, mutual information,and Lexicography.
Computational Linguistics,16(1), pp.
22?29.Dagan, Ido.
2000.
Contextual Word Similarity, inRob Dale, Hermann Moisl and Harold Somers(Eds.
), Handbook of Natural Language Process-ing, Marcel Dekker Inc, 2000, Chapter 19, pp.459-476.Dagan, Ido and Oren Glickman.
2004.
Probabilis-tic Textual Entailment: Generic Applied Model-ing of Language Variability.
PASCALWorkshop on Text Understanding and Mining.Dagan, Ido, Lillian Lee and Fernando Pereira.1999.
Similarity-based models of cooccurrenceprobabilities.
Machine Learning, 1999, Vol.34(1-3), special issue on Natural LanguageLearning, pp.
43-69.Grefenstette, Gregory.
1994.
Exploration in Auto-matic Thesaurus Discovery.
Kluwer AcademicPublishers.Harris, Zelig S. 1968.
Mathematical structures oflanguage.
Wiley, 1968.Hindle, D. 1990.
Noun classification from predi-cate-argument structures.
In Proc.
of ACL, pp.268?275.Lee, Lillian.
1997.
Similarity-Based Approaches toNatural Language Processing.
Ph.D. thesis, Har-vard University, Cambridge, MA.Lin, Dekang.
1993.
Principle-Based Parsing with-out Overgeneration.
In Proc.
of ACL-93, pages112-120, Columbus, Ohio, 1993.Lin, Dekang.
1998.
Automatic Retrieval and Clus-tering of Similar Words.
In Proc.
of COLING?ACL98, Montreal, Canada, August, 1998.Lin, Dekang and Patrick Pantel.
2001.
Discoveryof Inference Rules for Question Answering.Natural Language Engineering 7(4), pp.
343-360, 2001.Pereira, Fernando, Tishby Naftali, and Lee Lillian.1993.
Distributional clustering of Englishwords.
In Proc.
of ACL-93, pp.
183?190.Ruge,  Gerda.
1992.
Experiments on linguisti-cally-based term associations.
InformationProcessing & Management, 28(3), pp.
317?332.Weeds, Julie and David Weir.
2003.
A GeneralFramework for Distributional Similarity.
InProc.
of EMNLP-03.
Spain.nationstateislandregionarea12345territory*neighborcolony*portrepublic678910african_countryprovince*city*townkingdom1112131415*districteuropean_countryzonelandplace1617181920Table 7: Top-20 most similar words for country and their ranks in the similarity list by theRFF-based measure.
Incorrect similarities (non-substitutable) are marked with ?
*?.
