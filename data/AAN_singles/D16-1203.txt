Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1955?1960,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAnalyzing the Behavior of Visual Question Answering ModelsAishwarya Agrawal?, Dhruv Batra?,?, Devi Parikh?,?
?Virginia Tech ?Georgia Institute of Technology{aish, dbatra, parikh}@vt.eduAbstractRecently, a number of deep-learning basedmodels have been proposed for the task ofVisual Question Answering (VQA).
The per-formance of most models is clustered around60-70%.
In this paper we propose system-atic methods to analyze the behavior of thesemodels as a first step towards recognizing theirstrengths and weaknesses, and identifying themost fruitful directions for progress.
We an-alyze two models, one each from two ma-jor classes of VQA models ?
with-attentionand without-attention and show the similari-ties and differences in the behavior of thesemodels.
We also analyze the winning entry ofthe VQA Challenge 2016.Our behavior analysis reveals that despite re-cent progress, today?s VQA models are ?my-opic?
(tend to fail on sufficiently novel in-stances), often ?jump to conclusions?
(con-verge on a predicted answer after ?listening?to just half the question), and are ?stubborn?
(do not change their answers across images).1 IntroductionVisual Question Answering (VQA) is a recently-introduced (Antol et al, 2015; Geman et al, 2014;Malinowski and Fritz, 2014) problem where givenan image and a natural language question (e.g.,?What kind of store is this?
?, ?How many peopleare waiting in the queue??
), the task is to automati-cally produce an accurate natural language answer(?bakery?, ?5?).
A flurry of recent deep-learningbased models have been proposed for VQA (Antolet al, 2015; Chen et al, 2015; Yang et al, 2016;Xu and Saenko, 2016; Jiang et al, 2015; Andreaset al, 2016a; Wang et al, 2015; Kafle and Kanan,2016; Lu et al, 2016; Andreas et al, 2016b; Shihet al, 2016; Kim et al, 2016; Fukui et al, 2016;Noh and Han, 2016; Ilievski et al, 2016; Wu etal., 2016; Xiong et al, 2016; Zhou et al, 2015;Saito et al, 2016).
Curiously, the performance ofmost methods is clustered around 60-70% (com-pared to human performance of 83% on open-endedtask and 91% on multiple-choice task) with a mere5% gap between the top-9 entries on the VQA Chal-lenge 2016.1 It seems clear that as a first step tounderstand these models, to meaningfully comparestrengths and weaknesses of different models, to de-velop insights into their failure modes, and to iden-tify the most fruitful directions for progress, it is cru-cial to develop techniques to understand the behav-ior of VQA models.In this paper, we develop novel techniques tocharacterize the behavior of VQA models.
As con-crete instantiations, we analyze two VQA models(Lu et al, 2015; Lu et al, 2016), one each from twomajor classes of VQA models ?
with-attention andwithout-attention.
We also analyze the winning en-try (Fukui et al, 2016) of the VQA Challenge 2016.2 Related WorkOur work is inspired by previous works that diag-nose the failure modes of models for different tasks.
(Karpathy et al, 2016) constructed a series of ora-cles to measure the performance of a character level1http://www.visualqa.org/challenge.html1955language model.
(Hoiem et al, 2012) provided anal-ysis tools to facilitate detailed and meaningful inves-tigation of object detector performance.
This paperaims to perform behavior analyses as a first step to-wards diagnosing errors for VQA.
(Yang et al, 2016) categorize the errors made bytheir VQA model into four categories ?
model fo-cuses attention on incorrect regions, model focusesattention on appropriate regions but predicts incor-rect answers, predicted answers are different fromlabels but might be acceptable, labels are wrong.While these are coarse but useful failure modes, weare interested in understanding the behavior of VQAmodels along specific dimensions ?
whether theygeneralize to novel instances, whether they listen tothe entire question, whether they look at the image.3 Behavior AnalysesWe analyze the behavior of VQA models along thefollowing three dimensions ?Generalization to novel instances: We investi-gate whether the test instances that are incorrectlyanswered are the ones that are ?novel?
i.e., not sim-ilar to training instances.
The novelty of the test in-stances may be in two ways ?
1) the test question-image (QI) pair is ?novel?, i.e., too different fromtraining QI pairs; and 2) the test QI pair is ?famil-iar?, but the answer required at test time is ?novel?,i.e., answers seen during training are different fromwhat needs to be produced for the test QI pairs.Complete question understanding: To investi-gate whether a VQA model is understanding the in-put question or not, we analyze if the model ?listens?to only first few words of the question or the entirequestion, if it ?listens?
to only question (wh) wordsand nouns or all the words in the question.Complete image understanding: The absenceof a large gap between performance of language-alone and language + vision VQA models (Antol etal., 2015) provides evidence that current VQA mod-els seem to be heavily reliant on the language model,perhaps not really understanding the image.
In orderto analyze this behavior, we investigate whether thepredictions of the model change across images for agiven question.We present our behavioral analyses on the VQAdataset (Antol et al, 2015).
VQA is a large-scale free-form natural-language dataset containing?0.25M images, ?0.76M questions, and ?10M an-swers, with open-ended and multiple-choice modal-ities for answering the visual questions.
All the ex-perimental results are reported on the VQA valida-tion set using the following models trained on theVQA training set for the open-ended task ?CNN + LSTM based model without-attention(CNN+LSTM): We use the best performing modelof (Antol et al, 2015) (code provided by (Lu et al,2015)), which achieves an accuracy of 54.13% onthe VQA validation set.
It is a two channel model?
one channel processes the image (using Convolu-tional Neural Network (CNN) to extract image fea-tures) and the other channel processes the question(using Long Short-Term Memory (LSTM) recurrentneural network to obtain question embedding).
Theimage and question features obtained from the twochannels are combined and passed through a fullyconnected (FC) layer to obtain a softmax distribu-tion over the space of answers.CNN + LSTM based model with-attention(ATT): We use the top-entry on the VQA challengeleaderboard (as of June 03, 2016) (Lu et al, 2016),which achieves an accuracy of 57.02% on the VQAvalidation set.2 This model jointly reasons about im-age and question attention, in a hierarchical fashion.The attended image and question features obtainedfrom different levels of the hierarchy are combinedand passed through a FC layer to obtain a softmaxdistribution over the space of answers.VQA Challenge 2016 winning entry (MCB):This is the multimodal compact bilinear (mcb) pool-ing model from (Fukui et al, 2016) which won thereal image track of the VQA Challenge 2016.
Thismodel achieves an accuracy of 60.36% on the VQAvalidation set.3 In this model, multimodal compactbilinear pooling is used to predict attention over im-age features and also to combine the attended imagefeatures with the question features.
These combinedfeatures are passed through a FC layer to obtain asoftmax distribution over the space of answers.2Code available at https://github.com/jiasenlu/HieCoAttenVQA3Code available at https://github.com/akirafukui/vqa-mcb19563.1 Generalization to novel instancesDo VQA models make mistakes because test in-stances are too different from training ones?
To an-alyze the first type of novelty (the test QI pair isnovel), we measure the correlation between test ac-curacy and distance of test QI pairs from its k near-est neighbor (k-NN) training QI pairs.
For eachtest QI pair we find its k-NNs in the training setand compute the average distance between the testQI pair and its k-NNs.
The k-NNs are computedin the space of combined image + question embed-ding (just before passing through FC layer) for allthe three models (using euclidean distance metric forthe CNN+LSTM model and cosine distance metricfor the ATT and MCB models).The correlation between accuracy and averagedistance is significant (-0.41 at k=504 for theCNN+LSTM model and -0.42 at k=155 for theATT model).
A high negative correlation value tellsthat the model is less likely to predict correct an-swers for test QI pairs which are not very similarto training QI pairs, suggesting that the model isnot very good at generalizing to novel test QI pairs.The correlation between accuracy and average dis-tance is not significant for the MCB model (-0.14 atk=16) suggesting that MCB is better at generalizingto novel test QI pairs.We also found that 67.5% of mistakes made by theCNN+LSTM model can be successfully predictedby checking distance of test QI pair from its k-NNtraining QI pairs (66.7% for the ATT model, 55.08%for the MCB model).
Thus, this analysis not onlyexposes a reason for mistakes made by VQA mod-els, but also allows us to build human-like modelsthat can predict their own oncoming failures, andpotentially refuse to answer questions that are ?toodifferent?
from ones seen in past.To analyze the second type of novelty (the answerrequired at test time is not familiar), we compute thecorrelation between test accuracy and the averagedistance of the test ground truth (GT) answer withGT answers of its k-NN training QI pairs.
The dis-tance between answers is computed in the space of4k=50 leads to highest correlation5k=15 leads to highest correlation6k=1 leads to highest correlationFigure 1: Examples from test set where theCNN+LSTM model makes mistakes and their cor-responding nearest neighbor training instances.
Seesupplementary for more examples.average Word2Vec (Mikolov et al, 2013) vectors ofanswers.
This correlation turns out to be quite high(-0.62) for both CNN+LSTM and ATT models andsignificant (-0.47) for the MCB model.
A high neg-ative correlation value tells that the model tends toregurgitate answers seen during training.These distance features are also good at pre-dicting failures ?
74.19% of failures can be pre-dicted by checking distance of test GT answerwith GT answers of its k-NN training QI pairs forCNN+LSTM model (75.41% for the ATT model,70.17% for the MCB model).
Note that unlike theprevious analysis, this analysis only explains fail-ures but cannot be used to predict failures (since ituses GT labels).
See Fig.
1 for qualitative examples.From Fig.
1 (row1) we can see that the test QIpair is semantically quite different from its k-NNtraining QI pairs ({1st, 2nd, 3rd}-NN distances are{15.05, 15.13, 15.17}, which are higher than thecorresponding distances averaged across all successcases: {8.74, 9.23, 9.50.
}), explaining the mistake.Row2 shows an example where the model has seenthe same question in the training set (test QI pair issemantically similar to training QI pairs) but, since ithas not seen ?green cone?
for training instances (an-swers seen during training are different from whatneeds to be produced for the test QI pair), it is unableto answer the test QI pair correctly.
This shows thatcurrent models lack compositionality: the ability tocombine the concepts of ?cone?
and ?green?
(bothof which have been seen in training set) to answer?green cone?
for the test QI pair.
This composition-ality is desirable and central to intelligence.1957Figure 2: X-axis shows length of partial question (in %)fed as input.
Y-axis shows percentage of questions forwhich responses of these partial questions are the sameas full questions and VQA accuracy of partial questions.3.2 Complete question understandingWe feed partial questions of increasing lengths(from 0-100% of question from left to right).
Wethen compute what percentage of responses do notchange when more and more words are fed.Fig.
2 shows the test accuracy and percentage ofquestions for which responses remain same (com-pared to entire question) as a function of partialquestion length.
We can see that for 40% of thequestions, the CNN+LSTM model seems to haveconverged on a predicted answer after ?listening?
tojust half the question.
This shows that the modelis listening to first few words of the question morethan the words towards the end.
Also, the model has68% of the final accuracy (54%) when making pre-dictions based on half the original question.
Whenmaking predictions just based on the image, the ac-curacy of the model is 24%.
The ATT model seemsto have converged on a predicted answer after listen-ing to just half the question more often (49% of thetime), achieving 74% of the final accuracy (57%).The MCB model converges on a predicted answerafter listening to just half the question 45% of thetime, achieving 67% of the final accuracy (60%).See Fig.
3 for qualitative examples.We also analyze the change in responses of themodel?s predictions (see Fig.
4), when words of aparticular part-of-the-speech (POS) tag are droppedfrom the question.
The experimental results indi-cate that wh-words effect the model?s decisions themost (most of the responses get changed on drop-ping these words from the question), and that pro-nouns effect the model?s decisions the least.Figure 3: Examples where the CNN+LSTM model doesnot change its answer after first few question words.
Ondoing so, it is correct for some cases (the extreme left ex-ample) and incorrect for other cases (the remaining threeexamples).
See supplementary for more examples.Figure 4: Percentage of questions for which responsesremain same (compared to entire question) as a functionof POS tags dropped from the question.3.3 Complete image understandingDoes a VQA model really ?look?
at the image?
Toanalyze this, we compute the percentage of the time(say X) the response does not change across im-ages (e.g.,, answer for all images is ?2?)
for a givenquestion (e.g., ?How many zebras??)
and plot his-togram of X across questions (see Fig.
5).
We dothis analysis for questions occurring for atleast 25images in the VQA validation set, resulting in to-tal 263 questions.
The cumulative plot indicates thatfor 56% questions, the CNN+LSTM model outputsthe same answer for at least half the images.
This isfairly high, suggesting that the model is picking thesame answer no matter what the image is.
Promis-ingly, the ATT and MCB models (that do not workwith a holistic entire-image representation and pur-portedly pay attention to specific spatial regions inan image) produce the same response for at least halfthe images for fewer questions (42% for the ATTmodel, 40% for the MCB model).Interestingly, the average accuracy (see the VQAaccuracy plots in Fig.
5) for questions for whichthe models produce same response for >50% and<55% of the images is 56% for the CNN+LSTM1958Figure 5: Histogram of percentage of images for whichmodel produces same answer for a given question andits comparison with test accuracy.
The cumulative plotshows the % of questions for which model produces sameanswer for atleast x % of images.model (60% for the ATT model, 73% for the MCBmodel) which is more than the respective averageaccuracy on the entire VQA validation set (54.13%for the CNN+LSTM model, 57.02% for the ATTmodel, 60.36% for the MCB model).
Thus, pro-ducing the same response across images seems to bestatistically favorable.
Fig.
6 shows examples wherethe CNN+LSTM model predicts the same responseacross images for a given question.
The first rowshows examples where the model makes errors onseveral images by predicting the same answer for allimages.
The second row shows examples where themodel is always correct even if it predicts the sameanswer across images.
This is so because questionssuch as ?What covers the ground??
are asked foran image in the VQA dataset only when ground iscovered with snow (because subjects were lookingat the image while asking questions about it).
Thus,this analysis exposes label biases in the dataset.
La-bel biases (in particular, for ?yes/no?
questions) havealso been reported in (Zhang et al, 2016).4 ConclusionWe develop novel techniques to characterize the be-havior of VQA models, as a first step towards under-standing these models, meaningfully comparing thestrengths and weaknesses of different models, devel-oping insights into their failure modes, and identify-ing the most fruitful directions for progress.
Our be-havior analysis reveals that despite recent progress,today?s VQA models are ?myopic?
(tend to fail onsufficiently novel instances), often ?jump to conclu-sions?
(converge on a predicted answer after ?listen-ing?
to just half the question), and are ?stubborn?Figure 6: Examples where the predicted answers do notchange across images for a given question.
See supple-mentary for more examples.
(do not change their answers across images), withattention based models being less ?stubborn?
thannon-attention based models.As a final thought, we note that the somewhatpathological behaviors exposed in the paper are insome sense ?correct?
given the model architecturesand the dataset being trained on.
Ignoring optimiza-tion error, the maximum-likelihood training objec-tive is clearly intended to capture statistics of thedataset.
Our motive is simply to better understandcurrent generation models via their behaviors, anduse these observations to guide future choices ?
dowe need novel model classes?
or dataset with dif-ferent biases?
etc.
Finally, it should be clear thatour use of anthropomorphic adjectives such as ?stub-born?, ?myopic?
etc.
is purely for pedagogical rea-sons ?
to easily communicate our observations to ourreaders.
No claims are being made about today?sVQA models being human-like.AcknowledgementsWe would like to thank the EMNLP reviewers forvaluable feedback and Yash Goyal for sharing hiscode.
This work was supported in part by: NSFCAREER awards, ARO YIP awards, ICTAS JuniorFaculty awards, Google Faculty Research awards,awarded to both DB and DP, ONR grant N00014-14-1-0679, AWS in Education Research grant, NVIDIAGPU donation, awarded to DB, Paul G. AllenFamily Foundation Allen Distinguished Investiga-tor award, ONR YIP and Alfred P. Sloan Fellow-ship, awarded to DP.
The views and conclusionscontained herein are those of the authors and shouldnot be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed orimplied, of the U.S. Government or any sponsor.1959References[Andreas et al2016a] Jacob Andreas, Marcus Rohrbach,Trevor Darrell, and Dan Klein.
2016a.
Deep com-positional question answering with neural module net-works.
In CVPR.
1[Andreas et al2016b] Jacob Andreas, Marcus Rohrbach,Trevor Darrell, and Dan Klein.
2016b.
Learning tocompose neural networks for question answering.
InNAACL.
1[Antol et al2015] Stanislaw Antol, Aishwarya Agrawal,Jiasen Lu, Margaret Mitchell, Dhruv Batra,C.
Lawrence Zitnick, and Devi Parikh.
2015.Vqa: Visual question answering.
In ICCV.
1, 2[Chen et al2015] Kan Chen, Jiang Wang, Liang-ChiehChen, Haoyuan Gao, Wei Xu, and Ram Nevatia.2015.
ABC-CNN: an attention based convolutionalneural network for visual question answering.
CoRR,abs/1511.05960.
1[Fukui et al2016] Akira Fukui, Dong Huk Park, DaylenYang, Anna Rohrbach, Trevor Darrell, and MarcusRohrbach.
2016.
Multimodal compact bilinear pool-ing for visual question answering and visual ground-ing.
In EMNLP.
1, 2[Geman et al2014] Donald Geman, Stuart Geman, NeilHallonquist, and Laurent Younes.
2014.
A Visual Tur-ing Test for Computer Vision Systems.
In PNAS.
1[Hoiem et al2012] Derek Hoiem, Yodsawalai Chod-pathumwan, and Qieyun Dai.
2012.
Diagnosing errorin object detectors.
In ECCV.
2[Ilievski et al2016] Ilija Ilievski, Shuicheng Yan, andJiashi Feng.
2016.
A focused dynamic atten-tion model for visual question answering.
CoRR,abs/1604.01485.
1[Jiang et al2015] Aiwen Jiang, Fang Wang, Fatih Porikli,and Yi Li.
2015.
Compositional memory for visualquestion answering.
CoRR, abs/1511.05676.
1[Kafle and Kanan2016] Kushal Kafle and ChristopherKanan.
2016.
Answer-type prediction for visual ques-tion answering.
In CVPR.
1[Karpathy et al2016] Andrej Karpathy, Justin Johnson,and Fei-Fei Li.
2016.
Visualizing and understandingrecurrent networks.
In ICLR Workshop.
1[Kim et al2016] Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-WooHa, and Byoung-Tak Zhang.
2016.
Multimodal resid-ual learning for visual QA.
In NIPS.
1[Lu et al2015] Jiasen Lu, Xiao Lin, Dhruv Batra,and Devi Parikh.
2015.
Deeper lstm and nor-malized cnn visual question answering model.https://github.com/VT-vision-lab/VQA_LSTM_CNN.
1, 2[Lu et al2016] Jiasen Lu, Jianwei Yang, Dhruv Batra, andDevi Parikh.
2016.
Hierarchical question-image co-attention for visual question answering.
In NIPS.
1,2[Malinowski and Fritz2014] Mateusz Malinowski andMario Fritz.
2014.
A Multi-World Approach toQuestion Answering about Real-World Scenes basedon Uncertain Input.
In NIPS.
1[Mikolov et al2013] Tomas Mikolov, Kai Chen, GregCorrado, and Jeffrey Dean.
2013.
Efficient estima-tion of word representations in vector space.
In ICLR.3[Noh and Han2016] Hyeonwoo Noh and Bohyung Han.2016.
Training recurrent answering units with jointloss minimization for vqa.
CoRR, abs/1606.03647.
1[Saito et al2016] Kuniaki Saito, Andrew Shin, Yoshi-taka Ushiku, and Tatsuya Harada.
2016.
Dualnet:Domain-invariant network for visual question answer-ing.
CoRR, abs/1606.06108.
1[Shih et al2016] Kevin J. Shih, Saurabh Singh, and DerekHoiem.
2016.
Where to look: Focus regions for visualquestion answering.
In CVPR.
1[Wang et al2015] Peng Wang, Qi Wu, Chunhua Shen,Anton van den Hengel, and Anthony R. Dick.
2015.Explicit knowledge-based reasoning for visual ques-tion answering.
CoRR, abs/1511.02570.
1[Wu et al2016] Qi Wu, Peng Wang, Chunhua Shen, An-ton van den Hengel, and Anthony R. Dick.
2016.Ask me anything: Free-form visual question answer-ing based on knowledge from external sources.
InCVPR.
1[Xiong et al2016] Caiming Xiong, Stephen Merity, andRichard Socher.
2016.
Dynamic memory networksfor visual and textual question answering.
In ICML.
1[Xu and Saenko2016] Huijuan Xu and Kate Saenko.2016.
Ask, attend and answer: Exploring question-guided spatial attention for visual question answering.In ECCV.
1[Yang et al2016] Zichao Yang, Xiaodong He, JianfengGao, Li Deng, and Alexander J. Smola.
2016.
Stackedattention networks for image question answering.
InCVPR.
1, 2[Zhang et al2016] Peng Zhang, Yash Goyal, DouglasSummers-Stay, Dhruv Batra, and Devi Parikh.
2016.Yin and Yang: Balancing and answering binary visualquestions.
In CVPR.
5[Zhou et al2015] Bolei Zhou, Yuandong Tian, SainbayarSukhbaatar, Arthur Szlam, and Rob Fergus.
2015.Simple baseline for visual question answering.
CoRR,abs/1512.02167.
11960
