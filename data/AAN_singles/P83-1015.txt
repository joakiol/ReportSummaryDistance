On the Mathematical Properties of Linguistic TheoriesC.
tgm.lm~nd Pev-r=uttDept.
of Computer ScienceUntvermty of TorontoToronto, Ontario, Canada M5S IA4ASS~ACTMeta-theoretical results on the decidability, genera-tire capacity, and recognition complexity o~ several syn-tactic theories are surveyed These include context-freegrammars,  t ransformat ional  grammars, lexical func-t ional grammars ,  general ized phrase s t ructure  gram-mars, and tree adjunct grammars.i.
lmt~t iom.The development of new formalisms im which toexpress linguistic theories has been accompanied, at\[east since Chomsky and Miller's early work on context-free languages, by the study of the i r  nets - theory .
In par-tioular, numerous  results on the decidabttity, generativecapacity, and more recent ly  the co~aplexity of recogni-t ion of these formal isms have been publ ished (andrumoured!).
Strangely enough, much  less attentionseems to have been devoted to a discussion of thesignificance of these mathematical results.
As a prelim-tnary to the panel on formal properties which will addressthe signif icance issUe, it seemed appropriate to surveythe existing results.
Such is the modest gee\[ of thispaper.We wdt consider context-tree languages, transforma-tional grammars, lexicat functional grammars, general-ized phrase structure grammars, and tree adjunct gram-mars.
Although we will not examme them here, formalstudies of other syntactic theomes have been under-taken: e.g.
Warren \[51\] ~or Montague's PTQ \[30\], and Bor-gida \[71 for the stratifications\[ g rammars  Of Lamb \[25\].There follows a brief summary  of some comments  in theRterature about related empirical issues, but we avoidentirely the issue of whether one theory is more descrip-tively adequate than another.Z.
P,-elimLuary Def i l~onsWe assume the reader is familiar with the basicdefinitions of regular, context-tree (CF), context-sensitive~CS), recurstve, and recursively enumerable (r.e.
)languages and with their accepters as car~ be \[ound tn\[':-_\],Some elementary definitions \[rom complexity theorymay be useful.
?urt.her details may  be found tn \[2\]Complexity theory is the study of the resources requiredof algorithms, usuai~y space and time.
Let e(z) be a ~une-Lion, say the recognition Junction \[or a language i. Themost inter~t!n~ results we could obtain about )'would bea ~o~JeT bo%znd on ~he resources needed to compute f on amac~hine of a gLven architecture, say a yon NeumannThis research was sponsored by the National Science andEngineering Research Council of Canada under GrantAg2Rs.computer or a parallel array of neurons.
These resultsover whole classes of machines are very difficult toobtain, and none el any significance exist for parsiD.gproblems.Restricting ourselves to a specific machine modeland an algorithm M for j', we can ask about the cost.
(e.gtime or space) e(z) of executing M on a specific input z.~ Ica l ly  c is too flne-gra/ned to be useful: what one stu-dies instead ts a functio~ c w whose argument is a~.L'Iteger n denoting the s/zs of the input to !4, and whichgives some measure of the cost of processing inputs oflength n. Complexity theorists have been most interestedg% the a.~\]m~i~ot.ie behavlour of c~v, i.e.
the behaviour ofc~ as n gets \[alge.
:f one is interested in Upper bo~n~S on the behavlot:-of M, one usuai\[y defines c. (n) as the m.a:ru-n.um, of c(=over all inputs z of size n.~his is called the worst-caseconvexity hJumct/on for .&'.
Notice that other de~rutlon:are possible: one could define the expected eomp\[exity~otion c,(n) for /v/as the average of c(=) over all LnpuL.-.of length ~%.
c might be more useful than c~ if one hadan ~dea of what the distribution of ~nputs to M could be.Unfortunately, the introduction of probabiJistic con-siderations makes the study of expected complex:It)techmcally more difficult that of worst case comp\[exity?or a given problem, expected and worst case measuresmay be quite ditTerent.it is quite dlfTieult to get detailed descriptions ot c~and for many  purposes a cruder estimate ts sufficient.~"~'le next abstraction involves "lumping" classes of c wfunctions into simpler ones that more clearly demon-str~te their ~symptottc behavlour and are easier to mani-pulate.
This is the purpose of O-no~.Oon.
Let f(n) andg(n) be two ~ui%cttons.
\], ts said to be O(g) ,.f a constantmultiple of ~ is an upper bound for f, ~or ~tl\[ but a finitenumber  o~ values of n..~\[ore precisely, f ts O(g) ff ~here ,sare constants K and n O such that ~or all ~%>~e \],(n) <K'y('.
'~).Given an algorithn~.
M, we will say that tts '.verst-casetime complexity ts O(g) if the worst-ease time cost func-tion cw(.n ) :or M is O(g) Notice that this merely says thatalmost all ~nputs to M of s,ze n can be processed in timeat most a constant times g(n).
It does nat ~ay that alJLnputs requLre g\[~%) time, or even that any do even on M,let alne on any other machine that Lmp\[ements \],.
Also,if two algorithms A\] and A 2 are avaHab\[e for a function\]'.and \[\[ their worst-case complexity can be given respec-tively as OE, gl) and O(g~), and g2 < g2' tt may  still.be thecase that for a large number  of cases (maybe even for allcases one is likely to encounter in practice) that A 2 willbe the preferable algorithm, simply because the constantK!
for g!
may be much smaller than Kg for .q 8.98In examining known results about the recognitioncomplexity of various theories, it is useful to considerhow "robust" they are in the face of changes in themachine model from which they were derived.
Thesemodels can be divided into two classes: sequent ia l  modelsand paral lel  models.
Sequential  models \[2\] include thefamil iar single- and mult i - tape Turing Machines (TMs) aswell as Random Access Machines (RAMs) and Random/%:ces~ Stored PrograznMaehines (RASPs).
A RAM is Like aTM except that its working menory is random accessrather than sequential.
A RASP is like a RAM but storesits program in its memory.
Of all these models, it is mostlike a yon Neumann computer.All these sequential models can simulate each otherin ways that do not cause great changes in time complex-ity.
For example, a e-tape Turing Machine that runs intime O(t) can be simulated by a RAM in time O(t).
andconversely, a RAM runmng in O(t) can be simulated by ae-tape TM in time O(t~).
In fact, all familiar sequentialmodels are poIFnonm~Uy relate& they can su-nutate eachother with at most a polynomial toss in efficiency.Thus if a syntactic model is known to have a difficultrecognition problem on one sequential  model, then it willnot have a much easier one on another.TransforlTting a sequential algorithm to one on aparallel machine with a fixed number  Kof processors pro-rides at most a factor K improvement in speed.
Moreinteresting results are obtained when the number  of pro-cessors is allowed to grow with the size of the problem,e.g.
with the length of the string to be parsed.
If we viewthese processors as connected together in a circuit, vathinputs values entering at one end and outputs being pro-duced at the other, then a problem that has a solution ona ssq~ential machme in polynomial time and in space sw111 have a solution on a paraLLeL machine with a polyno-mial number  of processors and ci~-c~ da-ptA (or max-Lmum number  of processors data must  be passed throughfrom input to output) O(s 2) .
Since the depth of a parallelcircuit corresponds to the (parallel) ~/~te required tocomplete the computation, this means that a\[gorlthmswith sequential solutions requiring small space (such asdeterrnimstic CSLs) have fast parallel solutions.
For acomprehensive survey of parallel computation, seeCook\[9\].3.
Context-Free languages.Recognition techmques for context-free languagesare well-known ~3\].
The so-called "CK~ ~' or "dTnarmc pro-gramming" method is attributed by Hays \[~-51 to J Cocke,and Lt was discovered mdependentLy by Kasami ~5~.\] andYounger \[53\] who showed it to be O(nJ).
It requires thegrarm-nar to be in Chomsky Normal Form, and putting anarbitrary grammar  in CNF may square the size of thegrammar.Ear\[ey's algorithm recognizes strings in arbitraryCFGs in tlme O(n 3) and space O(rt2), and in time O(n 2) forunambiguous CF'Gs.
Graham, Harrison and Ruzzo \[/3\]glve an algorithm that tlnifies ~ and Ear{ey's \[/0\] algo-rithm, and discuss implementation details.Valiant \[50\] showed how to Interpret the Ck'Y algo-rithm as the finding of the transitive closure of a matrixand thus reduced CF recognition to matrix multiphca-tion, for which sub-cubic aJgorithms exist.
Because ofthe enormous constants of proport,onality associatedwith thls method, it is not likely to be of much practicaluse, either an implementation method or as a descrtp-tlon of the function of the brain.Ruzzo \[55\] has shown how CFLs can be recognized byboolean c ircuits  of depth O(Log(n)2), and thus that  paral-lel recognition can be done in time O(log(~)e).
Therequired circuit has size polynomial in ~.So as not to get mystified by the uppe~- bs~nW2 on CFrecogmtion, it is useful to remember  that no known CFLrequires more than linear time, nor is there a (non-constructive) proof of the existence of such a larg "-.=~.For an empirical comparison of various parsingmethods, see Slocum \[44\].4.
Tran~ormat ional  Gram.mr.From its earliest days, discussions of transforma-t/onal g rammar  (TG) have included mention of matterscomputational.Peters and Ritchie \[3S\] provided the first non-trivialresults on the generative power of TGs.
Their modelreflects the "Aspects" version quite closely, includingt ransformat ions that  could move and add const i tuentsand delete them subject to recoverability.
All transforma-tions are obligatory, and applied cycl)cally from the bot-tom up.
They show that every recursively enumerable(re.)
set can be generated by a TC using a conte?t-sensitive base.
The proof ts quite simple: the right-handsides of the type-0 rules that generate the r.e.
set arepadded with a new "blank" symbol to make them at leastas long as their left-hand sides.
Rules are added to allowthe blank symbols to commute  with all others.
Thesecontext-sensitive rules are then used as the base of a T0whose only transformation deletes the blank symbols.Thus if the transformational formalism itself is sup-posed to cA~te~'tze the grammatical strings of possiblenatural languages, then the only languages beingexcluded are those which are not enurnerabie under an~\]model of computation.At the expense of a considerably more intricateargu_rnest, the previous result can be strengthened \[32\]to show that every r.e.
set can be generated by acontext-free 5used TG, as long as a ~Iter (intersectionwith a regular set) can be applied to the phrase-markersoutput by the transformations.
In fact, the base gram-mar  can be 4,ndependent of the language being generated.The proof involves simulating a TM by a TG.
The transfor-mations first generate an "input tape" for the TM beingsimulated, and then apply the TM productions, one percycle of the grammar.
The filter insures that the basegrammar  generated just as many S nodes as necessary togenerate the input string and do the simulation.
Again, ifthe transformational formalism is supposed to character-ize the Dossibie natural languages, then the UniversalBase HYl~)th.esis \[31\] according to which all natural\[anguages can be generated from the same base gram-mar  ks empirically vacuous: an?#, recurs\[rely enumerablelanguage can.
:Teverai attempts were then made to find a restrictedform of the transformational model that was descmp-tively adequate arld yet whose generated languages arerecurslve (see e.g.
\[271).
Since a key part of the proof in\[32\] involves the use of a filter on the final derivationtrees, Feters and Ritchie examined the consequences offorbidding fi/%al filtering \[35\].
They show that if S is theonly recursive symbol in the CF base then the generatedlanguage L is predict~bL U en~zrte~-~bLe and ez'pone'rLtZalL.
Nbo~ndec?
A language L \[s pred ic tab ly  enunlerable if thereis an "easily" computable function t(n) that gives anupper bound on the number  of tape squares needed by itsenumerating TM to enumerate the first n elements of L.L is exponent/aUy bounded if there is a constant K suchthat for every string z in L there is another string z' in Lwhose length is at most Kt imes the length of z.99The class of non-filtering languages is quite unusual,including all the CFLs (obviously), but also some (but notall) Cb-l~s, some (but not all) reeursive languages, andsome (but not all) r.e.
languages.The source o~ non-recursivtty in transformational\[ygenerated languages is that transformations can deletearbitrarily large parts of the tree, thus producing surfacetrees arbitrarily smaller than the deep structure treesthey were derived from.
This ts what Chomsk'y's recover-ability of deletions condition was meant  to avoid.
In histhesis, Petrick \[36\] de6nes the following term~sal-\[em&d.h-i~cr,=a-inE condition on transformational deriva-tions: consider the followi~g two p-markers from aderivation, where the right one is derived from the leftone by applying the cycle of transformations to subtree cproducing the subtree z~ rs sContmuing the derivation, apply the cycle to tree t yield-ing tree ~.s $cycle 2A derivation satisfies the terminal-length-increasing con-dition if the yield of I~ is always \[ortger than the yield ofPetrick shows that if all recursion in the base"passes through S" and if all derivations satisfy theterrninal-\[ength-mcreasing condition, then the generatedlanguage is recursive.
Using a slightly more restrictedmodel of transformations, Rounds \[42\] strengthens thisresult by showing that the resulting languages are in factcontext-se nsitive.|n an unpublished paper, Myhill shows that Lf thecondition is weakened to terrnlnal-length-non-decreasing,then the resulting languages can be recognized in spaceat most ez-po~ent/o\], Ln the length of the input.
Thisimplies that the recognition can be done ~n at mostdouble-exponential time, but Rounds \[.~\] shows that notonly can recognition be done in ez-ponevtt/a/ t/raze, butthat every language recognizable in exponential time canbe generated by a TG satisfylng the terminal-length-non-decreasing condition and recoverability of deletionsThis Is a very stron~ resu\].t, because of the closureproperties of the class of exponential-time \[a_r~uages.
Tosee why this LS SO requires a ~ew more deflnitionsLet P be the class o~ all languages that can be recog-cuzed Ln polynomlaJ time on a deterministic TM, and NPthe class of all languages that can be recognized in poly-nnmiai time on a non-deterministic T~\[ P \[s obviouslycontained in NP, but the converse is not known, althoughthere is much evidence that is false.There is a class of problems, the so-called NP-complete problems, which are in NP and "as di~icuit" asany prob'..em in NP m the followLn~ sense: if czn!\] of themcould be shown to be m P, then art the problems m NPwould also be in P. One way to show thaL a language L LsNP-complete \[s to show that L is in NP and that everyother lar~uage L o in NP can be pol~omi~l ly  tr-allSfOlrl0sedinto L, i.e.
that there is a deterministic TM, operating inpolynomial time', that will transform an input tu to L intoan input %u 0 to L 0 such that m is in L if and only tu 0 ts LnL O.
In practice, to show that a \[an@uage is NP-complete,one shows that it ~s in NP, and that some already-knownNP-complete language can be polynomially transformedto it.All the known NP-comp\[et.e languages can be recog-nized in exponential time on a deterministic machine,and none are known to have sub-exponential solutions.Thus sint:e the restricted transformational languages ofRounds characterize the exponential languages, then i\[all of them were to be in P, then P would be equal to NPPutting it another way, i\[P is not equal to NP, then sometransformational languages (even those sat,sfytng theterrnlnal-le ng th-non-incre asin~ condition) have ~"tractable" (i.e.
polynomial time) recognition pro~ "cu::,son any deterministic TM.
Note that this result also holctsfor all the other ki%own sequential models of computa-tion, and even for parallel machines wlth as many as apoL%pto,rt/at number  o\[ processors.5.
L=xical FUnctional Grammar ,in part, transformational g rammar  seeks to accountfor a range of constraints or dependencles wtthm sen-tences.
Of particular interest are subcategorlzationdependencies and predicate-argunlent dependencies.These dependencies can hold over arbitrarily large dis-tances ~everal recent theories su~o=est difTerent u/ays ofaccounting for these dependencies, but without makinguse of transformations.
We will exa/'nine three o~ these,Lexica\[ Functional Grammar,  Generalized Phrase ~truc-ture Grammar,  and Tree Adjunct Grammars, m ttte nextfew sections.Lexica\[ Functional Grammar  ~LFG) of gap\[an andBresnan \[24\] aims to provtd~ a descriptively adequatesyrttactic formalism wlthout transformations.
All thework done by transformations is instead encoded tnstructures in the \[exlcon and in \[inks establishedbetween nodes in the constituent structureLFG languages are CS and properly include the CFLs\[2~\].
Berwlck \[5\] shows that a set of strings whose recog-nition problem is known to be NP-compIete, namely theset o\[ satisQable boolean formulas, Ls an LFG l&\[~uage.Therefore, as was the case for Rounds's restrlcted class ofTGs, tf P Ls not equal to NP, then some languages ~en-erated by \[-~s do not have polynomial t~me recognitionalgorithms indeed only quite "baste" parts of the LFGmecharusm are necessary to the reduction.
Thisincludes mechanlsms necessary for feature agreement,for forcing verbs to take certain cases, and \[exlcal ambt-==uity Thus no s,mp\[e chan~e to the formalism is likelyto avoid the combinatorlai consequences of the ~ullmechan ismBerw1ek has also examined the relation between LFGand the class of languages generated by iIldexed gram~t-\[I\], a class kllown to be a proper subset of the C~Ls,but including some NP-complete languages \[42\] Heela/ms (personal communication) that the indexedlanguages are a proper subset of the LFG languages.6.
General ized Phrase  Structure Grammar.In a series of papers, Gerald Gazdar and his col-leagues \[" l\] have argued for a joint account of the syntaxand semantics o\[ En~hsh like LFG in eschewing the use oftrans,formations but unlike it in positing, only one level oftO0syntactic description.
The syntactic apparatus is basedon a non-standard interpretation of phrase-structurerules and on the use of meta-rules.
The formal conse-quences of both these moves %ave been investigated.6.
I. No~ A~MmsmbtlityThere are two ways of interpreting the function of CFrules.
The first, and most usual, is as rules for ,-e~,T,3b/~gstrings.
Derivation trees can then be seen as canonicalrepresentatives of classes of derivations producing thes~me string, and di~lering only in the order  of applicationo~ the same productions.The second interpretation of CF rules is as con-straimts on derivation trees: a legal derivation tree is:he where each node is "admitted" by a rule, i.e.
eachnode dormnates a sequence of nodes in a way sanctionedby a rule.
For CF rules, the two interpretations obviouslygenerate the same strings ~,~ the same set of trees.Following a suggestion of McCawley's, Peters and~,itchle \[34\] showed that if one considered context-se~s~.ve rules from the node-admissibility point of view,the languages defined were still CF  Thus the use of CSrules in the base to impose sub-categorization restric-t/oRs, for example, does not increase the weak generatlvecapacity of the base component.
(For some different res-trictions of context-sensitive rules that guarantee thatonly CFLs will be generated, see Baker \[~:\].
)Rounds \[40\] gives a simpler proof of Peters and?,itchie's node-adrnisstbility result using the techniquesfrom tree-automata theory, a generalization to trees offmlte state automata theory for strings.
Just as a 0_rotestate automaton (FSA) accepts a strong by reading it onecharacter at a time, changing its state at each transi-tion, a finite state tree automaton (FETA) traverses trees,propagating states.
The top-dowln F~TA "attaches" a start-ing state (from a flnite set) to the root o\[ the tree.
Tran-sltions are  allowed by productions of the form(q, ,., ~)--> (q, .
.
.
.
.
~, .
.
)such that if state q is being applied to a node Labelledand dominatmg n descendants, then state ~i should beapplied to its ~th descendant.
Acceptance occurs if all\[eaves of the tree end up labelled with states in theaccepting subset.
The bottom-up Fs"rA is similar: start-\[ng states are attached to the \[eaves of the tree and theproductions are of the form(=, ~, q~ ..... q~)-> qindicating that if a node labelled a dommating n descen-dants each labelled wlth states ql to q,v then node a getslabelled ~th  state q..Acceptance occurs when the root islabelled by a state from the subset o\[ accepting states..As is the ease ~th  FSAs, F~TAs of both flavours canbe either deterministic or non-deterministic.
A set oftrees i~ sa~d to be recognizable if it is accepted by a non-deterministic bottom-up Fb-TA.
Again as with FSAs, anyset o~ trees accepted by a non-determlmstic bottom-up~A t.~ accepted by a deterministic bottom-up ~,~TA, butthe re:~ult does not hold for top-down F'5"FA.
although therecognizable sets are exactly the languages retognizedby non-determinlstic top-down FSTAs.A set of trees is local if it is the set of demvationtrees of a CF grammar  Clearly, every local set }s recog-nizable by a one-state bottom-up F~A that checks ateach node that it satis6es a CF production.
Also, theyield of a recogmzable set ol trees (the set of strings itgeneretes) is CF..4/though not all recognizable sets arelocal, hey can all be mapped into local sets by a simple~homo~norphic) mapping.Rounds's proof !41\] that CS rules under node-adnussibility generate only CFLs involves showing thatthe set of trees accepted by the rules is recognizable,i.e.
that there is a non-deterministic bottom-up FSTA thatcan check at each node that some node-admissibili~:ycondition holds there.
This requires checking that the"strictly context-free" part of the rule holds, and thatsome proper analysis o\[ the tree passing thr~"g '_ thenode ~atisfies the "context-sensitive" part of the rule.The ditlieulty comes h'om the fact that the bottom-up automaton cannot generate the set of proper ana-lyses, but must  instead propagate (in its state set) theproper analysis conditions necessary to "admit" thenodes of its subtrees.
It must, of course, also check thatthose rules get satisfied.A more intuitive proof using tree tr~nsduce~rs a wellas FSTAs ,s sketched inthe Appendix.Joshi and Levy \[21\]  strengthened Peters andRitchie's result by showing that the node admissibilityconditions could also include arbitrary Boolean combina-tions of ~mance conditions: a node could specify abounded set of \[abels that must occur immediately aboveit along a path to the root, or un~r'nediate\[y below it on apath to the frontier.In general the CF grammars  constructed \[n .theproof of weak equivalence to the CS grammars  undernode admissibility are much larger than the original, andnot useful for practical recognition.
Joshi, Levy and Yueh\[22\], however, show how Eariey's a/gomthm can beextended to a parser that uses the local constraintsdirectly.8.2.
MetaruJes.The second important mechan ism used by Gazdar\[ii\] is mp~es ,  or rules that apply to rules to produceother rules.
Using standard notation for CF rules, oneexample of a metarule that could replace the transforma-tion k~lown as "particle movement"  is:V--> VN Pt X ==> V--> VP~ ~-PRO\] XXhere  is a vamable behavmg like vamables in structuralanalyses of transformations.
If such vamables are res-tricted to being used as cbbTeviaticns, that is if they areonly allowed to range over a \]~n~te subset of strings overthe vocabulary, then closing the grammar  under themetarules produces only a 6nite set of derived rules, aridthus the generative power of the formalism is notincreased.
If, on the other hand, X is allowed to rangeover strings of unbounded length, as are the essential~ e s  of transformational theory, then the conse-quences are less clear.
It is well known, for example, thatI\[ the right-hand sides of phrase structure rules areallowed to be arbitrary regular expressions, then the gen-erated languages are still context-free.
Might somethinglike this not be happening wlth essential variables inmetarules?
It turns out not.The formal consequences of the presence of essen-tie/, variables in metarules depends on the preserice ofanother device, the so-called phantoms categories.
It maybe convenient in formulating metarules to allo~, in theleft-hand sides of rules, occurrences of syntacticcategories that are never introduced by the grammar,1.e that never appear m the mght-hand sldes of rules.
|nstandard CFLs, these are called %L.~eLess e?tego~es, andrules containing them can simply be dropped, with nochange Jngenerative capacity Not so ~th  metarules: itis possible for metarules to rewrite rules containingphantom categories into rules without them.
Such a dev-ice was proposed at one time as a ~ay to implement pas-tures in the GPSG framework.i01Uszkorelt and Peters \[49\] have shown that essentialvariables i.n metarules are powerful devices indeed: CFgrammars with metaru\[es that use at most one essentialvariable and allow phantom categories can generate allreeursively enumerab\[e sets.
Even if phantom categoriesare banned, as long as the use of at \[east one essentialvariab\[es \[s allowed, then some non-reeursive sets can begenerated .Possible restrictions on the use of metarules aresuggested in Oazdar and Pultum \[12\].
Sh ieber  et al\[45\]discuss some empirical consequences of these moves.7.
Tree Adjunct  ~ .The Tree Adjunct  Grammars  (TAGs) of Joshi  and  hisco l leagues  presents  a d i f ferent  way of account ing  for syn-tact ic  dependencies (\[17\], \[19\]).
A TAG conmsts of two(finite) sets of (finite) trees, the centre trees and thendjunet t rees .The cent re  t rees  cor respond to the  sur face  s t ruc -tu res  of the  "kerne l"  sentences  of the  languages .
Theroot  of the  ad junct  t rees  is label led with a non- te rmina lsymbo l  wh ich  also appears  e~cactiy once  on the  f ront ie r  ofthe  tree.
All o ther  f ront ie r  nodes are  labe l led wi th  ter rm-nai  symbols .
Der ivat ions in TAGs are  def ined by repeatedapp l i ca t ion  of the  operat ion  of ad|uneUon I~ c is a cent ret ree  conta in ing  an  occur rence  of a non- tern -ana l  ,4. and  ifis an  ad junct  t ree  whose root (and  one node n on  thef ronUer)  ;s label led ,4, then  the ad junct ion  of a to e is per-fo rmed by "detach ing"  f rom c the subt ree  ~ rooted  at A,a t tach ing  a \[n i ts  place, and  reat tach iug  t at  node ft.Ad junct ton  may then  be seen  as a t ree ana logue  of acontext-free dertvatlon for strings \[40\].
The string\[anguage.~ obtamed by taking the yletds of the treelanguages generated by TAGs are called Tree Adjunct~mgLu~es, or TALs.In TAGs all l ong-d i s tance  dependenc ies  are  the  resu l tof adjtmcttons separating nodes tb.~t at one point in thederivation were "cLose".
8oth crossing and non-crossingdepenctenctes can be represented \[).8\].
The formal pro-pert ies  of TAGs are  fully d i scussed  in \[30\].
\[52\], \ [~\ ] .
Ofparticular i n te res t  are the  ~ollo~ng.TALs properly contain the C~Ls ~nd are property con-rained \[n the indexed languages, which m turn are prop-erly contained m the CSLs.
Although the indexed{anguages contain NP-complete languages, TALs aremuch bet ter  behaved:  ~oshi and Yokomori  repor t  ~per-sonal eommunicationl an O(n ~) recognition a lgor i thmand conjecture that an O(n ~) bound may be possible.\[3.
A Pointer to \]~t~nl~lrieal DLseusmonsThe l i te ra ture  on the  emptmca\ [  i ssues  under iy iugthe formal results reported here ts not ex~enswe.Chomsky argues convincingly \[8\] that there  is noargument for natural languages neeess~.
'~l~j being recur-sive.
This, or course, is different from the possibdity that'~anguages are covtt~zgentty recurstve.
Putnam \[39\] givesthree reasons he claims "point in this direction": (i)'speaker~ can presumably classify sentences as accept-able or unacceptable, deviant or non-deviant, et cetera,wlthout reliance on extra-linguistic contexts.
There areof course exceptions to this rule ", (~) grammatical\[tyjudgements can be made for nonsense sentences, and \[S)grammars can be \[earned.
(e) and (S) are irrelevant and(i) contalns zts own counter-argument.Peters and Ritchie \[S~\] contains a suggestive buthardly open-and-shut case ~or contingent recurstvtty: (:)every TQ has an exponentially bounded cyehng ~unction,and thus generates only recurs\[re languages, (Z) everynatural }an?ua~e has a descriptively adequate TG, and (3)the comp\[exlty of \[anguages investigated so far ks typLca\[of the class.Hintikka\[16\] presents a very di\[~erent argumentagainst the recursivity of English based on the distribu-tion of the words ~r~y and evev-y.
I/is account of why JoA~\]cno~s e'ue'mjth?~g is grarnmatlcal whi\[e John ~c~o~s =~y-thing ks not is that =~y can appear only in contexts wherereplacing it by eve~ changes the meaning.
Taking mean-mg to be logical equivalence, this means that grammati-eality is dependent on the determination of logicalequivalence of logical formulas, an undecidable problem.Chomsk'y \[8\] argues that a simpler solution ks available,namely one that replaces logical equivalence by syntac-tic tdentLty of some kind of logical form.PuHum and Gazdar \[38\] \[s a thorough survey of, andargument against, published claims (mainly the "respec-tively" examples \[26\], Dutch cross-serial dependencies,and nominallzation in Mohawk \[,37\]) that some naturallanguages cannot be weakly generated by CF grammars.No cIalms are made about the strong adequacy of CFGs.9.
Seeking E~gnilleanee.When can the supporter of a weak (syntactic) formal-ism (i.e.
low recognition complexity, low gener.~tive capa-city) e\[alm that it superior to a competing more powerfulfo rmal i sm?Ling\[astir theories can differ along several dimen-sions, wtth generative capacity and recognition capacitybeing only two (albeit related) ones.
The evaluation musttake into consideration at \[east the fottovang others:Coverage.
Do the theories make the same ~rammat  otcal predictions ?Extensibdity.
The linguistic theory of which the syn-tactic theory ks a part will want to express well-formedness constraints other than syntactic ones Theseconstraints may be expressed over syntactic representa-tions, or over different representations, presumablyre la ted  to the syntactic ones.
One theory may make thisconnection possible when another does not.
This ofcourse underlies the arguments for strong desempttveadequacy,Also relevant here Ls how the tmguLstlc theory as awhole is decomposed.
The syntactm theory can obviouslybe made ampler by trans~ermng some of the explanatoryburden to a.nother constituent.
The c\[asmc e?amp\[e inprogramming languages is the constraint that all vam-ables must be declared before they are used.
This con-strain\[ cannot be Lmposed by a CFG but can be by anindexed grammar, at the cost of a d ramat ic  inc rease  inrecognltton complexity.
Typically, however, the require-ment  is slmply not cen~idered part of "syntax", whichthus remams CF, and imposed separately in this case,the overall recognitmn comp\[exlty remams ~ome low-order polynomial, Some arguments of this kind can befound m \[3t~\]Separating the eonstralnts into different sub-theome~ wlt\[ not tn general make the problem of recog-ntzmg strings that satisfy all the constraints any moreeHictent, but tt may  allow hailing the power of each con-stituent.
To take an e?treme example, every r.e.
setthe homomorphic  image of the intersectlon of \[~,)context-free languages,Implementation.
This Ls probably the most subtle s,~tof issues determining the sigmfieance of the \[orm,,lresults, and I don't claim to understand them.Comparison between theories requires agreementbetween the machine models used to derive the complex-ity results As mentioned above, the sequential modelsare all polynomtally related, and no problem not hawng a102polynomial time solution on a sequential machine islikely to have one on a parallel machine limited to atmost a polynomial number  o\[ processors, at least if P isnot equal to NP.
Both these results restrict the improve-ment  one can obtain by changing implementation, butare of little use in comparing algorithms of low complex-ity.
Berwick and Weinberg \[6\] give examples of how algo-rithms of low complexity may have different implementa-tions differing by large constant factors.
In particular,changes in the form of the grammar  and in its represen-tation may have this effect.But of more interest I believe is the fact that imple-mentation is often accompanied by some form ofresource limitation that has two effects.
First it is also achange in speeifieaZ~bn.
A context-free parser imple-mented with a bounded stack recognizes only a finite-state language.Second, very special implementations can be used ifone is willing to restrict the size of the probterrt to besolved, or even use special-purpose methods for limitedproblems.
Marcus's parser \[28\] with its bounded look-ahead is another good example.
Sentences parsable~nthin the allowed look-ahead have "quick" parses, butsome grammatical sentences, such as "garden path" sen-tences cannot be recognized without an extension to themechan ism that would distort the complexity measures.There is obviously much more of this story to betold.
Allow me to speculate as to how it might go.
We mayend up with a space of linguistic theories, differing in theidealization of the data they assume, in the way theydecompose constraints, and in the proceduralspecifications they postulate (I take it that two theoriesmay differ tn that the second simply provides more  detailthan the first as to how constraints specified by the firstare to be used.)
Our observations, in particular our meas-urements of necessary resources, are drawn from the"ultimate implementation", but this does not mean thatthe "ultimately low-level theory" is necessamly the mostreformative, witness many  examples in the physical sci-ences, or that less procedural theories are not usefulstepping stones to more procedural ones.It is also not clear that theories of different compu-tational power may not be useful as descriptions ofdifferent parts of the syntactic apparatus.
For example,tt may  be earner to learn statements o\[ constraintswithin the framework of a general machine.
The con-straints once learned might then be subjected totransformation to produce more ei=llcient special-purposeprocessors also imposing resource limitations.
Indeed,the "possible languages" of the future may be more com-plex than the present ones, just as earlier ones may havebeen syntactically s impler  Were ancient languages reg-ularOWhatever we decide to make of existing formalresults, Lt is clear that continumg contact with the com-plexity community is important.
The driving problemsthere are the P = NPquestion, the determination f lowerbounds, the study of time-space tradeot~s, and the com-plexity of parallel computations.
We still have somem~thodological house-cleaning to do, but I don't see howwe can avoid being affected by the outcome of theirinvestigations.~ITKNO~E~ENTSThanks to Bob Bet'wlck, Arawnd Joshi, Jim Hoover,and Stan Peters for their suggestions.APPE~IDIXRounds \[41\] proves that context-sensitive rulesunder node-admissibility generate only context-freelanguages by constructing a non-deterministic bottom-up.tree automaton to recognize the accepted trees.
Wesketch here a proof that makes use of several d~tev-r, tin/s-tic O"a.,~d'uc.e~s instead.FSTAs can be generalized so that instead of simplyaccepting or rejecting trees, they transform them, byadding constant trees, and deleting or duplicating sub-trees.
Such devices are called ~nite state tree transdue-erw (FSTT), and like the FSTA they can be top-down orbottom-up.
Flrst motivated as models of syntax-directedtranslations for compilers, they have been extensivelystudied (e.g.
\[47\], \[48\], \[40\]) but a simple subset issufficient here.The idea is this.
Let The the set of trees accepted bythe CS-based gram/nat.
Let t be in 71 F'STTs can be usedto label each node ~% of t with the set of all proper ana-lyses passing through n. It will then be simple to checkthat each node satisfies one of the node admissibilityconditions by sweeping through the labelled tree with abottom-up FSTA.The node labelling is done by two FST"Fs ~'l and r e. Letbe the max imum length of any left or right-context ofany node adITussibility condition.
Thus we need only labelnodes with sets of strings of length at most rrL, and over afinite alphabet there are only a fitute number  of suchstrings.r I operates bottom-up on a tree t, and labels eachnode 7t of t with three sets Pb'efz(n), Sufj~z(n), andY'zeld(n) of proper analyses: if P is the set of all properanalyses of the subtree rooted at vt, then Prefix(n) is theset of all substrings of length at most m that are prefixesof strings of P. Similarly, ,~tJ%z(n) is the set of allsuGixes of length at most ~,~., and Z'zetd(n) is the set of allstrings of P of length at most rrL.
It can easily be shownthat for any set of trees T, Tis recognizable if and only if~ /r) is.Applying to the output of "r j, the second transducer-rm operating top-down, labels each node rt with all theproper analyses going through n, i.e.
wlth a pair of setsof strings.
The first set ~nll contain all left-contexts ofnode n and the second all mght-eontexts, r 2 alsopreserves recogmzability.
A bottom-up FSTA can now bedefined to check at each node that both the context-~reepart of a ru/e as well as its context conditions aresatisfied.This argument also extends easily to cover the dotal-nance predicates of ioshi and Levy: transducers can beadded to label each node wlth all its "top contexts" andall its "bottom- contexts" "\['he final ?STA must thencheck that the nodes satlsf-y whatever Boolean combina-tion of dorrunanee and proper analysis predicates _arerequJred by the node admissibility m//es.REFEEENCES\[i\] Aho A.V., \[ndezed gT~rrLmars., aw eztensiovt o/ theco~ztezt-free ~rr=m~rs,  JACM 15, 647-67!.
i968.\[2\] Aho A.V., Hopcroft J.E.
and Ullman J.D., The Design andAnalysis of Comlmlter Algorith~q~ Addison-Wesley, Reading~ass, 1974.\[3\] Aho A.V., and Ullman J.D, The Theory of Parsing.Translation.
and CoKm@ili.g.
vol I: Parsing.
Prentice Hall,Englewood Cliffs N.J., 1972.103\[4\] Baker B.S., Arbitrary g'r~mm~r~ generat ing contezt-free languages, TR 11-72, Center for Research in Comput-ing Technology, Harvard Univ., 1972.\[5\] Berwick R. C., Comp~aZional com~lezi ty  ~nd lezical2u~t io~a l  s rammar,  t9th ACL, 198L\[6\] Berwick R. C. and Weinberg A., Pars/rig eff/c/.ency,com~tat ionn~ complezity, and the eva l~ l ion  of gram-rna//ca~ theor~s, l.ing.
Inq.
13, ,.65-191, 1962.\[7\] Borgida & T., Formal Studies of Strat/f icational Gram-mars, Pb_D Thesis, University o?
Toronto, 1977.\[8\] Chomsky N., R t~a and Representatinns.
ColumbiaUniversity Press, 1980.\[9\] Cook S. A., 7b~'ds  ~ cornpte~ity thso~ d of svnchzo-nm~s pa.v~ztlet com.~utat~, L'P.n-eignementMath,bmatique 27, 99-t24, !981.\[I0\] Eariey J., An effic-Lent contezt-free txtrs-mg algo-~hrn,  Comm_ or ACM 13.2, 94-10~, 1970.\[Ii\] Gazdar G., PhTsse stru=hzre ~rramanar, in Jacobson P.and Puilum G.
(eds.
), The Nature of Syntactic Represen-tation.
Reidel, 1982.\[12\] Gazdar G. and Pullum G., Generalized l~e  ~-uc-ttu~ C~"amr~ A'l~oretical Synopsis.
Indiana Univ.
Ling.Club, 1982.\[13\] Graham S. L., Harrison M. A.. and Ruzzo W.L., Aniwtprovaed co~tte=t-free recognizer, ACM Trans.
on Frog.lang.
and Systems, 2, 3, 415-462, 1980.\[14\] Hopcroft J.E.
and Ullman J., Introduction to Auto-mata Theory, ~ e s  and Computation, Addison Wes-ley, 1979.\[15\] Hays D.G., A~tornat~c l~g~age data ~'ocess~g.
InComputer Ai~Nication~ in the Behavioral Sciences, HBorko (ed.
), Prentice Hail, Englewood Cli~s N.J., 1962\[16\] Hintikka, ,I.K.K., Q~?n~e~ in  naPurai ~ang~ges:solute logicalprobtems 2, l .
i .= and PhiL, 153-L72, 1977.\[17\] Joshi ,% K., How much contezt-sensitiuit~j is requiredto pr~ride reasonable St~ttctur~ desc~ptio~s: treea~\]oining 9-ra~nmm-s, to appear in Dowry D., Karttunen L.add Zwicky A.
(eds.
), Natural l~-ua~e Processing:l~Chl ) i inuu i~c,  CoH~=m.ltatiollaJ.
and TheoRt~aJ.
Pl"Oper-ties, Cambridge Univ.
Press.\[18\] Joshi AK., F~to~.
.g  recurs-m~ ~nd d~pendencies: anaspect of Tree Ad\]o~rtY.~tg Ora~tmar,c and a compm-~son ~fsome format  properties of  TAG's, GPSG's, PLG'S and LFG's,these Proceedings, !983.\[19\] Josht &K. and Levy L.S:, P/mnse st~t~'e trees be~rnm'e ~t  t?~n you zvo~Id have tturught, 18th ACL, 1980.\[20\] Joshi A.K., Levy L.S.
and Takahasht M,, Tree adfunct@-ra~rnars, J. of Cor~p.
and Sys.
Sc.
"0, i, !36-163, "975.!21\] Joshi A.K., Levy L.S., Constv~z~nts on structura l48scr~tions:  local transfc,rmations, SIAM J. on Comput-e ,  1977.\[Z2\] Joshi A.K, Levy L.S.
and Yueh K., L~ca/cor~traints onprogrexr~m~ng ta~g~zges, Part 1.
~ttta~, Th.
Comp.
Sc.i2.2.65-290, :960\[~31 Josh/ A~ K. and Yokomori T, S0~ne character izat iontheore~-s fo~" tree ?~j~tct la'nguages and recognizsublesets.
\ [orthcoming\[24} Kaplan R. and Bresnan J., Lez~cal-F~nct~nal Gram-tnwr: a fozvrt~2 system fc, r gra~maZ~cal representation, inBre.~nan J (ed.
), The Mental Repe~sentation of Grammati-cad Relatinn~, M\]T Press, !
982.\[25\] Lamb S., OutlJ~e of StJratificational Gramr,'\]ar, George-town Umvermty Press, Washington, 1966.\[26\] Langendoen D.T., On the inadegu~cy o/ "/~jpe-2 andTMpe-3 gr~rruzrs for  human l~tg~ages, tn P.J.
Hopper(ed.)
~.udies m HistorieRl IJnguJ~cs: festsc\]~riJtt \[orW'~d P. Lehrrmrt, John Benjamin, Amsterdam, 159-171,1977.\[27\] LaPointe S., /{%zc~rsive~tess and deletiovt, \[Jno.
Al~d.3, 227-Z65, 1976.\[28\] Marcus M.P., A Theory of ~tacUc  Recognition forNaturaI Language, MIT Press, 1980.\[29\] Matthews R., Are the ~Fr~at~ca l  sentences of al,,ngtmge a rec~r~ve set?, Synthese 40, 2139-224, 1979.\[30\] Montague R., The pr ie r  treat~.ewt of T.Lm~t~ficationin 0rdina~J English, in Hintikka, J., Moravcsik J, andSuppes P (eds.
), Approaches to Natural Language, Reidel,Dordrecht, 1973.\[31\] Peters P.S.
and Ritchie R.W., A note on the ~iversaZbase ~othes~,  J. of IJn?~.~cs, 5, i50-2, 1969.\[32\] Peters PS.
and Rttchie R.W., On restmctzng tim basecomponent of t'r~wsform~ztiovtat g~zmma~s, Inf.
and Con-trol 18, 483-5-1, 1971.\[33\] Peters P.S.
arid Ritchie R.W., On the gevter~e powerof brensloz'rrta~iona~ grcntm~rs, Inf.
So.
6, 49-83, 1973.\[34\] Peters P.S.
and Ritchte R.W., Co.tort-sensitive~.t~rwd~--Ze cowst~tuevtt am~tyszs - co~tezt-free Languages.re'wLsited.
Math Sys.
Theory 6.
324-333, "9?3.\[35\] Peters P.S.
and Ritchie R.W., Ncrn-fitterivu\] and localfiZter~g gratnmm'.~, tn JK.K.
Hinttkka, JM.E.
Moravcsik,and P. Suppes (eds.)
Approaches to Natural Ls~b~ge,Reidel, 180-194.
1973.\[36\] Petrick S. R., A Recognition Procedure tot Transfor-mational Grammar~ Pb_D Thesis, .k-IT, 1965\[37\] Postal P.M., Lirtt~tations of  phrase-structure g~rtt-mats, m J.A.
Fodor and J.J. Katz (eds.
), Th~ structure oflanguage: Rean~nL/s in the philosophy of lan=~lage, Eng~e-wood Cliffs: Prentice Hall, 137-i51, 1964.\[38\] Pullum G.K. and Gazdar G., Natural and contezt-~ree~a~@'uages, I~.~_.
and Phil., 4, ~71-504.
1982.\[39\] Putnam H., So~te iss~s m the theory of ~ramrn.~r, ml~ .
of S~s ia  in ApI~lied Mathe~tics.
.AmericanMath.
Soc., 1961.\[40\] Rounds W. C., Mwptr~gs and gramr~tars o~ trees,Math.
Sys.
Th..4,3, 257-~87, '.9?0.\[41\] Rounds W. C., 7~'ee-ov-Lented proofs of some theoremson contezt - f ree and indezed lsmg~axjes, 2nd ACM b-'ymp, onTh.
Comp.
Sc.. 109-;-!6, 1979.\[4~} Rounds W. C., Cs~r~ptezi~j of rec,Jgni~ion in~rttev-r~diate-level langta~ges, 14th S\]Enp.
or* Sw. and Aut.13~ 1973.\[43\] Rounds W C., A graz~.z}~x~ticai cAwr~ter~.ation ofe~povtevttial-ti.te Imrtg~ages, ~ ~.
on ,Found.
cfCommp.
So., L35-!43.
1975.\[44\] b-qocum J., A practizat co~tpar/son of pav~ing stra-tegies, !9th ACt "_981.\[45\] Shieber S.M., Stucky S. U., Uszkoreit H. anc\[ RobinsonJ.
j., fb~naZ constraints on metctrtdes, these Proceed-ings, 1983.\[46\] Thatcher J W, Charazter~ng dew.ar ian trees ofcontext- free grarnrna~'s through a general izat ion of f inite~tovrtata  theov'g, J. of Comp.
and Sys.
Sc.
~, 3~.7-3~2.1967\[47\] Thatcher  J.W., C.ensv~/~zed ~ se~rttenti~t roach%nomaps, J. of Comp.
and Sys.
So.
4, 339-67, :970\[~8\] Thatcher J W., 7~'ee mLttozrtata: an infov-rrtal survey, \[nA.
Aho (ed.
), Currents in the theory of COml~ting, Pren-tice Hall, 148-172, !973.104\[49\] Uszkoreit H. and Peters P. S., Essm~t~aL var/ables inr r~ts~/es ,  forthcoming.\[50\] Valiant L., ~ener~ context-free recogTtition in lessthan cubic t~z~, L of Comp.
and b~Fs.
So.
i0, 308-315,1975.\[51\] Warren D. S., Syntax and Sem~nLics of Paz~ing: An~q~pficaUon to Montague C~ramm~m Ph.D Thesis, Universityof Michigan, 1979.\[52\] Yokomori T. and Joshi A. K., Serni-liztear~ty, p~z'Uch-baundea~ss ~td tree adjunct aTtguages, to appear \[n inf.Pr.
Letter& 1983.\[53\] Younger D. H., Recoqln52io~t ~nd pars~ztg of context-f~ee lamg'az~es ~ t/.~u~ nJ, Inf.
and Contl~l, I0, 2, 189-208,1967.\[54\] Kasami T., A~ e(T/c/ent recognition a~d s'tj.ntax a2go-r~thm /~r context-free laT~g~mges, Air Force CambridgeResearch Laboratory report AF-CRL-65-758, Bedford ,VLA,\[965.\[55\] Ruzzo W. L., On ur,:Lfo~'n, c'Lrc'~ cora\]c~ez~tj (extendedabstract), Proc.
of 20th $mnual Syrup.
on Found.
of Com.S=., 312-318, 1979.105
