Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 80?84,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsApplying UMLS for Distantly Supervised Relation DetectionRoland Roller and Mark StevensonUniversity of SheffieldRegent Court, 211 PortobelloS1 4DP Sheffield, UK{R.Roller,M.Stevenson}@dcs.shef.ac.ukAbstractThis paper describes first results usingthe Unified Medical Language System(UMLS) for distantly supervised relationextraction.
UMLS is a large knowledgebase which contains information aboutmillions of medical concepts and relationsbetween them.
Our approach is evaluatedusing existing relation extraction data setsthat contain relations that are similar tosome of those in UMLS.1 IntroductionDistant supervision has proved to be a popular ap-proach to relation extraction (Craven and Kum-lien, 1999; Mintz et al., 2009; Hoffmann et al.,2010; Nguyen and Moschitti, 2011).
It has theadvantage that it does not require manually anno-tated training data.
Distant supervision avoids thisby using information from a knowledge base toautomatically identify instances of a relation fromtext and use them in order to generate training datafor a relation extraction system.Distant supervision has already been appliedto the biomedical domain (Craven and Kumlien,1999; Thomas et al., 2011).
Craven and Kum-lien (1999) were the first to apply distant supervi-sion and used the Yeast Protein Database (YPD) todetect sentences containing subcellar-localizationrelations.
Thomas et al.
(2011) trained a clas-sifier for protein-protein interactions (PPI) usingthe knowledge base IntAct and evaluated their ap-proach on different PPI corpora.There have also been recent applications of dis-tant supervision outside the biomedical domain.The use of Freebase to train a classifier, e.g.
(Mintz et al., 2009; Riedel et al., 2010), has provedpopular.
Other, such as Hoffmann et al.
(2010),use Wikipedia info-boxes as the knowledge base.Applications of distant supervision face severalchallenges.
The main problem is ensuring thequality of the automatically identified training in-stances identified by the self-annotation.
The useof instances that have been incorrectly labelled aspositive can lower performance (Takamatsu et al.,2012).
Another problem arises when positive ex-amples are included in the set of negative train-ing instances, which can occur when informationis missing from the knowledge base (Min et al.,2013; Ritter et al., 2013; Xu et al., 2013).Evaluation of relation extraction systems thatuse distant supervision represents a further chal-lenge.
In the ideal case an annotated evaluation setis available.
Others, such as Ritter et al.
(2013) andHoffmann et al.
(2011), use Freebase as knowl-edge base and evaluate their classifier on an an-notated New York Times corpus.
However, if noevaluation set is available leave-out can be usedwhere the data identified using distant supervisionused for both training and testing (Hoffmann et al.,2010).This paper makes use of the Unified MedicalLanguage System (UMLS) as a knowledge sourcefor distant supervision.
It is widely used forbiomedical language processing and readily avail-able.
The advantage of UMLS is that it containsinformation about a wide range of different typesof relations and therefore has the potential to gen-erate a large number of relation classifiers.
To ourknowledge, it has not been used as a knowledgesource to train relation extraction systems.Evaluating such as wide range of relation clas-sifiers is not straightforward due to the lack ofgold-standard data.
As an alternative approach wemake use of existing annotated data sets and iden-tify ones which contain relations that are similar tothose included in UMLS.The next section provides a short description ofUMLS.
We then describe how we acquire existingdata sets to evaluate certain relations.
In section 4we present our first results using UMLS for distantsupervision.802 Unified Medical Language SystemThe Unified Medical Language System1is a setof files and software which combines differentbiomedical vocabularies, knowledge bases andstandards.
The Metathesaurus is a database withinUMLS which contains several million biomedicaland health related names and concepts and rela-tionships among them.
All different names of aconcept are unified by the Concept Unique Identi-fiers (CUI).
MRREL is a subset of the Metathe-saurus and involves different relationships be-tween different medical concepts defined by a pairof CUIs.
Many of them are child-parent rela-tionships, express a synonymy or are vaguely de-fined as broader or narrower relation.
Other re-lations are more specific, such as has location ordrug contraindicated for.
This work focuses onmore specific types of relations.3 Acquiring Evaluation Data SetsWe examined a number of relation extraction datasets in order to identify ones that could be used toevaluate our system.
The aim is to find a data setthat is annotated with relations that are similar tosome of those found in the UMLS.
If an appropri-ate relation can be identified then a relation extrac-tion system can be trained using information fromthe UMLS and evaluated using the data set.To determine whether a data set is suitable weused MetaMap (Aronson and Lang, 2010) to iden-tify the CUIs for each related item.
We then com-pared each pair against the MRREL table to deter-mine whether it is included as a relation.
To in-crease coverage we also included parent and childnodes in the mapping process.Table 1 shows the mappings obtained for twoof the data sets: the DDI 2011 data set (Segura-Bedmar et al., 2011) and the data set described byRosario and Hearst (2004).The DDI data set contains information aboutdrug-drug interactions and includes a single re-lation (DDI).
The relations it contained weremapped onto 701 CUI pairs.
266 (37.9%) of thesemappings could be matched to the MRREL rela-tion has contraindicated drug.
Many of the CUIpairs could also be mapped to the isa relationshipin MRREL, but this is a very general relationshipand the matches are caused by the large number ofthese in UMLS rather than it being a reasonable1https://www.nlm.nih.gov/research/umls/match for the DDI relation.The data set described by Rosario and Hearst(2004) focuses on different relationships betweentreatments and diseases.
The two most com-mon relations TREAT FOR DIS (TREAT), denot-ing the treatment for a particular disease, and PRE-VENT (PREV), which indicates that a treatmentcan be used to prevent a disease.
The MRRELisa relationship also matches many of these re-lations, again due to its prevalence in MRREL.Other MRREL relations (may be prevented byand may be treated by) match fewer CUI pairs butseem to be better matches for the TREAT andPREV relations.Relation MRRELDDI (701) has contraindicated drug (266),isa (185), may treat (57),has contraindication (51)PREV (41) isa (11), may be prevented by (5)TREAT (741) isa (172), may be treated by (118)Table 1: Relation mapping to MRRELIt is important to note that it is not always possi-ble to find a CUI mapping for each entity and themapping process means that the mapping cannotbe guaranteed to be correct in all cases.
High cov-erage does not necessarily mean that a corpus isvery similar to a certain MRREL relation, just thatmany of the CUI pairs which have been mappedto the related entities in the corpus occur often to-gether in a certain MRREL relation.
However, inthe absence of any other suitable evaluation datawe assume that high coverage is an indicator thatthe relations are strongly similar and use these twodata sets for evaluation.4 Distant Supervision using UMLSIn this section we carry out two different dis-tant supervised experiments using UMLS.
Thefirst experiment will be evaluated on a subsetof the DDI 2011 training data set using theMRREL relation has contraindicated drug andhas contraindication.
The second experimentuses the MRREL relations may be treated by andmay be prevented by and are evaluated on theRosario & Hearst data set.We use 7,500,000 Medline abstracts annotatedwith CUIs using MetaMap (choosing the bestmapping as annotation) as a corpus for distant su-pervision.
Our information extraction platformbased on a system developed for the BioNLP81Shared Task 2013 (Roller and Stevenson, 2013).In contrast to our previous work, our classificationprocess relies on the Shallow Linguistic Kernel(Giuliano et al., 2006) in combination with Lib-SVM (Chang and Lin, 2011) taking the kernel asinput.4.1 Experiment 1: DDI 2011The DDI 2011 data set was split into training andtest sets for the experiments.
Table 2 presentsresults that place the distant supervision perfor-mance in context.
The naive classification ap-proach predicts all candidate pairs as positive.
Thesupervised approach is trained on the training set,using the same kernel method as our distant su-pervised experiments and evaluated on the test set.This represents the performance that can be ob-tained using manually labelled training data andcan be considered as an upper bound for distantsupervision.Method Prec.
/ Recall / F1naive 0.098 / 1.000 / 0.178supervised 0.428 / 0.702 / 0.532Table 2: DDI 2011 baseline resultsThe distant supervision approach requires pairsof positive and negative CUI to be identified.These pairs are used to identify positive and nega-tive examples of the target relation from a corpus.Pairs which occur in our target MRREL relationare used as positive CUI pairs.
Negative pairs aregenerated by selecting pairs of CUIs that are occurin any other MRREL relation.Sentences containing these CUI pairs are iden-tified in the subset of the MetaMapped Medline.In the basic setup (basic), sentences containing apositive pair will be considered as a positive train-ing example.
There are many cases where just theoccurrence of a positive MRREL pair does not ex-press the target relation.
In an effort to removethis noisy data we apply some simple heuristics.The first discards all training instances with morethan five words (5w) between the two entities, anapproach similar to one applied by Takamatsu etal.
(2012).
The second discards positive sentencescontaining a comma between the related entities(com).
We found that commas often indicate a sen-tence containing a list of items (e.g.
genes or dis-eases) and that these sentences do not form goodtraining examples due to the multiple relations thatare possible when there are several items.
Finallywe also apply a combination of both techniques(5w+com).1000 positive examples were generated usingeach approach and used for training.
Although itwould be possible to generate more examples forsome approaches, for example basic, applying thecombination of techniques (5w+com) significantlyreduces the number of instances available.Method has contraindication has contraindicated(P./R./F1) drug (P./R./F1)basic 0.146 / 0.371 / 0.210 0.158 / 0.598 / 0.2505w 0.109 / 0.641 / 0.187 0.207 / 0.487 / 0.290com 0.212 / 0.560 / 0.308 0.177 / 0.498 / 0.2615w+com 0.207 / 0.487 / 0.291 0.214 / 0.471 / 0.294Table 3: Evaluation with DDI 2011Table 3 presents results of the experiments.The results show that all applied techniques forboth MRREL relations outperform the naive ap-proach.
The best results in terms of F1 scorefor the has contraindication MRREL relationare obtained using the com selection technique.Applying just 5w leads to worse results thanusing the basic approach.
The situation forhas contraindicated drug is different.
The classi-fier provides for all techniques a better F1 scorethan the basic approach.
The best results areachieved by using 5w+com.
It is interesting to see,that both MRREL relations provide similar aver-age classification results, even if both relations aredifferent from the target relation and cover com-pletely different CUI pairs.
It is also interest-ing that the MRREL relation has contraindicationhas a lower coverage to the DDI relation thanhas contraindicated drug, but provides slightlybetter results overall.
A problem with the distantsupervised classification of these two MRREL re-lations is their low occurrence in our Medline sub-set.
Using more training data will often lead tobetter results.
In our case, if we apply the com-bined selection technique, there are fewer positivetraining instances than are available to the super-vised approach, making it difficult to outperformthe supervised approach.4.2 Experiment 2: Rosario & HearstThe second experiment addresses the prob-lem of detecting the MRREL relationsmay be prevented by and may be treated by.Parts of the Rosario & Hearst data set are usedto evaluate this relation.
This data set differsin structure from the DDI data set.
Instead of82annotating the entities in the sentence accordingto its relation, the annotations in the data setindicate whether a certain relation occurs in thesentence.
This data set does not contain anynegative examples.
If a sentence contains twoentities, it will always describe a certain relation.A supervised classifier is created by dividing thedata set into training and test sets.
The test setcontains 253 different sentences (221 describea TREAT relation, 15 a PREV relation and 17involve other relationships).
Positive and negativeCUI pairs are selected in a different way to theprevious experiment.
The two most frequentrelations in the data set are TREAT and PREV.A classifier for a particular relation is trainedusing sentences annotated with the correspondingMRREL relation as positive instances.
Negativeinstances are identified using the other relation.For example, the classifier for the TREAT relationis trained using positive examples identifiedusing may be treated by with negative examplesgenerated using may be treated by.Table 4 shows the baseline results on the dataset using a naive and a supervised approach on thetwo original relations TREAT and PREV.
Perfor-mance of the naive approach for TREAT is veryhigh since the majority of sentences in the data setare annotated with that relation.Data Set Method Prec.
/ Recall / F1TREATnaive 0.874 / 1.000 / 0.933supervised 0.944 / 0.923 / 0.934PREVnaive 0.059 / 1.000 / 0.112supervised 0.909 / 0.667 / 0.769Table 4: Rosario & Hearst baseline resultsTable 5 shows the results for the various dis-tant supervision approaches.
Again, 1000 positivetraining examples were used to train the classifier.Since the F-Score of the naive and the supervisedapproaches of TREAT are very high, it is difficultto compete with the may be treated by distant su-pervised classifier.
However, considering that just15.9% of the TREAT instance pairs of the train-ing set match the MRREL may be treated by re-lation, the results are promising.
Furthermore, theprecision of all may be treated by distant super-vised experiments outperform the naive approach.The best results are achieved using com as selec-tion technique.The experiments using the PREV relation forevaluation are more interesting.
Due to its lowoccurrence in the test set it is more difficult todetect this relation.
The distant supervised clas-sifier trained with the may be prevented by rela-tion easily outperforms the naive approach.
Thebest overall F1 scoer results are achieved usingthe 5w technique.
As expected the distant super-vised results are outperformed by the supervisedapproach.
However, the recall for all distantly su-pervised approaches are at least as high as thoseobtained using the supervised approach.may be treated by may be prevented byevaluated on TREAT evaluated on PREVMethod (P./R./F1) (P./R./F1)basic 0.926 / 0.733 / 0.818 0.286 / 0.667 / 0.4005w 0.925 / 0.783 / 0.848 0.407 / 0.733 / 0.524com 0.928 / 0.819 / 0.870 0.222 / 0.800 / 0.3485w+com 0.924 / 0.769 / 0.840 0.361 / 0.867 / 0.510Table 5: Evaluation with Rosario & Hearst dataset5 Conclusion and DiscussionIn this paper we presented first results usingUMLS to train a distant supervised relational clas-sifier.
Evaluation was carried out using existingevaluation data sets since no resources directly an-notated with UMLS relations were available.
Weshowed that using a distantly supervised classifiertrained on MRREL relations similar to those foundin the evaluation data set provides promising re-sults.Overall, our system works with some compo-nents which should be improved to achieve betterresults.
First, we rely on a cheap and fast anno-tation using MetaMap, which might produce an-notation errors.
In addition, the use of noisy dis-tant supervised training data decreases the classi-fication quality.
An improvement of the selectionprocess and an improvement of the classificationmethod, such as Chowdhury and Lavelli (2013),could lead to better classification results.
In futurewe would also like to make further use of existingdata sets with similar relations to those of interestto evaluate distant supervision approaches.AcknowledgementsThe authors are grateful to the Engineeringand Physical Sciences Research Council forsupporting the work described in this paper(EP/J008427/1).83ReferencesA.
Aronson and F. Lang.
2010.
An overview ofMetaMap: historical perspective and recent ad-vances.
Journal of the American Medical Associ-ation, 17(3):229?236.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Md.
Faisal Mahbub Chowdhury and Alberto Lavelli.2013.
Fbk-irst : A multi-phase kernel based ap-proach for drug-drug interaction detection and clas-sification that exploits linguistic information.
InSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM), Volume 2: Proceedingsof the Seventh International Workshop on Seman-tic Evaluation (SemEval 2013), pages 351?355, At-lanta, Georgia, USA, June.
Association for Compu-tational Linguistics.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informa-tion from text sources.
In In Proceedings of the Sev-enth International Conference on Intelligent Systemsfor Molecular Biology (ISMB), pages 77?86.
AAAIPress.Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-mano.
2006.
Exploiting shallow linguistic infor-mation for relation extraction from biomedical liter-ature.
In In Proc.
EACL 2006.Raphael Hoffmann, Congle Zhang, and Daniel S.Weld.
2010.
Learning 5000 relational extractors.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 286?295, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics, ACL ?11, pages 541?550.Bonan Min, Ralph Grishman, Li Wan, Chang Wang,and David Gondek.
2013.
Distant supervision forrelation extraction with an incomplete knowledgebase.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 777?782, Atlanta, Georgia, June.Association for Computational Linguistics.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2 - Volume 2, ACL ?09, pages 1003?1011,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Truc-Vien T. Nguyen and Alessandro Moschitti.
2011.End-to-end relation extraction using distant super-vision from external semantic repositories.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 277?282, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Proceedings of the EuropeanConference on Machine Learning and KnowledgeDiscovery in Databases (ECML PKDD ?10).Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-zioni.
2013.
Modeling missing data in distant su-pervision for information extraction.
In Associationfor Computational Linguistics Vol.
1 (TACL).Roland Roller and Mark Stevenson.
2013.
Identi-fication of genia events using multiple classifiers.In Proceedings of BioNLP Shared Task 2013 Work-shop, Sofia, Bulgaria, August.
Association for Com-putational Linguistics.Barbara Rosario and Marti A. Hearst.
2004.
Classi-fying semantic relations in bioscience texts.
In Pro-ceedings of the 42nd Annual Meeting on Associationfor Computational Linguistics, ACL ?04, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Isabel Segura-Bedmar, Paloma Martnez, and DanielSnchez-Cisneros.
2011.
The 1st ddi extraction-2011 challenge task: Extraction of drug-drug inter-actions from biomedical texts.
In Proceedings ofDDI Extraction-2011 challenge task., pages 1?9.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervi-sion for relation extraction.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers - Volume 1, ACL?12, pages 721?729, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Philippe Thomas, Ill?es Solt, Roman Klinger, and UlfLeser.
2011.
Learning protein protein interactionextraction using distant supervision.
In Proceedingsof Robust Unsupervised and Semi-Supervised Meth-ods in Natural Language Processing, pages 34?41.Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-ishman.
2013.
Filling knowledge base gaps for dis-tant supervision of relation extraction.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 665?670, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.84
