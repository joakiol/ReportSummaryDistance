R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
507 ?
518, 2005.?
Springer-Verlag Berlin Heidelberg 2005Exploring Syntactic Relation Patternsfor Question AnsweringDan Shen1,2, Geert-Jan M. Kruijff 1, and Dietrich Klakow21Department of Computational Linguistics, Saarland University,Building 17, Postfach 15 11 50, 66041 Saarbruecken, Germany{dshen, gj}@coli.uni-sb.de2Lehrstuhl Sprach Signal Verarbeitung,Saarland University,Building 17, Postfach 15 11 50, 66041 Saarbruecken, Germany{dietrich.klakow}@lsv.uni-saarland.deAbstract.
In this paper, we explore the syntactic relation patterns for open-domain factoid question answering.
We propose a pattern extraction method toextract the various relations between the proper answers and different types ofquestion words, including target words, head words, subject words and verbs,from syntactic trees.
We further propose a QA-specific tree kernel to partiallymatch the syntactic relation patterns.
It makes the more tolerant matching be-tween two patterns and helps to solve the data sparseness problem.
Lastly, weincorporate the patterns into a Maximum Entropy Model to rank the answercandidates.
The experiment on TREC questions shows that the syntactic rela-tion patterns help to improve the performance by 6.91 MRR based on the com-mon features.1   IntroductionQuestion answering is to find answers for open-domain natural language questions ina large document collection.
A typical QA system usually consists of three basicmodules: 1.
Question Processing (QP) Module, which finds some useful informationfrom questions, such as expected answer type and key words; 2.
Information Retrieval(IR) Module, which searches a document collection to retrieve a set of relevant sen-tences using the key words; 3.
Answer Extraction (AE) Module, which analyzes therelevant sentences using the information provided by the QP module and identify theproper answer.
In this paper, we will focus on the AE module.In order to find the answers, some evidences, such as expected answer types andsurface text patterns, are extracted from answer sentences and incorporated in the AEmodule using a pipelined structure, a scoring function or some statistical-based meth-ods.
However, the evidences extracted from plain texts are not sufficient to identify aproper answer.
For examples, for ?Q1910: What are pennies made of?
?, the expectedanswer type is unknown; for ?Q21: Who was the first American in space?
?, the sur-face patterns may not detect the long-distance relations between the question keyphrase ?the first American in space?
and the answer ?Alan Shepard?
in ??
that car-ried Alan Shepard on a 15 - minute suborbital flight in 1961 , making him the first508 D. Shen, G.-J.M.
Kruijff, and D. KlakowAmerican in space.?
To solve these problems, more evidences need to be extractedfrom the more complex data representations, such as parse trees.In this paper, we explore the syntactic relation patterns (SRP) for the AE module.An SRP is defined as a kind of relation between a question word and an answer can-didate in the syntactic tree.
Different from the textual patterns, the SRPs capture therelations based on the sentence syntactic structure rather than the sentence surface.Therefore, they may get the deeper understanding of the relations and capture the longrange dependency between words regardless of their ordering and distance in thesurface text.
Based on the observation of the task, we find that the syntactic relationsbetween different types of question words and answers vary a lot with each other.
Weclassify the question words into four classes, including target words, head words,subject phrases and verbs, and generate the SRPs for them respectively.
Firstly, wegenerate the SRPs from the training data and score them based on the support andconfidence measures.
Next, we propose a QA-specific tree kernel to calculate thesimilarity between two SRPs in order to match the patterns from the unseen data intothe pattern set.
The tree kernel makes the partial matching between two patterns andhelps to solve the data sparseness problem.
Lastly, we incorporate the SRPs into aMaximum Entropy Model along with some common features to classify the answercandidates.
The experiment on TREC questions shows that the syntactic relationpatterns improve the performance by 6.91 MRR based on the common features.Although several syntactic relations, such as subject-verb and verb-object, havebeen also considered in some other systems, they are basically extracted using a smallnumber of hand-built rules.
As a result, they are limited and costly.
In our task, weautomatically extract the various relations between different question words and an-swers and more tolerantly match the relation patterns using the tree kernel.2   Related WorkThe relations between answers and question words have been explored by many suc-cessful QA systems based on certain sentence representations, such as word sequence,logic form, parse tree, etc.In the simplest case, a sentence is represented as a sequence of words.
It is as-sumed that, for certain type of questions, the proper answers always have certainsurface relations with the question words.
For example, ?Q: When was X born?
?, theproper answers often have such relation ?<X> ( <Answer>--?
with the questionphrase X .
[14] first used a predefined pattern set in QA and achieved a good per-formance at TREC10.
[13] further developed a bootstrapping method to learn thesurface patterns automatically.
When testing, most of them make the partial matchingusing regular expression.
However, such surface patterns strongly depend on theword ordering and distance in the text and are too specific to the question type.LCC [9] explored the syntactic relations, such as subject, object, prepositional at-tachment and adjectival/adverbial adjuncts, based on the logic form transformation.Furthermore they used a logic prover to justify the answer candidates.
The prover isaccurate but costly.Most of the QA systems explored the syntactic relations on the parse tree.
Sincesuch relations do not depend on the word ordering and distance in the sentence, theymay cope with the various surface expressions of the sentence.
ISI [7] extracted theExploring Syntactic Relation Patterns for Question Answering 509relations, such as ?subject-verb?
and ?verb-object?, in the answer sentence tree andcompared with those in the question tree.
IBM?s Maximum Entropy-based model[10] integrated a rich feature set, including words co-occurrence scores, named entity,dependency relations, etc.
For the dependency relations, they considered some prede-fined relations in trees by partial matching.
BBN [15] also considered the verb-argument relations.However, most of the current QA systems only focus on certain relation types,such as verb-argument relations, and extract them from the syntactic tree using someheuristic rules.
Therefore, extracting such relations is limited in a very local contextof the answer node, such as its parent or sibling nodes, and does not involve longrange dependencies.
Furthermore, most of the current systems only concern the rela-tions to certain type of question words, such as verb.
In fact, different types of ques-tion words may have different indicative relations with the proper answers.
In thispaper, we will automatically extract more comprehensive syntactic relation patternsfor all types of question words, partially match them using a QA-specific tree kerneland evaluate their contributions by integrating them into a Maximum Entropy Model.3   Syntactic Relation Pattern GeneratingIn this section, we will discuss how to extract the syntactic relation patterns.
Firstly,we briefly introduce the question processing module which provides some necessaryinformation to the answer extraction module.
Secondly, we generate the dependencytree of the answer sentence and map the question words into the tree using a ModifiedEdit Distance (MED) algorithm.
Thirdly, we define and extract the syntactic relationpatterns in the mapped dependency tree.
Lastly, we score and filter the patterns.3.1   Question Processing ModuleThe key words are extracted from the questions.
Considering that different key wordsmay have different syntactic relations with the answers, we divide the key words intothe following four types:1.
Target Words, which are extracted from what / which questions.
Such words indi-cate the expected answer types, such as ?party?
in ?Q1967: What party led ???.2.
Head Words, which are extracted from how questions.
Such words indicate theexpected answer heads, such as ?dog?
in the ?Q210: How many dogs pull ???3.
Subject Phrases, which are extracted from all types of questions.
They are the basenoun phrases of the questions except the target words and the head words.4.
Verbs, which are the main verbs extracted from non-definition questions.The key words described above are identified and classified based on the questionparse tree.
We employ the Collins Parser [2] to parse the questions and the answersentences.3.2   Question Key Words MappingFrom this section, we start to introduce the AE module.
Firstly, the answer sentencesare tagged with named entities and parsed.
Secondly, the parse trees are transformed510 D. Shen, G.-J.M.
Kruijff, and D. Klakowto the dependency trees based on a set of rules.
To simplify a dependency tree, somespecial rules are used to remove the non-useful nodes and dependency information.The rules include1.
Since the question key words are always NPs and verbs, only the syntactic rela-tions between NP and NP / NP and verb are considered.2.
The original form of Base Noun Phrase (BNP) is kept and the dependency relationswithin the BNPs are not considered, such as adjective-noun.
A base noun phrase isdefined as the smallest noun phrase in which there are no noun phrases embedded.An example of the dependency tree is shown in Figure 1.
We regard all BNPnodes and leaf nodes as answer candidates.Fig.
1.
Dependency tree and Tagged dependency treeNext, we map the question key words into the simplified dependency trees.
Wepropose a weighted edit distance (WED) algorithm, which is to find the similaritybetween two phrases by computing the minimal cost of operations needed to transformone phrase into the other, where an operation is an insertion, deletion, or substitution.Different from the commonly-used edit distance algorithm [11], the WED definesthe more flexible cost function which incorporates the morphological and semanticalternations of the words.
The morphological alternations indicate the inflections ofnoun/verb.
For example, for Q2149: How many Olympic gold medals did Carl Lewiswin?
We map the verb win to the nominal winner in the answer sentence ?CarlLewis, winner of nine Olympic gold medals, thinks that ??.
The morphological alter-nations are found based on a stemming algorithm and the ?derivationally relatedforms?
in WordNet [8].
The semantic alternations consider the synonyms of thewords.
Some types of the semantic relations in WordNet enable the retrieval of syno-nyms, such as hypernym, hyponym, etc.
For example, for Q212: Who invented theelectric guitar?
We may map the verb invent to its direct hypernym create in answersentences.
Based on the observation of the task, we set the substitution costs of thealternations as follows: Identical words have cost 0; Words with the same morpho-logical root have cost 0.2; Words with the hypernym or hyponym relations have costtagged dependency tree dependency treeliveBNPNER_PEREllingtonBNPNER_LOCBNPWashington his early NNP 20sNER_DATVERVER: the verb of the questionSUB: the subject words of the questionTGT_HYP: the hypernym of the target word of the questionliveBNPNER_PERSUBEllingtonBNPNER_LOCTGT_HYPBNPWashington his early NNP 20sNER_DATQ1916: What city did Duke Ellington live in?A: Ellington lived in Washington until his early 20s.Exploring Syntactic Relation Patterns for Question Answering 5110.4; Words in the same SynSet have cost 0.6; Words with subsequence relations havecost 0.8; otherwise, words have cost 1.
Figure 1 also shows an example of the taggeddependency tree.3.3   Syntactic Relation Pattern ExtractionA syntactic relation pattern is defined as the smallest subtree which covers an answercandidate node and one question key word node in the dependency tree.
To capturedifferent relations between answer candidates and different types of question words,we generate four pattern sets, called PSet_target, PSet_head, PSet_subject andPSet_verb, for the answer candidates.
The patterns are extracted from the trainingdata.
Some pattern examples are shown in Table 1.
For a question Q, there are a setof relevant sentences SentSet.
The extraction process is as follows:1. for each question Q in the training data2.
question processing model extract the key words of Q3.
for each sentence s in SentSeta) parse sb) map the question key words into the parse treec) tag all BNP nodes in the parse tree as answer candidates.d) for each answer candidate (ac) nodefor each question word (qw) nodeextract the syntactic relation pattern (srp) for ac and qwadd srp to PSet_target, PSet_head, PSet_subject orPSet_verb based on the types of qw.Table 1.
Examples of the patterns in the four pattern setsPatternSet  Patterns Sup.
Conf.
(NPB~AC~TGT) 0.55 0.22(NPB~AC~null (NPB~null~TGT)) 0.08 0.06 PSet_target(NPB~null~null (NPB~AC~null) (NPB~null~TGT)) 0.02 0.09PSet_head (NPB~null~null (CD~AC~null) (NPB~null~HEAD)) 0.59 0.67(VP~null~null (NPB~null~SUB) (NPB~null~null(NPB~AC~null)))0.04 0.33PSet_subject(NPB~null~null (NPB~null~SUB) (NPB~AC~null)) 0.02 0.18PSet_verb (VP~null~VERB (NPB~AC~null)) 0.18 0.163.4   Syntactic Relation Pattern ScoringThe patterns extracted in section 3.3 are scored by support and confidence measures.Support and confidence measures are most commonly used to evaluate the associationrules in the data mining area.
The support of a rule is the proportion of times the ruleapplies.
The confidence of a rule is the proportion of times the rule is correct.
In ourtask, we score a pattern by measuring the strength of the association rule from thepattern to the proper answer (the pattern is matched => the answer is correct).
Let pibe any pattern in the pattern set PSet ,512 D. Shen, G.-J.M.
Kruijff, and D. Klakowthe number of  in which  is correctsupport( )the size ofp acipi PSet=the number of  in which  is correctconfidence( )the number ofp acipi pi=We score the patterns in the PSet_target, PSet_head, PSet_subject and PSet_verbrespectively.
If the support value is less than the threshold supt or the confidencevalue is less than the threshold conft , the pattern is removed from the set.
In the ex-periment, we set supt 0.01 and conft  0.5.
Table 1 lists the support and confidence ofthe patterns.4   Syntactic Relation Pattern MatchingSince we build the pattern sets based on the training data in the current experiment,the pattern sets may not be large enough to cover all of the unseen cases.
If we makethe exact match between two patterns, we will suffer from the data sparseness prob-lem.
So a partial matching method is required.
In this section, we will propose a QA-specific tree kernel to match the patterns.A kernel function 1 2( , ) : [0, ]K x x ?
?X X R , is a similarity measure betweentwo objects 1x and 2x with some constraints.
It is the most important component ofkernel methods [16].
Tree kernels are the structure-driven kernels used to calculatethe similarity between two trees.
They have been successfully accepted in the naturallanguage processing applications, such as parsing [4], part of speech tagging andnamed entity extraction [3], and information extraction [5, 17].
To our knowledge,tree kernels have not been explored in answer extraction.Suppose that a pattern is defined as a tree T with nodes 0 1{ , , ..., }nt t t  and each nodeit is attached with a set of attributes 0 1{ , , ..., }ma a a , which represent the local charac-teristics of ti .
In our task, the set of the attributes include Type attributes, Ortho-graphic attributes and Relation Role attributes, as shown in Table 2.
Figure 2 showsan example of the pattern tree T_ac#target.The core idea of the tree kernel ( , )1 2K T T  is that the similarity between two treesT1 and T2 is the sum of the similarity between their subtrees.
It can be calculated bydynamic programming and can capture the long-range relations between two nodes.The kernel we use is similar to [17] except that we define a task-specific matchingfunction and similarity function, which are two primitive functions to calculate thesimilarity between two nodes in terms of their attributes.Matching function1 if .
.
and .
.
( , )0 otherwisei j i ji jt type t type t role t rolem t t= ==??
?Exploring Syntactic Relation Patterns for Question Answering 513Similarity function0{ ,..., }( , ) ( .
, .
)i j i jma a as t t f t a t a?= ?where, ( .
, .
)i jf t a t a  is a compatibility function between two feature values.
.
( .
, .
)1   if0   otherwisei ji jt a t af t a t a ==??
?Table 2.
Attributes of the nodesAttributes ExamplesPOS tag CD, NNP, NN?
Typesyntactic tag NP, VP, ?Is Digit?
DIG, DIGALLIs Capitalized?
CAP, CAPALLOrthographiclength of phrase LNG1, LNG2#3, LNGgt3Role1 Is answer candidate?
true, falseRole2 Is question key words?
true, falseFig.
2.
An example of the pattern tree T_ac#target5   ME-Based Answer ExtractionIn addition to the syntactic relation patterns, many other evidences, such as namedentity tags, may help to detect the proper answers.
Therefore, we use maximum en-tropy to integrate the syntactic relation patterns and the common features.5.1   Maximum Entropy Model[1] gave a good description of the core idea of maximum entropy model.
In our task,we use the maximum entropy model to rank the answer candidates for a question,T_ac#targetQ1897: What is the name of the airport in Dallas Ft. Worth?S: Wednesday morning, the low temperature at the Dallas-Fort Worth Inter-national Airport was 81 degrees.t4 t3 t2T: BNPO: nullR1: trueR2: falset1Dallas-FortT: NNPO: CAPALLR1: falseR2: falseInternationalT: JJO: CAPALLR1: falseR2: falseAirportT: NNPO: CAPALLR1: falseR2: truet0WorthT: NNPO: CAPALLR1: falseR2: false514 D. Shen, G.-J.M.
Kruijff, and D. Klakowwhich is similar to [12].
Given a question q and a set of possible answer candi-dates 1 2{ , ... }nac ac ac , the model outputs the answer 1 2{ , ... }nac ac ac ac?
with themaximal probability from the answer candidate set.
We define M feature func-tions 1 2( ,{ , ... }, ),  m=1,...,Mm nf ac ac ac ac q .
The probability is modeled as1 211 21 2' 1exp[ ( ,{ , ... }, ))]( | { , ... }, )exp[ ( ',{ , ... }, )]Mm m nmn Mm m nac mf ac ac ac ac qP ac ac ac ac qf ac ac ac ac q??==?=?
?where, (m=1,...,M)m?
are the model parameters, which are trained with General-ized Iterative Scaling [6].
A Gaussian Prior is used to smooth the ME model.Table 3.
Examples of the common featuresFeatures Examples ExplanationNE#DAT_QT_DAT ac is NE (DATE) and qtarget is DATE NENE#PER_QW_WHO ac is NE (PERSON) and qword is WHOSSEQ_Q ac is a subsequence of questionCAP_QT_LOC ac is capitalized and qtarget is LOCATIONOrtho-graphicLNGlt3_QT_PER the length of ac ?
3 and qtarget is PERSONCD_QT_NUM syn.
tag of ac is CD and qtarget is NUM SyntacticTag  NNP_QT_PER syn.
tag of ac is NNP and qtarget is PERSONTriggers TRG_HOW_DIST ac matches the trigger words for HOW questions whichask for distance5.2   FeaturesFor the baseline maximum entropy model, we use four types of common features:1.
Named Entity Features: For certain question target, if the answer candidate istagged as certain type of named entity, one feature fires.2.
Orthographic Features: They capture the surface format of the answer candi-dates, such as capitalizations, digits and lengths, etc.3.
Syntactic Tag Features: For certain question target, if the word in the answercandidate belongs to a certain syntactic / POS type, one feature fires.4.
Triggers: For some how questions, there are always some trigger words which areindicative for the answers.
For example, for ?Q2156: How fast does Randy John-son throw?
?, the word ?mph?
may help to identify the answer ?98-mph?
in ?John-son throws a 98-mph fastball.
?Table 3 shows some examples of the common features.
All of the features are thebinary features.
In addition, many other features, such as the answer candidate fre-quency, can be extracted based on the IR output and are thought as the indicativeevidences for the answer extraction [10].
However, in this paper, we are to evaluatethe answer extraction module independently, so we do not incorporate such featuresin the current model.Exploring Syntactic Relation Patterns for Question Answering 515In order to evaluate the effectiveness of the automatically generated syntactic rela-tion patterns, we also manually build some heuristic rules to extract the relation fea-tures from the trees and incorporate them into the baseline model.
The baselinemodel uses 20 rules.
Some examples of the hand-extracted relation features arelisted as follows,z If the ac node is the same of the qtarget node, one feature fires.z If the ac node is the sibling of the qtarget node, one feature fires.z If the ac node is the child of the qsubject node, one feature fires.z ?Next, we will discuss the use of the syntactic relation features.
Firstly, for eachanswer candidate, we extract the syntactic relations between it and all mapped ques-tion key words in the sentence tree.
Then for each extracted relation, we match it inthe pattern set PSet_target, PSet_head, PSet_subject or PSet_verb.
A tree kerneldiscussed in Section 4 is used to calculate the similarity between two patterns.
Fi-nally, if the maximal similarity is above a threshold ?
, the pattern with the maximalsimilarity is chosen and the corresponding feature fires.
The experiments will evalu-ate the performance and the coverage of the pattern sets based on different ?
values.6   ExperimentWe apply the AE module to the TREC QA task.
Since this paper focuses on the AEmodule alone, we only present those sentences containing the proper answers to theAE module based on the assumption that the IR module has got 100% precision.
TheAE module is to identify the proper answers from the given sentence collection.We use the questions of TREC8, 9, 2001 and 2002 for training and the questions ofTREC2003 for testing.
The following steps are used to generate the data:1.
Retrieve the relevant documents for each question based on the TREC judgments.2.
Extract the sentences, which match both the proper answer and at least one ques-tion key word, from these documents.3.
Tag the proper answer in the sentences based on the TREC answer patterns.In TREC 2003, there are 413 factoid questions in which 51 questions (NIL ques-tions) are not returned with the proper answers by TREC.
According to our datageneration process, we cannot provide data for those NIL questions because we can-not get the sentence collections.
Therefore, the AE module will fail on all of the NILquestions and the number of the valid questions should be 362 (413 ?
51).
In theexperiment, we still test the module on the whole question set (413 questions) to keepconsistent with the other?s work.
The training set contains 1252 questions.
The per-formance of our system is evaluated using the mean reciprocal rank (MRR).
Fur-thermore, we also list the percentages of the correct answers respectively in terms ofthe top 5 answers and the top 1 answer returned.
No post-processes are used to adjustthe answers in the experiments.In order to evaluate the effectiveness of the syntactic relation patterns in the answerextraction, we compare the modules based on different feature sets.
The first MEmodule ME1 uses the common features including NE features, Orthographic features,516 D. Shen, G.-J.M.
Kruijff, and D. KlakowSyntactic Tag features and Triggers.
The second ME module ME2 uses the commonfeatures and some hand-extracted relation features, described in Section 5.2.
Thethird module ME3 uses the common features and the syntactic relation patterns whichare automatically extracted and partial matched with the methods proposed in Section3 and 4.
Table 4 shows the overall performance of the modules.
Both ME2 and ME3outperform ME1 by 3.15 MRR and 6.91 MRR respectively.
This may indicate thatthe syntactic relations between the question words and the answers are useful for theanswer extraction.
Furthermore, ME3 got the higher performance (+3.76 MRR) thanME2.
The probable reason may be that the relations extracted by some heuristic rulesin ME2 are limited in the very local contexts of the nodes and they may not be suffi-cient.
On the contrary, the pattern extraction methods we proposed can explore thelarger relation space in the dependency trees.Table 4.
Overall performanceME1 ME2 ME3Top1  44.06 47.70 51.81Top5 53.27 55.45 58.85MRR 47.75 50.90 54.66Table 5.
Performances for two pattern matching methodsPartialMatch  ExactMatch( ?
=1) ?
=0.8 ?
=0.6 ?
=0.4 ?
=0.2 ?
=0Top1 50.12 51.33 51.81 51.57 50.12 50.12Top5 57.87 58.37 58.85 58.60 57.16 57.16MRR 53.18 54.18 54.66 54.41 52.97 52.97Furthermore, we evaluate the effectiveness of the pattern matching method in Sec-tion 4.
We compare two pattern matching methods: the exact matching (ExactMatch)and the partial matching (PartialMatch) using the tree kernel.
Table 5 shows theperformances for the two pattern matching methods.
For PartialMatch, we alsoevaluate the effect of the parameter ?
(described in Section 5.2) on the performance.In Table 5, the best PartialMatch ( ?
= 0.6) outperforms ExactMatch by 1.48 MRR.Since the pattern sets extracted from the training data is not large enough to cover theunseen cases, ExactMatch may have too low coverage and suffer with the data sparse-ness problem when testing, especially for PSet_subject (24.32% coverage using Ex-actMatch vs. 49.94% coverage using PartialMatch).
In addition, even the model withExactMatch is better than ME2 (common features + hand-extracted relations) by 2.28MRR.
It indicates that the relation patterns explored with the method proposed inSection 3 are more effective than the relations extracted by the heuristic rules.Table 6 shows the size of the pattern sets PSet_target, PSet_head, PSet_subjectand PSet_verb and their coverage for the test data based on different ?
values.PSet_verb gets the low coverage (<5% coverage).
The probable reason is that theverbs in the answer sentences are often different from those in the questions, thereforeonly a few question verbs can be matched in the answer sentences.
PSet_head alsogets the relatively low coverage since the head words are only exacted from howquestions and there are only 49/413 how questions with head words in the test data.Exploring Syntactic Relation Patterns for Question Answering 517Table 6.
Size and coverage of the pattern setscoverage (*%)  size?
=1 ?
=0.8 ?
=0.6 ?
=0.4 ?
=0.2 ?
=0PSet_target 45 49.85 53.73 57.01 58.14 58.46 58.46PSet_head 42 5.82 6.48 6.69 6.80 6.80 6.80PSet_subject 123 24.32 44.82 49.94 51.29 51.84 51.84PSet_verb 125 2.21 3.49 3.58 3.58 3.58 3.58We further evaluate the contributions of different types of patterns.
We respec-tively combine the pattern features in different pattern set and the common features.Some findings can be concluded from Table 7: All of the patterns have the positiveeffects based on the common features, which indicates that all of the four types of therelations are helpful for answer extraction.
Furthermore, P_target (+4.21 MRR) andP_subject (+2.47 MRR) are more beneficial than P_head (+1.25 MRR) and P_verb(+0.19 MRR).
This may be explained that the target and subject patterns may havethe effect on the more test data than the head and verb patterns since PSet_target andPSet_subject have the higher coverage for the test data than PSet_head andPSet_verb, as shown in Table 6.Table 7.
Performance on feature combinationCombination of features MRRcommon features 47.75common features + P_target 51.96common features + P_head 49.00common features + P_subject 50.22common features + P_verb 47.947   ConclusionIn this paper, we study the syntactic relation patterns for question answering.
Weextract the various syntactic relations between the answers and different types ofquestion words, including target words, head words, subject words and verbs andscore the extracted relations based on support and confidence measures.
We furtherpropose a QA-specific tree kernel to partially match the relation patterns from theunseen data to the pattern sets.
Lastly, we incorporate the patterns and some com-mon features into a Maximum Entropy Model to rank the answer candidates.
Theexperiment shows that the syntactic relation patterns improve the performance by6.91 MRR based on the common features.
Moreover, the contributions of the pat-tern matching methods are evaluated.
The results show that the tree kernel-basedpartial matching outperforms the exact matching by 1.48 MRR.
In the future, weare to further explore the syntactic relations using the web data rather than thetraining data.518 D. Shen, G.-J.M.
Kruijff, and D. KlakowReferences1.
Berger, A., Della Pietra, S., Della Pietra, V.: A maximum entropy approach to natural lan-guage processing.
Computational Linguistics (1996), vol.
22, no.
1, pp.
39-712.
Collins, M.: A New Statistical Parser Based on Bigram Lexical Dependencies.
In: Pro-ceedings of ACL-96 (1996) 184-1913.
Collins, M.: New Ranking Algorithms for Parsing and Tagging: Kernel over DiscreteStructures, and the Voted Perceptron.
In: Proceeings of ACL-2002 (2002).4.
Collins, M., Duffy, N.: Convolution Kernels for Natural Language.
Advances in NeuralInformation Processing Systems 14, Cambridge, MA.
MIT Press (2002)5.
Culotta, A., Sorensen, J.: Dependency Tree Kernels for Relation Extraction.
In: Proceed-ings of ACL-2004 (2004)6.
Darroch, J., Ratcliff, D.: Generalized iterative scaling for log-linear models.
The annualsof Mathematical Statistics (1972), vol.
43, pp.
1470-14807.
Echihabi, A., Hermjakob, U., Hovy, E., Marcu, D., Melz, E., Ravichandran, D.: Multiple-Engine Question Answering in TextMap.
In: Proceedings of the TREC-2003 Conference,NIST (2003)8.
Fellbaum, C.: WordNet - An Electronic Lexical Database.
MIT Press, Cambridge, MA(1998)9.
Harabagiu, S., Moldovan, D., Clark, C., Bowden, M., Williams, J., Bensley, J.: AnswerMining by Combining Extraction Techniques with Abductive Reasoning.
In: Proceedingsof the TREC-2003 Conference, NIST (2003)10.
Ittycheriah, A., Roukos, S.: IBM's Statistical Question Answering System - TREC 11.
In:Proceedings of the TREC-2002 Conference, NIST (2002)11.
Levenshtein, V. I.: Binary Codes Capable of Correcting Deletions, Insertions and Rever-sals.
Doklady Akademii Nauk SSSR 163(4) (1965) 845-84812.
Ravichandran, D., Hovy, E., Och, F. J.: Statistical QA - Classifier vs. Re-ranker: What'sthe difference?
In: Proceedings of Workshop on Multilingual Summarization and QuestionAnswering, ACL (2003)13.
Ravichandran, D., Hovy, E.: Learning Surface Text Patterns for a Question AnsweringSystem.
In: Proceedings of ACL-2002 (2002) 41-4714.
Soubbotin, M. M., Soubbotin, S. M.: Patterns of Potential Answer Expressions as Clues tothe Right Answer.
In: Proceedings of the TREC-10 Conference, NIST (2001)15.
Xu, J., Licuanan, A., May, J., Miller, S., Weischedel, R.: TREC 2002 QA at BBN: AnswerSelection and Confidence Estimation.
In: Proceedings of the TREC-2002 Conference,NIST (2002)16.
Vapnik, V.: Statistical Learning Theory, John Wiley, NY, (1998) 732.17.
Zelenko, D., Aone, C., Richardella, A.: Kernel Methods for Relation Extraction.
Journal ofMachine Learning Research (2003) 1083-1106.
