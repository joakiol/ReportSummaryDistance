Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 1?10,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsGraph-based alignment of narratives for automated neurological assessmentEmily T. Prud?hommeaux and Brian RoarkCenter for Spoken Language UnderstandingOregon Health & Science University{emilypx,roarkbr}@gmail.comAbstractNarrative recall tasks are widely used in neu-ropsychological evaluation protocols in or-der to detect symptoms of disorders suchas autism, language impairment, and demen-tia.
In this paper, we propose a graph-basedmethod commonly used in information re-trieval to improve word-level alignments inorder to align a source narrative to narra-tive retellings elicited in a clinical setting.From these alignments, we automatically ex-tract narrative recall scores which can then beused for diagnostic screening.
The signifi-cant reduction in alignment error rate (AER)afforded by the graph-based method resultsin improved automatic scoring and diagnos-tic classification.
The approach described hereis general enough to be applied to almost anynarrative recall scenario, and the reductions inAER achieved in this work attest to the po-tential utility of this graph-based method forenhancing multilingual word alignment andalignment of comparable corpora for morestandard NLP tasks.1 IntroductionMuch of the work in biomedical natural languageprocessing has focused on mining information fromelectronic health records, clinical notes, and medicalliterature, but NLP is also very well suited for ana-lyzing patient language data, in terms of both con-tent and linguistic features, for neurological eval-uation.
NLP-driven analysis of clinical languagedata has been used to assess language development(Sagae et al, 2005), language impairment (Gabaniet al, 2009) and cognitive status (Roark et al, 2007;Roark et al, 2011).
These approaches rely on the ex-traction of syntactic features from spoken languagetranscripts in order to identify characteristics of lan-guage use associated with a particular disorder.
Inthis paper, rather than focusing on linguistic fea-tures, we instead propose an NLP-based method forautomating the standard manual method for scoringthe Wechsler Logical Memory (WLM) subtest of theWechsler Memory Scale (Wechsler, 1997) with theeventual goal of developing a screening tool for MildCognitive Impairment (MCI), the earliest observableprecursor to dementia.
During standard administra-tion of the WLM, the examiner reads a brief narra-tive to the subject, who then retells the story to theexaminer, once immediately upon hearing the storyand a second time after a 30-minute delay.
The ex-aminer scores the retelling in real time by countingthe number of recalled story elements, each of whichcorresponds to a word or short phrase in the sourcenarrative.
Our method for automatically extractingthe score from a retelling relies on an alignment be-tween substrings in the retelling and substrings inthe original narrative.
The scores thus extracted canthen be used for diagnostic classification.Previous approaches to alignment-based narra-tive analysis (Prud?hommeaux and Roark, 2011a;Prud?hommeaux and Roark, 2011b) have relied ex-clusively on modified versions of standard wordalignment algorithms typically applied to large bilin-gual parallel corpora for building machine transla-tion models (Liang et al, 2006; Och et al, 2000).Scores extracted from the alignments produced us-ing these algorithms achieved fairly high classifi-1cation accuracy, but the somewhat weak alignmentquality limited performance.
In this paper, we com-pare these word alignment approaches to a new ap-proach that uses traditionally-derived word align-ments between retellings as the input for graph-based exploration of the alignment space in order toimprove alignment accuracy.
Using both earlier ap-proaches and our novel method for word alignment,we then evaluate the accuracy of automated scoringand diagnostic classification for MCI.Although the alignment error rates for our datamight be considered high in the context of buildingphrase tables for machine translation, the alignmentsproduced using the graph-based method are remark-ably accurate given the small size of our trainingcorpus.
In addition, these more accurate alignmentslead to gains in scoring accuracy and to classificationperformance approaching that of manually derivedscores.
This method for word alignment and scoreextraction is general enough to be easily adaptedto other tests used in neuropsychological evalua-tion, including not only those related to narrative re-call, such as the NEPSY Narrative Memory subtest(Korkman et al, 1998) but also picture descriptiontasks, such as the Cookie Theft picture descriptiontask of the Boston Diagnostic Aphasia Examination(Goodglass et al, 2001) or the Renfrew Bus Story(Glasgow and Cowley, 1994).
In addition, this tech-nique has the potential to improve word alignmentfor more general NLP tasks that rely on small cor-pora, such as multilingual word alignment or wordalignment of comparable corpora.2 BackgroundThe act of retelling or producing a narrative taps intoa wide array of cognitive functions, not only mem-ory but also language comprehension, language pro-duction, executive function, and theory of mind.
Theinability to coherently produce or recall a narrativeis therefore associated with many different cogni-tive and developmental disorders, including demen-tia, autism (Tager-Flusberg, 1995), and language im-pairment (Dodwell and Bavin, 2008; Botting, 2002).Narrative tasks are widely used in neuropsycholog-ical assessment, and many commonly used instru-ments and diagnostic protocols include a task in-volving narrative recall or production (Korkman etal., 1998; Wechsler, 1997; Lord et al, 2002).In this paper, we focus on evaluating narrative re-call within the context of Mild Cognitive Impair-ment (MCI), the earliest clinically significant pre-cursor of dementia.
The cognitive and memoryproblems associated with MCI do not necessarilyinterfere with daily living activities (Ritchie andTouchon, 2000) and can therefore be difficult todiagnose using standard dementia screening tools,such as the Mini-Mental State Exam (Folstein et al,1975).
A definitive diagnosis of MCI requires anextensive interview with the patient and a familymember or caregiver.
Because of the effort requiredfor diagnosis and the insensitivity of the standardscreening tools, MCI frequently goes undiagnosed,delaying the introduction of appropriate treatmentand remediation.
Early and unobtrusive detectionwill become increasingly important as the elderlypopulation grows and as research advances in delay-ing and potentially stopping the progression of MCIinto moderate and severe dementia.Narrative recall tasks, such as the test used in re-search presented here, the Wechsler Logical Mem-ory subtest (WLM), are often used in conjunctionwith other cognitive measures in attempts to identifyMCI and dementia.
Multiple studies have demon-strated a significant difference in performance on theWLM between subjects with MCI and typically ag-ing controls, particularly in combination with testsof verbal fluency and memory (Storandt and Hill,1989; Peterson et al, 1999; Nordlund et al, 2005).The WLM can also serve as a cognitive indicator ofphysiological characteristics associated with symp-tomatic Alzheimers disease, even in the absence ofpreviously reported dementia (Schmitt et al, 2000;Bennett et al, 2006).Some previous work on automated analysis of theWLM has focused on using the retellings as a sourceof linguistic data for extracting syntactic and pho-netic features that can distinguish subjects with MCIfrom typically aging controls (Roark et al, 2011).There has been some work on automating scoringof other narrative recall tasks using unigram overlap(Hakkani-Tur et al, 2010), but Dunn et al (2002)are among the only researchers to apply automatedmethods to scoring the WLM for the purpose ofidentifying dementia, using latent semantic analysisto measure the semantic distance between a retelling2Dx n Age EducationMCI 72 88.7 14.9 yrNon-MCI 163 87.3 15.1 yrTable 1: Subject demographic data.and the source narrative.
Although scoring automa-tion is not typically used in a clinical setting, theobjectivity offered by automated measures is par-ticularly important for tests like the WLM, whichare often administered by practitioners working in acommunity setting and serving a diverse population.Researchers working on NLP tasks such as para-phrase extraction (Barzilay and McKeown, 2001),word-sense disambiguation (Diab and Resnik,2002), and bilingual lexicon induction (Sahlgren andKarlgren, 2005), often rely on aligned parallel orcomparable corpora.
Recasting the automated scor-ing of a neuropsychological test as another NLP taskinvolving the analysis of parallel texts, however, is arelatively new idea.
We hope that the methods pre-sented here will both highlight the flexibility of tech-niques originally developed for standard NLP tasksand attract attention to the wide variety of biomed-ical data sources and potential clinical applicationsfor these techniques.3 Data3.1 SubjectsThe data examined in this study was collected fromparticipants in a longitudinal study on brain agingat the Layton Aging and Alzheimers Disease Cen-ter at the Oregon Health and Science University(OHSU), including 72 subjects with MCI and 163typically aging seniors roughly matched for age andyears of education.
Table 1 shows the mean ageand mean years of education for the two diagnos-tic groups.
There were no significant between-groupdifferences in either measure.Following (Shankle et al, 2005), we assign a di-agnosis of MCI according to the Clinical DementiaRating (CDR) (Morris, 1993).
A CDR of 0.5 corre-sponds to MCI (Ritchie and Touchon, 2000), whilea CDR of zero indicates the absence of MCI or anydementia.
The CDR is measured via the Neurobe-havioral Cognitive Status Examination (Kiernan etal., 1987) and a semi-structured interview with thepatient and a family member or caregiver that allowsthe examiner to assess the subject in several key ar-eas of cognitive function, such as memory, orienta-tion, problem solving, and personal care.
The CDRhas high inter-annotator reliability (Morris, 1993)when conducted by trained experts.
It is crucial tonote that the calculation of CDR is completely inde-pendent of the neuropsychological test investigatedin this paper, the Wechsler Logical Memory subtestof the Wechsler Memory Scale.
We refer readers tothe above cited papers for a further details.3.2 Wechsler Logical Memory TestThe Wechsler Logical Memory subtest (WLM) ispart of the Wechsler Memory Scale (Wechsler,1997), a diagnostic instrument used to assess mem-ory and cognition in adults.
In the WLM, the subjectlistens to the examiner read a brief narrative, shownin Figure 1.
The subject then retells the narrative tothe examiner twice: once immediately upon hearingit (Logical Memory I, LM-I) and again after a 30-minute delay (Logical Memory II, LM-II).
The nar-rative is divided into 25 story elements.
In Figure 1,the boundaries between story elements are denotedby slashes.
The examiner notes in real time whichstory elements the subject uses.
The score that is re-ported under standard administration of the task isa summary score, which is simply the raw numberof story elements recalled.
Story elements do notneed to be recalled verbatim or in the correct tempo-ral order.
The published scoring guidelines describethe permissible substitutions for each story element.The first story element, Anna, can be replaced in theretelling with Annie or Ann, while the 16th storyelement, fifty-six dollars, can be replaced with anynumber of dollars between fifty and sixty.An example LM-I retelling is shown in Figure 2.According to the published scoring guidelines, thisretelling receives a score of 12, since it contains thefollowing 12 elements: Anna, employed, Boston, asa cook, was robbed of, she had four, small children,reported, station, touched by the woman?s story,took up a collection, and for her.3.3 Word alignment dataThe Wechsler Logical Memory immediate and de-layed retellings for all of the 235 experimental sub-jects were transcribed at the word level.
We sup-3Anna / Thompson / of South / Boston / em-ployed / as a cook / in a school / cafeteria /reported / at the police / station / that she hadbeen held up / on State Street / the night be-fore / and robbed of / fifty-six dollars.
/ Shehad four / small children / the rent was due /and they hadn?t eaten / for two days.
/ The po-lice / touched by the woman?s story / took upa collection / for her.Figure 1: Text of WLM narrative segmented into 25 storyelements.Ann Taylor worked in Boston as a cook.
Andshe was robbed of sixty-seven dollars.
Is thatright?
And she had four children and reportedat the some kind of station.
The fellow wassympathetic and made a collection for her sothat she can feed the children.Figure 2: Sample retelling of the Wechsler narrative.plemented the data collected from our experimentalsubjects with transcriptions of retellings from 26 ad-ditional individuals whose diagnosis had not beenconfirmed at the time of publication or who didnot meet the eligibility criteria for this study.
Par-tial words, punctuation, and pause-fillers were ex-cluded from all transcriptions used for this study.The retellings were manually scored according topublished guidelines.
In addition, we manually pro-duced word-level alignments between each retellingand the source narrative presented in Figure 1.Word alignment for phrase-based machine trans-lation typically takes as input a sentence-alignedparallel corpus or bi-text, in which a sentence onone side of the corpus is a translation of the sen-tence in that same position on the other side of thecorpus.
Since we are interested in learning how toalign words in the source narrative to words in theretellings, our primary parallel corpus must consistof source narrative text on one side and retellingtext on the other.
Because the retellings containomissions, reorderings, and embellishments, we areobliged to consider the full text of the source narra-tive and of each retelling to be a ?sentence?
in theparallel corpus.We compiled three parallel corpora to be used forthe word alignment experiments:?
Corpus 1: A roughly 500-line source-to-retelling corpus consisting of the source narra-tive on one side and each retelling on the other.?
Corpus 2: A roughly 250,000-line pairwiseretelling-to-retelling corpus, consisting of ev-ery possible pairwise combination of retellings.?
Corpus 3: A roughly 900-line word identitycorpus, consisting of every word that appearsin every retelling and the source narrative.The explicit parallel alignments of word identitiesthat compose Corpus 3 are included in order to en-courage the alignment of a word in a retelling to thatsame word in the source, if it exists.The word alignment techniques that we use areentirely unsupervised.
Therefore, as in the casewith most experiments involving word alignment,we build a model for the data we wish to evalu-ate using that same data.
We do, however, use theretellings from the 26 individuals who were not ex-perimental subjects as a development set for tuningthe various parameters of our system, which is de-scribed below.4 Word Alignment4.1 Baseline alignmentWe begin by building two word alignment modelsusing the Berkeley aligner (Liang et al, 2006), astate-of-the-art word alignment package that relieson IBM mixture models 1 and 2 (Brown et al, 1993)and an HMM.
We chose to use the Berkeley aligner,rather than the more widely used Giza++ alignmentpackage, for this task because its joint training andposterior decoding algorithms yield lower alignmenterror rates on most data sets and because it offersfunctionality for testing an existing model on newdata and for outputting posterior probabilities.
Thesmaller of our two Berkeley-generated models istrained on Corpus 1 (the source-to-retelling parallelcorpus described above) and ten copies of Corpus3 (the word identity corpus).
The larger model istrained on Corpus 1, Corpus 2 (the pairwise retellingcorpus), and 100 copies of Corpus 3.
Both modelsare then tested on the 470 retellings from our 235 ex-perimental subjects.
In addition, we use both mod-els to align every retelling to every other retelling sothat we will have all pairwise alignments availablefor use in the graph-based model.4Figure 3: Depiction of word graph.The first two rows of Table 2 show the preci-sion, recall, F-measure, and alignment error rate(AER) (Och and Ney, 2003) for these two Berkeleyaligner models.
We note that although AER for thelarger model is lower, the time required to train themodel is significantly larger.
The alignments gen-erated by the Berkeley aligner serve not only as abaseline for comparison but also as a springboardfor the novel graph-based method of alignment wewill now discuss.4.2 Graph-based refinementGraph-based methods, in which paths or randomwalks are traced through an interconnected graph ofnodes in order to learn more about the nodes them-selves, have been used for various NLP tasks in in-formation extraction and retrieval, including web-page ranking (PageRank (Page et al, 1999)) and ex-tractive summarization (LexRank (Erkan and Radev,2004; Otterbacher et al, 2009)).
In the PageRank al-gorithm, the nodes of the graph are web pages andthe edges connecting the nodes are the hyperlinksleading from those pages to other pages.
The nodesin the LexRank algorithm are sentences in a docu-ment and the edges are the similarity scores betweenthose sentences.
The likelihood of a random walkthrough the graph starting at a particular node andending at another node provides information aboutthe relationship between those two nodes and the im-portance of the starting node.In the case of our graph-based method for wordalignment, each node represents a word in one of theretellings or in the source narrative.
The edges areFigure 4: Changes in AER as ?
increases.the normalized posterior-weighted alignments thatthe Berkeley aligner proposes between each wordand (1) words in the source narrative, and (2) wordsin the other retellings, as depicted in Figure 3.
Start-ing at a particular node (i.e., a word in one of theretellings), our algorithm can either walk from thatnode to another node in the graph or to a word inthe source narrative.
At each step in the walk, thereis a set probability ?
that determines the likelihoodof transitioning to another retelling word versus aword in the source narrative.
When transitioning toa retelling word, the destination word is chosen ac-cording to the posterior probability assigned by theBerkeley aligner to that alignment.
When the walkarrives at a source narrative word, that word is thenew proposed alignment for the starting word.For each word in each retelling, we perform 1000of these random walks, thereby generating a distri-bution for each retelling word over all of the wordsin the source narrative.
The new alignment for theword is the source word with the highest frequencyin that distribution.We build two graphs on which to carry out theserandom walks: one graph is built using the align-ments generated by the smaller Berkeley alignmentmodel, and the other is built from the alignmentsgenerated by the larger Berkeley alignment model.Alignments with posterior probabilities of 0.5 orgreater are included as edges within the graph, sincethis is the default posterior threshold used by theBerkeley aligner.
The value of ?, the probability ofwalking to a retelling word node rather than a sourceword, is tuned to the development set of retellings,5Model P R F AERBerkeley-Small 72.1 79.6 75.6 24.5Berkeley-Large 78.6 80.5 79.5 20.5Graph-Small 77.9 81.2 79.5 20.6Graph-Large 85.4 76.9 81.0 18.9Table 2: Aligner performance comparison.discussed in Section 3.3.
Figure 4 shows how AERvaries according to the value of ?
for the two graph-based approaches.Each of these four alignment models produces,for each retelling, a set of word pairs containing oneword from the original narrative and one word fromthe retelling.
The manual gold alignments for the235 experimental subjects were evaluated againstthe alignments produced by each of the four models.Table 2 shows the accuracy of word alignment us-ing these two graph-based models in terms of preci-sion, accuracy, F-measure, and alignment error rate,alongside the same measures for the two Berkeleymodels.
We see that each of the graph-based modelsoutperforms the Berkeley model of the same size.The performance of the small graph-based model isespecially remarkable since it an AER comparableto the large Berkeley model while requiring signif-icantly fewer computing resources.
The differencein processing time between the two approaches wasespecially remarkable: the graph-based model com-pleted in only a few minutes, while the large Berke-ley model required 14 hours of training.Figures 5 and 6 show the results of aligningthe retelling presented in Figure 2 using the smallBerkeley model and the large graph-based model,respectively.
Comparing these two alignments, wesee that the latter model yields more precise align-ments with very little loss of recall, as is borne outby the overall statistics shown in Table 2.5 ScoringThe published scoring guidelines for the WLM spec-ify the source words that compose each story ele-ment.
Figure 7 displays the source narrative withthe element IDs (A?
Y ) and word IDs (1?
65) ex-plicitly labeled.
Element Q, for instance, consists ofthe words 39 and 40, small children.
Using this in-formation, we extract scores from the alignments asfollows: for each word in the original narrative, if[A anna1] [B thompson2] [C of3 south4][D boston5] [E employed6] [F as7 a8cook9] [G in10 a11 school12] [H cafeteria13][I reported14] [J at15 the16 police17] [Kstation18] [L that19 she20 had21 been22 held23up24] [M on25 state26 street27] [N the28night29 before30] [O and31 robbed32 of33] [Pfifty-six34 dollars35] [Q she36 had37 four38][R small39 children40] [S the41 rent42 was43due44] [T and45 they46 had47 n?t48 eaten49][U for50 two51 days52] [V the53 police54] [Wtouched55 by56 the57 woman?s58 story59] [Xtook60 up61 a62 collection63] [Y for64 her65]Figure 7: Text of Wechsler Logical Memory narrativewith story-element labeled bracketing and word IDs.anna(1) : Athompson(2) : Bemployed(6) : Eboston(5) : Dcook(9) : Frobbed(32) : Ofifty-six(34) : Pfour(38) : Qchildren(40) : Rreported(14) : Istation(18) : Ktook(60) : Xcollection(63) : Xfor(64) : Yher(65) : YFigure 8: Source content words from the alignment inFigure 6 with corresponding story element IDs.that word is aligned to a word in the retelling, thestory element that it is associated with is consideredto be recalled.
Figure 8 shows the story elementsextracted from the word alignments in Figure 6.When we convert alignments to scores in this way,any alignment can be mapped to an element, even analignment between function words such as the andof, which would be unlikely to indicate that the storyelement had been recalled.
To avoid such scoring er-rors, we disregard any word-alignment pair contain-ing a source function word.
The two exceptions tothis rule are the final two words, for her, which arenot content words but together make a single storyelement.The element-level scores induced from the fourword alignments for all 235 experimental sub-jects were evaluated against the manual per-elementscores.
We report the precision, recall, and f-measure for all four alignment models in Table 3.
Inaddition, report Cohen?s kappa as a measure of reli-ability between our automated scores and the man-ually assigned scores.
We see that as AER im-proves, scoring accuracy also improves, with thelarge graph-based model outperforming all othermodels in terms of precision, f-measure, and inter-6ann(1) : anna(1)worked(3) : employed(6)in(4) : in(10)boston(5) : boston(5)as(6) : as(7)a(7) : a(8)cook(8) : cook(9)and(9) : and(31)robbed(12) : robbed(32)of(13) : of(33)dollars(15) : dollars(35)is(16) : was(43)that(17) : that(19)and(19) : and(45)she(20) : she(36)had(21) : had(37)four(22) : four(38)children(23) : children(40)reported(25) : reported(14)at(26) : at(15)the(27) : the(16)some(28) : police(17)station(31) : station(18)made(37) : up(61)made(37) : took(60)a(38) : a(62)collection(39) : collection(63)for(40) : for(64)her(41) : her(65)so(42) : woman?s(58)she(44) : she(20)Figure 5: Word alignment generated by the small Berkeley alignment model with retelling words italicized.ann(1) : anna(1)taylor(2) : thompson(2)worked(3) : employed(6)in(4) : in(10)boston(5) : boston(5)as(6) : as(7)a(7) : a(8)cook(8) : cook(9)robbed(12) : robbed(32)of(13) : of(33)sixty-seven(14) : fifty-six(34)dollars(15) : dollars(35)she(20) : she(36)had(21) : had(37)four(22) : four(38)children(23) : children(40)reported(25) : reported(14)at(26) : at(15)the(27) : the(16)station(31) : station(18)made(37) : took(60)a(38) : a(62)collection(39) : collection(63)for(40) : for(64)her(41) : her(65)Figure 6: Word alignment generated by the large graph-based model with retelling words italicized.Model P R F ?Berkeley-Small 87.2 88.9 88.0 76.1Berkeley-Large 86.8 90.7 88.7 77.1Graph-Small 84.7 93.6 88.9 76.9Graph-Big 88.8 89.3 89.1 78.3Table 3: Scoring accuracy results.rater reliability.
The scoring accuracy levels re-ported here are comparable to the levels of inter-rateragreement typically reported for the WLM, and re-liability between our automated scores and the man-ual scores, as measured by Cohen?s kappa, is wellwithin the ranges reported in the literature (Johnsonet al, 2003).
As will be shown in the following sec-tion, scoring accuracy is very important for achiev-ing high diagnostic classification accuracy, which isthe ultimate goal of this work.6 Diagnostic ClassificationAs discussed in Section 2, poor performance on theWechsler Logical Memory test is associated withMild Cognitive Impairment.
We now use the scoreswe have extracted from the word alignments as fea-tures with a support vector machine (SVM) to per-form diagnostic classification for distinguishing sub-jects with MCI from those without.
For each of the235 experimental subjects, we generate 2 summaryscores: one for the immediate retelling and one forthe delayed retelling.
The summary score rangesfrom 0, indicating that no elements were recalled,to 25, indicating that all elements were recalled.
Inaddition to the summary score, we also provide theSVM with a vector of 50 per-element scores: foreach of the 25 element in each of the two retellingsper subject, there is a vector element with the valueof 0 if the element was not recalled, or 1 if the el-ement was recalled.
Since previous work has indi-cated that certain elements may be more powerful intheir ability to predict the presence of MCI, we ex-pect that giving the SVM these per-elements scoresmay improve classification performance.
To trainand test our classifiers, we use the WEKA API (Hallet al, 2009) and LibSVM (Chang and Lin, 2011),with a second-order polynomial kernel and defaultparameter settings.We evaluate the performance of the SVMs us-ing a leave-pair-out validation scheme (Cortes et al,2007; Pahikkala et al, 2008).
In the leave-pair-outtechnique, every pairing between a negative exam-ple and a positive example is tested using a classi-fier trained on all of the remaining examples.
Theresulting pairs of scores can be used to calculatethe area under the receiver operating characteristic(ROC) curve (Egan, 1975), which is a plot of thefalse positive rate of a classifier against its true pos-itive rate.
The area under this curve (AUC) has a7Model Summ.
(s.d.)
Elem.
(s.d.
)Manual Scores 73.3 (3.76) 81.3 (3.32)Berkeley-Small 73.7 (3.74) 77.9 (3.52)Berkeley-Big 75.1 (3.67) 79.2 (3.45)Graph-Small 74.2 (3.71) 78.9 (3.47)Graph-Big 74.8 (3.69) 78.6 (3.49)Table 4: Classification accuracy results (AUC).value of 0.5 when the classifier performs at chanceand a value 1.0 when perfect classification accuracyis achieved.Table 4 shows the classification results for thescores derived from the four alignment models alongwith the classification results using the examiner-assigned manual scores.
It appears that, in all cases,the per-element scores are more effective than thesummary scores in classifying the two diagnosticgroups.
In addition, we see that our automatedscores have classificatory power comparable to thatof the manual gold scores, and that as scoring ac-curacy increases from the small Berkeley model tothe graph-based models and bigger models, classifi-cation accuracy improves.
This suggests both thataccurate scores are crucial for accurate classifica-tion and that pursuing even further improvements inword alignment is likely to result in improved di-agnostic differentiation.
We note that although thelarge Berkeley model achieved the highest classi-fication accuracy, this very slight margin of differ-ence may not justify its significantly greater compu-tational requirements.7 Conclusions and Future WorkThe work presented here demonstrates the utilityof adapting techniques drawn from a diverse set ofNLP research areas to tasks in biomedicine.
In par-ticular, the approach we describe for automaticallyanalyzing clinically elicited language data showspromise as part of a pipeline for a screening tool forMild Cognitive Impairment.
Our novel graph-basedapproach to word alignment resulted in large reduc-tions in alignment error rate.
These reductions in er-ror rate in turn led to human-level scoring accuracyand improved diagnostic classification.As we have mentioned, the methods outlined hereare general enough to be used for other episodicrecall and description scenarios.
Although the re-sults are quite robust, several enhancements and im-provements should be made before we apply the sys-tem to other tasks.
First, although we were able toachieve decent word alignment accuracy, especiallywith our graph-based approach, many alignment er-rors remain.
As shown in Figure 4, the graph-basedalignment technique could potentially result in anAER of as low as 11%.
We expect that our deci-sion to select as a new alignment the most frequentsource word over the distribution of source words atthe end of 1000 walks could be improved, since itdoes not allow for one-to-many mappings.
In addi-tion, it would be worthwhile to experiment with sev-eral posterior thresholds, both during the decodingstep of the Berkeley aligner and in the graph edges.In order to produce a viable clinical screeningtool, it is crucial that we incorporate speech recogni-tion in the pipeline.
Our very preliminary investiga-tion into using ASR to generate transcripts for align-ment seems promising and surprisingly robust to theproblems that might be expected when working withnoisy audio.
In our future work, we also plan to ex-amine longitudinal data for individual subjects to seewhether our techniques can detect subtle differencesin recall and coherence between a recent retellingand a series of earlier baseline retellings.
Since themetric commonly used to quantify the progressionof dementia, the Clinical Dementia Rating, relies onobserved changes in cognitive function over time,longitudinal analysis of performance on the Wech-sler Logical Memory task may be the most promis-ing application for our research.ReferencesRegina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Pro-ceeding of ACL.D.A.
Bennett, J.A.
Schneider, Z. Arvanitakis, J.F.
Kelly,N.T.
Aggarwal, R.C.
Shah, and R.S.
Wilson.
2006.Neuropathology of older persons without cognitiveimpairment from two community-based studies.
Neu-rology, 66:1837?844.Nicola Botting.
2002.
Narrative as a tool for the assess-ment of linguistic and pragmatic impairments.
ChildLanguage Teaching and Therapy, 18(1).Peter Brown, Vincent Della Pietra, Stephen Della Pietra,and Robert Mercer.
1993.
The mathematics of statis-8tical machine translation: Parameter estimation.
Com-putational Linguistics, 19(2):263?311.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2(27):1?27.Corinna Cortes, Mehryar Mohri, and Ashish Rastogi.2007.
An alternative ranking problem for search en-gines.
In Proceedings of WEA2007, LNCS 4525,pages 1?21.
Springer-Verlag.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proceedings of ACL.Kristy Dodwell and Edith L. Bavin.
2008.
Childrenwith specific language impair ment: an investigation oftheir narratives and memory.
International Journal ofLanguage and Communication Disorders, 43(2):201?218.John C. Dunn, Osvaldo P. Almeida, Lee Barclay, AnnaWaterreus, and Leon Flicker.
2002.
Latent seman-tic analysis: A new method to measure prose recall.Journal of Clinical and Experimental Neuropsychol-ogy, 24(1):26?35.James Egan.
1975.
Signal Detection Theory and ROCAnalysis.
Academic Press.Gu?nes Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in text sum-marization.
J. Artif.
Intell.
Res.
(JAIR), 22:457?479.M.
Folstein, S. Folstein, and P. McHugh.
1975.
Mini-mental state - a practical method for grading the cog-nitive state of patients for the clinician.
Journal of Psy-chiatric Research, 12:189?198.Keyur Gabani, Melissa Sherman, Thamar Solorio, andYang Liu.
2009.
A corpus-based approach for theprediction of language impairment in monolingual En-glish and Spanish-English bilingual children.
In Pro-ceedings of NAACL-HLT, pages 46?55.Cheryl Glasgow and Judy Cowley.
1994.
RenfrewBus Story test - North American Edition.
CentrevilleSchool.H Goodglass, E Kaplan, and B Barresi.
2001.
BostonDiagnostic Aphasia Examination.
3rd ed.
Pro-Ed.Dilek Hakkani-Tur, Dimitra Vergyri, and Gokhan Tur.2010.
Speech-based automated cognitive status as-sessment.
In Proceedings of Interspeech, pages 258?261.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1).David K. Johnson, Martha Storandt, and David A. Balota.2003.
Discourse analysis of logical memory recall innormal aging and in dementia of the alzheimer type.Neuropsychology, 17(1):82?92.R.J.
Kiernan, J. Mueller, J.W.
Langston, and C. VanDyke.
1987.
The neurobehavioral cognitive sta-tus examination, a brief but differentiated approach tocognitive assessment.
Annals of Internal Medicine,107:481?485.Marit Korkman, Ursula Kirk, and Sally Kemp.
1998.NEPSY: A developmental neuropsychological assess-ment.
The Psychological Corporation.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT NAACL.Catherine Lord, Michael Rutter, Pamela DiLavore, andSusan Risi.
2002.
Autism Diagnostic ObservationSchedule (ADOS).
Western Psychological Services.John Morris.
1993.
The clinical dementia rating(CDR): Current version and scoring rules.
Neurology,43:2412?2414.A Nordlund, S Rolstad, P Hellstrom, M Sjogren,S Hansen, and A Wallin.
2005.
The goteborg mcistudy: mild cognitive impairment is a heterogeneouscondition.
Journal of Neurology, Neurosurgery andPsychiatry, 76:1485?1490.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och, Christoph Tillmann, , and HermannNey.
2000.
Improved alignment models for statisti-cal machine translation.
In Proceedings of ACL, pages440?447.Jahna Otterbacher, Gu?nes Erkan, and Dragomir R. Radev.2009.
Biased lexrank: Passage retrieval using randomwalks with question-based priors.
Inf.
Process.
Man-age., 45(1):42?54.Lawrence Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1999.
The pagerank citation ranking:Bringing order to the web.
Technical Report 1999-66, Stanford InfoLab, November.
Previous number =SIDL-WP-1999-0120.Tapio Pahikkala, Antti Airola, Jorma Boberg, and TapioSalakoski.
2008.
Exact and efficient leave-pair-outcross-validation for ranking RLS.
In Proceedings ofAKRR 2008, pages 1?8.Ronald Peterson, Glenn Smith, Stephen Waring, RobertIvnik, Eric Tangalos, and Emre Kokmen.
1999.
Mildcognitive impairment: Clinical characterizations andoutcomes.
Archives of Neurology, 56:303?308.Emily T. Prud?hommeaux and Brian Roark.
2011a.Alignment of spoken narratives for automated neu-ropsychological assessment.
In Proceedings of ASRU.Emily T. Prud?hommeaux and Brian Roark.
2011b.
Ex-traction of narrative recall patterns for neuropsycho-logical assessment.
In Proceedings of Interspeech.Karen Ritchie and Jacques Touchon.
2000.
Mild cogni-tive impairment: Conceptual basis and current noso-logical status.
Lancet, 355:225?228.9Brian Roark, Margaret Mitchell, and Kristy Holling-shead.
2007.
Syntactic complexity measures for de-tecting mild cognitive impairment.
In Proceedings ofthe ACL 2007 Workshop on Biomedical Natural Lan-guage Processing (BioNLP), pages 1?8.Brian Roark, Margaret Mitchell, John-Paul Hosom,Kristina Hollingshead, and Jeffrey Kaye.
2011.
Spo-ken language derived measures for detecting mildcognitive impairment.
IEEE Transactions on Audio,Speech and Language Processing, 19(7):2081?2090.Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005.Automatic measurement of syntactic development inchild language.
In Proceedings of ACL, pages 197?204.Magnus Sahlgren and Jussi Karlgren.
2005.
Automaticbilingual lexicon acquisition using random indexingof parallel corpora.
Natural Language Engineering,11(3).F.A.
Schmitt, D.G.
Davis, D.R.
Wekstein, C.D.
Smith,J.W.
Ashford, and W.R. Markesbery.
2000.
Preclini-cal ad revisited: Neuropathology of cognitively normalolder adults.
Neurology, 55:370?376.William R. Shankle, A. Kimball Romney, Junko Hara,Dennis Fortier, Malcolm B. Dick, James M. Chen,Timothy Chan, and Xijiang Sun.
2005.
Methodsto improve the detection of mild cognitive impair-ment.
Proceedings of the National Academy of Sci-ences, 102(13):4919?4924.Martha Storandt and Robert Hill.
1989.
Very mild seniledementia of the alzheimers type: Ii psychometric testperformance.
Archives of Neurology, 46:383?386.Helen Tager-Flusberg.
1995.
Once upon a ribbit: Storiesnarrated by autistic children.
British journal of devel-opmental psychology, 13(1):45?59.David Wechsler.
1997.
Wechsler Memory Scale - ThirdEdition Manual.
The Psychological Corporation.10
