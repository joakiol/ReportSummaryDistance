Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 68?78,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAutomatic evaluation of spoken summaries: the case of languageassessmentAnastassia Loukina, Klaus Zechner, Lei ChenEducational Testing Service (ETS)Princeton, NJ 08541, USAaloukina@ets.org, kzechner@ets.org, lchen@ets.orgAbstractThis paper investigates whether ROUGE, apopular metric for the evaluation of au-tomated written summaries, can be ap-plied to the assessment of spoken sum-maries produced by non-native speakersof English.
We demonstrate that ROUGE,with its emphasis on the recall of infor-mation, is particularly suited to the as-sessment of the summarization quality ofnon-native speakers?
responses.
A stan-dard baseline implementation of ROUGE-1 computed over the output of the au-tomated speech recognizer has a Spear-man correlation of ?
= 0.55 with experts?scores of speakers?
proficiency (?
= 0.51for a content-vector baseline).
Further in-creases in agreement with experts?
scorescan be achieved by using types instead oftokens for the computation of word fre-quencies for both candidate and referencesummaries, as well as by using multiplereference summaries instead of a singleone.
These modifications increase the cor-relation with experts?
scores to a Spear-man correlation of ?
= 0.65.
Furthermore,we found that the choice of reference sum-maries does not have any impact on per-formance, and that the adjusted metric isalso robust to errors introduced by auto-mated speech recognition (?
= 0.67 for hu-man transcriptions vs. ?
= 0.65 for speechrecognition output).1 IntroductionIn this paper we explore whether metrics com-monly used for the automated evaluation of writ-ten summaries can be used to evaluate spokensummaries in the context of language assessment.The performance of automatic summarizationsystems is routinely evaluated using content met-rics such as ROUGE (Lin and Rey, 2004), whichmeasures the n-gram overlap between the candi-date summary and a set of reference summaries(see also Rankel et al.
(2013) for historical back-ground).
ROUGE is a recall-oriented metric in-spired by its precision-oriented counterpart BLEU,developed to evaluate machine translations (Pap-ineni et al., 2002).
Recent research in this area hasbeen focused on identifying the most reliable vari-ants of ROUGE and best practices in the applicationof the metric (Owczarzak et al., 2012; Rankel etal., 2013).
These studies (reviewed in more detailin Section 2.1) showed that less commonly usedvariants of ROUGE may in fact be more consistentwith human judgments, at least in the context ofautomatic summary evaluation.Beyond the research in automatic summariza-tion systems, ROUGE has also been used to eval-uate written summaries in the context of educa-tional assessment.
Madnani et al.
(2013) showedthat one of the variants of ROUGE, in combinationwith other metrics, performed consistently wellfor the automated scoring of written responses tosummary tasks produced by middle- and high-school students.
They did not investigate the effectof using other variants of ROUGE.In this paper, we explore whether ROUGE can beused to automatically evaluate the content cover-age of spoken summaries produced by non-nativespeakers in the context of language assessment.As in case of automatic text summaries, the hu-man raters who score these responses are askedto assess whether the summary accurately con-veys the information contained in the stimulus.While the length of the spoken responses is moreloosely constrained than in case of automatic textsummaries, human raters do not penalize for ex-traneously irrelevant language.
Therefore recall-oriented ROUGE is an attractive evaluation metricfor this task.At the same time, unlike automatic text sum-68maries, spoken summaries are abstractive and of-ten contain ungrammatical sequences, repetitions,repairs, and other disfluencies.
Further ?noise?is introduced by transcription errors generated bythe automated speech recognition system.
In thisstudy, we assess whether (a) ROUGE is robustagainst this type of noise; (b) how many refer-ence summaries are necessary to obtain reliableevaluation; and (c) how the choice of specific ref-erence summaries affects the performance of themetric (Section 4.1).
We also assess which vari-ants of ROUGE have the most agreement with hu-man judgments on this type of summary and whatadjustments can be made to mitigate the effectsof disfluencies and errors introduced by automatedspeech recognition (Section 4.2).
Finally, we testhow well our adjusted variant of ROUGE can pre-dict the human scores on unseen data (Section4.3).2 Related work2.1 The application of ROUGE to evaluationof automatic text summarizationThere exist various versions of ROUGE which dif-fer in terms of the length of their n-grams, the useof skip-bigrams, the application of stemming, andthe exclusion of stop-words.
Several studies havecompared these variants to identify those mostconsistent with human judgments.
In earlier work,Lin (2004) reported that variants based on uni-grams and skip-bigrams (ROUGE-SU4) or bigramsalone (ROUGE-2) performed best.
ROUGE-2 wasalso identified as the best variant more recentlyby Owczarzak et al.
(2012).
Rankel et al.
(2013)found that linear combinations of these metricswith ROUGE based on longer n-grams are more ac-curate in finding significantly different systems.Previous work also explored various methodsof text pre-processing prior to the computation ofROUGE, including stemming and the removal ofstop-words, neither of which had any substantialeffect on the performance of ROUGE (Lin and Rey,2004; Owczarzak et al., 2012).
Owczarzak et al.
(2012) reported that the agreement with humanjudgments was, in fact, higher if the stop-wordswere retained.All applications discussed so far used ROUGEto evaluate the textual summarization of writtentexts.
There have also been attempts to applythis metric to text summaries of speech data withmixed results (see Nenkova and McKeown (2011)for a review).
ROUGE performed reasonably wellfor the evaluation of text summaries of spoken pre-sentations (Hirohata et al., 2005), but was not cor-related with the summary accuracy of summariesof meetings or conversations (although see (Pennand Zhu, 2008)).Most of this work was performed on extractivesummaries produced by summarization systemsthat used multiple summaries to evaluate each sys-tem.
In this study, we explore the application ofROUGE to the evaluation of abstractive summariesproduced by students in a language assessmentcontext with an aim of producing a separate evalu-ation for each summary.
Furthermore, the fact thatthese are spoken responses adds an extra layer ofcomplexity to the analysis, therefore the results ofprevious studies cannot directly be applied to thisnew context.2.2 Previous approaches to the contentevaluation of spoken summaries forassessment purposesThe research on the automated scoring of con-tent accuracy in a language assessment has pri-marily focused on the evaluation of written essays.Most previous approaches in this area have usedso-called ?bag-of-words?-based models, gleanedfrom the discipline of information retrieval.
Thebasic idea is that an essay is considered to behighly content relevant to a given topic when itcontains words that are similar to those seen inpreviously collected essays with high human-raterscores.
For instance, Attali and Burstein (2006)used a vector-space model to compute the co-sine similarities between word vectors found inan essay to be automatically scored and word vec-tors comprising previously scored essays with thesame human-rater score.
In a similar vein, Foltzet al.
(1999) computed a compressed vector spacebased on singular value decomposition for a setof document-word vectors, called latent semanticanalysis, and then computed similarity scores foressays based on this more compact representation.It should be noted, though, that since all of thesemodels do not take word sequences into account,they must be considered knowledge-poor in thatthey cannot distinguish between syntactic rolesor a list of random words versus a well-formedsentence.
In operational systems, such bag-of-words similarity features are combined with fea-tures which evaluate grammar and other aspects69of language use; therefore a random list of con-tent words is unlikely to lead to a high overallscore.
However, finer-grained distinctions such asnegations or subject-object relationships betweenwords are often lost.Applications of these methods to spontaneousspeech in spoken-language assessments have beenconducted much more recently as this domain oflanguage assessment relies on the output of Au-tomatic Speech Recognition systems (ASR) thattypically have a fairly high word-error rate.
Theseerrors can negatively affect the accuracy of themethods developed for written responses.
Fur-thermore, spoken responses differ in many proper-ties from written ones (Biber et al., 2004) and thevalidity of existing methods for assessing speechneeds to be established before they can be usedfor operational scoring.Xie et al.
(2012) presented experiments usingcontent features on spontaneous-speech data basedon vector-space models, latent semantic analysis,as well as point-wise mutual information.
Someof these content features showed higher correla-tions with human scores than features measuringother aspects of speaking proficiency, such as flu-ency or pronunciation.
Chen and Zechner (2012)also used a vector space model for the scoring ofspontaneous speech, but extended it by using theontological information contained in WordNet.
Fi-nally, Xiong et al.
(2013) used a variety of ap-proaches to capture the content of spontaneous re-sponses from the same corpus that we are investi-gating in this paper.
Approaches varied from com-puting the overlap between key words in the stim-uli and responses to a more traditional vector spacemodel based on content vector analysis.While these approaches have good correlationswith human scores, they have a number of short-comings.
The best performing method suggestedby Xiong et al.
(2013) requires the manual annota-tion of the relevant key words for each prompt be-fore the computation of the metric.
Vector spacemodels do not have this limitation, but they requirea substantial number of reference summaries toachieve consistent results.
Supporting this point,Chen (2013) showed that at least 50 reference re-sponses were necessary to obtain moderate agree-ment between the cosine similarity measure andhuman judgments, with further improvement inagreement as the number of reference responsesis increased to 200.
These limitations pose prac-tical difficulties when new items are added to thetests: the computation of content metrics for eachnew item requires either a manual annotation or arelatively large number of reference responses.ROUGE appears promising in this context sinceit does not have either of these limitations.
First,the computation of ROUGE does not require man-ual annotation.
Second, research on the evalua-tion of written summaries suggests that relativelyfew reference summaries may be necessary to ob-tain reliable results, e.g., only four references wereused for the summary evaluation at the Text Anal-ysis Conference (Rankel et al., 2013).
In addition,the recall-based nature of ROUGE is well-alignedwith the evaluation criteria for these responses.Therefore in this paper, we explore whether any ofthe variants of ROUGE can be successfully appliedto the content scoring of spoken summaries andwhat modifications may be necessary to achieveoptimal performance.3 Data and methodology3.1 Description of the corpusThe study is based on a corpus of responsescollected during the pilot administration of theTOEFLR?JuniorTMComprehensive test, an inter-national assessment of English proficiency tar-geted at middle-school students aged from 11 to15 (see also Xiong et al.
(2013) who used a subsetof this corpus).The corpus used in this study included 5,934spoken responses produced by 1,611 speakers; alllearners of English as a foreign language residingin different countries.
In addition to a read-aloudtask that was not relevant for this paper, the speak-ers were presented with four other tasks.
First, thespeakers were asked to describe a sequence of sixpictures.
For the remaining three taks, the speak-ers listened to one announcement and two frag-ments from a lecture and were then asked to sum-marize the content of what they heard.
The stu-dents were provided with a list of concepts that testtakers were expected to cover in their responses.For example, a student may have listened toa teacher giving an assignment in history class.1This assignment required the class to go to the li-brary, look up information about the water supplyin old and modern cities, answer the questions ontheir worksheet, and write a short paragraph about1http://toefljr.caltesting.org/sampletest/s-historylesson.html70their findings.
The students were then asked to re-spond to the following prompt:Imagine that your classmate was notin class today.
Tell your classmateabout what the history teacher askedthe students to do.
Be sure to talk aboutthe following:- the library- the worksheet- the homeworkThe corpus contained responses to 24 differentprompts with 6 different sets of prompts.
Eachspeaker only answered one set of prompts giving4 responses per speaker.
The recording time foreach response was limited to 60 seconds.
The ac-tual number of words varied between participantswith an average 72 words per response (?
= 29).From the originally recorded 6,444 responses,we excluded from further analysis 510 responses(about 8%), which contained either no speech orwhere the quality of the recording was too low forfurther analysis.
All remaining 5,934 responseswere scored on a scale of 1-4 by two expert humanraters on a holistic scale that reflects all aspectsof speaking proficiency, including pronunciation,grammar, and content coverage.2For content cov-erage, the raters were asked to consider whetherthe key information contained in the prompt wasconveyed accurately or, in case of the picture de-scription prompt, whether the story was complete.When the difference in the scores assigned by thetwo raters was greater than 1, the final score wasassigned by an adjudicator.The corpus was divided into non-overlappingtraining and testing partitions.
The training par-tition contained 3,337 responses from 915 speak-ers and the test partition contained 2,597 spokenresponses from 696 speakers.
Both partitions in-cluded responses for the same prompts but therewas no speaker overlap.All responses were converted to text usinga state-of-the-art automatic speech recognizer(ASR) with constrained vocabulary (see Evaniniand Wang (2013) for further details).
To evalu-ate the effect of the errors that may have been in-troduced by the ASR system, all responses were2see http://www.ets.org/s/toefl junior/pdf/toefl juniorcomprehensive speaking scoring guides.pdf for the scoringrubricstranscribed manually by professional human tran-scribers.
Comparison with the human transcrip-tion showed that the ASR word error rate for thiscorpus was 26.5% for picture narration tasks and29.4% for the summarization tasks.3.2 Computation of the metricsEvaluation metrics.
ROUGE was computed usingequation (1) as an n-gram (grn) overlap betweencandidate summary and each summary (S) fromthe set of reference summaries (RS).ROUGEN=?S?RS?grn?SCountoverlap(grn)?S?RS?grn?SCount(grn)(1)We used n-grams whereby n was in a rangefrom 1 to 4 (ROUGE 1-4) and a combinationof unigrams with skip-bigrams with maximumstep of four words (ROUGE-SU1-4).
Finally,we also computed a combined measure ROUGE-ALL which is the geometrical mean of ROUGE-1?ROUGE-4, computed by using the same smoothingprocedure as for BLEU (Papineni et al., 2002).We used the cosine distance (CVA) between theresponse and reference summaries as a baselinemetric as this metric is commonly used for eval-uating document similarity in the context of lan-guage assessment.
CVA was computed as the co-sine distance between candidate responses and thesame reference responses as used for the com-putation of ROUGE.
All term frequencies wereweighted using tf-idf where tf is the frequency ofa term in a given response and idf is the inversedocument frequency.
idf frequencies were com-puted based on all of the responses in the corpus.Reference summaries.
The reference sum-maries were selected from responses with thehighest human rater final score (4).
This approachis similar to using system outputs as pseudo-models for the evaluation of machine-translationor automatic-summarization systems (cf.
Louisand Nenkova (2013)).
It has also been success-fully applied to the content assessment of writtenanswers by Madnani et al.
(2013) who used onerandomly selected highly scored summary as a ref-erence summary.Since previous work on summarization eval-uation showed that multiple summaries increasethe reliability of evaluations (Louis and Nenkova,2013; Nenkova and McKeown, 2011), we tested71how many summaries were necessary to achieveconsistent results.
We therefore computed ROUGEfor each response using up to 10 randomly se-lected responses with final score of 4.
To inves-tigate the effect that different choices of referencesummaries may have on the metrics, we repeatedthe analysis for 20 randomly selected sets of refer-ence responses.The corpus did not contain a sufficient num-ber of responses with the maximum score for eachprompt.
Therefore, this part of the analysis wasbased on a subset of 1,784 responses selected fromthe training partition.
This set included only 12prompts for which human raters assigned a scoreof 4 to more than 11 responses.Text preprocessing.
For the evaluation of writ-ten summaries, ROUGE is usually computed usingthe raw counts of all of the terms.
In addition to us-ing this classical approach using unstemmed terms(?all?
), we also computed ROUGE using three otherapproaches: (1) excluding all stop-words (?Non-stop?
); (2) setting the frequency of all n-gramswithin each summary to 1, that is, counting typesinstead of tokens (?Types?
); (3) excluding all stop-words and counting types only (?Non-stop types?
).Finally, we computed all of these ROUGE variantsusing raw text as well as lemmatized text.
As a re-sult, we computed 72 different variants of ROUGEfor each response and each combination of refer-ence summaries: nine different types of ROUGE(eight different n-gram lengths and ROUGE-ALL)computed using four different methods of text pro-cessing and two possible approaches to lemmati-zation.
All of the computations were done both onASR and manual transcriptions.3.3 EvaluationWe computed the Spearman?s rank correlation be-tween the metric and the holistic score assignedby the first rater to identify the best method ofcomputing ROUGE and the optimal number of ref-erences.
Performance of the metric may be af-fected by properties of the prompt (cf.
(Nenkovaand Louis, 2008)), therefore we first analyzed eachprompt separately and then selected the variantsthat achieved the highest performance across allof the prompts.
Since correlation coefficients arenot normally distributed, we used several non-parametric methods to identify significant differ-ences including non-parametric bootstrapping andnon-parametric ANOVAs.
These analyses weredone using the data from the training partition ofthe corpus.We then evaluated how well the selected vari-ants of ROUGE predicted human scores using a lin-ear regression model trained on all of the data fromthe training partition using pooled data from all ofthe prompts.
The model was tested on an unseentest partition that had not been used for any of theanalyses.Finally, we tested whether the new metrics im-proved the performance of the automated scoringengine for spoken responses.
The current systemassigns scores based on the linear combination offeatures with empirical weights obtained by train-ing scoring models on scores assigned by expertraters (Zechner et al., 2009; Higgins et al., 2011).Current features measure various aspects of speak-ing proficiency such as fluency, pronunciation, andgrammar usage.
The performance of the system isevaluated with correlations and quadratic kappasbetween the scores assigned by the human ratersand rounded predicted scores.4 ResultsAll analyses were performed twice: each for met-rics computed using ASR and manual transcrip-tions.
We found that although the exact valuesof the correlation coefficients differed across thesetwo transcriptions, the overall pattern of resultsremained the same.
There was also a high cor-relation in metric values between the two typesof transcription (Pearson?s r for different types ofROUGE varied between 0.81 for ROUGE-4 and 0.9for ROUGE-1).
Since automated scoring relies onthe output of automatic speech recognition, all nu-merical results reported in the main text of thissection are based on ASR output.
The tables re-port the numbers for both ASR and manual tran-scriptions.4.1 Number and choice of referenceresponsesNumber of references.
To identify the optimalnumber of references for each prompt and met-rics, we first found Nbest, which had the high-est correlation with human scores and then iden-tified the lowest number of reference summariesfor which the correlation coefficient was not sig-nificantly lower than the correlation coefficient forNbest.Comparisons between different correlations72were performed using the general method sug-gested by Zou (2007) for comparing overlappingcorrelations as implemented by Baguley (2012,p.224) but we used bootstrapped confidence inter-vals (Wilcox, 2009).
Confidence intervals for eachcorrelation coefficient were constructed using pi-geonhole bootstrapping (Owen, 2007) with 1,000samples.
For each N reference, we pooled thevalues computed for 20 randomly selected sets ofdifferent reference summaries.
We then indepen-dently sampled responses and sets of referencesand selected values at each bootstrap repetitionat the intersection of the two samples.
The con-fidence intervals were constructed using the ad-justed percentile method (Davison and Hinkley,1997, p. 203-213).
Since this analysis is more sen-sitive to Type II errors (?false negatives?
), we setthe significance threshold at ?
= 0.15.The optimal number of references varied be-tween prompts, metrics, and methods of compu-tation, but never exceeded 8.
On average, opti-mal performance was achieved with 3 references.More references were required to achieve optimalperformance for ROUGE based on longer n-grams(using the Kruskal-Wallis test, a non-parametricanalysis of variance, p < 2.2 ?
10?16).
For ex-ample, two references on average were requiredto achieve reliable results for ROUGE-1, but forROUGE-4 this number was four references.
Therequired number of references was also signifi-cantly dependent on the prompt (Kruskal-Wallistest, p < 2.2 ?
10?16) with averages varying be-tween two and four.
When the number of ref-erences was equal to or greater than the optimalnumber, there were no significant differences inthe correlation coefficients across the different ref-erence models.For the analysis in the following section eachof the 72 variants of ROUGE for each prompt wascomputed using the optimal N references identi-fied for this variant and prompt.4.2 Types of ROUGE and different methods ofcomputationThe correlation coefficients between the summa-rization metrics and human ratings depended onthe length of n-grams (Kruskal-Wallis test p <2.2 ?
10?16).
While all types of ROUGE were pos-itively correlated with human ratings, the corre-lation coefficients were the highest for ROUGE-1and ROUGE-SU2-4, which performed significantlybetter than ROUGE-3-4 and the combined mea-sures ROUGE-ALL (post-hoc Tukey HSD test onranked observations, p varied from p < 1 ?
10?10to 2.804 ?
10?4).
The average correlations acrossthe different types of text pre-processing for ASRand manual transcriptions are shown in Table 1.Metrics ASR output ManualROUGE-1 0.616 0.637ROUGE-SU4 0.592 0.608ROUGE-SU3 0.595 0.609ROUGE-SU2 0.594 0.613ROUGE-SU1 0.598 0.619ROUGE-ALL 0.523 0.527ROUGE-2 0.553 0.560ROUGE-3 0.468 0.461ROUGE-4 0.366 0.357Table 1: Average correlation coefficient with hu-man scores (Spearman?s ?)
across different meth-ods of computation for ROUGE based on n-gramsof different lengths.
The table shows the resultsfor metrics computed based on ASR and manualtranscriptions.The effect of text pre-processing differedacross the metrics: for metrics that relied onconsecutive n-grams with n>2, the removal ofstop-words led to further drops in performance(Kruskal-Wallis test p = 4.4 ?
10?5).
For ROUGEbased on unigrams and skip-bigrams, countingonly type frequencies led to a significant im-provement in performance (Kruskal-Wallis test,p = 0.00017).
Correlations for the different typesof pre-processing for the measures that performedthe best are given in Table 2.
Lemmatization didnot make a significant difference to metric perfor-mance.Pre-processing ASR ouput ManualAll 0.573 0.606Non-stop 0.585 0.600Non-stop types 0.601 0.617Types 0.622 0.634Table 2: Average correlation coefficient withhuman proficiency score (Spearman?s ?)
acrossROUGE-1 and ROUGE-SU1-4 for different meth-ods of text processing.
The table shows the resultsfor metrics computed based on ASR output andmanual transcriptions.73Finally, a summarization metric performed bet-ter on tasks that required the test takers tosummarize an announcement or lecture (average??
= 0.653 for ROUGE-1 and ROUGE-SU1-4) ratherthan on tasks that required them to describe a pic-ture sequence (average ??
= 0.437, Mann-Whitney-Wilcox test, a non-parametric test for comparingtwo independent samples, p < 2.2 ?
10?16).4.3 Evaluation of the final modelAnalysis by prompt showed that the variants ofROUGE that included unigram counts (ROUGE andROUGE-SU1-4) had the best correlations with hu-man scores across all prompts.
Further improve-ment in their performance was obtained by count-ing type frequencies only and by using several ref-erence summaries.
The optimal N references forthese variants of ROUGE varied between prompts,but never exceeded four which was therefore se-lected as the optimal N references for this corpus.Based on these results we computed ROUGE-1 metrics for all responses in the original train-ing partition using four randomly selected, highlyscored responses for each prompt and ?types?method of pre-processing.
We then comparedit with two baselines: (1) cosine distance (CVA)computed using type frequencies only and thesame four references, and (2) na?
?ve implementa-tion of ROUGE-1 computed using one randomlyselected reference summary and raw frequencies(tokens).
The newly adjusted version of ROUGE-1 metrics performed significantly above the base-lines (using Zou?s method for the comparison ofoverlapping correlations with confidence intervalsconstructed at ?
= 0.001).
The correlation coeffi-cients are shown in Table 3.Metric ASR output ManualNew ROUGE-1 0.652 0.673Base ROUGE-1 0.55 0.589CVA 0.508 0.451Table 3: Correlation coefficients with humanscores (Spearman?s ?)
for the entire training parti-tion for the newly adjusted version of ROUGE andthe baseline metrics.
The table shows the resultsfor metrics computed based on ASR and manualtranscriptions.We then trained a standard linear regressionmodel using the human scores as the dependentvariables and summarization metrics as indepen-dent variables.
The accuracy of prediction wasevaluated using two metrics as suggested, for ex-ample, by Williamson et al.
(2012): quadraticweighted kappa (?)
and Pearson?s correlation co-efficient (r) between the observed and predictedscores.
For computation of ?, the predicted scoreswere trimmed to the range of human scores androunded to the nearest integer.Repeated 10-fold cross-validation on the train-ing partition showed that a model based onROUGE-1 produced averages of r?
= 0.65(?
= 0.031) and ??
= 0.54 (?
= 0.036).
The modelbased on a linear combination of several ROUGEvariants using longer n-grams and a recursive fea-ture elimination (Kuhn and Johnson, 2013, p. 480)did not show any improvement in the performanceas compared to a model based on a single ROUGE-1.Finally, we tested the performance of the met-rics on an unseen test set that had not been usedfor any previous analyses.
We tested both themodel based solely on the content metric as wellas on the performance of the content metrics incombination with 11 other features used for theautomated scoring of spoken responses that mea-sure pronunciation accuracy, prosody, fluency, andgrammar.
These results are presented in Table4.
Note that the performance of the content-onlymodel based on the new ROUGE-1 was in linewith the estimates obtained on the training set.Zou?s method for comparing overlapping correla-tions showed that in all cases, the difference be-tween the model based on an adjusted ROUGE andthe baselines was significant at ?
= 0.001.
In linewith previous results, the models based on manualtranscriptions showed better agreement with hu-man scores than the models based on ASR output.Table 4 shows that the addition of content met-rics lead to relatively small increase in the perfor-mance of the integrated models.
This is due to thefact that for most speakers different aspects of pro-ficiency tend to be correlated.
For example, morefluent speakers also achieve higher ROUGE scores(the correlation between ROUGE and pronuncia-tion accuracy (Chen et al., 2009) is r = 0.62).
Asa result, a model which measures only one as-pect of performance such as fluency may some-times reach near optimal performance and addingfurther predictors leads to a relatively small gain.When interpreting these results, it is important tobear in mind that empirical performance is only74ModelASR Manualr ?
r ?Content onlyCVA 0.492 0.340 0.469 0.303Base ROUGE 0.587 0.440 0.632 0.489New ROUGE 0.655 0.540 0.700 0.590Integrated modelNo content 0.678 0.565 0.678 0.565CVA 0.691 0.600 0.698 0.602Base ROUGE 0.700 0.597 0.719 0.610New ROUGE 0.715 0.617 0.738 0.652Table 4: Performance of the linear regressionmodel based on one content metric and an ?inte-grated?
model based on 11 features that measurepronunciation, fluency, and grammar before andafter the addition of ?Base ROUGE,?
?CVA?
or ?NewROUGE.?
The table shows the correlation coeffi-cients (Pearson?s r) and quadratic weighted kappakappas (?)
between the predicted scores and hu-man ratings for the unseen test set.
The agree-ment between the two expert raters on this datasetis ?
= 0.69.one aspect of evaluation of automated scoring sys-tems.
In addition to high agreement with hu-man scores, operational automatic scoring systemsalso need to show good construct representationby covering different aspects of speaker perfor-mance (Williamson et al., 2012).
This requirementensures the validity of automated scores and pre-vents future test-takers from fine-tuning their per-formance to one particular feature measured by thescoring system.
Therefore the addition of ROUGEto the automated scoring model serves both goals:it improves the agreement with human raters andalso expands the construct coverage of the model.5 DiscussionSummarization metrics can be successfully usedto evaluate spoken summaries in the context oflanguage assessment.
Although the na?
?ve imple-mentation of ROUGE had good agreement with thescores assigned by human raters, several modifica-tions led to a further increase in the performance.Some of our findings show common patternswith what has previously been reported for writtensummaries.
ROUGE-1, ROUGE-SU4 and ROUGE-2 are the three variants of ROUGE most com-monly used for the evaluation of automatic textsummaries.
Our results showed that the first twoof these measures (ROUGE-1 and ROUGE-SU4)were also most suitable for content assessmentof spoken responses.
We note that both of thesemeasures include unigram counts.
More recently,Rankel et al.
(2013) and Owczarzak et al.
(2012)reported that metrics based on longer consecutiven-grams or linear combinations of different vari-ants are more accurate.
We did not find this for ourdata.
Since our data represents abstractive sum-maries, poor performances of longer n-grams isnot surprising.
Finally, as in the case of writtensummaries, there was no effect of lemmatizationwhile the removal of stop-words sometimes led toa decrease in performance.Similar to written summaries, the use of morethan one reference summary improved the perfor-mance.
We found that the optimal number of refer-ence summaries varied between prompts and met-rics.
For ROUGE-1, this number never exceededfour across all prompts in our corpus.
Further-more, we found that the choice of reference sum-maries from the pool of highly scored responseshad no significant effect on the performance of themetric.In addition to good agreement with humanscores, metrics used for automated scoring alsoneed to match the construct of interest, as definedby the assessment program (Williamson et al.,2012).
The scoring guidelines for the tasks usedin this paper ask raters to judge whether the keyinformation contained in the prompt has been con-veyed accurately.
A notable difference betweenROUGE and previously used metrics is that as arecall measure, ROUGE does not penalize for thelack of precision.
Our results suggest that a recall-oriented approach has better agreement with hu-man judgments than cosine distance which com-bines both precision and recall.Recall-based approaches are sensitive to thelength of candidate responses.
In the case of auto-matic summary evaluation, the length of the sum-maries is limited to a predefined number of words.In this data, the length of the responses is limitedmore loosely by the time available to record theresponse and the actual number of words variedbetween the responses.
Therefore, a recall-basedapproach may produce inflated scores by assign-ing higher metric values to a response which con-tains multiple repetitions of the same n-gram aslong as the n-gram occurs several times in the ref-erence response.
The common occurrence of re-75pairs and repetitions in spoken speech further ag-gravates this problem further.
We addressed thisissue by only counting type frequencies, whichalso improved agreement with human judgments.The adjusted metric had better agreement withhuman judgments than other ?bag-of-words?
ap-proaches such as the cosine-based measure com-monly used for content scoring that requires amuch larger set of model responses than ROUGE.It also performed equally well on human and ASRtranscriptions and did not require any manual an-notation of the data.
We also found that the per-formance of ROUGE depended on the task: weobtained better agreement for tasks that requiredthe student to summarize a stimulus rather thantasks that required the student to describe a se-quence of pictures.
While in both cases the stu-dents produced short summary-like texts, the pic-ture description task allowed for greater variabil-ity between the responses than the summarizationtask and, therefore, recall-oriented comparisonswith highly-scored responses showed less agree-ment with human scores.As a ?bag-of-words?
approach, ROUGE-1 hasthe same shortcomings as other methods discussedin Section 2.2 in that it doesn?t distinguish be-tween syntactic roles.
While variants based onlonger n-grams could in theory address this, ourresults showed that neither a linear nor a geomet-ric combination of these variants with ROUGE-1improved agreement with human scores.
This is-sue has also been acknowledged in the context ofnon-extractive text summarization and new met-rics such as AutoSummEng (Giannakopoulos andKarkaletsis, 2011) have been developed to addressit.
Future research will include the conceptualiza-tion and development of metrics that can addressthe content accuracy of spoken summaries beyondthe ?bag-of-words?
approach.6 ConclusionIn this paper we applied ROUGE, a recall-basedmetrics for evaluation of written summaries to theautomatic assessment of spoken summaries pro-duced by non-native speakers of English.
We per-formed a thorough evaluation of different types ofROUGE by varying the length of n-grams, vari-ous methods of frequency computation, and text-preprocessing.
We also explored the effect of thenumber of reference summaries.
We found thatthe standard baseline implementation of ROUGE-1computed over the output of the automated speechrecognizer showed good agreement with expertratings and performed better than the cosine sim-ilarity measure commonly used for the evaluationcontent of spoken responses.
A further increase inagreement with human ratings could be achievedby using types instead of tokens for the frequencycomputation of both candidate and reference sum-maries.
We also found that the use of several refer-ence summaries improves the performance of themetric, but only four reference summaries werenecessary to achieve reliable results.AcknowledgmentsWe would like to thank Keelan Evanini, NitinMadnani, Xinhao Wang, Derrick Higgins andthree anonymous reviewers for their helpful com-ments and suggestions and Ren?e Lawless for edit-ing assistance.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater V. 2.
The Journal of Technology,Learning and Assessment, 4(3):1?30.Thomas Baguley.
2012.
Serious Stats: A guide to ad-vanced statistics for the behavioral sciences.
Pal-grave Macmillan.Douglas Biber, Susan M. Conrad, Randi Reppen, PatByrd, Marie Helt, Victoria Clark, Viviana Cortes,Eniko Csomay, and Alfredo Urzua.
2004.
Rep-resenting language use in the university: analysisof TOEFL 2000 Spoken and Written academic lan-guage corpus.
Educational Testing Service, Prince-ton.Miao Chen and Klaus Zechner.
2012.
Using an on-tology for improved automated content scoring ofspontaneous non-native speech.
In Proceedings ofthe 7th Workshop on Building Educational Applica-tions Using NLP, pages 86?94, Stroudsburg, PA. As-sociation for Computational Linguistics.Lei Chen, Klaus Zechner, and Xiaoming Xi.
2009.
Im-proved pronunciation features for construct-drivenassessment of non-native spontaneous speech.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, NAACL ?09, pages 442?449, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Lei Chen.
2013.
Applying unsupervised learning tosupport vector space model based speaking assess-ment.
Proceedings of the 8th Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, Atlanta, Georgia, pages 58?62.76Anthony C. Davison and David V. Hinkley.
1997.Bootstrap Methods and their Application (Cam-bridge Series in Statistical and Probabilistic Mathe-matics).
Cambridge University Press.Keelan Evanini and Xinhao Wang.
2013.
Automatedspeech scoring for non-native middle school stu-dents with multiple task types.
Proceedings of In-terspeech 2013, Lyon, France, pages 2435?2439.Peter W. Foltz, Darrell Laham, and Thomas K. Lan-dauer.
1999.
Automated essay scoring: applica-tions to educational technology.
In B. Collis andR.
Oliver, editors, Proceedings of World Confer-ence on Educational Multimedia, Hypermedia andTelecommunications 1999, pages 939?944.George Giannakopoulos and Vangelis Karkaletsis.2011.
AutoSummENG and MeMoG in Evaluat-ing Guided Summaries.
In TAC 2011 Workshop,Gaithersburg, MD, USA.
NIST.Derrick Higgins, Xiaoming Xi, Klaus Zechner, andDavid Williamson.
2011.
A three-stage approachto the automated scoring of spontaneous spoken re-sponses.
Computer Speech & Language, 25(2):282?306, April.Makoto Hirohata, Yousuke Shinnaka, Koji Iwano, andSadaoke Furui.
2005.
Sentence extraction-basedpresentation summarization techniques and evalua-tion metrics.
In Acoustics, Speech, and Signal Pro-cessing, 2005.
Proceedings.
(ICASSP ?05).
IEEE In-ternational Conference on, volume 1, pages 1065?1068.Max Kuhn and Kjell Johnson.
2013.
Applied Predic-tive Modeling.
Springer.Chin-Yew Lin and Marina Rey.
2004.
ROUGE:A package for automatic evaluation of summaries.In Stan Szpakowicz Marie-Francine Moens, edi-tor, Text Summarization Branches Out: Proceedingsof the ACL-04 Workshop, pages 74?81, Barcelona,Spain.
Association for Computational Linguistics.Chin-Yew Lin.
2004.
Looking for a few good metrics:Automatic summarization evaluation - how manysamples are enough.
In Proceedings of the NTCIRWorkshop, pages 1765?1776, Tokyo.Annie Louis and A Nenkova.
2013.
Automaticallyassessing machine summary content without a goldstandard.
Computational Linguistics, 39(2):267?300.Nitin Madnani, Jill Burstein, John Sabatini, and TenahaO?Reilly.
2013.
Automated scoring of a summary-writing task designed to measure reading compre-hension.
In Proceedings of the 8th Workshop on In-novative Use of NLP for Building Educational Ap-plications, pages 163?168, Atlanta, Georgia.
Asso-ciation for Computational Linguistics.Ani Nenkova and Annie Louis.
2008.
Can you sum-marize this?
Identifying correlates of input difficultyfor generic multi-document summarization.
In Pro-ceedings of the ACL-08: HLT, pages 825?833.
As-sociation for Computational Linguistics.Ani Nenkova and Kathleen McKeown.
2011.
Auto-matic summarization.
Foundations and Trends inInformation Retrieval, 5(2-3):103?233.Karolina Owczarzak, John M. Conroy, Hoa TrangDang, and Ani Nenkova.
2012.
An assessment ofthe accuracy of automatic evaluation in summariza-tion.
In Proceedings of workshop on evaluation met-rics and system comparison for automatic summa-rization., pages 1?9, Stroudsburg, PA. Associationfor Computational Linguistics.Art B. Owen.
2007.
The pigeonhole bootstrap.
TheAnnals of Applied Statistics, 1(2):386?411.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU : a Method for AutomaticEvaluation of Machine Translation.
In Proceed-ings of the 40th Annual Meeting of the ACL, pages311?318, Philadelphia, PA. Association for Compu-tational Linguistics.Gerald Penn and Xiaodan Zhu.
2008.
A CriticalReassessment of Evaluation Baselines for SpeechSummarization.
In in Proceedings of RANLP work-shop on Crossing Barriers in Text SummarizationResearch, number June, pages 470?478, Columbus,Ohio, June.
Association for Computational Linguis-tics.Peter A. Rankel, John.
M. Conroy, Hoa Trang Dang,and Ani Nenkova.
2013.
A decade of automaticcontent evaluation of news summaries: reassessingthe state of the art.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics, Sofia, Bulgaria, August 4-9, 2013, pages131?136, Sofia.
Association for Computational Lin-guistics.Rand R. Wilcox.
2009.
Comparing Pearson correla-tions: dealing with heteroscedasticity and nonnor-mality.
Communications in Statistics - Simulationand Computation, 38(10):2220?2234.David M. Williamson, Xiaoming Xi, and F. Jay Breyer.2012.
A framework for evaluation and use of au-tomated scoring.
Educational measurement: issuesand practice, 31(1):2?13.Shasha Xie, Keelan Evanini, and Klaus Zechner.
2012.Exploring content features for automated speechscoring.
In NAACL HLT ?12 Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 103?111.Wenting Xiong, Keelan Evanini, Klaus Zechner, andLei Chen.
2013.
Automated content scoring of77spoken responses containing multiple parts with fac-tual information.
In Pierre Badin, Thomas Hue-ber, G?erard Bailly, Didier Demolin, and Franc?oiseRaby, editors, Proceedings of SLaTE 2013, Greno-ble, France, pages 137?142, Grenoble.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoringof non-native spontaneous speech in tests of spokenEnglish.
Speech Communication, 51(10):883?895.Guang Yong Zou.
2007.
Toward using confidenceintervals to compare correlations.
Psychologicalmethods, 12(4):399?413.78
