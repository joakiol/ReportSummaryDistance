CONSTRAINT GRAMMAR AS A FRAMEWORKFOR PARSING RUNNING TEXTFred KarlssonUniversity of HelsinkiDepartment of General LinguisticsHallituskatu 11SF-00100 HelsinkiFinlande-mail: KARLSS?N@FINUH.bitnet1.
OutlineGrammars which are used in parsers are oftendirectly imported from autonomous grammar theoryand descriptive practice that were not exercised forthe explicit purpose of parsing.
Parsers have beendesigned for English based on e.g.
Government andBinding Theory, Generalized Phrase StructureGrammar, and LexicaI-Functional Grammar.
Wepresent a formalism to be used for parsing where thegrammar statements are closer to real text senten-ces and more directly address some notorious pars-ing problems, especially ambiguity.
The formalismis a linguistic one.
It relies on transitional probabilitiesin an indirect way.
The probabilities are not part ofthe description.The descriptive statements, constraints, do nothave the ordinary task of defining the notion 'correctsentence in L'.
They are less categorical in nature,more closely tied to morphological features, andmore directly geared towards the basic task of pars-ing.
We see this task as one of inferring surfacestructure from a stream of concrete tokens in abasically bottom-up mode.
Constraints are formu-lated on the basis of extensive corpus studies.
Theymay reflect absolute, ruleqike facts, or probabilistictendencies where a certain risk is judged to beproper to take.
Constraints of the former rule-liketype are of course preferable.The ensemble of constraints for language L con-stitute a Constraint Grammar (CG) for L. A CG isintended to be used by the Constraint GrammarParser CGP, implemented as a Lisp interpreter.Our input tokens to CGP are morphologically ana-lyzed word-forms.
One central idea is to maximizethe use of morphological information for parsingpurposes.
All relevant structure is assigned directlyvia lexicon, morphology, and simple mappings frommorphology to syntax.
\]he task of the constraints isbasically to discard as many alternatives aspossible, the optimum being a fully disambiguatedsentence with one syntactic reading only.The second central idea is to treat morphologicaldisambiguation and syntactic labelling by the samemechanism of discarding improper alternatives.168A good parsing formalism should satisfy many re-quirements: the constraints should be declarativerather than procedural, they should be able to copewith any real-world text-sentence (i.e.
with runningtext, not just with linguists' laboratory sentences),they should be clearly separated from the programcode by which they are executed, the formalismshould be language-independent, it should be rea~sonably easy to implement (optimally as finite-stateautomata), and it should also be efficient to run.
TheCG formalism adheres to these desiderata.2.
Breaking up the problem of parsingThe problem of parsing running text may be brokenup into six subproblems or 'modules':?
preprocessing,?
morphological analysis,?
local morphological disambiguation,?
morphosyntactic mapping,?
context-dependent morphological disambigua-tion,, determination of intrasentential clause boun-daries,, disalnbiguation of surface syntactic functions.The first four of these modules are executed se-quentially, optimally followed by parallel execution ofthe last three modules which constitute 'syntaxproper'.
We have a five-stage parsing-process.In this general setting, CG is the formalism of thefifth stage, syntax proper.
The same CG constraintformalism is used to disambiguate morphologicaland syntactic ambiguities, and to locate clause boun-daries in a complex sentence.
Parts of the CG for-rnalism are used also in morphosyntactic mapping.Real texts are full with idiosyncracies in regard toheadings, footnotes, paragraph structure, interpunc-tuation, use of upper and lower case, etc.
Suchphenomena must be properly normalized.
Further-more several purely linguistic phenomena must besomehow dealt with prior to single-word morphologi-cal analysis, especially idioms and other more or lessfixed multi-word expressions.
(It would e.g.
make nosense to subject the individual words of the express-ion in spite of to plain morphological analysis.)
Theexistence of an adequate preprocessor is here sim-ply taken for granted.We concentrate on morphological analysis, clauseboundary determination, morphological disambigua-tion, and syntactic function assignment.
Viewing theproblem of parsing in turn from one or another ofthese angles clarifies many intricacies.
The subprob-lems take more manageable proportions and makepossible a novel type of modularity.Morphological analysis is relatively independent.CGP is always supplied with adequate morphologi-cal input.
The morphological analyzers are designedaccording to Koskenniemi's (1983) two-level model.Currently our Research Unit has morphological ana-lyzers available for English (41,000 lexicon entries),Finnish (37,000 entries), and Swedish (42,000 en-tries).
Below are two morphologically analyzed Eng-lish word-forms, a has one reading, move four.
Theset of readings for a word-form we call a cohort.
Allreadings in a cohort have the base-form initially or+the line.
Upper-case strings are morphological fea-tures except for those containing the designatedinitial character "@" which denotes that the stringfollowing it is the name of a syntactic function, hereemanating from the lexicon.
"@DN>" = determineras modif ier of the next noun to the right,"@+FMAINV" = finite main verb, "@-FMAINV" =non-finite main verb as member of a verb chain,"@<NQM-FMAINV" = non-finite main verb as post-modifier of a nominal:aa" DET CENTR ART INDEF @DN>"movemove" N NOM SG"move "V SUBJUNCTIVE @+FMAINV"move "V IMP @+FMAINV"move" V INF @-FMAINV @<NOM-FMAINV"described by recursive links back to the main lexicon.Consider the cohort of the Swedish word-form fru-kosten ("_ " = compound boundary, frukost 'break-fast', fru 'mrs', kost'nutrition', ko 'cow', sten 'stone'):frukostenfrukost" N UTR DEF SG NOM"fru_kost" N UTR DEF SG NOM "fru ko sten" N UTR INDEF SG NOM "By 'local disambiguation' we refer to constraints orstrategies that make it possible to discard somereadings just by local inspection of the current co-hort, without invoking any contextual information.The present cohort contains three readings.
An in-teresting local disambiguation strategy can now bestated: "Discard all readings with more than thesmallest number of compound boundaries occurringin the current cohort".
This strategy properly discardsthe readings "fru_kost" and "fru ko sten".
I havefound this principle to be very close to perfect.A similar principle holds for derivation: "Discardreadings with derivational elements if there is at leastone non-derived reading available in the cohort".Other local disambiguation strategies comparemultiple compound readings in terms of how prob-able their part of speech structure is (NNN, ANN,NVN, AAV, etc.
).Local disambiguation is a potent module.
TheSwedish morphological analyzer was applied to atext containing some 840,000 word-form tokens.
Thefollowing table shows cohort size N(r) in the firstcolumn.
The second and third columns sllow thenumber of cohorts with the respective number ofreadings before (a) and after (b) local disambigua-tion.
E.g., before local disambiguation there were3830 word-forms with 6 readings but after localdisambiguation only 312.Here, disambiguation refers to reduction of mor-phological ambiguities, optimally down to cohort size= 1.
Sense disambiguation is not included (presentlyour lexical items have no sense descriptions).The subproblems of morphosyntactic mapping,morphological disambiguation, clause boundary lo-cation, and syntactic function determination are in-terrelated.
E.g., for disambiguation it is useful toknow the boundaries of the current clause, and toknow as much as possible about its syntactic struc-ture.
An important aspect of the general problem isto work out the precise relations between thesemodules.3.
Local disambiguationMorphological ambiguities may be due to intersec-tion of forms of the same or of different lexical entries,or to intersection of recursive compound paths.
Thelatter phenomenon arises if productive compoundformation in e.g.
Finnish, German, and Swedish isN(r) (a) (b)0 13957 139571 440035 4879942 253779 2362983 55857 447824 38062 290535 24135 189116 3830 3127 9551 89138 541 239 232 4710 72 211 46 512 12413 1514 2815+ 33Out of roughly 1,5 million readings assigned bymorphology (1.8 readings/word-form), local disam-169biguation discards more than 100,000.
Especiallydramatic the drop is for highly ambiguous words.4.
Morphosyntactic mappingAfter local disambiguation, each word in the sen-tence undergoes morphosyntactic mapping, i.e.
it isassigned at least one syntactic label, perhaps sev-eral if a unique label is not possible to assign.
Thismapping will be discussed in connection with thesyntactic constraints in section 7.5.
Context-dependent disambiguationconstraintsThe CG formalism will first be illustrated by con-text-dependent disambiguation constraints.
Sets ofgrammatical features are needed in the constraintsfor the purpose of genei'alization.
Each set declara-tion consists of a set name followed by the elementsof that set.
The elements are (strings of) featuresand/or base-forms occurring in readings:(DET "DET")(N "N")(TO "to")(PREMOD "A" "DET")(NOMHEAD "N NOM" "PRON NOM")(VFIN "V PRES" "V PAST" "V IMP" "V SUBJUNC-TIVE")Each constraint is a quadruple consisting of do-main, operator, target, and context condition(s).
Anexample:(@w =0 "PREP" (-1 DET))other readings.
The operators are here defined in theprocedural mode as performing operations.
Con-ceptually they just express constraints.The context conditions are defined relative to thetarget reading in position 0.
Position 1 is one word tothe right of 0,-3 three words to the left of 0, etc.
(Such straightforward positions we call absolute.
)Each context condition is a triple consisting ofpolarity, position, and set.
'Polarity' is either NOT or nothing (i.e.
positive),'position' is a legal position number, and 'set' is adeclared set name.An asterisk ..... (functionally and mnemotechnicallyreminiscent of the Kleene star) prefixed to positionnumber n refers to some position rightwards of n (ifn is positive), or some position leftwards of n (if n isnegative), in both cases including n, up to the nextsentence boundary (or clause boundary, if enforcedin clause boundary mode, cf.
below).
The asteriskconvention thus enables the description of un-bounded dependencies.Examples: (1 N) requires there to be a reading withthe feature "N" for the next word-form.
(NOT *-1VFIN) states: nowhere leftwards in this sentence isthere a reading with any of the feature combinationsdefining finite verbs.
The condition ensemble (1PREMOD) (2 N) (3 VFIN) requires there to be areading with either "A" or "DET" in position 1, with"N" in position 2, and with one of the VFIN readingsin position 3.
Here are two more context-dependentdisambiguation constraints for English:(@w =0 VFIN (-1 TO))("that" =!
"<Rel>" (-1 NOMHEAD) (1 VFIN))stating that if a word (@w) has a reading with thefeature "PREP", this very reading is discarded (=0)iff the preceding word (i.e.
the word in position -1)has a reading with the feature "DET".The domain points out some element o be disam-biguated, e.g.
(the readings of) a particular word-form.
The designated domain @w is a variable overany word-form, used when the target reading ispicked by feature(s) only.The target defines which reading the constraint isabout.
The target may refer to one particular eading,such as "V PRES -SG3", or to all members of adeclared set, such as VFIN.The operator defines which operation to performon the reading(s).
There are three disambiguationoperators, here treated in order of decreasingstrength.
The operator '=!!'
indicates that the targetreading is the correct one iff all context conditionsare satisfied; all other readings should be discarded.If the context conditions are not satisfied, the targetreading itself is discarded.
The operator '=!'
indicatesthat the target reading is the correct one iff all contextconditions are satisfied, all other readings are dis-carded.
The operator '=0' discards the target readingiff the context conditions are satisfied, it leaves allThe first one discards all finite verb readings imme-diately after the base-form to (itself either a preposi-tion or an infinitive mark).
VFIN is a declared set.
Theconstraint is applicable to all strings declared tobelong to this set.The second constraint states that the proper read-ing of the word thatis relative pronoun (i.e.
a readingcontaining the string "<Rel>", itself an inherent fea-ture emanating from the lexicon) immediately after anominal head and immediately belore a finite verb.There is also a mechanism available for expressingrelative position with reference to variable positionsestablished via unbounded dependencies.
Let con-dition ('1 VFIN) be satisfied at absolute position 5,i.e.
at the fifth word to the right.
Then (L-1 N) wouldrequire there to be a feature "N" in absolute position4, (L* N) would establish a second unbounded de-pendency somewhere left of position 5 (but right ofposition 0), i.e.
looking for satisfaction at one ofpositions 4,3,2,1.Often context conditions work on ambiguous co-horts, i.e.
one reading satisfiesthe condition, but thisreading perhaps is not the correct one in the firstplace.
If so, should a risk be taken?
The CG formal-ism makes this a matter of deliberate choice.
All170 3constraints so far treated allow the context condi-tions to be satisfied by ambiguous context cohorts.By appending the character C to the position num-ber, one requires the respective condition to besatisfied only if the cohort being tested is itself un-ambiguous.
This is called careful mode, e.g.
:classical repertoire of heads and modifiers.
CG syn-tax maps morphological categories and word orderinformation onto syntactic labels.The designated syntactic subsets of verb chainelements, head labels, and modifier labels should beestablished.
For English, these include e.g.
:(@w =0 VFIN (-1C TO))For many constraints it is necessary to require thatthey do not apply over clause boundaries.
Thisclause boundary mode is effected by appendingeither of the atoms **CLB (ordinary mode) or**CLB-C (careful mode) after the last context condition.Clause boundary mode is typically used in conjunc-tion with unbounded contexts.A template mechanism is available for expressingpartial generalizations.
E.g., a template "&NP" couldbe declared to contain the alternatives ((N)), ((A) (N))((DET) .
(N)) ((DET) (A) (N)), etc.
Then the template&NP could be used in the context parl of any con-straint.
At run-time all alternative realizations of &NPwould be properly considered.Every constraint embodies a true statement.
Oc-casionally the constraints might seem quite down-to-earth and even 'trivial', given mainstreamconceptions of what constitutes a 'linguistically sig-nificant generalization'.
But the very essence of CGis that low-level constraints (i) are easily express-ible, and (it) prove to be effective in parsing.6.
Constraints for intrasentential clauseboundariesClause boundary constraints establish locations ofclause boundaries.
They are important especially forthe formulation of proper syntactic constraints.
E.g.,the syntactic constraint "there is only one finite predi-cate in a simplex non-coordinated clause" presup-poses that clause boundary locations are known.Clause boundaries occur i.a.
as the inherent fea-ture "<**CLB>" in the input stream.
E.g.
subjunctionsare lexically marked by this feature.
But many boun-daries must be spotted by specific constraints.Clause boundary constraints have the special oper-ator "=**CLB" stating that there is a clause boundarybefore the word specified by the target.E.g., given that conjunctions are lexically markedby the inherent feature "<Conj>", the constraint:(@w =**CLB "<Conj>" (1 NOMHEAD) (2 VFIN))states that there is a clause boundary before con-junction instances that precede a NOMHEAD fol-lowed by a finite verb (e.g., before the conjunction ina sentence such as John eats and Bill drinks).7.
Syntactic constraintsCG syntax is based on dependency and shouldassign flat, functional, surface labels, optimallyone to each word-form.
The labels are roughly theverb chain members: @+FAUXV (finite auxil-iary V), @-FAUXV (non-finite auxiliary V),@+FMAINV (finite main V), @-FMAINV (non-finite main V) ....?
nominal heads: @SUB J, @OBJ, @I-OBJ,@PCOMPL-S (subj.
pred.
compl.
), @PCOMPL-O (obj.
pred.
compl.
), @ADVL (adverbial) ....?
nominal modifiers: AN> (adjective as premodi~tier to N), DN> (determiner aspremodifier toN),<NOM (t-mstmodifier to nominal), A> (premodi-tier to A), <P (postmodifier toP) ....A verb chain such as has been reading gets thelabels @+FAUXV @-FAUXV @-FMAINV.
In thesentenceShe boughtthe car, she is @SUBJ and car@OBJ.Certain verb chain and head labels may occurmaximally once in a simplex clause.
This restrictionwe call the Uniqueness Principle.
At least@+FAUXV, @+FMAINV, @SUBJ, @OBJ, @I-OBJ,@PCOMPL-S, and @PCOMPL-O obey this restric-tion.
Many constraints may be based on consequen-ces of the Uniqueness Principle.
E.g., if amorphologically and syntactically unambiguous@SUBJ has been identified in a clause, all otherinstances of @SUBJ occurring in syntactically am-biguous readings of that clause may be discarded.Modifier and complement labels point in the direc-tion (right ">", left "<") of the respective head whichis identified by its part-of-speech label.
E.g., the label@<P is assigned to a prepositional complementsuch as park in in the park.
Our analysis of modifierand complement labels is more delicate than intraditional grammar, cf.
the premodifiers AN>, DN>,NN>, GN> (genitival).In Constraint Grammar, syntactic labels are as-signed in three steps.
The basic strategy is: Do asmuch as possible as early as possible.The first step is to provide as many syntactic labelsas possible in the lexicon (including morphology).For entries having a reduced set of labels (comparedto what that morphological class normally has),those labels will be listed in the lexicon.
Thus, outputfrom lexicon and morphology will indicate that he is@SUB J, that him is either @OBJ, @I-OBJ, or @<P(NB: a considerably reduced subset of all nominalhead functions), that went is @+FMAINV, etc.The second step is morphosyntactic mappingFor all readings that remain after local disambigua-tion and do not yet have any syntactic function label,simple mapping statements tell, for each relevantmorphological feature, or combination of features,what its range of syntactic labels is.
This may be4 171compared to traditional grammar book statementssuch as "the syntactic functions of nou ns are subject,object, indirect object .... ".CG contains one enrichment of this scheme.
Amapping statement may be constrained by the con-text condition mechanism specified in section 5.Thus, a mapping statement is a triple <morphologi-cal feature(s), context condition(s), syntactic func-tion(s)>.
The first element is a feature stringoccurring in a morphological reading, the second iseither NIL (no conditions) or a list of sublists each ofwhich is a legal context condition.
Finally the requi-site grammatical function label(s) are listed.
Here aresome mapping statements without context condi-tions, providing a maximal set of labels:("PRON GEN" NIL GN> @PCOMPL-S@ PCOM PL-O)("A" NIL AN> @PCOMPL-S @PCOMPL-O@SUBJ @OBJ @I-OBJ)("N NOM" NIL @SUBJ @OBJ @I-OBJ@PCOMPL-S @PCOMPL-O @APP @NN> @<P)A pronoun in the genitive case is either prenominalgenitival modifier, subject predicate complement, orobject predicate complement.
An adjective is pre-nominal adjectival modifier, predicate complement,subject, object, or indirect object (the last three func-tions refer to occurrences of adjectives as 'nomi-nalized heads'), etc.Often morphosyntactic mappings may be consid-erably constrained by imposing context conditions :("N NOM" ((1 N)) @NN>)("N NOM" ((-1 PREP)) @<P)("INF" ((-2 N) (-1 TO)) @<NOM-FMAINV)These state that a noun in the nominative casepremodifies (@NN>) a subsequently following noun(in compounds, cf.
computer screen), that a noun inthe nominative case after a preposition is @<P, andthat an infinitive preceded by a noun + to post-modifies that noun.In this way, the task of syntactic analysis is simpli-fied as much as possible, as early as possible.Superfluous alternatives are not even introduced intothe parsing of a particular clause if it is clear at theoutset, i.e.
either in the lexicon or at the stage ofmorphosyntactic mapping, that certain labels areincompatible with the clausal context at hand.There may be several mapping statements for thesame morphological feature(s), e.g.
"N NOM".
Map-ping statements with more narrowly specified con-texts have precedence over more generalstatements.
In the present implementation of CGP,the mapping statements apply in plain linear order.The last mapping statement for a particular feature1"/2provides the worst case, i.e.
the maximal assort-ment of function labels for that feature.Every word-form will have at least one syntacticlabel after morphosyntactic mapping, and allpossible syntactic ambiguities have also now beenintroduced.In step three, syntactic constraints reduce syn-tactic ambiguities where such exist due either tolexical information (cf.
the infinitive move above), orto morphosyntactic mapping.
Syntactic constraintsdiscard the remaining superfluous syntactic labels.Syntactic constraints differ from context-dependentdisambiguation constraints only by having one of thesyntactic operators '=s!
', or '=sO' (where s indicatesthat the constraint is a syntactic one).
Their seman-tics is identical to that of the disambiguation con-straints:(@w =sO "@+FMAINV" (*-1 VFIN))(@w =sO "@+FMAINV" ('1 VFIN))(@w =s!
"@SUBJ" (0 NOMHEAD) (NOT "1 NOM-HEAD) ('1 VFIN)(NOT *-1 NOMHEAD))The first two constraints discard @+FMAINV as asyntactic alternative if there is a unique finite mainverb either to the left or to the right in the sameclause.
The third constraint prescribes that @SUBJis the correct label for a noun or pronoun (NOM-HEAD in target position, i.e.
position 0), with a finiteverb somewhere to the right in the same clause andno similar noun or pronoun either left or right (--woman -- laughed --).Maximal profit is extracted from the UniquenessPrinciple.
At each syntactic step (before mapping,after mapping, and after the application of a syntacticconstraint hat affects the labels obeying the Unique-hess Principle), each clause is checked for eventualviolations of this principle.
In this way many ambigu-ous primary labels may be safely discarded.Here is an example sentence, fully analyzed andunambiguous in all respects but the one syntacticambiguity remaining for the word in:BillBill "<Proper> N NOM SG "@SUBJsawsee" <SVO> V PAST" @+FMAINVthethe" DET" @DN>littlelittle" A ABS" @AN>dogdog" N NOM SG" @OBJinin" PREP "@<NOM @ADVLthethe" DET" @DN>parkpark" N NOM SG "@<P5There is no non-semantic way of resolving theattachment ambiguity of the adverbial in the park.This ambiguity is therefore properly unresolved.In CGP, all ambiguities 'are there' after morpho-syntactic mapping and require no additional process-ing load.
Notice in passing that CGP makes aninteresting prediction which might be relevant fromthe viewpoint of mental language processing.
Dis-ambiguation, i.e.
finding a unique interpretation byapplying constraints, requires 'more effort' than leav.oing all or many ambiguities unresolved (in whichcase constraints were not applied).
Parsers basedon autonomous grammars tend to work in the oppo-site way (the more ambiguities, the more rules toapply and trees to construct).In CGP, there is precisely one output for eachsentence regardless of how many unresolved ambi-guities there might be pending in it.
This output is anannotated linear, flat string of word-forms, base-forms, inherent features, morphological features,and syntactic function labels, all of the same formaltype.
The dependency structure of the sentence isexpressed by the pointers and parts of speech of thesyntactic labels.There is no proliferation of parsetrees, often encountered in other types of parsers,even if morphological and/or syntactic ambiguitiesare left unresolved.8.
ImplementationI have written an interpreter in strict Common Lispfor parsing with constraint grammars.
This is whatwe call the Constraint Grammar Parser (CGP).
CGPcurrently runs on Unix workstations under LucidCommon Lisp and Allegro Common Lisp.
A PCversion with the same functionality runs under mu-Lisp on ordinary XT/AT machines.CGP takes two inputs, a constraint file with setdeclarations, mapping statements, context-depend-ent disambiguation constraints, syntactic con-straints, etc., and a text file with morphologicallyanalyzed word-forms (cf.
section 2).The optimal implementation of constraint grammarparsing would be in terms of finite-state machines(cf.
Kimmo Koskenniemi, COLING-90 Proceedings,Vol.
2).9.
DiscussionThe CG formalism has so far been extensivelyapplied only to English context-dependent disam-biguation and syntax.Presently some 400 context-dependent disam-biguation constraints have been formulated for Eng-lish by Atro Voutilainen, Juha Heikkil~, and ArtoAnttila.
These constraints prune 95-97 % of all mor-phological ambiguities in running English text, de-pending upon the complexity of the text.
This level(which is not linal as work on the most recalcitrantproblems proceeds) has been achieved using plainmorphological information, i.e.
information present in\[he cohorts of neighbouring words.
No syntacticfunctions have been used.
No enorrnous amounts ofdisambiguation constraints will thus be needed, the'finar order of magnitude might be some 500.
Weconsider this number surprisingly small.
It shows thedescriptive power of low-level morphology-basedconstraints.The most successful achievements so far in thedomain of large-scale morphological disambigua-tion of running text have been those for Englishreported by Garside, Leech, and Sampson (1987),on tagging the LOB corpus, and Church (1988), onassigning part-of-speech labels and parsing nounphrases.
Success rates ranging between 95-99%are reported, depending on how 'success' is defined.These approaches are probabilistic and based ontransitional probabilities calculated from extensivepretagged corpora.As for morphological disambiguation, CGP hasachieved almost the same success rate.
In compari-son, we first note that CG provides a formalism,based on ordinary linguistic concepts, that is applic-able to any language.
Work on Finnish and Swedishis in progress.
Second, CG fully integrates morpho-logy and surface syntax within the same formalism.Our present success rate for syntax, with some 220mapping statements and 25 syntactic constraints, isslightly above 90% (words with unique syntacticlabels).
The remaining words have more than onesyntactic label (one of which is correct).ReferencesChurch, K. 1988.
"A Stochastic Parts Program andNoun Phrase Parser for Running Text."
SecondCon-ference on Applied Natural Language Processing,Proceedings of the Conference, ACL 1988, pp.136-143.Garside, Roger, Leech, Geoffrey, and Sampson,Geoffrey 1987 (eds.
), 7he Computational Analysis ofEnglish.
Longman, London and New York.Karlsson, Fred 1989.
"Parsing and ConstraintGrammar".
Manuscript, Research Unit for Computa-tional Linguistics, University of Helsinki.Koskenniemi, Kimmo 1983.
Two-Level Morpho-logy.
A General Computational Model for Word-FormRecognition and Production.
Department of GeneralLinguistics, University of Helsinki, Publications No.13.AcknowledgementsThis research was supported by the Academy ofFinland in 1985-89, and by the Technology Develop-ment Centre of Finland (TEKES) in 1989-90.
Part ofit belongs to the ESPRIT II project SIMPR (2083).
Iam indebted to Kimmo Koskenniemi for help in thefield of morphological analysis, and to Atro Vouti-lainen, Juha Heikkil~, and Arto Anttila for help intesting the formalism.6 173
