Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863?869,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsGenerative Incremental Dependency Parsing with Neural NetworksJan Buys1and Phil Blunsom1,21Department of Computer Science, University of Oxford2Google DeepMind{jan.buys,phil.blunsom}@cs.ox.ac.ukAbstractWe propose a neural network model forscalable generative transition-based de-pendency parsing.
A probability distri-bution over both sentences and transi-tion sequences is parameterised by a feed-forward neural network.
The model sur-passes the accuracy and speed of previ-ous generative dependency parsers, reach-ing 91.1% UAS.
Perplexity results showa strong improvement over n-gram lan-guage models, opening the way to the ef-ficient integration of syntax into neuralmodels for language generation.1 IntroductionTransition-based dependency parsers that performincremental local inference with a discrimina-tive classifier offer an appealing trade-off be-tween speed and accuracy (Nivre, 2008; Zhangand Nivre, 2011; Choi and Mccallum, 2013).Recently neural network transition-based depen-dency parsers have been shown to give state-of-the-art performance (Chen and Manning, 2014;Dyer et al., 2015; Weiss et al., 2015).
However,the downstream integration of syntactic structurein language understanding and generation tasks isoften done heuristically.Neural networks have also been shown to bepowerful generative models for language mod-elling (Bengio et al., 2003; Mikolov et al., 2010)and machine translation (Kalchbrenner and Blun-som, 2013; Devlin et al., 2014; Sutskever et al.,2014).
However, currently these models lackawareness of syntax, which limits their ability toinclude longer-distance dependencies even whenpotentially unbounded contexts are used.In this paper we propose a generative model forincremental parsing that offers an efficient way toincorporate syntactic information into a generativemodel.
It relies on the strength of neural networksto overcome sparsity in the long conditioning con-texts required for an accurate model, while also of-fering a principled approach to learn dependency-based word representations (Levy and Goldberg,2014; Bansal et al., 2014).Generative models for graph-based dependencyparsing (Eisner, 1996; Wallach et al., 2008) aremuch less accurate than their discriminative coun-terparts.
Syntactic language models based onPCFGs (Roark, 2001; Charniak, 2001) and incre-mental parsing (Chelba and Jelinek, 2000; Emamiand Jelinek, 2005) have been proposed for speechrecognition and machine translation.
However,these models are also limited in either scalability,expressiveness, or both.
A generative transition-based dependency parser based on recurrent neu-ral networks (Titov and Henderson, 2007) obtainshigh accuracy, but training and decoding is pro-hibitively expensive.We perform efficient linear-time decoding witha particle filtering-based beam-search methodwhere derivations after pruned after every wordgeneration and the beam size depends on the un-certainty in the model (Buys and Blunsom, 2015).The model obtains 91.1% UAS on the WSJ,which is 0.2% UAS better than the previous high-est accuracy generative dependency parser (Titovand Henderson, 2007), while also being muchmore efficient.
As a language model its perplex-ity reaches 111.8, a 23% reduction over an n-gram baseline, when combining supervised train-ing with unsupervised fine-tuning.
Finally, we findthat the model is able to generate sentences thatdisplay both local and syntactic coherence.8632 Generative Transition-based ParsingOur parsing model is based on transition-basedarc-standard projective dependency parsing (Nivreand Scholz, 2004).
The generative formulationis similar to previous generative transition-basedparsers (Titov and Henderson, 2007; Cohen et al.,2011; Buys and Blunsom, 2015), and also relatedto the joint tagging and parsing model of Bohnetand Nivre (2012).The model predicts a sequence of parsing tran-sitions: A shift transition generates a word (andits POS tag), while a reduce transition adds an arc(i, l, j), where i is the head node, j the dependentand l is the dependency label.The joint probability distribution over a sen-tence with words w1:n, tags t1:nand a transitionsequence a1:2nis defined asn?i=1(p(ti|hmi)p(wi|ti,hmi)mi+1?j=mi+1p(aj|hj)),where miis the number of transitions that havebeen performed when (ti, wi) is shifted and hjisthe conditioning context at the jth transition.A parser configuration (?, ?,A) for sentence sconsists of a stack ?
of indices in s, an index ?
tothe next word to be generated, and a set of arcs A.The stack elements are referred to as ?1, .
.
.
, ?|?|,where ?1is the top element.
For any node a,lc1(a) refers to the leftmost child of a in A, andrc1(a) to its rightmost child.
A root node is addedto the beginning of the sentence, and the headword of the sentence (we assume there is only one)is the dependent of the root.The initial configuration is ([], 0, ?
), while Aterminal configuration is reached when ?
> |s|and |?| = 1.The transition types are shift, left-arc and right-arc.
Shift generates the next word of the sentenceand pushes it on the stack.
Left-arc adds an arc(?1, l, ?2) and removes ?2from the stack.
Right-arc adds (?2, l, ?1) and pops ?1.The parsing strategy adds arcs bottom-up.
Ina valid transition sequence the last transition is aright-arc from the root to the head word, and theroot node is not involved in any other dependen-cies.
We use an oracle to extract transition se-quences from the training data: The oracle prefersreduce over shift transitions when both may leadto a valid derivation.Order Elements1 ?1, ?2, ?3, ?42 lc1(?1), rc1(?1), lc1(?2), rc1(?2)lc2(?1), rc2(?1), lc2(?2), rc2(?2)3 lc1(lc1(?1)), rc1(rc1(?1))lc1(lc1(?2)), rc1(rc1(?2))Table 1: Conditioning context elements for neuralnetwork input: First, second and third order de-pendencies are used.3 Neural Network ModelOur probability model is based on neural net-work language models with distributed represen-tations (Bengio et al., 2003; Mnih and Hinton,2007), as well as feed-forward neural networkmodels for transition-based dependency pars-ing (Chen and Manning, 2014; Weiss et al., 2015).We estimate the distributions p(ti|hi), p(wi|ti,hi)and p(aj|hj) with neural networks with shared in-put and hidden layers but separate output layers.The templates for the conditioning context usedare defined in Table 1.
In the templates we ob-tain sentence indexes, which are then mapped tothe corresponding words, tags and labels (for thedependencies of 2nd and 3rd order elements).
Theneural network allows us to include a large numberof elements without suffering from sparsity.In the input layer we make use of additive rep-resentations (Botha and Blunsom, 2014) so thatfor each word input position i we can include theword type, tag and other features, and learn inputrepresentations for each of these.
Each contextfeature f has an input representation qf?
RD.The composite representation is computed as qi=?f??
(wi)qf, where ?
(wi) are the word features.The hidden layer is then defined as?
(h) = g(L?j=1Cjqhj),where Cj?
RD?Dare transformation matricesdefined for each position in sequence h, L = |h|and g is a (usually non-linear) activation functionapplied element-wise.
The matrices Cjcan be ap-proximated to be diagonal to reduce the numberof model parameters and speed up the model byavoiding expensive matrix multiplications.For the output layer predicting the next transi-tion a, the hidden layer is mapped with a scoring864function?
(a,h) = kTa?
(h) + ea,where kais the transition output representationand eais the bias weight.
The score is normalisedwith the soft-max function:p(a|h) =exp(?(a,h))?a??Aexp(?
(a?,h)).The output layer for predicting the next tag hasa similar form, using the scoring function?
(t,h) = tTt?
(h) + otfor tag representation ttand bias ot.The probability p(w|t,h) can be estimatedsimilarly.
However, to reduce the computa-tional cost of normalising over the entire vocab-ulary, we factorize the probability as P (w|h) =P (c|t,h)P (w|c, t,h), where c = c(w) is theunique class of word w. For each c, let ?
(c) bethe set of words in that class.
The vocabulary isclustered into approximately?|V | classes usingBrown clustering (Brown et al., 1992), reducingthe number of items to sum over in the normal-isation factor from O(|V |) to O(?|V |).
Class-based factorization has been shown to be an effec-tive strategy in normalizing neural language mod-els (Baltescu and Blunsom, 2015),The class prediction score is defined as?
(c,h) = sTc?
(h) + dc, where sc?
RDis theoutput weight vector for class c and dcis the classbias weight.
The output layer then consists of asoftmax function for p(c|h) and another softmaxfor the word predictionp(w|c,h) =exp(?(w,h))?w???(c)exp(?
(w?,h)),where ?
(w,h) = rTw?
(h)+bwis the word scoringfunction with output word representation rwandbias weight bw.The model is trained with minibatch stochas-tic gradient descent (SGD) with Adagrad (Duchiet al., 2011) and L2 regularisation, to minimisethe negative log likelihood of the joint distribu-tion over parsed training sentences.
For our ex-periments we train the model while the trainingobjective improves, and choose the parameters ofthe iteration with the best development set accu-racy (early stopping).
The model obtains high ac-curacy with only a few training iterations.4 DecodingBeam-search decoders for transition-based pars-ing (Zhang and Clark, 2008) keep a beam of par-tial derivations, advancing each derivation by onetransition at a time.
When the size of the beamexceeds a set threshold, the lowest-scoring deriva-tions are removed.
However, in an incrementalgenerative model we need to compare derivationswith the same number of words shifted, rather thantransitions performed.
To let the decoding time re-main linear, we also need to bound the total num-ber of reduce transitions that can be performedover all derivations between two shift transitions.To achieve this, we use a decoding method re-cently proposed for generative incremental pars-ing (Buys and Blunsom, 2015) based on particlefiltering (Doucet et al., 2001), a sequential MonteCarlo sampling method.In the algorithm, a fixed number of particlesare divided among the partial derivations in thebeam.
Suppose iwords have been shifted in all thederivations on the beam.
To predict the next tran-sition from derivation dj, its particles are dividedaccording to p(a|h).
In practice, adding only shiftand the most likely reduce transition leads to al-most no accuracy loss.
After all the derivationshave been advanced to shift word i+1, a selectionstep is performed: The number of particles of eachderivation is redistributed according to its proba-bility, weighted by its current number of particles.Some derivations may be assigned 0 particles, inwhich case they are removed.The particle filtering method lets the beam sizedepend of the uncertainty of the model, somewhatsimilar to Choi and Mccallum (2013), while fixingthe total number of particles constrains the decod-ing time to be linear.
The particle filter also allowus to sample outputs, and to marginalise over thesyntax when generating.5 ExperimentsWe evaluate our model for parsing and languagemodelling on the English Penn Treebank (Marcuset al., 1993) WSJ parsing setup1.
Constituencytrees are converted to projective CoNLL syntac-tic dependencies (Johansson and Nugues, 2007)with the LTH converter2.
For some experiments1Training on sections 02-21, development on section 22,and testing on section 23.2http://nlp.cs.lth.se/software/treebank converter/865Activation UAS LASlinear 88.40 86.48rectifier 89.99 88.31tanh 90.91 89.22sigmoid 91.48 89.94Table 2: Parsing accuracies using different neuralnetwork activation functions.we also use the Stanford dependency representa-tion (De Marneffe and Manning, 2008) (SD)3.Our neural network implementation is partlybased on the OxLM neural language modellingframework (Baltescu et al., 2014).
The model pa-rameters are initialised randomly by drawing froma Gaussian distribution with mean 0 and variance0.1, except for the bias weights, which are ini-tialised by the unigram distributions of their out-put.
We use minibatches of size 128, the L2 regu-larization parameter is 10, and the word represen-tation and hidden layer of size is 256.
The Ada-grad learning rate is initialised to 0.05.POS tags for the development and test sets areobtained with the Stanford POS tagger (Toutanovaet al., 2003), with 97.5% test set accuracy.
Wordsthat occur only once in the training data are treatedas unknown words.
Unknown words are replacedby tokens representing morphological surface fea-tures (based on capitalization, numbers, punctua-tion and common suffixes) similar to those usedin the implementation of generative constituencyparsers (Klein and Manning, 2003).5.1 Parsing resultsWe report unlabelled attachment score (UAS) andlabelled attachment score (LAS) in our results,excluding punctuation.
On the development set,we consider the effect of the choice of activationfunction (Table 2), finding that a sigmoid activa-tion (logistic function) performs best, following bytanh.
Under our training setup the model can ob-tain up to 91.0 UAS after only 1 training iteration,thereby performing pure online learning.We found that including third order depen-dencies in the conditioning context performs just0.1% UAS better than including only first and sec-ond order dependencies.
Including additional ele-ments does not improve performance further.
Themodel can obtain 91.18 UAS, 89.02 LAS when3Converted with version 3.4.1 of the Stanford parser,available at http::/nlp.stanford.edu/software/lex-parser.shtml.Model UAS LASWallach et al.
(2008) 85.7 -Titov and Henderson (2007) 90.93 89.42NN-GenDP 91.11 89.41Chen and Manning (2014) 92.0 90.7Table 3: Parsing accuracies for dependencyparsers on the WSJ test set, CoNLL dependencies.trained only on words, not POS tags.
Dependencyparsers that do not use distributed representationstend to rely much more on the tags.Test set results comparing generative depen-dency parsers are given in Table 3 (our model isrefered to as NN-GenDP).
The graph-based gen-erative baseline (Wallach et al., 2008), parame-terised by Pitman-Yor Processes, is quite weak.Our model outperforms the generative model ofTitov and Henderson (2007), which we retrainedon our dataset, by 0.2%, despite that model be-ing able to condition on arbitrary-sized contexts.The decoding speed of our model is around 20 sen-tences per second, against less than 1 sentence persecond for Titov and Henderson?s model.
Usingdiagonal transformation matrices further increasesour model?s speed, but reduces parsing accuracy.On the Stanford dependency representation ourmodel obtains 90.63% UAS, 88.27% LAS.
Al-though this performance is promising, it is stillbelow the discriminative neural network models ofDyer et al.
(2015) and Weiss et al.
(2015), who ob-tained 93.1% UAS and 94.0% UAS respectively.5.2 Language modellingWe also evaluate our parser as a language model,on the same WSJ data used for the parsing eval-uation4.
We perform unlabelled parsing, as ex-periments show that including labels in the con-ditioning context has a very small impact on per-formance.
Neither do we use POS tags, as theyare too expensive to predict in language genera-tion applications.Perplexity results on the WSJ are given in Ta-ble 4.
As baselines we report results on modifiedKnesser-Ney (Kneser and Ney, 1995) and neu-ral network 5-gram models.
For our dependency-based language models we report perplexitiesbased on the most likely parse found by the de-coder, which gives an upper bound on the true4However instead of using multiple unknown wordclasses, we replace all numbers by 0 and have a single un-known word token.866the u.s. union board said revenue rose 11 % to $ NUM million , or $ NUM a share .mr.
bush has UNK-ed a plan to buy the company for $ NUM to NUM million , or $ NUM a share .the plan was UNK-ed by the board ?s decision to sell its $ NUM million UNK loan loan funds .in stocks coming months , china ?s NUM shares rose 10 cents to $ NUM million , or $ NUM a share .in the case , mr. bush said it will sell the company business UNK concern to buy the company .it was NUM common shares in addition , with $ NUM million , or $ NUM a share , according to mr. bush .in the first quarter , 1989 shares closed yesterday at $ NUM , mr. bush has increased the plan .last year ?s retrenchment price index index rose 11 cents to $ NUM million , or $ NUM million is asked .last year earlier , net income rose 11 million % to $ NUM million , or 91 cents a share .the u.s. union has UNK-ed $ NUM million , or 22 cents a share , in 1990 , payable nov. 9 .Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.Model PerplexityKN 5-gram 145.7NN 5-gram 142.5NN-GenDP 132.2NN-GenDP + unsup 111.8Table 4: WSJ Language modelling test results.We compare our model, with and without unsu-pervised tuning, to n-gram baselines.value of the model perplexity.First we only perform standard supervised train-ing with the model - this already leads to an im-provement of 10 perplexity points over the neu-ral n-gram model.
Second we consider a train-ing setup where we first perform 5 supervised it-erations, and then perform unsupervised training,treating the transition sequence as latent.
For eachminibatch parse trees are sampled with a parti-cle filter.
This approach further improves the per-plexity to 111.8, a 23% reduction relative to theKnesser-Ney model.The unsupervised training stage lets the parsingaccuracy fall from 91.48 to 89.49 UAS.
We pos-tulate that the model is learning to make small ad-justments to favour of parsing structures that ex-plain the data better than the annotated parse trees,leading to the improvement in perplexity.To test the scalability of our model, we alsotrained it on a larger unannotated corpus ?
a sub-set (of around 7 million words) of the billion wordlanguage modeling benchmark dataset (Chelba etal., 2013).
After training the model on the WSJ,we parsed the unannotated data with the model,and continued to train on the obtained parses.We observed a small increase in perplexity, from203.5 for a neural n-gram model to 200.7 for thegenerative dependency model.
We expect largerimprovements when training on more data andwith more sophisticated inference.To evaluate our generative model qualitatively,we perform unconstrained generation of sentences(and parse trees) from the model, and found thatsentences display a higher degree of syntactic co-herence than sentences generated by an n-grammodel.
See Table 5 for examples generated by themodel.
The highest-scoring sentences of length 20or more are given, from 1000 samples generated.Note that the generation includes unknown wordtokens (here NUM, UNK and UNK-ed are used).6 ConclusionWe presented an incremental generative depen-dency parser that can obtain accuracies competi-tive with discriminative models.
The same modelcan be applied as an efficient syntactic languagemodel, and for future work it should be integratedinto language generation tasks such as machinetranslation.AcknowledgementsWe acknowledge the financial support of the Ox-ford Clarendon Fund and the Skye Foundation.ReferencesPaul Baltescu and Phil Blunsom.
2015.
Pragmatic neu-ral language modelling in machine translation.
InProceedings of NAACL-HTL, pages 820?829.Paul Baltescu, Phil Blunsom, and Hieu Hoang.
2014.Oxlm: A neural language modelling framework formachine translation.
The Prague Bulletin of Mathe-matical Linguistics, 102(1):81?92, October.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proceedings of the ACL.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.867Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-ceedings of EMNLP-CONLL, pages 1455?1465.Jan A. Botha and Phil Blunsom.
2014.
Compositionalmorphology for word representations and languagemodelling.
In Proceedings of the 31st InternationalConference on Machine Learning (ICML).Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.Jan Buys and Phil Blunsom.
2015.
A Bayesian modelfor generative transition-based dependency parsing.arXiv preprint arXiv:1506.04334.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of ACL, pages124?131.
Association for Computational Linguis-tics.Ciprian Chelba and Frederick Jelinek.
2000.
Struc-tured language modeling.
Computer Speech & Lan-guage, 14(4):283?332.Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,Thorsten Brants, Phillipp Koehn, and Tony Robin-son.
2013.
One billion word benchmark for measur-ing progress in statistical language modeling.
Tech-nical report, Google.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of EMNLP.Jinho D. Choi and Andrew Mccallum.
2013.Transition-based dependency parsing with selec-tional branching.
In Proceedings of ACL.Shay B. Cohen, Carlos G?omez-Rodr?
?guez, and GiorgioSatta.
2011.
Exact inference for generative proba-bilistic non-projective dependency parsing.
In Pro-ceedings of EMNLP, pages 1234?1245.Marie-Catherine De Marneffe and Christopher D Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of ACL,pages 1370?1380.Arnaud Doucet, Nando De Freitas, and Neil Gordon.2001.
Sequential Monte Carlo methods in practice.Springer.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Joural of MachineLearning Research, 12:2121?2159.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of ACL 2015.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Pro-ceedings of COLING, pages 340?345.Ahmad Emami and Frederick Jelinek.
2005.
A neuralsyntactic language model.
Machine Learning, 60(1-3):195?227.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In 16th Nordic Conference of Computa-tional Linguistics, pages 105?112, Tartu, Estonia.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In Proceedingsof EMNLP, pages 1700?1709.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of ACL,pages 423?430.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
InICASSP, volume 1, pages 181?184.
IEEE.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of ACL:Short Papers.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th International Conferenceon Machine learning, pages 641?648.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedingsof COLING.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational linguistics,27(2):249?276.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems, pages 3104?3112.868Ivan Titov and James Henderson.
2007.
A latent vari-able model for generative dependency parsing.
InProceedings of the Tenth International Conferenceon Parsing Technologies, pages 144?155.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT-NAACL, pages 173?180.Hanna M Wallach, Charles Sutton, and Andrew Mc-Callum.
2008.
Bayesian modeling of dependencytrees using hierarchical Pitman-Yor priors.
In ICMLWorkshop on Prior Knowledge for Text and Lan-guage Processing.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural net-work transition-based parsing.
In Proceedings ofACL 2015.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-basedand transition-based dependency parsing.
In Pro-ceedings of EMNLP, pages 562?571.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of ACL-HLT: Short papers, pages 188?193.869
