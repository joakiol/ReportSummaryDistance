Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 38?47,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsNatural Language Descriptions of Visual Scenes:Corpus Generation and AnalysisMuhammad Usman Ghani Khan Rao Muhammad Adeel Nawab Yoshihiko GotohUniversity of Sheffield, United Kingdom{ughani, r.nawab, y.gotoh}@dcs.shef.ac.ukAbstractAs video contents continue to expand, it isincreasingly important to properly annotatevideos for effective search, mining and re-trieval purposes.
While the idea of anno-tating images with keywords is relativelywell explored, work is still needed for anno-tating videos with natural language to im-prove the quality of video search.
The fo-cus of this work is to present a video datasetwith natural language descriptions which isa step ahead of keywords based tagging.We describe our initial experiences with acorpus consisting of descriptions for videosegments crafted from TREC video data.Analysis of the descriptions created by 13annotators presents insights into humans?interests and thoughts on videos.
Such re-source can also be used to evaluate auto-matic natural language generation systemsfor video.1 IntroductionThis paper presents our experiences in manu-ally constructing a corpus, consisting of naturallanguage descriptions of video segments craftedfrom a small subset of TREC video1 data.
Ina broad sense the task can be considered oneform of machine translation as it translates videostreams into textual descriptions.
To date thenumber of studies in this field is relatively smallpartially because of lack of appropriate datasetfor such task.
Another obstacle may be inher-ently larger variation for descriptions that can beproduced for videos than a conventional transla-tion from one language to another.
Indeed hu-mans are very subjective while annotating video1www-nlpir.nist.gov/projects/trecvid/streams, e.g., two humans may produce quite dif-ferent descriptions for the same video.
Based onthese descriptions we are interested to identify themost important and frequent high level features(HLFs); they may be ?keywords?, such as a par-ticular object and its position/moves, used for asemantic indexing task in video retrieval.
MostlyHLFs are related to humans, objects, their movesand properties (e.g., gender, emotion and action)(Smeaton et al, 2009).In this paper we present these HLFs in the formof ontologies and provides two hierarchical struc-tures of important concepts ?
one most relevantfor humans and their actions, and another for nonhuman objects.
The similarity of video descrip-tions is quantified using a bag of word model.
Thenotion of sequence of events in a video was quan-tified using the order preserving sequence align-ment algorithm (longest common subsequence).This corpus may also be used for evaluation ofautomatic natural language description systems.1.1 BackgroundThe TREC video evaluation consists of on-goingseries of annual workshops focusing on a list ofinformation retrieval (IR) tasks.
The TREC videopromotes research activities by providing a largetest collection, uniform scoring procedures, anda forum for research teams interested in present-ing their results.
The high level feature extrac-tion task aims to identify presence or absence ofhigh level semantic features in a given video se-quence (Smeaton et al, 2009).
Approaches tovideo summarisation have been explored usingrushes video2 (Over et al, 2007).2Rushes are the unedited video footage, sometimes re-ferred to as a pre-production video.38TREC video also provides a variety of metadata annotations for video datasets.
For the HLFtask, speech recognition transcripts, a list of mas-ter shot references, and shot IDs having HLFs inthem are provided.
Annotations are created forshots (i.e., one camera take) for the summarisa-tion task.
Multiple humans performing multipleactions in different backgrounds can be shown inone shot.
Annotations typically consist of a fewphrases with several words per phrase.
Humanrelated features (e.g., their presence, gender, age,action) are often described.
Additionally, cameramotion and camera angle, ethnicity informationand human?s dressing are often stated.
On theother hand, details relating to events and objectsare usually missing.
Human emotion is anothermissing information in many of such annotations.2 Corpus CreationWe are exploring approaches to natural languagedescriptions of video data.
The step one of thestudy is to create a dataset that can be used fordevelopment and evaluation.
Textual annotationsare manually generated in three different flavours,i.e., selection of HLFs (keywords), title assign-ment (a single phrase) and full description (mul-tiple phrases).
Keywords are useful for identifica-tion of objects and actions in videos.
A title, in asense, is a summary in the most compact form; itcaptures the most important content, or the theme,of the video in a short phrase.
On the other hand,a full description is lengthy, comprising of severalsentences with details of objects, activities andtheir interactions.
Combination of keywords, a ti-tle, and a full descriptions will create a valuableresource for text based video retrieval and sum-marisation tasks.
Finally, analysis of this datasetprovides an insight into how humans generate nat-ural language description for video.Most of previous datasets are related to spe-cific tasks; PETS (Young and Ferryman, 2005),CAVIAR (Fisher et al, 2005) and Terrascope(Jaynes et al, 2005) are for surveillance videos.KTH (Schuldt et al, 2004) and the Hollywood ac-tion dataset (Marszalek et al, 2009) are for hu-man action recognition.
MIT car dataset is foridentification of cars (Papageorgiou and Poggio,1999).
Caltech 101 and Caltech 256 are imagedatasets with 101 and 256 object categories re-spectively (Griffin et al, 2007) but there is noinformation about human actions or emotions.There are some datasets specially generated forscene settings such as MIT outdoor scene dataset(Oliva and Torralba, 2009).
Quattoni and Tor-ralba (2009) created indoor dataset with 67 differ-ent scenes categories.
For most of these datasetsannotations are available in the form of keywords(e.g., actions such as sit, stand, walk).
They weredeveloped for keyword search, object recognitionor event identification tasks.
Rashtchian et al(2010) provided an interesting dataset of 1000 im-ages which contain natural language descriptionsof those images.In this study we select video clips from TRECvideo benchmark for creating annotations.
Theyinclude categories such as news, meeting, crowd,grouping, indoor/outdoor scene settings, traffic,costume, documentary, identity, music, sports andanimals videos.
The most important and proba-bly the most frequent content in these videos ap-pears to be a human (or humans), showing theiractivities, emotions and interactions with otherobjects.
We do not intend to derive a datasetwith a full scope of video categories, which is be-yond our work.
Instead, to keep the task manage-able, we aim to create a compact dataset that canbe used for developing approaches to translatingvideo contents to natural language description.Annotations were manually created for a smallsubset of data prepared form the rushes videosummarisation task and the HLF extraction taskfor the 2007 and 2008 TREC video evaluations.It consisted of 140 segments of videos ?
20 seg-ments for each of the following seven categories:Action videos: Human posture is visible and hu-man can be seen performing some actionsuch as ?sitting?, ?standing?, ?walking?
and?running?.Close-up: Human face is visible.
Facial expres-sions and emotions usually define mood ofthe video (e.g., happy, sad).News: Presence of an anchor or reporters.
Char-acterised by scene settings such as weatherboards at the background.Meeting: Multiple humans are sitting and com-municating.
Presence of objects such aschairs and a table.Grouping: Multiple humans interaction scenesthat do not belong to a meeting scenario.
A39table or chairs may not be present.Traffic: Presence of vehicles such as cars, busesand trucks.
Traffic signals.Indoor/Outdoor: Scene settings are more obvi-ous than human activities.
Examples may bepark scenes and office scenes (where com-puters and files are visible).Each segment contained a single camera shot,spanning between 10 and 30 seconds in length.Two categories, ?Close-up?
and ?Action?, aremainly related to humans?
activities, expressionsand emotions.
?Grouping?
and ?Meeting?
de-pict relation and interaction between multiple hu-mans.
?News?
videos explain human activitiesin a constrained environment such as a broad-cast studio.
Last two categories, ?Indoor/Outdoor?and ?Traffic?, are often observed in surveillancevideos.
They often shows for humans?
interac-tion with other objects in indoor and outdoor set-tings.
TREC video annotated most video seg-ments with a brief description, comprising of mul-tiple phrases and sentences.
Further, 13 humansubjects prepared additional annotation for thesevideo segments, consisting of keywords, a titleand a full description with multiple sentences.They are referred to as hand annotations in therest of this paper.2.1 Annotation ToolThere exist several freely available video anno-tation tools.
One of the popular video anno-tation tool is Simple Video Annotation tool3.It allows to place a simple tag or annotationon a specified part of the screen at a particulartime.
The approach is similar to the one used byYouTube4.
Another well-known video annotationtool is Video Annotation Tool5.
A video can bescrolled for a certain time period and place anno-tations for that part of the video.
In addition, anannotator can view a video clip, mark a time seg-ment, attach a note to the time segment on a videotimeline, or play back the segment.
?Elan?
anno-tation tool allows to create annotations for bothaudio and visual data using temporal information(Wittenburg et al, 2006).
During that annotationprocess, a user selects a section of video using the3videoannotation.codeplex.com/4www.youtube.com/t/annotations about5dewey.at.northwestern.edu/ppad2/documents/help/video.htmlFigure 1: Video Description Tool (VDT).
An anno-tator watches one video at one time, selects all HLFspresent in the video, describes a theme of the video asa title and creates a full description for important con-tents in the video.timeline capability and writes annotation for thespecific time.We have developed our own annotation tool be-cause of a few reasons.
None of existing annota-tion tools provided the functionality of generat-ing a description and/or a title for a video seg-ment.
Some tools allows selection of keywords ina free format, which is not suitable for our pur-pose of creating a list of HLFs.
Figure 1 showsa screen shot of the video annotation tool devel-oped, which is referred to as Video DescriptionTool (VDT).
VDT is simple to operate and assistannotators in creating quality annotations.
Thereare three main items to be annotated.
An anno-tator is shown one video segment at one time.Firstly a restricted list of HLFs is provided foreach segment and an annotator is required to se-lect all HLFs occurring in the segment.
Second,a title should be typed in.
A title may be a themeof the video, typically a phrase or a sentence withseveral words.
Lastly, a full description of videocontents is created, consisting of several phrasesand sentences.
During the annotation, it is pos-sible to stop, forward, reverse or play again thesame video if required.
Links are provided fornavigation to the next and the previous videos.
Anannotator can delete or update earlier annotationsif required.402.2 Annotation ProcessA total of 13 annotators were recruited to createtexts for the video corpus.
They were undergradu-ate or postgraduate students and fluent in English.It was expected that they could produce descrip-tions of good quality without detailed instructionsor further training.
A simple instruction set wasgiven, leaving a wide room for individual inter-pretation about what might be included in the de-scription.
For quality reasons each annotator wasgiven one week to complete the full set of videos.Each annotator was presented with a completeset of 140 video segments on the annotation toolVDT.
For each video annotators were instructedto provide?
a title of one sentence long, indicating themain theme of the video;?
description of four to six sentences, relatedto what are shown in the video;?
selection of high level features (e.g., male,female, walk, smile, table).The annotations are made with open vocabulary?
that is, they can use any English words as longas they contain only standard (ASCII) characters.They should avoid using any symbols or computercodes.
Annotators were further guided not to useproper nouns (e.g., do not state the person name)and information obtained from audio.
They werealso instructed to select all HLFs appeared in thevideo.3 Corpus Analysis13 annotators created descriptions for 140 videos(seven categories with 20 videos per category),resulting in 1820 documents in the corpus.
Thetotal number of words is 30954, hence the av-erage length of one document is 17 words.
Wecounted 1823 unique words and 1643 keywords(nouns and verbs).Figure 2 shows a video segment for a meet-ing scene, sampled at 1 fps (frame per second),and three examples for hand annotations.
Theytypically contain two to five phrases or sentences.Most sentences are short, ranging between two tosix words.
Descriptions for human, gender, emo-tion and action are commonly observed.
Occa-sionally minor details for objects and events arealso stated.
Descriptions for the background areHand annotation 1(title) interview in the studio;(description) three people are sitting on a red ta-ble; a tv presenter is interviewing his guests; he istalking to the guests; he is reading from papers infront of him; they are wearing a formal suit;Hand annotation 2(title) tv presenter and guests(description) there are three persons; the one ishost; others are guests; they are all men;Hand annotation 3(title) three men are talking(description) three people are sitting around thetable and talking each other;Figure 2: A montage showing a meeting scene in anews video and three sets of hand annotations.
Inthis video segment, three persons are shown sitting onchairs around a table ?
extracted from TREC video?20041116 150100 CCTV4 DAILY NEWS CHN33050028?.often associated with objects rather than humans.It is interesting to observe the subjectivity with thetask; the variety of words were selected by indi-vidual annotators to express the same video con-tents.
Figure 3 shows another example of a videosegment for a human activity and hand annota-tions.3.1 Human Related FeaturesAfter removing function words, the frequency foreach word was counted in hand annotations.
Twoclasses are manually defined; one class is relateddirectly to humans, their body structure, identity,action and interaction with other humans.
(An-other class represents artificial and natural objectsand scene settings, i.e., all the words not directlyrelated to humans, although they are importantfor semantic understanding of the visual scene ?described further in the next section.)
Note thatsome related words (e.g., ?woman?
and ?lady?
)were replaced with a single concept (?female?
);concepts were then built up into a hierarchicalstructure for each class.Figure 4 presents human related informationobserved in hand annotations.
Annotators paidfull attention to human gender information as thenumber of occurrences for ?female?
and ?male?
is41Figure 4: Human related information found in 13 hand annotations.
Information is divided into structures (gen-der, age, identity, emotion, dressing, grouping and body parts) and activities (facial, hand and body).
Each boxcontains a high level concept (e.g., ?woman?
and ?lady?
are both merged into ?female?)
and the number of itsoccurrences.Hand annotation 1(title) outdoor talking scene;(description) young woman is sitting on chair inpark and talking to man who is standing next toher;Hand annotation 2(title) A couple is talking;(description) two person are talking; a lady is sit-ting and a man is standing; a man is wearing ablack formal suit; a red bus is moving in the street;people are walking in the street; a yellow taxi ismoving in the street;Hand annotation 3(title) talk of two persons;(description) a man is wearing dark clothes; he isstanding there; a woman is sitting in front of him;they are saying to each other;Figure 3: A montage of video showing a human activ-ity in an outdoor scene and three sets of hand annota-tions.
In this video segment, a man is standing whilea woman is sitting in outdoor ?
from TREC video?20041101 160000 CCTV4 DAILY NEWS CHN 41504210?.the highest among HLFs.
This highlights our con-clusion that most interesting and important HLFis humans when they appear in a video.
On theother hand age information (e.g., ?old ?, ?young?,?child ?)
was not identified very often.
Names forhuman body parts have mixed occurrences rang-ing from high (?hand ?)
to low (?moustache?).
Sixbasic emotions ?
anger, disgust, fear, happiness,sadness, and surprise as discussed by Paul Ek-man6 ?
covered most of facial expressions.Dressing became an interesting feature whena human was in a unique dress such as a formalsuit, a coloured jacket, an army or police uni-form.
Videos with multiple humans were com-mon, and thus human grouping information wasfrequently recognised.
Human body parts wereinvolved in identification of human activities; theyincluded actions such as standing, sitting, walk-ing, moving, holding and carrying.
Actions re-lated to human body and posture were frequentlyidentified.
It was rare that unique human identi-ties, such as police, president and prime minister,were described.
This may indicate that a viewermight want to know a specific type of an objectto describe a particular situation instead of gener-alised concepts.3.2 Objects and Scene SettingsFigure 5 shows the hierarchy created for HLFsthat did not appear in Figure 4.
Most of the wordsare related to artificial objects.
Humans inter-act with these objects to complete an activity ?6en.wikipedia.org/wiki/Paul Ekman42Figure 5: Artificial and natural objects and scene set-tings were summarised into six groups.e.g., ?man is sitting on a chair?, ?she is talkingon the phone?, ?he is wearing a hat ?.
Natural ob-jects were usually in the background, providingthe additional context of a visual scene ?
e.g.,?human is standing in the jungle, ?sky is clear to-day?.
Place and location information (e.g., room,office, hospital, cafeteria) were important as theyshow the position of humans or other objects inthe scene ?
e.g., ?there is a car on the road, ?peo-ple are walking in the park ?.Colour information often plays an importantpart in identifying separate HLFs ?
e.g., ?a manin black shirt is walking with a woman with greenjacket?, ?she is wearing a white uniform ?.
Thelarge number of occurrences for colours indicateshuman?s interest in observing not only objects butalso their colour scheme in a visual scene.
Somehand descriptions reflected annotator?s interest inscene settings shown in the foreground or in thebackground.
Indoor/outdoor scene settings werealso interested in by some annotators.
These ob-servations demonstrate that a viewer is interestedin high level details of a video and relationshipsbetween different prominent objects in a visualscene.3.3 Spatial RelationsFigure 6 presents a list of the most frequent wordsand phrases related to spatial relations found inhand annotations.
Spatial relations between HLFsare important when explaining the semantics ofvisual scenes.
Their effective use leads to thesmooth description.
Spatial relations can be cate-gorised intoin (404); with (120); on (329); near (68); around(63); at (55); on the left (35); in front of (24);down (24); together (24); along (16); beside (16);on the right (16); into (14); far (11); between (10);in the middle (10); outside (8); off (8); over (8);pass-by (8); across (7); inside (7); middle (7); un-der (7); away (6); after (7)Figure 6: List of frequent spatial relations with theircounts found in hand annotations.static: relations between stationary objects;dynamic: direction and path of moving objects;inter-static and dynamic: relations between movingand not moving objects.Static relations can establish the scene settings(e.g., ?chairs around a table?
may imply an indoorscene).
Dynamic relations are used for finding ac-tivities present in the video (e.g., ?a man is run-ning with a dog?).
Inter-static and dynamic rela-tions are a mixture of stationary and non station-ary objects; they explain semantics of the com-plete scene (e.g., ?persons are sitting on the chairsaround the table?
indicates a meeting scene).3.4 Temporal RelationsVideo is a class of time series data formed withhighly complex multi dimensional contents.
Letvideo X be a uniformly sampled frame sequenceof length n, denoted by X = {x1, .
.
.
, xn}, andeach frame xi gives a chronological position ofthe sequence (Figure 7).
To generate full de-scription of video contents, annotators use tempo-ral information to join descriptions of individualframes.
For example,A man is walking.
After sometime he en-ters the room.
Later on he is sitting on thechair.Based on the analysis of the corpus, we describetemporal information in two flavors:1. temporal information extracted from activi-ties of a single human;2. interactions between multiple humans.Most common relations in video sequences are?before?, ?after?, ?start ?
and ?finish ?
for single hu-mans, and ?overlap?, ?during?
and ?meeting?
formultiple humans.Figure 8 presents a list of the most frequentwords in the corpus related to temporal relations.It can be observed that annotators put much focus43Figure 7: Illustration of a video as a uniformly sam-pled sequence of length n. A video frame is denotedby xi, whose spatial context can be represented in thed dimensional feature space.single human:then (25); end (24); before (22); after (16); next(12); later on (12); start (11); previous (11);throughout (10); finish (8); afterwards (6); priorto (4); since (4)multiple humans:meet (114); while (37); during (27); at the sametime (19); overlap (12); meanwhile (12); through-out (7); equals (4)Figure 8: List of frequent temporal relations with theircounts found in hand annotations.on keywords related to activities of multiple hu-mans as compared to single human cases.
?Meet?keyword has the highest frequency, as annota-tors usually consider most of the scenes involvingmultiple humans as the meeting scene.
?While?keyword is mostly used for showing separate ac-tivities of multiple humans such as ?a man is walk-ing while a woman is sitting?.3.5 Similarity between DescriptionsA well-established approach to calculating humaninter-annotator agreement is kappa statistics (Eu-genio and Glass, 2004).
However in the currenttask it is not possible to compute inter-annotatoragreement using this approach because no cat-egory was defined for video descriptions.
Fur-ther the description length for one video can varyamong annotators.
Alternatively the similarity be-tween natural language descriptions can be calcu-lated; an effective and commonly used measureto find the similarity between a pair of documentsis the overlap similarity coefficient (Manning andSchu?tze, 1999):Simoverlap(X,Y ) =|S(X,n) ?
S(Y, n)|min(|S(X,n)|, |S(Y, n)|)where S(X,n) and S(Y, n) are the set of distinctn-grams in documents X and Y respectively.
It isa similarity measure related to the Jaccard index(Tan et al, 2006).
Note that when a set X is a sub-set of Y or the converse, the overlap coefficient isequal to one.
Values for the overlap coefficientrange between 0 and 1, where ?0?
presents the sit-uation where documents are completely differentand ?1?
describes the case where two documentsare exactly the same.Table 1 shows the average overlap similarityscores for seven scene categories within 13 handannotations.
The average was calculated fromscores for individual description, that was com-pared with the rest of descriptions in the samecategory.
The outcome demonstrate the fact thathumans have different observations and interestswhile watching videos.
Calculation were repeatedwith two conditions; one with stop words re-moved and Porter stemmer (Porter, 1993) applied,but synonyms NOT replaced, and the other withstop words NOT removed, but Porter stemmer ap-plied and synonyms replaced.
It was found thelatter combination of preprocessing techniques re-sulted in better scores.
Not surprisingly synonymreplacement led to increased performance, indi-cating that humans do express the same conceptusing different terms.The average overlap similarity score was higherfor ?Traffic?
videos than for the rest of categories.Because vehicles were the major entity in ?Traf-fic?
videos, rather than humans and their actions,contributing for annotators to create more uniformdescriptions.
Scores for some other categorieswere lower.
It probably means that there are moreaspects to pay attention when watching videos in,e.g., ?Grouping?
category, hence resulting in thewider range of natural language expressions pro-duced.3.6 Sequence of Events MatchingVideo is a class of time series data which canbe partitioned into time aligned frames (images).These frames are tied together sequentially andtemporally.
Therefore, it will be useful to knowhow a person captures the temporal informationpresent in a video.
As the order is preserved in asequence of events, a suitable measure to quantifysequential and temporal information of a descrip-tion is the longest common subsequence (LCS).This approach computes the similarity betweena pair of token (i.e., word) sequences by simplycounting the number of edit operations (insertionsand deletions) required to transform one sequenceinto the other.
The output is a sequence of com-mon elements such that no other longer string is44Action Close-up Indoor Grouping Meeting News Trafficunigram (A) 0.3827 0.3913 0.4217 0.3809 0.3968 0.4378 0.4687(B) 0.4135 0.4269 0.4544 0.4067 0.4271 0.4635 0.5174bigram (A) 0.1483 0.1572 0.1870 0.1605 0.1649 0.1872 0.1765(B) 0.2490 0.2616 0.2877 0.2619 0.2651 0.2890 0.2825trigram (A) 0.0136 0.0153 0.0301 0.0227 0.0219 0.0279 0.0261(B) 0.1138 0.1163 0.1302 0.1229 0.1214 0.1279 0.1298Table 1: Average overlapping similarity scores within 13 hand annotations.
For each of unigram, bigram andtrigram, scores are calculated for seven categories in two conditions: (A) stop words removed and Porter stemmerapplied, but synonyms NOT replaced; (B) stop words NOT removed, but Porter stemmer applied and synonymsreplaced.raw synonym keywordAction 0.3782 0.3934 0.3955Close-up 0.4181 0.4332 0.4257Indoor 0.4248 0.4386 0.4338Grouping 0.3941 0.4104 0.3832Meeting 0.3939 0.4107 0.4124News 0.4382 0.4587 0.4531Traffic 0.4036 0.4222 0.4093Table 2: Similarity scores based on the longest com-mon subsequence (LCS) in three conditions: scoreswithout any preprocessing (raw), scores after synonymreplacement (synonym), and scores by keyword com-parison (keyword).
For keyword comparison, verbsand nouns were presented as keywords after stemmingand removing stop words.available.
In the experiments, the LCS score be-tween word sequences is normalised by the lengthof the shorter sequence.Table 2 presents results for identifying se-quences of events in hand descriptions using theLCS similarity score.
Individual descriptionswere compared with the rest of descriptions in thesame category and the average score was calcu-lated.
Relatively low scores in the table indicatethe great variation in annotators?
attention on thesequence of events, or temporal information, in avideo.
Events described by one annotator may nothave been listed by another annotator.
The Newsvideos category resulted in the highest similarityscore, confirming the fact that videos in this cate-gory are highly structured.3.7 Video ClassificationTo demonstrate the application of this corpus withnatural language descriptions, a supervised docu-ment classification task is outlined.
Tf-idf scorecan express textual document features (Dumais etal., 1998).
Traditional tf-idf represents the rela-tion between term t and document d. It providesa measure of the importance of a term within aparticular document, calculated astfidf(t, d) = tf(t, d) ?
idf(d) (1)where the term frequency tf(t, d) is given bytf(t, d) =Nt,d?kNk,d(2)In the above equation Nt,d is the number of occur-rences of term t in document d, and the denomina-tor is the sum of the number of occurrences for allterms in document d, that is, the size of the docu-ment |d|.
Further the inverse document frequencyidf(d) isidf(d) = logNW (t)(3)where N is the total number of documents in thecorpus and W (t) is the total number of documentcontaining term t.A term-document matrix X is presented byT ?
D matrix tfidf(t, d).
In the experimentNaive Bayes probabilistic supervised learning al-gorithm was applied for classification using Wekamachine learning library (Hall et al, 2009).
Ten-fold cross validation was applied.
The perfor-mance was measured using precision, recall andF1-measure (Table 3).
F1-measure was low for?Grouping?
and ?Action?
videos, indicating thedifficulty in classifying these types of natural lan-guage descriptions.
Best classification resultswere achieved for ?Traffic?
and ?Indoor/Outdoor?scenes.
Absence of humans and their actionsmight have contributed obtaining the high clas-sification scores.
Human actions and activitieswere present in most videos in various categories,hence the ?Action?
category resulted in the low-est results.
?Grouping?
category also showed45precision recall F1-measureAction 0.701 0.417 0.523Close-up 0.861 0.703 0.774Grouping 0.453 0.696 0.549Indoor 0.846 0.915 0.879Meeting 0.723 0.732 0.727News 0.679 0.823 0.744Traffic 0.866 0.869 0.868average 0.753 0.739 0.736Table 3: Results for supervised classification using thetf-idf features.Figure 9: The average overlap similarity scores fortitles and for descriptions.
?uni?, ?bi?, and ?tri?
indi-cate the unigram, bigram, and trigram based similarityscores, respectively.
They were calculated without anypreprocessing such as stop word removal or synonymreplacement.weaker result; it was probably because process-ing for interaction between multiple people, withtheir overlapped actions, had not been fully devel-oped.
Overall classification results are encourag-ing which demonstrates that this dataset is a goodresource for evaluating natural language descrip-tion systems of short videos.3.8 Analysis of Title and DescriptionA title may be considered a very short form ofsummary.
We carried out further experiments tocalculate the similarity between a title and a de-scription manually created for a video.
The lengthof a title varied between two to five words.
Figure9 shows the average overlapping similarity scoresbetween titles and descriptions.
It can be ob-served that, in general, scores for titles were lowerthan those for descriptions, apart from ?News?and ?Meeting?
videos.
It was probably caused bythe short length of titles; by inspection we foundphrases such as ?news video?
and ?meeting scene?for these categories.Another experiment was performed for classi-fication of videos based on title information only.Figure 10 shows comparison of classification per-Figure 10: Video classification by titles, and by de-scriptions.formance with titles and with descriptions.
Wewere able to make correct classification in manyvideos with titles alone, although the performancewas slightly less for titles only than for descrip-tions.4 Conclusion and Future WorkThis paper presented our experiments using a cor-pus created for natural language description ofvideos.
For a small subset of TREC video data inseven categories, annotators produced titles, de-scriptions and selected high level features.
Thispaper aimed to characterise the corpus based onanalysis of hand annotations and a series of exper-iments for description similarity and video clas-sification.
In the future we plan to develop au-tomatic machine annotations for video sequencesand compare them against human authored anno-tations.
Further, we aim to annotate this corpusin multiple languages such as Arabic and Urduto generate a multilingual resource for video pro-cessing community.AcknowledgementsM U G Khan thanks University of Engineering &Technology, Lahore, Pakistan and R M A Nawabthanks COMSATS Institute of Information Tech-nology, Lahore, Pakistan for funding their workunder the Faculty Development Program.46ReferencesS.
Dumais, J. Platt, D. Heckerman, and M. Sahami.1998.
Inductive learning algorithms and represen-tations for text categorization.
In Proceedings ofthe seventh international conference on Informationand knowledge management, pages 148?155.
ACM.B.D.
Eugenio and M. Glass.
2004.
The kappa statis-tic: A second look.
Computational linguistics,30(1):95?101.R.
Fisher, J. Santos-Victor, and J. Crowley.
2005.Caviar: Context aware vision using image-based ac-tive recognition.G.
Griffin, A. Holub, and P. Perona.
2007.
Caltech-256 object category dataset.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The weka data min-ing software: an update.
ACM SIGKDD Explo-rations Newsletter, 11(1):10?18.C.
Jaynes, A. Kale, N. Sanders, and E. Gross-mann.
2005.
The terrascope dataset: A scriptedmulti-camera indoor video surveillance dataset withground-truth.
In Proceedings of the IEEE Workshopon VS PETS, volume 4.
Citeseer.Christopher Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.M.
Marszalek, I. Laptev, and C. Schmid.
2009.
Ac-tions in context.A.
Oliva and A. Torralba.
2009.
Mit outdoor scenedataset.P.
Over, A.F.
Smeaton, and P. Kelly.
2007.
Thetrecvid 2007 bbc rushes summarization evaluationpilot.
In Proceedings of the international work-shop on TRECVID video summarization, pages 1?15.
ACM.C.
Papageorgiou and T. Poggio.
1999.
A trainableobject detection system: Car detection in static im-ages.
Technical Report 1673, October.
(CBCLMemo 180).M.F.
Porter.
1993.
An algorithm for suffix stripping.Program: electronic library and information sys-tems, 14(3):130?137.A.
Quattoni and A. Torralba.
2009.
Recognizing in-door scenes.C.
Rashtchian, P. Young, M. Hodosh, and J. Hocken-maier.
2010.
Collecting image annotations usingamazon?s mechanical turk.
In Proceedings of theNAACL HLT 2010 Workshop on Creating Speechand Language Data with Amazon?s MechanicalTurk, pages 139?147.
Association for Computa-tional Linguistics.C.
Schuldt, I. Laptev, and B. Caputo.
2004.
Recog-nizing human actions: A local svm approach.
InPattern Recognition, 2004.
ICPR 2004.
Proceed-ings of the 17th International Conference on, vol-ume 3, pages 32?36.
IEEE.A.F.
Smeaton, P. Over, and W. Kraaij.
2009.
High-level feature detection from video in trecvid: a5-year retrospective of achievements.
MultimediaContent Analysis, pages 1?24.P.N.
Tan, M. Steinbach, V. Kumar, et al 2006.
Intro-duction to data mining.
Pearson Addison WesleyBoston.P.
Wittenburg, H. Brugman, A. Russel, A. Klassmann,and H. Sloetjes.
2006.
Elan: a professional frame-work for multimodality research.
In Proceedings ofLREC, volume 2006.
Citeseer.D.P.
Young and J.M.
Ferryman.
2005.
Pets metrics:On-line performance evaluation service.
In JointIEEE International Workshop on Visual Surveil-lance and Performance Evaluation of Tracking andSurveillance (VS-PETS), pages 317?324.47
