Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 225?228, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsJoint Parsing and Semantic Role LabelingCharles Sutton and Andrew McCallumDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003 USA{casutton,mccallum}@cs.umass.eduAbstractA striking feature of human syntactic pro-cessing is that it is context-dependent, thatis, it seems to take into account seman-tic information from the discourse con-text and world knowledge.
In this paper,we attempt to use this insight to bridgethe gap between SRL results from goldparses and from automatically-generatedparses.
To do this, we jointly performparsing and semantic role labeling, usinga probabilistic SRL system to rerank theresults of a probabilistic parser.
Our cur-rent results are negative, because a locally-trained SRL model can return inaccurateprobability estimates.1 IntroductionAlthough much effort has gone into developingstatistical parsing models and they have improvedsteadily over the years, in many applications thatuse parse trees errors made by the parser are a ma-jor source of errors in the final output.
A promisingapproach to this problem is to perform both pars-ing and the higher-level task in a single, joint prob-abilistic model.
This not only allows uncertaintyabout the parser output to be carried upward, suchas through an k-best list, but also allows informa-tion from higher-level processing to improve pars-ing.
For example, Miller et al (2000) showed thatperforming parsing and information extraction in ajoint model improves performance on both tasks.
Inparticular, one suspects that attachment decisions,which are both notoriously hard and extremely im-portant for semantic analysis, could benefit greatlyfrom input from higher-level semantic analysis.The recent interest in semantic role labeling pro-vides an opportunity to explore how higher-level se-mantic information can inform syntactic parsing.
Inprevious work, it has been shown that SRL systemsthat use full parse information perform better thanthose that use shallow parse information, but thatmachine-generated parses still perform much worsethan human-corrected gold parses.The goal of this investigation is to narrow the gapbetween SRL results from gold parses and from au-tomatic parses.
We aim to do this by jointly perform-ing parsing and semantic role labeling in a singleprobabilistic model.
In both parsing and SRL, state-of-the-art systems are probabilistic; therefore, theirpredictions can be combined in a principled way bymultiplying probabilities.
In this paper, we rerankthe k-best parse trees from a probabilistic parser us-ing an SRL system.
We compare two reranking ap-proaches, one that linearly weights the log proba-bilities, and the other that learns a reranker overparse trees and SRL frames in the manner of Collins(2000).Currently, neither method performs better thansimply selecting the top predicted parse tree.
Wediscuss some of the reasons for this; one reason be-ing that the ranking over parse trees induced by thesemantic role labeling score is unreliable, becausethe model is trained locally.2 Base SRL SystemOur approach to joint parsing and SRL begins witha base SRL system, which uses a standard architec-ture from the literature.
Our base SRL system is acascade of maximum-entropy classifiers which se-lect the semantic argument label for each constituentof a full parse tree.
As in other systems, we usethree stages: pruning, identification, and classifica-tion.
First, in pruning, we use a deterministic pre-processing procedure introduced by Xue and Palmer(2004) to prune many constituents which are almostcertainly not arguments.
Second, in identification,a binary MaxEnt classifier is used to prune remain-ing constituents which are predicted to be null with225Base features [GJ02]Path to predicateConstituent typeHead wordPositionPredicateHead POS [SHWA03]All conjunctions of aboveTable 1: Features used in base identification classi-fier.high probability.
Finally, in classification, a multi-class MaxEnt classifier is used to predict the argu-ment type of the remaining constituents.
This clas-sifer also has the option to output NULL.It can happen that the returned semantic argu-ments overlap, because the local classifiers take noglobal constraints into account.
This is undesirable,because no overlaps occur in the gold semantic an-notations.
We resolve overlaps using a simple recur-sive algorithm.
For each parent node that overlapswith one of its descendents, we check which pre-dicted probability is greater: that the parent has itslocally-predicted argument label and all its descen-dants are null, or that the descendants have their op-timal labeling, and the parent is null.
This algorithmreturns the non-overlapping assignment with glob-ally highest confidence.
Overlaps are uncommon,however; they occurred only 68 times on the 1346sentences in the development set.We train the classifiers on PropBank sections 02?21.
If a true semantic argument fails to matchany bracketing in the parse tree, then it is ignored.Both the identification and classification models aretrained using gold parse trees.
All of our features arestandard features for this task that have been usedin previous work, and are listed in Tables 1 and 2.We use the maximum-entropy implementation in theMallet toolkit (McCallum, 2002) with a Gaussianprior on parameters.3 Reranking Parse Trees Using SRLInformationHere we give the general framework for the rerank-ing methods that we present in the next section.
Wewrite a joint probability model over semantic framesF and parse trees t given a sentence x asp(F, t|x) = p(F |t,x)p(t|x), (1)where p(t|x) is given by a standard probabilisticparsing model, and p(F |t,x) is given by the base-line SRL model described previously.Base features [GJ02]Head wordConstituent typePositionPredicateVoiceHead POS [SHWA03]From [PWHMJ04]Parent Head POSFirst word / POSLast word / POSSibling constituent type / head word / head POSConjunctions [XP03]Voice & PositionPredicate & Head wordPredicate & Constituent typeTable 2: Features used in baseline labeling classifier.Parse Trees Used SRL F1Gold 77.11-best 63.9Reranked by gold parse F1 68.1Reranked by gold frame F1 74.2Simple SRL combination (?
= 0.5) 56.9Chosen using trained reranker 63.6Table 3: Comparison of Overall SRL F1 on devel-opment set by the type of parse trees used.In this paper, we choose (F ?, t?)
to approximatelymaximize the probability p(F, t|x) using a rerankingapproach.
To do the reranking, we generate a list ofk-best parse trees for a sentence, and for each pre-dicted tree, we predict the best frame using the baseSRL model.
This results in a list {(F i, ti)} of parsetree / SRL frame pairs, from which the rerankerchooses.
Thus, our different reranking methods varyonly in which parse tree is selected; given a parsetree, the frame is always chosen using the best pre-diction from the base model.The k-best list of parses is generated using DanBikel?s (2004) implementation of Michael Collins?parsing model.
The parser is trained on sections 2?21 of the WSJ Treebank, which does not overlapwith the development or test sets.
The k-best list isgenerated in Bikel?s implementation by essentiallyturning off dynamic programming and doing veryaggressive beam search.
We gather a maximum of500 best parses, but the limit is not usually reachedusing feasible beam widths.
The mean number ofparses per sentence is 176.4 Results and DiscussionIn this section we present results on several rerank-ing methods for joint parsing and semantic role la-226beling.
Table 3 compares F1 on the development setof our different reranking methods.
The first fourrows in Table 3 are baseline systems.
We presentbaselines using gold trees (row 1 in Table 3) andpredicted trees (row 2).
As shown in previous work,gold trees perform much better than predicted trees.We also report two cheating baselines to explorethe maximum possible performance of a rerankingsystem.
First, we report SRL performance of ceil-ing parse trees (row 3), i.e., if the parse tree from thek-best list is chosen to be closest to the gold tree.This is the best expected performance of a parsereranking approach that maximizes parse F1.
Sec-ond, we report SRL performance where the parsetree is selected to maximize SRL F1, computingusing the gold frame (row 4).
There is a signifi-cant gap both between parse-F1-reranked trees andSRL-F1-reranked trees, which shows promise forjoint reranking.
However, the gap between SRL-F1-reranked trees and gold parse trees indicates thatreranking of parse lists cannot by itself completelyclose the gap in SRL performance between gold andpredicted parse trees.4.1 Reranking based on score combinationEquation 1 suggests a straightforward method forreranking: simply pick the parse tree from the k-bestlist that maximizes p(F, t|x), in other words, add thelog probabilities from the parser and the base SRLsystem.
More generally, we consider weighting theindividual probabilities ass(F, t) = p(F |t,x)1??p(t|x)?.
(2)Such a weighted combination is often used in thespeech community to combine acoustic and lan-guage models.This reranking method performs poorly, however.No choice of ?
performs better than ?
= 1, i.e.,choosing the 1-best predicted parse tree.
Indeed, themore weight given to the SRL score, the worse thecombined system performs.
The problem is that of-ten a bad parse tree has many nodes which are obvi-ously not constituents: thus p(F |t,x) for such a badtree is very high, and therefore not reliable.
As moreweight is given to the SRL score, the unlabeled re-call drops, from 55% when ?
= 0 to 71% when?
= 1.
Most of the decrease in F1 is due to the dropin unlabeled recall.4.2 Training a reranker using global featuresOne potential solution to this problem is to addfeatures of the entire frame, for example, to voteagainst predicted frames that are missing key argu-ments.
But such features depend globally on the en-tire frame, and cannot be represented by local clas-sifiers.
One way to train these global features is tolearn a linear classifier that selects a parse / framepair from the ranked list, in the manner of Collins(2000).
Reranking has previously been applied tosemantic role labeling by Toutanova et al (2005),from which we use several features.
The differencebetween this paper and Toutanova et al is that in-stead of reranking k-best SRL frames of a singleparse tree, we are reranking 1-best SRL frames fromthe k-best parse trees.Because of the the computational expense oftraining on k-best parse tree lists for each of 30,000sentences, we train the reranker only on sections 15?18 of the Treebank (the same subset used in previ-ous CoNLL competitions).
We train the rerankerusing LogLoss, rather than the boosting loss usedby Collins.
We also restrict the reranker to consideronly the top 25 parse trees.This globally-trained reranker uses all of the fea-tures from the local model, and the following globalfeatures: (a) sequence features, i.e., the linear se-quence of argument labels in the sentence (e.g.A0_V_A1), (b) the log probability of the parse tree,(c) has-arg features, that is, for each argument typea binary feature indicating whether it appears in theframe, (d) the conjunction of the predicate and has-arg feature, and (e) the number of nodes in the treeclassified as each argument type.The results of this system on the development setare given in Table 3 (row 6).
Although this performsbetter than the score combination method, it is stillno better than simply taking the 1-best parse tree.This may be due to the limited training set we usedin the reranking model.
A base SRL model trainedonly on sections 15?18 has 61.26 F1, so in com-parison, reranking provides a modest improvement.This system is the one that we submitted as our offi-cial submission.
The results on the test sets are givenin Table 4.5 Summing over parse treesIn this section, we sketch a different approach tojoint SRL and parsing that does not use rerank-ing at all.
Maximizing over parse trees can meanthat poor parse trees can be selected if their se-mantic labeling has an erroneously high score.
Butwe are not actually interested in selecting a goodparse tree; all we want is a good semantic frame.This means that we should select the semantic frame227Precision Recall F?=1Development 64.43% 63.11% 63.76Test WSJ 68.57% 64.99% 66.73Test Brown 62.91% 54.85% 58.60Test WSJ+Brown 67.86% 63.63% 65.68Test WSJ Precision Recall F?=1Overall 68.57% 64.99% 66.73A0 69.47% 74.35% 71.83A1 66.90% 64.91% 65.89A2 64.42% 61.17% 62.75A3 62.14% 50.29% 55.59A4 72.73% 70.59% 71.64A5 50.00% 20.00% 28.57AM-ADV 55.90% 49.60% 52.57AM-CAU 76.60% 49.32% 60.00AM-DIR 57.89% 38.82% 46.48AM-DIS 79.73% 73.75% 76.62AM-EXT 66.67% 43.75% 52.83AM-LOC 50.26% 53.17% 51.67AM-MNR 54.32% 51.16% 52.69AM-MOD 98.50% 95.46% 96.96AM-NEG 98.20% 94.78% 96.46AM-PNC 46.08% 40.87% 43.32AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 72.15% 67.43% 69.71R-A0 0.00% 0.00% 0.00R-A1 0.00% 0.00% 0.00R-A2 0.00% 0.00% 0.00R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 0.00% 0.00% 0.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 0.00% 0.00% 0.00R-AM-MNR 0.00% 0.00% 0.00R-AM-TMP 0.00% 0.00% 0.00V 99.21% 86.24% 92.27Table 4: Overall results (top) and detailed results onthe WSJ test (bottom).that maximizes the posterior probability: p(F |x) =?t p(F |t,x)p(t|x).
That is, we should be sum-ming over the parse trees instead of maximizing overthem.
The practical advantage of this approach isthat even if one seemingly-good parse tree does nothave a constituent for a semantic argument, manyother parse trees in the k-best list might, and allare considered when computing F ?.
Also, no sin-gle parse tree need have constituents for all of F ?
;because it sums over all parse trees, it can mix andmatch constituents between different trees.
The op-timal frame F ?
can be computed by an O(N3) pars-ing algorithm if appropriate independence assump-tions are made on p(F |x).
This requires designingan SRL model that is independent of the bracketingderived from any particular parse tree.
Initial experi-ments performed poorly because the marginal modelp(F |x) was inadequate.
Detailed exploration is leftfor future work.6 Conclusion and Related WorkIn this paper, we have considered several methodsfor reranking parse trees using information from se-mantic role labeling.
So far, we have not beenable to show improvement over selecting the 1-bestparse tree.
Gildea and Jurafsky (Gildea and Jurafsky,2002) also report results on reranking parses usingan SRL system, with negative results.
In this paper,we confirm these results with a MaxEnt-trained SRLmodel, and we extend them to show that weightingthe probabilities does not help either.Our results with Collins-style reranking are toopreliminary to draw definite conclusions, but the po-tential improvement does not appear to be great.
Infuture work, we will explore the max-sum approach,which has promise to avoid the pitfalls of max-maxreranking approaches.AcknowledgementsThis work was supported in part by the Center for IntelligentInformation Retrieval, in part by National Science Foundationunder NSF grants #IIS-0326249 ond #IIS-0427594, and in partby the Defense Advanced Research Projec ts Agency (DARPA),through the Department of the Interior, NBC, Acquisition Ser-vices Division, under contract number NBCHD030010.
Anyopinions, findings and conclusions or recommendations ex-pressed in this material are the author(s) and do not necessarilyreflect those of the sponsor.ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics.Michael Collins.
2000.
Discriminative reranking for natu-ral language parsing.
In Proc.
17th International Conf.
onMachine Learning, pages 175?182.
Morgan Kaufmann, SanFrancisco, CA.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3):245?288.Andrew Kachites McCallum.
2002.
Mallet: A machine learn-ing for language toolkit.
http://mallet.cs.umass.edu.Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M.Weischedel.
2000.
A novel use of statistical parsing to ex-tract information from text.
In ANLP 2000, pages 226?233.Mihai Surdeanu, Sanda Harabagiu, John Williams, and PaulAarseth.
2003.
Using predicate-argument structures for in-formation extraction.
In ACL-2003.Kristina Toutanova, Aria Haghighi, and Christopher D. Man-ning.
2005.
Joint learning improves semantic role labeling.In ACL 2005.Nianwen Xue and Martha Palmer.
2004.
Calibrating featuresfor semantic role labeling.
In Proceedings of 2004 Confer-ence on Empirical Methods in Natural Language Process-ing.228
