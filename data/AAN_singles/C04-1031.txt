Word to word alignment strategiesJo?rg TiedemannDepartment of Linguistics and PhilologyUppsala UniversityUppsala/Swedenjoerg@stp.ling.uu.seAbstractWord alignment is a challenging task aim-ing at the identification of translational re-lations between words and multi-word unitsin parallel corpora.
Many alignment strate-gies are based on links between single words.Different strategies can be used to find theoptimal word alignment using such one-to-one word links including relations betweenmulti-word units.
In this paper seven algo-rithms are compared using a word alignmentapproach based on association clues andan English-Swedish bitext together with ahandcrafted reference alignment used forevaluation.1 IntroductionWord alignment is the task of identifying trans-lational relations between words in parallel cor-pora with the aim of re-using them in natu-ral language processing.
Typical applicationsthat make use of word alignment techniquesare machine translation and multi-lingual lex-icography.
Several approaches have been pro-posed for the automatic alignment of words andphrases using statistical techniques and align-ment heuristics, e.g.
(Brown et al, 1993; Vogelet al, 1996; Garc?
?a-Varea et al, 2002; Ahren-berg et al, 1998; Tiedemann, 1999; Tufis andBarbu, 2002; Melamed, 2000).
Word align-ment usually includes links between so-calledmulti-word units (MWUs) in cases where lex-ical items cannot be split into separated wordswith appropriate translations in another lan-guage.
See for example the alignment betweenan English sentence and a Swedish sentence il-lustrated in figure 1.
There are MWUs in bothlanguages aligned to corresponding translationsin the other language.
The Swedish compound?mittplatsen?
corresponds to three words in En-glish (?the middle seat?)
and the English verb?dislike?
is translated into a Swedish particleverb ?tycker om?
(English: like) that has beennegated using ?inte?.
Most approaches modelJag tar mittplatsen, vilket jag inte tycker om, men det g?r mig inte s?
mycket.I take the middle seat, which I dislike, but  I am not really put out.Figure 1: A word alignment example from SaulBellow ?To Jerusalem and back: a personal ac-count?
(Bellow, 1976) and its Swedish transla-tion (Bellow, 1977) (the Bellow corpus).word alignment as links between words in thesource language and words in the target lan-guage as indicated by the arrows in figure 1.However, in cases like the English expression?I am not really put out?
which corresponds tothe Swedish expression ?det go?r mig inte s?a my-cket?
there is no proper way of connecting singlewords with each other in order to express thisrelation.
In some approaches such relations areconstructed in form of an exhaustive set of linksbetween all word pairs included in both expres-sions (Melamed, 1998; Mihalcea and Pedersen,2003).
In other approaches complex expressionsare identified in a pre-processing step in orderto handle them as complex units in the samemanner as single words in alignment (Smadjaet al, 1996; Ahrenberg et al, 1998; Tiedemann,1999).The one-to-one word linking approach seemsto be very limited.
However, single word linkscan be combined in order to describe links be-tween multi-word units as illustrated in figure1.
In this paper we investigate different align-ment strategies using this approach1.
For thiswe apply clue alignment introduced in the nextsection.2 Word alignment with cluesThe clue alignment approach has been pre-sented in (Tiedemann, 2003).
Alignment cluesrepresent probabilistic indications of associa-1A similar study on statistical alignment models isincluded in (Och and Ney, 2003).tions between lexical items collected from dif-ferent sources.
Declarative clues can be takenfrom linguistic resources such as bilingual dic-tionaries.
They may also include pre-definedrelations between lexical items based on cer-tain features such as parts of speech.
Estimatedclues are derived from the parallel data using,for example, measures of co-occurrence (e.g.
theDice coefficient (Smadja et al, 1996)), statisti-cal alignment models (e.g.
IBM models fromstatistical machine translation (Brown et al,1993)), or string similarity measures (e.g.
thelongest common sub-sequence ratio (Melamed,1995)).
They can also be learned from pre-viously aligned training data using linguisticand contextual features associated with aligneditems.
Relations between certain word classeswith respect to the translational association ofwords belonging to these classes is one exampleof such clues that can be learned from alignedtraining data.
In our experiments, for example,we will use clues that indicate relations betweenlexical items based on their part-of-speech tagsand their positions in the sentence relative toeach other.
They are learned from automati-cally word-aligned training data.The clue alignment approach implements away of combining association indicators on aword-to-word level.
The combination of cluesresults in a two-dimensional clue matrix.
Thevalues in this matrix express the collected evi-dence of an association between word pairs inbitext segments taken from a parallel corpus.Word alignment is then the task of identifyingthe best links according to the associations indi-cated in the clue matrix.
Several strategies forsuch an alignment are discussed in the followingsection.3 Alignment strategiesA clue matrix summarizes information from var-ious sources that can be used for the identifica-tion of translation relations.
However, there isno obvious way to utilize this information forword alignment as we explicitly include multi-word units (MWUs) in our approach.
The cluematrix in figure 2 has been obtained for a bi-text segment from our English-Swedish test cor-pus (the Bellow corpus) using a set of weighteddeclarative and estimated clues.There are many ways of ?clustering?
wordstogether and there is no obvious maximizationprocedure for finding the alignment optimumwhen MWUs are involved.
The alignment pro-ingen visar sa?rskilt mycket t?alamodno 29 0 0 1 9one 16 2 1 1 13is 1 13 1 2 0very 0 2 18 17 1patient 2 1 4 12 6Figure 2: A clue matrix (all values in %).cedure depends very much on the definition ofan optimal alignment.
The best alignment forour example would probably be the set of thefollowing links:links ={no one ingenis patient visar t?alamodvery sa?rskilt mycket}A typical procedure for automatic word align-ment is to start with one-to-one word links.Links that have common source or target lan-guage words are called overlapping links.
Setsof overlapping links, which do not overlap withany other link outside the set, are called linkclusters (LC).
Aligning words one by one of-ten produces overlaps and in this way implic-itly creates aligned multi-word-units as part oflink clusters.
A general word-to-word align-ment L for a given bitext segment with Nsource language words (s1s2...sN ) and M tar-get language words (t1t2...tM ) can be formallydescribed as a set of links L = {L1, L2, ..., Lx}with Lx = [sx1 , tx2 ] , x1 ?
{1..N}, x2 ?
{1..M}.This general definition allows varying num-bers of links (0 ?
x ?
N ?
M) within possiblealignments L. It is not straightforward how tofind the optimal alignment as L may includedifferent numbers of links.3.1 Directional alignment modelsOne word-to-word alignment approach is to as-sume a directional word alignment model simi-lar to the models in statistical machine trans-lation.
The directional alignment model as-sumes that there is at most one link for eachsource language word.
Using alignment clues,this can be expressed as the following optimiza-tion problem: L?D = argmaxLD?Nn=1 C(LDn )where LD = {LD1 , LD2 , .., LDN} is a set of linksLDn =[sn, taDn]with aDn ?
{1..M} and C(LDn )is the combined clue value for the linked itemssn and taDn .
In other words, word alignmentis the search for the best link for each sourcelanguage word.
Directional models do not al-low multiple links from one item to several tar-get items.
However, target items can be linkedto multiple source language words as they canbe aligned to the same target language word.The direction of alignment can easily be re-versed, which leads to the inverse directionalalignment: L?I = argmaxLI?Mm=1 C(LIm) withlinks LIm =[saIm , tm]and aIm ?
{1..N}.
In theinverse directional alignment, source languagewords can be linked to multiple words but notthe other way around.
The following figure il-lustrates directional alignment models appliedto the example in figure 2:L?D =??????
?no ingenone ingenis visarvery sa?rskiltpatient mycket??????
?LCD =????
?no one ingenis visarvery sa?rskiltpatient mycket????
?Using the inverse directional alignment strategywe would obtain the following links:L?I =??????
?no ingenis visarvery sa?rskiltvery mycketone t?alamod??????
?LCI =????
?no ingenis visarvery sa?rskilt mycketone t?alamod????
?3.2 Combined directional alignmentDirectional link sets can be combined in severalways.
The union of link sets (L??
= L?D ?
L?I)usually causes many overlaps and, hence, verylarge link clusters.
On the other hand, an inter-section of link sets (L??
= L?D ?
L?I) removes alloverlaps and leaves only highly confident one-to-one word links behind.
Using the same examplefrom above we obtain the following alignments:L??
=??????????????
?no ingenone ingenone t?alamodis visarvery sa?rskiltvery mycketpatient mycket???????????????LC?
={no one ingen t?alamodis visarvery patient sa?rskilt mycket}The intersection of links produces the followingsets:L??
={no ingenis visarvery sa?rskilt}LC?
= L?
?The union and the intersection of links do notproduce satisfactory results as seen in the ex-ample.
Another alignment strategy is a refinedcombination of link sets (L?R = {L?D ?
L?I} ?
{LR1 , ..., LRr }) as suggested by (Och and Ney,2000b).
In this approach, the intersection oflinks is iteratively extended by additional linksLRr which pass one of the following two con-straints:?
A new link is accepted if both items in thelink are not yet algned.?
Mapped on a two-dimensional bitext space,the new link is either vertically or horizon-tally adjacent to an existing link and thenew link does not cause any link to be adja-cent to other links in both dimensions (hor-izontally and vertically).Applying this approach to the example, we get:L?R =??????????
?no ingenis visarvery sa?rskiltvery mycketone ingenpatient t?alamod??????????
?LCR =????
?no one ingenis visarvery sa?rskilt mycketpatient t?alamod????
?3.3 Competitive linkingAnother alignment approach is the compet-itive linking approach proposed by Melamed(Melamed, 1996).
In this approach, one as-sumes that there are only one-to-one wordlinks.
The alignment is done in a greedy ?best-first?
search manner where links with the high-est association scores are aligned first, and thealigned items are then immediately removedfrom the search space.
This process is re-peated until no more links can be found.
Inthis way, the optimal alignment (L?C) for non-overlapping one-to-one links is found.
The num-ber of possible links in an alignment is reducedto min(N,M).
Using competitive linking withour example we yield:L?C =??????
?no ingenvery sa?rskiltis visarone t?alamodpatient mycket??????
?LCC = L?C3.4 Constrained best-first alignmentAnother iterative alignment approach has beenproposed in (Tiedemann, 2003).
In this ap-proach, the link LBx = [sx1 , tx2 ] with the high-est score in the clue matrix C?
(sx1 , tx2) =maxsi,tj (C(si, tj)) is added to the set of linkclusters if it fulfills certain constraints.
The topscore is removed from the matrix (i.e.
set tozero) and the link search is repeated until nomore links can be found.
This is basically aconstrained best-first search.
Several constraintsare possible.
In (Tiedemann, 2003) an adja-cency check is suggested, i.e.
overlapping linksare accepted only if they are adjacent to otherlinks in one and only one existing link cluster.Non-overlapping links are always accepted (i.e.a non-overlapping link creates a new link clus-ter).
Other possible constraints are clue valuethresholds, thresholds for clue score differencesbetween adjacent links, or syntactic constraints(e.g.
that link clusters may not cross phraseboundaries).
Using a best-first search strategywith the adjacency constraint we obtain the fol-lowing alignment:L?B =??????????????
?no ingenvery sa?rskiltvery mycketone ingenis visarpatient mycketpatient t?alamod??????????????
?LCB ={no one ingenis visarvery patient sa?rskilt mycket t?alamod}3.5 SummaryNone of the alignment approaches describedabove produces the preferred reference align-ment in our example using the given clue ma-trix.
However, simple iterative procedures comevery close to the reference and produce ac-ceptable alignments even for multi-word units,which is promising for an automatic clue align-ment system.
Directional alignment models de-pend very much on the relation between thesource and the target language.
One direc-tion usually works better than the other, e.g.an alignment from English to Swedish is bet-ter than Swedish to English because in En-glish terms and concepts are often split intoseveral words whereas Swedish tends to con-tain many compositional compounds.
Symmet-ric approaches to word alignment are certainlymore appropriate for general alignment systemsthan directional ones.4 Evaluation methodologyWord alignment quality is usually measured interms of precision and recall.
Often, previouslycreated gold standards are used as reference datain order to simplify automatic tests of alignmentattempts.
Gold standards can be re-used for ad-ditional test runs which is important when ex-amining different parameter settings.
However,recall and precision derived from information re-trieval have to be adjusted for the task of wordalignment.
The main difficulty with these mea-sures in connection with word alignment ariseswith links between MWUs that cause partiallycorrect alignments.
It is not straightforwardhow to judge such links in order to computeprecision and recall.
In order to account forpartiality we use a slightly modified version ofthe partiality score Q proposed in (Ahrenberget al, 2000)2:Qprecisionx =|algxsrc ?
corrxsrc|+ |algxtrg ?
corrxtrg||algxsrc|+ |algxtrg|Qrecallx =|algxsrc ?
corrxsrc|+ |algxtrg ?
corrxtrg||corrxsrc|+ |corrxtrg|The set of algxsrc includes all source languagewords of all proposed links if at least one ofthem is partially correct with respect to the ref-erence link x from the gold standard.
Similarly,algxtrg refers to all the proposed target languagewords.
corrxsrc and corrxtrg refer to the sets ofsource and target language words in link x ofthe gold standard.
Using the partiality valueQ, we can define the recall and precision met-rics as follows:Rmwu =?Xx=1 Qrecallx|correct|, Pmwu =?Xx=1 Qprecisionx|aligned|A balanced F-score can be used to combineboth, precision and recall:Fmwu = (2 ?
Pmwu ?Rmwu)/(Pmwu +Rmwu).2Qx ?
0 for incorrect links for both, precision andrecall.65707580859060  65  70  75  80  85  90recallprecisiondifferent search strategiesdice+ppgizagiza+ppdice+ppgizagiza+ppF=80%F=70%competitiveunionintersectionrefinedbest-firstdirectionalinverseFigure 3: Different alignment search strategies.
Clue alignment settings: dice+pp, giza, andgiza+pp.
Alignment strategies: directional (LD), inverse directional (LI), union (L?
), intersec-tion (L?
), refined (LR), competitive linking (LC), and constrained best-first (LB).Alternative measures for the evaluation ofone-to-one word links have been proposed in(Och and Ney, 2000a; Och and Ney, 2003).However, these measures require completelyaligned bitext segments as reference data.
Ourgold standards include random samples fromthe corpus instead (Ahrenberg et al, 2000).Furthermore, we do not split MWU links as pro-posed by (Och and Ney, 2000a).
Therefore, themeasures proposed above are a natural choicefor our evaluations.5 ExperimentsSeveral alignment search strategies have beendiscussed in the previous sections.
Our cluealigner implements these strategies in order totest their impact on the alignment performance.In the experiments we used one of our English-Swedish bitext from the PLUG corpus (S?agvallHein, 2002), the novel ?To Jerusalem and back:A personal account?
by Saul Bellow.
This cor-pus is fairly small (about 170,000 words) andtherefore well suited for extensive studies ofalignment parameters.
For evaluation, a goldstandard of 468 manually aligned links is used(Merkel et al, 2002).
It includes 122 links withMWUs either on the source or on the targetside (= 26% of the gold standard).
109 linkscontain source language MWUs, 59 links targetlanguage MWUs, and 46 links MWUs in bothlanguages.
10 links are null links, i.e.
a linkof one word to an empty string.
Three differ-ent clue types are used for the alignment: theDice coefficient (dice), lexical translation proba-bilities derived from statistical translation mod-els (giza) using the GIZA++ toolbox (Och andNey, 2003), and, finally, POS/relative-word-position-clues learned from previous alignments(pp).
Alignment strategies are compared on thebasis of three different settings: dice+pp, giza,and giza+pp.
In figure 3, the alignment resultsare shown for the three clue settings using dif-ferent search strategies as discussed earlier.5.1 DiscussionFigure 3 illustrates the relation between pre-cision and recall when applying different algo-rithms.
As expected, the intersection of direc-tional alignment strategies yields the highestprecision at the expense of recall, which is gen-erally lower than for the other approaches.
Con-trary to the intersection, the union of directionallinks produces alignments with the highest re-call values but lower precision than all othersearch algorithms.
Too many (partially) incor-rect MWUs are included in the union of direc-tional links.
The intersection on the other handincludes only one-to-one word links that tendto be correct.
However, many links are missedin this strategy evident in the low recall val-giza+pp non-MWU MWU-links (122 in total) English MWU Swedish MWU bothP R correct partial P R P R P R P Rdirectional 82.66 88.73 19 77 59.23 67.10 58.04 69.84 68.93 70.16 68.85 77.52inverse 81.93 87.28 5 50 64.39 59.74 64.68 57.87 67.62 72.57 69.21 71.78union 80.13 91.62 21 88 55.61 73.24 55.57 73.29 63.40 81.74 65.50 84.25intersection 91.67 85.84 0 98 74.35 52.44 73.56 53.44 83.04 60.31 83.33 64.89competitive 88.44 88.44 0 105 64.66 58.59 66.11 59.71 72.13 67.94 77.66 73.23refined 84.61 88.15 28 78 65.91 72.61 65.53 72.70 74.37 80.56 75.86 83.03best-first 85.07 89.02 28 79 66.40 73.40 66.23 73.77 75.38 81.35 77.52 84.48Table 1: Evaluations of different link types for the setting giza+pp.ues.
Directional alignment strategies generallyyield lower F-values than other refined symmet-ric alignment strategies.
Their implementationis straightforward but the results are highly de-pendent on the language pair under consider-ation.
The differences between the two align-ment directions in our example are surprisinglyinconsistent.
Using the giza clues both align-ment results are very close in terms of preci-sion and recall whereas a larger difference canbe observed using the other two clue settingswhen applying different directional alignmentstrategies.
Competitive linking is somewhatin between the intersection approach and thetwo symmetric approaches, ?best-first?
and ?re-fined?.
This could also be expected as competi-tive linking only allows non-overlapping one-to-one word links.
The refined bi-directional align-ment approach and the constrained best-firstapproach are almost identical in our exampleswith a more or less balanced relation betweenprecision and recall.
One advantage of the best-first approach is the possibility of incorporatingdifferent constraints that suit the current task.The adjacency check is just one of the possi-ble constraints.
For example, syntactic criteriacould be applied in order to force linked itemsto be complete according to existing syntacticmarkup.
Non-contiguous elements could also beidentified using the same approach simply byremoving the adjacency constraint.
However,this seems to increase the noise significantly ac-cording to experiments not shown in this paper.Further investigations on optimizing alignmentconstraints for certain tasks have to be done inthe future.
Focusing on MWUs, the numbers intable 1 show a clear picture about the difficul-ties of all approaches to find correct MWU links.Symmetric alignment strategies like refined andbest-first produce in general the best results forMWU links.
However, the main portion of suchlinks is only partially correct even for these ap-proaches.
Using our partiality measure, theintersection of directional alignments still pro-duces the highest precision values when consid-ering MWU links only even though no MWUsare included in these alignments at all.
The bestresults among MWU links are achieved for theones including MWUs in both languages.
How-ever, these results are still significantly lowerthan for single-word links (non-MWU).6 ConclusionsAccording to our results different alignmentstrategies can be chosen to suit particularneeds.
Concluding from the experiments, re-strictive methods like the intersection of direc-tional alignments or competitive linking shouldbe chosen if results with high precision are re-quired (which are mostly found among one-to-one word links).
This is, for example, thecase in automatic extraction of bilingual lexi-cons where noise should be avoided as much aspossible.
A strong disadvantage of these ap-proaches is that they do not include MWUs atall.
Other strategies should be chosen for appli-cations, which require a comprehensive cover-age as, for example, machine translation.
Sym-metric approaches such as the refined combi-nation of directional alignments and the con-strained best-first alignment strategy yield thehighest overall performance.
They produce thebest balance between precision and recall andthe highest scores in terms of F-values.ReferencesLars Ahrenberg, Magnus Merkel, and MikaelAndersson.
1998.
A simple hybrid aligner forgenerating lexical correspondences in paralleltexts.
In Christian Boitet and Pete White-lock, editors, Proceedings of the 36th AnnualMeeting of the Association for ComputationalLinguistics and the 17th International Con-ference on Computational Linguistics, pages29?35, Montreal, Canada.Lars Ahrenberg, Magnus Merkel, Anna S?agvallHein, and Jo?rg Tiedemann.
2000.
Evaluationof word alignment systems.
In Proceedingsof the 2nd International Conference on Lan-guage Resources and Evaluation, volume III,pages 1255?1261, Athens, Greece.Saul Bellow.
1976.
From Jerusalem and back:a personal account.
The Viking Press, NewYork, USA.Saul Bellow.
1977.
Jerusalem tur och retur.Bonniers, Stockholm.
Translation of CajLundgren.Peter F. Brown, Stephen A. Della Pietra, Vin-cent J. Della Pietra, and Robert L. Mercer.1993.
The mathematics of statistcal machinetranslation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311, June.Ismael Garc?
?a-Varea, Franz Josef Och, Her-mann Ney, and Francisco Casacuberta.
2002.Improving alignment quality in statisticalmachine translation using context-dependentmaximum entropy models.
In Proceedings ofthe 19th International Conference on Compu-tational Linguistics, pages 1051?1054, Taipei,Taiwan, August.I.
Dan Melamed.
1995.
Automatic evaluationand uniform filter cascades for inducing n-best translation lexicons.
In David Yarovskyand Kenneth Church, editors, Proceedings ofthe 3rd Workshop on Very Large Corpora,pages 184?198, Boston, MA.
Association forComputational Linguistics.I.
Dan Melamed.
1996.
Automatic constructionof clean broad-coverage lexicons.
In Proceed-ings of the 2nd Conference the Association forMachine Translation in the Americas, pages125?134, Montreal, Canada.I.
Dan Melamed.
1998.
Annotation style guidefor the Blinker project, version 1.0.
IRCSTechnical Report 98-06, University of Penn-sylvania, Philadelphia, PA.I.
Dan Melamed.
2000.
Models of transla-tional equivalence among words.
Computa-tional Linguistics, 26(2):221?249, June.Magnus Merkel, Mikael Andersson, and LarsAhrenberg.
2002.
The PLUG link annotator- interactive construction of data from par-allel corpora.
In Lars Borin, editor, ParallelCorpora, Parallel Worlds.
Rodopi, Amster-dam, New York.
Proceedings of the Sympo-sium on Parallel Corpora, Department of Lin-guistics, Uppsala University, Sweden,1999.Rada Mihalcea and Ted Pedersen.
2003.
Anevaluation exercise for word alignment.
InWorkshop on Building and Using ParallelTexts: Data Driven Machine Translation andBeyond, pages 1?10, Edmonton, Canada,May.Franz-Josef Och and Hermann Ney.
2000a.A comparison of alignment models for sta-tistical machine translation.
In Proceed-ings of the 18th International Conference onComputational Linguistics, pages 1086?1090,Saarbru?cken, Germany, July.Franz Josef Och and Hermann Ney.
2000b.
Im-proved statistical alignment models.
In Proc.of the 38th Annual Meeting of the Associ-ation for Computational Linguistics, pages440?447.Franz Josef Och and Hermann Ney.
2003.
Asystematic comparison of various statisticalalignment models.
Computational Linguis-tics, 29(1):19?51.Anna S?agvall Hein.
2002.
The PLUG project:Parallel corpora in Linko?ping, Uppsala, andGo?teborg: Aims and achievements.
In LarsBorin, editor, Parallel Corpora, ParallelWorlds.
Rodopi, Amsterdam, New York.Proceedings of the Symposium on ParallelCorpora, Department of Linguistics, UppsalaUniversity, Sweden,1999.Frank A. Smadja, Kathleen R. McKeown, andVasileios Hatzivassiloglou.
1996.
Translatingcollocations for bilingual lexicons: A statis-tical approach.
Computational Linguistics,22(1), pages 1?38.Jo?rg Tiedemann.
1999.
Word alignment -step by step.
In Proceedings of the 12thNordic Conference on Computational Lin-guistics, pages 216?227, University of Trond-heim, Norway.Jo?rg Tiedemann.
2003.
Combining clues forword alignment.
In Proceedings of the 10thConference of the European Chapter of theAssociation for Computational Linguistics(EACL), pages 339?346, Budapest, Hungary,April.Dan Tufis and Ana-Maria Barbu.
2002.
Lexicaltoken alignment: Experiments, results andapplications.
In Proceedings from The 3rdInternational Conference on Language Re-sources and Evaluation, pages 458?465, LasPalmas, Spain.Stephan Vogel, Hermann Ney, and ChristophTillmann.
1996.
HMM-based word alignmentin statistical translation.
In Proceedings ofthe 16th International Confernece on Compu-tational Linguistics, pages 836?841, Copen-hagen, Denmark.
