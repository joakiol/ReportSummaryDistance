Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236?244,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsPreference Grammars: Softening Syntactic Constraintsto Improve Statistical Machine TranslationAshish Venugopal Andreas Zollmann Noah A. Smith Stephan VogelLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{ashishv,zollmann,nasmith,vogel}@cs.cmu.eduAbstractWe propose a novel probabilistic syn-choronous context-free grammar formalismfor statistical machine translation, in whichsyntactic nonterminal labels are representedas ?soft?
preferences rather than as ?hard?matching constraints.
This formalism allowsus to efficiently score unlabeled synchronousderivations without forgoing traditionalsyntactic constraints.
Using this score as afeature in a log-linear model, we are ableto approximate the selection of the mostlikely unlabeled derivation.
This helpsreduce fragmentation of probability acrossdifferently labeled derivations of the sametranslation.
It also allows the importance ofsyntactic preferences to be learned alongsideother features (e.g., the language model)and for particular labeling procedures.
Weshow improvements in translation quality onsmall and medium sized Chinese-to-Englishtranslation tasks.1 IntroductionProbabilistic synchronous context-free grammars(PSCFGs) define weighted production rules that areautomatically learned from parallel training data.
Asin classical CFGs, these rules make use of nontermi-nal symbols to generalize beyond lexical modelingof sentences.
In MT, this permits translation and re-ordering to be conditioned on more abstract notionsof context.
For example,VP?
ne VB1 pas # do not VB1represents the discontiguous translation of theFrench words ?ne?
and ?pas?
to ?do not?, in the con-text of the labeled nonterminal symbol ?VB?
(rep-resenting syntactic category ?verb?).
Translationwith PSCFGs is typically expressed as the problemof finding the maximum-weighted derivation consis-tent with the source sentence, where the scores aredefined (at least in part) by R-valued weights asso-ciated with the rules.
A PSCFG derivation is a syn-chronous parse tree.Defining the translation function as finding thebest derivation has the unfortunate side effect offorcing differently-derived versions of the same tar-get sentence to compete with each other.
In otherwords, the true score of each translation is ?frag-mented?
across many derivations, so that each trans-lation?s most probable derivation is the only one thatmatters.
The more Bayesian approach of findingthe most probable translation (integrating out thederivations) instantiates an NP-hard inference prob-lem even for simple word-based models (Knight,1999); for grammar-based translation it is knownas the consensus problem (Casacuberta and de laHiguera, 2000; Sima?an, 2002).With weights interpreted as probabilities, themaximum-weighted derivation is the maximum aposteriori (MAP) derivation:e?
?
argmaxemaxdp(e, d | f)where f is the source sentence, e ranges over tar-get sentences, and d ranges over PSCFG deriva-tions (synchronous trees).
This is often describedas an approximation to the most probable transla-tion, argmaxe?d p(e, d | f).
In this paper, wewill describe a technique that aims to find the mostprobable equivalence class of unlabeled derivations,rather than a single labeled derivation, reducing thefragmentation problem.
Solving this problem ex-actly is still an NP-hard consensus problem, but weprovide approximations that build on well-knownPSCFG decoding methods.
Our model falls some-where between PSCFGs that extract nonterminalsymbols from parse trees and treat them as part of236the derivation (Zollmann and Venugopal, 2006) andunlabeled hierarchical structures (Chiang, 2005); wetreat nonterminal labels as random variables chosenat each node, with each (unlabeled) rule express-ing ?preferences?
for particular nonterminal labels,learned from data.The paper is organized as follows.
In Section 2,we summarize the use of PSCFG grammars fortranslation.
We describe our model (Section 3).Section 4 explains the preference-related calcula-tions, and Section 5 addresses decoding.
Experi-mental results using preference grammars in a log-linear translation model are presented for two stan-dard Chinese-to-English tasks in Section 6.
We re-view related work (Section 7) and conclude.2 PSCFGs for Machine TranslationProbabilistic synchronous context-free grammars(PSCFGs) are defined by a source terminal set(source vocabulary) TS , a target terminal set (targetvocabulary) TT , a shared nonterminal set N and asetR of rules of the form: X ?
?
?, ?,w?
where?
X ?
N is a labeled nonterminal referred to as theleft-hand-side of the rule.?
?
?
(N ?
TS)?
is the source side of the rule.?
?
?
(N ?
TT )?
is the target side of the rule.?
w ?
[0,?)
is a nonnegative real-valued weightassigned to the rule.For visual clarity, we will use the # character to sep-arate the source side of the rule ?
from the targetside ?.
PSCFG rules also have an implied one-to-one mapping between nonterminal symbols in ?
andnonterminals symbols in ?.
Chiang (2005), Zoll-mann and Venugopal (2006) and Galley et al (2006)all use parameterizations of this PSCFG formalism1.Given a source sentence f and a PSCFG G, thetranslation task can be expressed similarly to mono-lingual parsing with a PCFG.
We aim to find themost likely derivation d of the input source sentenceand read off the English translation, identified bycomposing ?
from each rule used in the derivation.This search for the most likely translation under the1Galley et al (2006) rules are formally defined as tree trans-ducers but have equivalent PSCFG forms.MAP approximation can be defined as:e?
= tgt(argmaxd?D(G):src(d)=fp(d))(1)where tgt(d) is the target-side yield of a derivationd, and D(G) is the set of G?s derivations.
Using ann-gram language model to score derivations and rulelabels to constraint the rules that form derivations,we define p(d) as log-linear model in terms of therules r ?
R used in d as:p(d) = pLM(tgt(d))?0 ?
( m?i=1pi(d)?i)?psyn(d)?m+1/Z(~?
)pi(d) =?r?Rhi(r)freq(r;d) (2)psyn(d) ={ 1 if d respects label constraints0 otherwise (3)where ~?
= ?0 ?
?
?
?m+1 are weights that reflect therelative importance of features in the model.
Thefeatures include the n-gram language model (LM)score of the target yield sequence, a collection of mrule feature functions hi : R ?
R?0, and a ?syn-tax?
feature that (redundantly) requires every non-terminal token to be expanded by a rule with thatnonterminal on its left-hand side.
freq(r; d) denotesthe frequency of the rule r in the derivation d. Notethat ?m+1 can be effectively ignored when psyn isdefined as in Equation 3.
Z(~?)
is a normalizationconstant that does not need to be computed duringsearch under the argmax search criterion in Equa-tion 1.
Feature weights ~?
are trained discrimina-tively in concert with the language model weightto maximize the BLEU (Papineni et al, 2002) au-tomatic evaluation metric via Minimum Error RateTraining (MERT) (Och, 2003).We use the open-source PSCFG rule extractionframework and decoder from Zollmann et al (2008)as the framework for our experiments.
The asymp-totic runtime of this decoder is:O(|f |3[|N ||TT |2(n?1)]K) (4)where K is the maximum number of nonterminalsymbols per rule, |f | the source sentence length, and237n is the order of the n-gram LM that is used to com-pute pLM.
This constant factor in Equation 4 arisesfrom the dynamic programming item structure usedto perform search under this model.
Using notationfrom Chiang (2007), the corresponding item struc-ture is:[X, i, j, q(?)]
: w (5)whereX is the nonterminal label of a derivation, i, jdefine a span in the source sentence, and q(?)
main-tains state required to compute pLM(?).
Under theMAP criterion we can discard derivations of lowerweight that share this item structure, but in practicewe often require additional lossy pruning to limit thenumber of items produced.
The Syntax-AugmentedMT model of Zollmann and Venugopal (2006), forinstance, produces a very large nonterminal set us-ing ?slash?
(NP/NN?
the great) and ?plus?
labels(NP+VB ?
she went) to assign syntactically mo-tivated labels for rules whose target words do notcorrespond to constituents in phrase structure parsetrees.
These labels lead to fragmentation of prob-ability across many derivations for the same targetsentence, worsening the impact of the MAP approx-imation.
In this work we address the increased frag-mentation resulting from rules with labeled nonter-minals compared to unlabeled rules (Chiang, 2005).3 Preference GrammarsWe extend the PSCFG formalism to include soft ?la-bel preferences?
for unlabeled rules that correspondto alternative labelings that have been encounteredin training data for the unlabeled rule form.
Thesepreferences, estimated via relative frequency countsfrom rule occurrence data, are used to estimate thefeature psyn(d), the probability that an unlabeledderivation can be generated under traditional syn-tactic constraints.
In classic PSCFG, psyn(d) en-forces a hard syntactic constraint (Equation 3).
Inour approach, label preferences influence the valueof psyn(d).3.1 Motivating exampleConsider the following labeled Chinese-to-EnglishPSCFG rules:(4) S ?
(??
VB1 #a place where I can VB1(3) S ?
(??
VP1 #a place where I can VP1(2) SBAR ?
(??
VP1 #a place where I can VP1(1) FRAG ?
(??
AUX1 #a place where I can AUX1(8) VB ?
m # eat(1) VP ?
m # eat(1) NP ?
m # eat(10) NN ?
m # dishwhere the numbers are frequencies of the rule fromthe training corpus.
In classical PSCFG we can thinkof the nonterminals mentioned in the rules as hardconstraints on which rules can be used to expand aparticular node; e.g., a VP can only be expanded bya VP rule.
In Equation 2, psyn(d) explicitly enforcesthis hard constraint.
Instead, we propose softeningthese constraints.
In the rules below, labels are rep-resented as soft preferences.
(10) X ?
(??
X1 #a place where I can X1??
?p(H0 = S, H1 = VB | r) = 0.4p(H0 = S, H1 = VP | r) = 0.3p(H0 = SBAR, H1 = VP | r) = 0.2p(H0 = FRAG, H1 = AUX | r) = 0.1???
(10) X ?
m # eat{ p(H0 = VB | r) = 0.8p(H0 = VP | r) = 0.1p(H0 = NP | r) = 0.1}(10) X ?
m # dish{ p(H0 = NN | r) = 1.0 }Each unlabeled form of the rule has an associateddistribution over labels for the nonterminals refer-enced in the rule; the labels are random variablesHi, with H0 the left-hand-side label.
These un-labeled rule forms are simply packed representa-tions of the original labeled PSCFG rules.
In ad-dition to the usual features hi(r) for each rule, esti-mated based on unlabeled rule frequencies, we now238have label preference distributions.
These are esti-mated as relative frequencies from the labelings ofthe base, unlabeled rule.
Our primary contributionis how we compute psyn(d)?the probability thatan unlabeled derivation adheres to traditional syn-tactic constraints?for derivations built from prefer-ence grammar rules.
By using psyn(d) as a featurein the log-linear model, we allow the MERT frame-work to evaluate the importance of syntactic struc-ture relative to other features.The example rules above highlight the potentialfor psyn(d) to affect the choice of translation.
Thetranslation of the Chinese word sequence  ( ??
m can be performed by expanding the non-terminal in the rule ?a place where I can X1?
witheither ?eat?
or ?dish.?
A hierarchical system (Chi-ang, 2005) would allow either expansion, relying onfeatures like pLM to select the best translation sinceboth expansions occurred the same number of timesin the data.A richly-labeled PSCFG as in Zollmann andVenugopal (2006) would immediately reject the rulegenerating ?dish?
due to hard label matching con-straints, but would produce three identical, compet-ing derivations.
Two of these derivations would pro-duce S as a root symbol, while one derivation wouldproduce SBAR.
The two S-labeled derivations com-pete, rather than reinforce the choice of the word?eat,?
which they both make.
They will also com-pete for consideration by any decoder that prunesderivations to keep runtime down.The rule preferences indicate that VB and VP areboth valid labels for the rule translating to ?eat?, andboth of these labels are compatible with the argu-ments expected by ?a place where I can X1?.
Al-ternatively, ?dish?
produces a NN label which isnot compatible with the arguments of this higher-up rule.
We design psyn(d) to reflect compatibilitybetween two rules (one expanding a right-hand sidenonterminal in the other), based on label preferencedistributions.3.2 Formal definitionProbabilistic synchronous context-free preferencegrammars are defined as PSCFGs with the follow-ing additional elements:?
H: a set of implicit labels, not to be confusedwith the explicit label set N .?
pi: H ?
N , a function that associates each im-plicit label with a single explicit label.
We cantherefore think ofH symbols as refinements ofthe nonterminals inN (Matsusaki et al, 2005).?
For each rule r, we define a probability distri-bution over vectors ~h of implicit label bindingsfor its nonterminals, denoted ppref(~h | r).
~hincludes bindings for the left-hand side nonter-minal (h0) as well as each right-hand side non-terminal (h1, ..., h|~h|).
Each hi ?
H.When N ,H are defined to include just a singlegeneric symbol as in (Chiang, 2005), we produce theunlabeled grammar discussed above.
In this work,we define?
N = {S,X}?
H = {NP,DT,NN ?
?
? }
= NSAMTwhere N corresponds to the generic labels of Chi-ang (2005) and H corresponds to the syntacticallymotivated SAMT labels from (Zollmann and Venu-gopal, 2006), and pi maps all elements of H toX .
We will use hargs(r) to denote the set of all~h = ?h0, h1, ..., hk?
?
Hk+1 that are valid bindingsfor the rule with nonzero preference probability.The preference distributions ppref from each ruleused in d are used to compute psyn(d) as describednext.4 Computing feature psyn(d)Let us view a derivation d as a collection of nonter-minal tokens nj , j ?
{1, ..., |d|}.
Each nj takes anexplicit label in N .
The score psyn(d) is a product,with one factor per nj in the derivation d:psyn(d) =|d|?j=1?j (6)Each ?j factor considers the two rules that nj partic-ipates in.
We will refer to the rule above nonterminaltoken nj as rj (the nonterminal is a child in this rule)and the rule that expands nonterminal token j as rj .The intuition is that derivations in which thesetwo rules agree (at each j) about the implicit label239for nj , in H are preferable to derivations in whichthey do not.
Rather than making a decision aboutthe implicit label, we want to reward psyn when rjand rj are consistent.
Our way of measuring thisconsistency is an inner product of preference distri-butions:?j ?
?h?Hppref(h | rj)ppref(h | rj) (7)This is not quite the whole story, because ppref(?
| r)is defined as a joint distribution of all the implicitlabels within a rule; the implicit labels are not in-dependent of each other.
Indeed, we want the im-plicit labels within each rule to be mutually consis-tent, i.e., to correspond to one of the rule?s preferredlabelings, for both hargs(r) and hargs(r).Our approach to calculating psyn within the dy-namic programming algorithm is to recursively cal-culate preferences for each chart item based on (a)the smaller items used to construct the item and(b) the rule that permits combination of the smalleritems into the larger one.
We describe how the pref-erences for chart items are calculated.
Let a chartitem be denoted [X, i, j, u, ...] where X ?
N and iand j are positions in the source sentence, andu : {h ?
H | pi(h) = X} ?
[0, 1](where ?h u(h) = 1) denotes a distribution overpossible X-refinement labels.
We will refer to itbelow as the left-hand-side preference distribution.Additional information (such as language modelstate) may also be included; it is not relevant here.The simplest case is for a nonterminal token njthat has no nonterminal children.
Here the left-hand-side preference distribution is simply given byu(h) = ppref(h | rj) .and we define the psyn factor to be ?j = 1.Now consider the dynamic programming stepof combining an already-built item [X, i, j, u, ...]rooted by explicit nonterminal X , spanning sourcesentence positions i to j, with left-hand-side prefer-ence distribution u, to build a larger item rooted byY through a rule r = Y ?
??X1?
?, ?X1?
?, w?withpreferences ppref(?
| r).2 The new item will have2We assume for the discussion that ?, ??
?
T ?S and ?, ??
?signature [Y, i ?
|?|, j + |?
?|, v, ...].
The left-hand-side preferences v for the new item are calculated asfollows:v(h) = v?(h)?h?
v?(h?
)where (8)v?
(h) = ?h??H:?h,h??
?hargs(r)ppref(?h, h??
| r)?
u(h?
)Renormalizing keeps the preference vectors on thesame scale as those in the rules.
The psyn factor ?,which is factored into the value of the new item, iscalculated as:?
= ?h??H:?h,h???hargs(r)u(h?)
(9)so that the value considered for the new item is w ??
?
..., where factors relating to pLM, for example,may also be included.
Coming back to our example,if we let r be the leaf rule producing ?eat?
at sharednonterminal n1, we generate an item with:u = ?u(VB) = 0.8, u(VP) = 0.1, u(NP) = 0.1?
?1 = 1Combining this item with X ?
?
 ( ?
?
X1# a place where I can X1 ?
as r2 at nonterminal n2generates a new target item with translation ?a placewhere I can eat?, ?2 = 0.9 and v as calculated inFig.
1.
In contrast, ?2 = 0 for the derivation wherer is the leaf rule that produces ?dish?.This calculation can be seen as a kind of single-pass, bottom-up message passing inference methodembedded within the usual dynamic programmingsearch.5 Decoding ApproximationsAs defined above, accurately computing psyn(d) re-quires extending the chart item structure with u. Formodels that use the n-gram LM feature, the itemstructure would be:[X, i, j, q(?
), u] : w (10)Since u effectively summarizes the choice of rulesin a derivation, this extension would partition theT ?T .
If there are multiple nonterminals on the right-hand sideof the rule, we sum over the longer sequences in hargs(r) andinclude appropriate values from the additional ?child?
items?preference vectors in the product.240v?
(S) = ppref (?h = S, h?
= VB?
| r)u(VB) + ppref (?h = S, h?
= VP?
| r)u(VP) = (0.4?
0.8) + (0.3?
0.1) = 0.35v?
(SBAR) = p(?h = SBAR, h?
= VP?
| r)u(VP) = (0.2?
0.1) = 0.02v = ?v(S) = 0.35/(v?
(S) + v?
(SBAR)), v(SBAR) = 0.02/v?
(S) + v?(SBAR)?
= ?v(S) = 0.35/0.37, v(SBAR) = 0.02/0.37?
?2 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9Figure 1: Calculating v and ?2 for the running example.search space further.
To prevent this partitioning, wefollow the approach of Venugopal et al (2007).
Wekeep track of u for the best performing derivationfrom the set of derivations that share [X, i, j, q(?
)]in a first-pass decoding.
In a second top-down passsimilar to Huang and Chiang (2007), we can recal-culate psyn(d) for alternative derivations in the hy-pergraph; potentially correcting search errors madein the first pass.We face another significant practical challengeduring decoding.
In real data conditions, the sizeof the preference vector for a single rule can be veryhigh, especially for rules that include multiple non-terminal symbols that are located on the left andright boundaries of ?.
For example, the Chinese-to-English rule X ?
?
X1 ?
X2 # X1 ?s X2 ?
hasover 24K elements in hargs(r) when learned for themedium-sized NIST task used below.
In order tolimit the explosive growth of nonterminals duringdecoding for both memory and runtime reasons, wedefine the following label pruning parameters:?
?R: This parameter limits the size of hargs(r) tothe ?R top-scoring preferences, defaulting othervalues to zero.?
?L: This parameter is the same as ?R but appliedonly to rules with no nonterminals.
The stricter of?L and ?R is applied if both thresholds apply.?
?P : This parameter limits the number labels initem preference vectors (Equation 8) to the ?Pmost likely labels during decoding, defaultingother preferences to zero.6 Empirical ResultsWe evaluate our preference grammar model onsmall (IWSLT) and medium (NIST) data Chinese-to-English translation tasks (described in Table 1).IWSLT is a limited domain, limited resource task(Paul, 2006), while NIST is a broadcast news taskwith wide genre and domain coverage.
We use asubset of the full training data (67M words of En-glish text) from the annual NIST MT Evaluation.Development corpora are used to train model pa-rameters via MERT.
We use a variant of MERT thatprefers sparse solutions where ?i = 0 for as manyfeatures as possible.
At each MERT iteration, a sub-set of features ?
are assigned 0 weight and optimiza-tion is repeated.
If the resulting BLEU score is notlower, these features are left at zero.All systems are built on the SAMT frameworkdescribed in Zollmann et al (2008), using a tri-gram LM during search and the full-order LM dur-ing a second hypergraph rescoring pass.
Reorder-ing limits are set to 10 words for all systems.
Prun-ing parameters during decoding limit the number ofderivations at each source span to 300.The system ?Hier.?
uses a grammar with a singlenonterminal label as in Chiang (2005).
The system?Syntax?
applies the grammar from Zollmann andVenugopal (2006) that generates a large number ofsyntactically motivated nonterminal labels.
For theNIST task, rare rules are discarded based on theirfrequency in the training data.
Purely lexical rules(that include no terminal symbols) that occur lessthan 2 times, or non-lexical rules that occur less than4 times are discarded.IWSLT task: We evaluate the preference gram-mar system ?Pref.?
with parameters ?R = 100,?L = 5, ?P = 2.
Results comparing systems Pref.to Hier.
and Syntax are shown in Table 2.
Auto-matic evaluation results using the preference gram-mar translation model are positive.
The preferencegrammar system shows improvements over both theHier.
and Syntax based systems on both unseen eval-uation sets IWSLT 2007 and 2008.
The improve-ments are clearest on the BLEU metric (matchingthe MERT training criteria).
On 2007 test data,Pref.
shows a 1.2-point improvement over Hier.,while on the 2008 data, there is a 0.6-point improve-ment.
For the IWSLT task, we report additional au-241System Name Words in Target Text LM singleton 1-n-grams (n) Dev.
TestIWSLT 632K 431K (5) IWSLT06 IWSLT07,08NIST 67M 102M (4) MT05 MT06Table 1: Training data configurations used to evaluate preference grammars.
The number of words in the target textand the number of singleton 1-n-grams represented in the complete model are the defining statistics that characterizethe scale of each task.
For each LM we also indicate the order of the n-gram model.System DevBLEU(lpen) ?2007BLEU(lpen) ?2008BLEU(lpen) ?2008WER ?2008 PER?2008MET.
?2008GTM ?Hier.
28.0(0.89)37.0(0.89)45.9(0.91)44.5 39.9 61.8 70.7Syntax 30.9(0.96)35.5(0.94)45.3(0.95)45.7 40.4 62.1 71.5Pref.
28.3(0.88)38.2(0.90)46.3(0.91)43.8 40.0 61.7 71.2Table 2: Translation quality metrics on the IWSLT translation task, with IWSLT 2006 as the development corpora, andIWSLT 2007 and 2008 as test corpora.
Each metric is annotated with an ?
if increases in the metric value correspondto increase in translation quality and a ?
if the opposite is true.
We also list length penalties for the BLEU metric toshow that improvements are not due to length optimizations alone.tomatic evaluation metrics that generally rank thePref.
system higher than Hier.
and Syntax.
As a fur-ther confirmation, our feature selection based MERTchooses to retain ?m+1 in the model.
While theIWSLT results are promising, we perform a morecomplete evaluation on the NIST translation task.NIST task: This task generates much larger rulepreference vectors than the IWSLT task simply dueto the size of the training corpora.
We build sys-tems with both ?R = 100, 10 varying ?P .
Vary-ing ?P isolates the relative impact of propagatingalternative nonterminal labels within the preferencegrammar model.
?L = 5 for all NIST systems.
Pa-rameters ?
are trained via MERT on the ?R = 100,?L = 5, ?P = 2 system.
BLEU scores for eachpreference grammar and baseline system are shownin Table 3, along with translation times on the testcorpus.
We also report length penalties to show thatimprovements are not simply due to better tuning ofoutput length.The preference grammar systems outperform theHier.
baseline by 0.5 points on development data,and upto 0.8 points on unseen test data.
While sys-tems with ?R = 100 take significantly longer totranslate the test data than Hier., setting ?R = 10takes approximately as long as the Syntax based sys-tem but produces better slightly better results (0.3points).The improvements in translation quality with thepreference grammar are encouraging, but how muchof this improvement can simply be attributed toMERT finding a better local optimum for parame-ters ??
To answer this question, we use parameters??
optimized by MERT for the preference grammarsystem to run a purely hierarchical system, denotedHier.(??
), which ignores the value of ?m+1 duringdecoding.
While almost half of the improvementcomes from better parameters learned via MERT forthe preference grammar systems, 0.5 points can bestill be attributed purely to the feature psyn.
In addi-tion, MERT does not set parameter ?m+1 to 0, cor-roborating the value of the psyn feature again.
Notethat Hier.(??)
achieves better scores than the Hier.system which was trained via MERT without psyn.This highlights the local nature of MERT parametersearch, but also points to the possibility that train-ing with the feature psyn produced a more diversederivation space, resulting in better parameters ?.We see a very small improvement (0.1 point) by al-lowing the runtime propagation of more than 1 non-terminal label in the left-hand side posterior distribu-tion, but the improvement doesn?t extend to ?P = 5.Improved integration of the feature psyn(d) into de-coding might help to widen this gap.242TestDev.
Test timeSystem BLEU (lpen) BLEU (lpen) (h:mm)Baseline SystemsHier.
34.1 (0.99) 31.8 (0.95) 0:12Syntax 34.7 (0.99) 32.3 (0.95) 0:45Hier.(??)
- 32.1 (0.95) 0:12Preference Grammar: ?R = 100?P = 1 - 32.5 (0.96) 3:00?P = 2 34.6 (0.99) 32.6 (0.95) 3:00?P = 5 - 32.5 (0.95) 3:20Preference Grammar: ?R = 10?P = 1 - 32.5 (0.95) 1:03?P = 2 - 32.6 (0.95) 1:10?P = 5 - 32.5 (0.95) 1:10Table 3: Translation quality and test set translation time(using 50 machines with 2 tasks per machine) measuredby the BLEU metric for the NIST task.
NIST 2006 isused as the development (Dev.)
corpus and NIST 2007 isused as the unseen evaluation corpus (Test).
Dev.
scoresare reported for systems that have been separately MERTtrained, Pref.
systems share parameters from a singleMERT training.
Systems are described in the text.7 Related WorkThere have been significant efforts in the both themonolingual parsing and machine translation liter-ature to address the impact of the MAP approxi-mation and the choice of labels in their respectivemodels; we survey the work most closely related toour approach.
May and Knight (2006) extract n-best lists containing unique translations rather thanunique derivations, while Kumar and Byrne (2004)use the Minimum Bayes Risk decision rule to se-lect the lowest risk (highest BLEU score) translationrather than derivation from an n-best list.
Trombleet al (2008) extend this work to lattice structures.All of these approaches only marginalize over alter-native candidate derivations generated by a MAP-driven decoding process.
More recently, work byBlunsom et al (2007) propose a purely discrimina-tive model whose decoding step approximates theselection of the most likely translation via beamsearch.
Matsusaki et al (2005) and Petrov et al(2006) propose automatically learning annotationsthat add information to categories to improve mono-lingual parsing quality.
Since the parsing task re-quires selecting the most non-annotated tree, the an-notations add an additional level of structure thatmust be marginalized during search.
They demon-strate improvements in parse quality only when avariational approximation is used to select the mostlikely unannotated tree rather than simply strippingannotations from the MAP annotated tree.
In ourwork, we focused on approximating the selection ofthe most likely unlabeled derivation during search,rather than as a post-processing operation; the meth-ods described above might improve this approxima-tion, at some computational expense.8 Conclusions and Future WorkWe have proposed a novel grammar formalism thatreplaces hard syntactic constraints with ?soft?
pref-erences.
These preferences are used to compute amachine translation feature (psyn(d)) that scores un-labeled derivations, taking into account traditionalsyntactic constraints.
Representing syntactic con-straints as a feature allows MERT to train the cor-responding weight for this feature relative to othersin the model, allowing systems to learn the relativeimportance of labels for particular resource and lan-guage scenarios as well as for alternative approachesto labeling PSCFG rules.This approach takes a step toward addressingthe fragmentation problems of decoding based onmaximum-weighted derivations, by summing thecontributions of compatible label configurationsrather than forcing them to compete.
We have sug-gested an efficient technique to approximate psyn(d)that takes advantage of a natural factoring of deriva-tion scores.
Our approach results in improvementsin translation quality on small and medium resourcetranslation tasks.
In future work we plan to focus onmethods to improve on the integration of the psyn(d)feature during decoding and techniques that allow usconsider more of the search space through less prun-ing.AcknowledgementsWe appreciate helpful comments from three anony-mous reviewers.
Venugopal and Zollmann were sup-ported by a Google Research Award.
Smith was sup-ported by NSF grant IIS-0836431.243ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2007.A discriminative latent variable model for statisticalmachine translation.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics (ACL).Francisco Casacuberta and Colin de la Higuera.
2000.Computational complexity of problems on probabilis-tic grammars and transducers.
In Proc.
of the 5th Inter-national Colloquium on Grammatical Inference: Al-gorithms and Applications.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe Annual Meeting of the Association for Compua-tional Linguistics (ACL).David Chiang.
2007.
Hierarchical phrase based transla-tion.
Computational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inferences and training ofcontext-rich syntax translation models.
In Proceed-ings of the Annual Meeting of the Association for Com-puational Linguistics (ACL), Sydney, Australia.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the Annual Meeting of the Associationfor Compuational Linguistics (ACL).Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, Squibs and Discussion.Shankar Kumar and William Byrne.
2004.
Min-imum Bayes-risk decoding for statistical machinetranslation.
In Proceedings of the Human Lan-guage Technology and North American Association forComputational Linguistics Conference (HLT/NAACL),Boston,MA, May 27-June 1.Takuya Matsusaki, Yusuke Miyao, and Junichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL).Jonathan May and Kevin Knight.
2006.
A better N-bestlist: Practical determinization of weighted finite treeautomata.
In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational LinguisticsConference (HLT/NAACL).Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of theAnnual Meeting of the Association for CompuationalLinguistics (ACL).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of theAnnual Meeting of the Association for CompuationalLinguistics (ACL).Michael Paul.
2006.
Overview of the IWSLT 2006 eval-uation campaign.
In Proceedings of the InternationalWorkshop on Spoken Language Translation (IWSLT).Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the AnnualMeeting of the Association for Compuational Linguis-tics (ACL).Khalil Sima?an.
2002.
Computational complexity ofprobabilistic disambiguation.
Grammars, 5(2):125?151.Roy Tromble, Shankar Kumar, Franz Och, and WolfgangMacherey.
2008.
Lattice minimum Bayes-risk decod-ing for statistical machine translation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Ashish Venugopal, Andreas Zollmann, and Stephan Vo-gel.
2007.
An efficient two-pass approach toSynchronous-CFG driven statistical MT.
In Proceed-ings of the Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistics Conference (HLT/NAACL).Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, HLT/NAACL, New York, June.Andreas Zollmann, Ashish Venugopal, Franz J. Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalMT.
In Proceedings of the Conference on Computa-tional Linguistics (COLING).244
