Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 236?246,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsIntrinsic Subspace Evaluation of Word Embedding RepresentationsYadollah Yaghoobzadeh and Hinrich Sch?utzeCenter for Information and Language ProcessingUniversity of Munich, Germanyyadollah@cis.lmu.deAbstractWe introduce a new methodology for in-trinsic evaluation of word representations.Specifically, we identify four fundamen-tal criteria based on the characteristics ofnatural language that pose difficulties toNLP systems; and develop tests that di-rectly show whether or not representationscontain the subspaces necessary to satisfythese criteria.
Current intrinsic evalua-tions are mostly based on the overall simi-larity or full-space similarity of words andthus view vector representations as points.We show the limits of these point-basedintrinsic evaluations.
We apply our evalu-ation methodology to the comparison of acount vector model and several neural net-work models and demonstrate importantproperties of these models.1 IntroductionDistributional word representations or embeddingsare currently an active area of research in nat-ural language processing (NLP).
The motivationfor embeddings is that knowledge about words ishelpful in NLP.
Representing words as vocabularyindexes may be a good approach if large train-ing sets allow us to learn everything we need toknow about a word to solve a particular task; butin most cases it helps to have a representation thatcontains distributional information and allows in-ferences like: ?above?
and ?below?
have similarsyntactic behavior or ?engine?
and ?motor?
havesimilar meaning.Several methods have been introduced to assessthe quality of word embeddings.
We distinguishtwo different types of evaluation in this paper: (i)extrinsic evaluation evaluates embeddings in anNLP application or task and (ii) intrinsic evalu-ation tests the quality of representations indepen-dent of a specific NLP task.Each single word is a combination of a largenumber of morphological, lexical, syntactic, se-mantic, discourse and other features.
Its em-bedding should accurately and consistently repre-sent these features, and ideally a good evaluationmethod must clarify this and give a way to analyzethe results.
The goal of this paper is to build suchan evaluation.Extrinsic evaluation is a valid methodology, butit does not allow us to understand the propertiesof representations without further analysis; e.g., ifan evaluation shows that embedding A works bet-ter than embedding B on a task, then that is not ananalysis of the causes of the improvement.
There-fore, extrinsic evaluations do not satisfy our goals.Intrinsic evaluation analyzes the generic qualityof embeddings.
Currently, this evaluation mostlyis done by testing overall distance/similarity ofwords in the embedding space, i.e., it is basedon viewing word representations as points andthen computing full-space similarity.
The assump-tion is that the high dimensional space is smoothand similar words are close to each other.
Sev-eral datasets have been developed for this purpose,mostly the result of human judgement; see (Baroniet al, 2014) for an overview.
We refer to theseevaluations as point-based and as full-space be-cause they consider embeddings as points in thespace ?
sub-similarities in subspaces are generallyignored.Point-based intrinsic evaluation computes ascore based on the full-space similarity of twowords: a single number that generally does notsay anything about the underlying reasons for alower or higher value of full-space similarity.
Thismakes it hard to interpret the results of point-basedevaluation and may be the reason that contradic-tory results have been published; e.g., based on236point-based evaluation, some papers have claimedthat count-based representations perform as wellas learning-based representations (Levy and Gold-berg, 2014a).
Others have claimed the opposite(e.g., Mikolov et al (2013), Pennington et al(2014), Baroni et al (2014)).Given the limits of current evaluations, we pro-pose a new methodology for intrinsic evaluationof embeddings by identifying generic fundamen-tal criteria for embedding models that are impor-tant for representing features of words accuratelyand consistently.
We develop corpus-based testsusing supervised classification that directly showwhether the representations contain the informa-tion necessary to meet the criteria or not.
Thefine-grained corpus-based supervision makes thesub-similarities of words important by looking atthe subspaces of word embeddings relevant to thecriteria, and this enables us to give direct insightsinto properties of representation models.2 Related WorkBaroni et al (2014) evaluate embeddings on dif-ferent intrinsic tests: similarity, analogy, synonymdetection, categorization and selectional prefer-ence.
Schnabel et al (2015) introduce tasks withmore fine-grained datasets.
These tasks are unsu-pervised and generally based on cosine similarity;this means that only the overall direction of vec-tors is considered or, equivalently, that words aremodeled as points in a space and only their full-space distance/closeness is considered.
In con-trast, we test embeddings in a classification set-ting and different subspaces of embeddings are an-alyzed.
Tsvetkov et al (2015) evaluate embed-dings based on their correlations with WordNet-based linguistic embeddings.
However, correla-tion does not directly evaluate how accurately andcompletely an application can extract a particularpiece of information from an embedding.Extrinsic evaluations are also common (cf.
(Liand Jurafsky, 2015; K?ohn, 2015; Lai et al, 2015)).Li and Jurafsky (2015) conclude that embed-ding evaluation must go beyond human-judgementtasks like similarity and analogy.
They suggest toevaluate on NLP tasks.
K?ohn (2015) gives similarsuggestions and also recommends the use of su-pervised methods for evaluation.
Lai et al (2015)evaluate embeddings in different tasks with differ-ent setups and show the contradictory results ofembedding models on different tasks.
Idiosyn-crasies of different downstream tasks can affectextrinsic evaluations and result in contradictions.3 Criteria for word representationsEach word is a combination of different proper-ties.
Depending on the language, these propertiesinclude lexical, syntactic, semantic, world knowl-edge and other features.
We call these propertiesfacets.
The ultimate goal is to learn representa-tions for words that accurately and consistentlycontain these facets.
Take the facet gender (GEN)as an example.
We call a representation 100% ac-curate for GEN if information it contains aboutGEN is always accurate; we call the representation100% consistent for GEN if the representation ofevery word that has a GEN facet contains this in-formation.We now introduce four important criteria that arepresentation must satisfy to represent facets ac-curately and consistently.
These criteria are ap-plied across different problems that NLP applica-tions face in the effective use of embeddings.Nonconflation.
A word embedding must keepthe evidence from different local contexts sepa-rate ?
?do not conflate?
?
because each contextcan infer specific facets of the word.
Embeddingsfor different word forms with the same stem, likeplural and singular forms or different verb tenses,are examples vulnerable to conflation because theyoccur in similar contexts.Robustness against sparseness.
One aspect ofnatural language that poses great difficulty for sta-tistical modeling is sparseness.
Rare words arecommon in natural language and embedding mod-els must learn useful representations based on asmall number of contexts.Robustness against ambiguity.
Another cen-tral problem when processing words in NLP islexical ambiguity (Cruse, 1986; Zhong and Ng,2010).
Polysemy and homonymy of words canmake it difficult for a statistical approach to gen-eralize and infer well.
Embeddings should fullyrepresent all senses of an ambiguous word.
Thiscriterion becomes more difficult to satisfy as dis-tributions of senses become more skewed, but arobust model must be able to overcome this.Accurate and consistent representation ofmultifacetedness.
This criterion addresses set-tings with large numbers of facets.
It is basedon the following linguistic phenomenon, a phe-nomenon that occurs frequently crosslinguistically237(Comrie, 1989).
(i) Words have a large numberof facets, including phonetic, morphological, syn-tactic, semantic and topical properties.
(ii) Eachfacet by itself constitutes a small part of the over-all information that a representation should cap-ture about a word.4 Experimental setup and resultsWe now design experiments to directly evaluateembeddings on the four criteria.
We proceed asfollows.
First, we design a probabilistic contextfree grammar (PCFG) that generates a corpus thatis a manifestation of the underlying phenomenon.Then we train our embedding models on the cor-pus.
The embeddings obtained are then evaluatedin a classification setting, in which we apply a lin-ear SVM (Fan et al, 2008) to classify embeddings.Finally, we compare the classification results fordifferent embedding models and analyze and sum-marize them.Selecting embedding models.
Since this paperis about developing a new evaluation methodol-ogy, the choice of models is not important as longas the models can serve to show that the proposedmethodology reveals interesting differences withrespect to the criteria.On the highest level, we can distinguish twotypes of distributional representations.
Countvectors (Sahlgren, 2006; Baroni and Lenci,2010; Turney and Pantel, 2010) live in a high-dimensional vector space in which each dimen-sion roughly corresponds to a (weighted) countof cooccurrence in a large corpus.
Learned vec-tors are learned from large corpora using machinelearning methods: unsupervised methods such asLSI (e.g., Deerwester et al (1990), Levy andGoldberg (2014b)) and supervised methods suchas neural networks (e.g., Mikolov et al (2013))and regression (e.g., Pennington et al (2014)).
Be-cause of the recent popularity of learning-basedmethods, we consider one count-based and fivelearning-based distributional representation mod-els.The learning-based models are: (i) vLBL(henceforth: LBL) (vectorized log-bilinear lan-guage model) (Mnih and Kavukcuoglu, 2013),(ii) SkipGram (henceforth: SKIP) (skipgram bag-of-word model), (iii) CBOW (continuous bag-of-word model (Mikolov et al, 2013), (iv) Struc-tured SkipGram (henceforth SSKIP), (Ling et al,2015) and CWindow (henceforth CWIN) (contin-1 P (aV b|S) = 1/42 P (bV a|S) = 1/43 P (aWa|S) = 1/84 P (aWb|S) = 1/85 P (bWa|S) = 1/86 P (bWb|S) = 1/87 P (vi|V ) = 1/5 0 ?
i ?
48 P (wi|W ) = 1/5 0 ?
i ?
4Figure 1: Global conflation grammar.
Words vioccur in a subset of the contexts of words wi, butthe global count vector signatures are the same.uous window model) (Ling et al, 2015).
Thesemodels learn word embeddings for input and tar-get spaces using neural network models.For a given context, represented by the inputspace representations of the left and right neigh-bors ~vi?1and ~vi+1, LBL, CBOW and CWIN pre-dict the target space ~viby combining the contexts.LBL combines ~vi?1and ~vi+1linearly with posi-tion dependent weights and CBOW (resp.
CWIN)combines them by adding (resp.
concatenation).SKIP and SSKIP predict the context words vi?1or vi+1given the input space ~vi.
For SSKIP, con-text words are in different spaces depending ontheir position to the input word.
In summary,CBOW and SKIP are learning embeddings usingbag-of-word (BoW) models, but the other three,CWIN, SSKIP and LBL, are using position depen-dent models.
We use word2vec1for SKIP andCBOW, wang2vec2for SSKIP and CWIN, andLai et al (2015)?s implementation3for LBL.The count-based model is position-sensitivePPMI, Levy and Goldberg (2014a)?s explicit vec-tor space representation model.4For a vocabularyof size V , the representation ~w of w is a vectorof size 4V , consisting of four parts correspondingto the relative positions r ?
{?2,?1, 1, 2} withrespect to occurrences of w in the corpus.
Theentry for dimension word v in the part of ~w cor-responding to relative position r is the PPMI (pos-itive pointwise mutual information) weight of wand v for that relative position.
The four parts ofthe vector are length normalized.
In this paper, weuse only two relative positions: r ?
{?1, 1}, soeach ~w has two parts, corresponding to immediateleft and right neighbors.1code.google.com/archive/p/word2vec2github.com/wlin12/wang2vec3github.com/licstar/compare4bitbucket.org/omerlevy/hyperwords2384.1 NonconflationGrammar.
The PCFG grammar shown in Fig-ure 1 generates viwords that occur in two typesof contexts: a-b (line 1) and b-a (line 2); and wiwords that also occur in these two contexts (lines4 and 5), but in addition occur in a-a (line 3) andb-b (line 6) contexts.
As a result, the set of con-texts in which viand wioccur is different, but ifwe simply count the number of occurrences in thecontexts, then viand wicannot be distinguished.Dataset.
We generated a corpus of 100,000 sen-tences.
Words that can occur in a-a and b-b con-texts constitute the positive class, all other wordsthe negative class.
The words v3, v4, w3, w4wereassigned to the test set, all other words to the train-ing set.Results.
We learn representations of words byour six models and train one SVM per model; ittakes a word representation as input and outputs+1 (word can occur in a-a/b-b) or -1 (it cannot).The SVMs trained on PPMI and CBOW repre-sentations assigned all four test set words to thenegative class; in particular, w3and w4were in-correctly classified.
Thus, the accuracy of clas-sification for these models (50%) was not betterthan random.
The SVMs trained on LBL, SSKIP,SSKIP and CWIN representations assigned allfour test set words to the correct class: v3and v4were assigned to the negative class and w3and w4were assigned to the positive class.Discussion.
The property of embedding mod-els that is relevant here is that PPMI is an aggre-gation model, which means it calculates aggregatestatistics for each word and then computes the fi-nal word embedding from these aggregate statis-tics.
In contrast, all our learning-based models areiterative models: they iterate over the corpus andeach local context of a word is used as a traininginstance for learning its embedding.For iterative models, it is common to use com-position of words in the context, as in LBL,CBOW and CWIN.
Non-compositional iterativemodels like SKIP and SSKIP are also popular.Aggregation models can also use composite fea-tures from context words, but these features aretoo sparse to be useful.
The reason that the modelof Agirre et al (2009) is rarely used is precisely itsinability to deal with sparseness.
All widely useddistributional models employ individual word oc-currences as basic features.The bad PPMI results are explained by the fact1 P (AV B|S) = 1/22 P (CWD|S) = 1/23 P (ai|A) = 1/10 0 ?
i ?
94 P (bi|B) = 1/10 0 ?
i ?
95 P (ci|C) = 1/10 0 ?
i ?
96 P (di|D) = 1/10 0 ?
i ?
97 P (vi|V ) = 1/10 0 ?
i ?
98 P (wi|W ) = 1/10 0 ?
i ?
99 L?= L(S)10 ?
{aiuibi|0 ?
i ?
9}11 ?
{cixidi|0 ?
i ?
9}Figure 2: In language L?, frequent viand rare uioccur in a-b contexts; frequent wiand rare xioc-cur in c-d contexts.
Word representations shouldencode possible contexts (a-b vs. c-d) for both fre-quent and rare words.that it is an aggregation model: the PPMI modelcannot distinguish two words with the same globalstatistics ?
as is the case for, say, v3and w3.
Thebad result of CBOW is probably connected to itsweak (addition) composition of context, althoughit is an iterative compositional model.
Simple rep-resentation of context words with iterative updat-ing (through backpropagation in each training in-stance), can influence the embeddings in a waythat SKIP and SSKIP get good results, althoughthey are non-compositional.As an example of conflation occurring in theEnglish Wikipedia, consider this simple example.We replace all single digits by ?7?
in tokenization.We learn PPMI embeddings for the tokens and seethat among the one hundred nearest neighbors of?7?
are the days of the week, e.g., ?Friday?.
As anexample of a conflated feature consider the word?falls?
occurring immediately to the right of thetarget word.
The weekdays as well as single dig-its often have the immediate right neighbor ?falls?in contexts like ?Friday falls on a public holiday?and ?2 out of 3 falls match?
?
tokenized as ?7 outof 7 falls match?
?
in World Wrestling Entertain-ment (WWE).
The left contexts of ?Friday?
and?7?
are different in these contexts, but the PPMImodel does not record this information in a waythat would make the link to ?falls?
clear.4.2 Robustness against sparsenessGrammar.
The grammar shown in Figure 2 gen-erates frequent viand rare uiin a-b contexts (lines1 and 9); and frequent wiand rare xiin c-d con-texts (lines 2 and 10).
The language generated bythe PCFG on lines 1?8 is merged on lines 9?11with the ten contexts a0u0b0.
.
.
a9u9b9(line 9)239and the ten contexts c0x0d0.
.
.
c9x9d9(line 10);that is, each of the uiand xioccurs exactly oncein the merged language L?, thus modeling the phe-nomenon of sparseness.Dataset.
We generated a corpus of 100,000 sen-tences using the PCFG (lines 1?8) and added the20 rare sentences (lines 9?11).
We label all wordsthat can occur in c-d contexts as positive and allother words as negative.
The singleton words uiand xiwere assigned to the test set, all other wordsto the training set.Results.
After learning embeddings with differ-ent models, the SVM trained on PPMI representa-tions assigned all twenty test words to the negativeclass.
This is the correct decision for the ten ui(since they cannot occur in a c-d context), but theincorrect decision for the xi(since they can occurin a c-d context).
Thus, the accuracy of classifica-tion was 50% and not better than random.
TheSVMs trained on learning-based representationsclassified all twenty test words correctly.Discussion.
Representations of rare words inthe PPMI model are sparse.
The PPMI represen-tations of the uiand xionly contain two nonzeroentries, one entry for an aior ci(left context) andone entry for a bior di(right context).
Given thissparseness, it is not surprising that representationsare not a good basis for generalization and PPMIaccuracy is random.In contrast, learning-based models learn that theai, bi, ciand diform four different distributionalclasses.
The final embeddings of the aiafter learn-ing is completed are all close to each other andthe same is true for the other three classes.
Oncethe similarity of two words in the same distribu-tional class (say, the similarity of a5and a7) hasbeen learned, the contexts for the ui(resp.
xi) lookessentially the same to embedding models as thecontexts of the vi(resp.wi).
Thus, the embeddingslearned for the uiwill be similar to those learnedfor the vi.
This explains why learning-based repre-sentations achieve perfect classification accuracy.This sparseness experiment highlights an im-portant difference between count vectors andlearned vectors.
Count vector models are lessrobust in the face of sparseness and noise be-cause they base their representations on individ-ual contexts; the overall corpus distribution isonly weakly taken into account, by way of PPMIweighting.
In contrast, learned vector modelsmake much better use of the overall corpus distri-1 P (AV1B|S) =10/202 P (CW1D|S)=9/203 P (CW2D|S)=?
?1/204 P (AW2B|S) =(1?
?
)?1/205 P (ai|A) =1/10 0 ?
i ?
96 P (bi|B) =1/10 0 ?
i ?
97 P (ci|C) =1/10 0 ?
i ?
98 P (di|D) =1/10 0 ?
i ?
99 P (vi|V1) =1/50 0 ?
i ?
4910 P (wi|W1) =1/45 5 ?
i ?
4911 P (wi|W2) =1/5 0 ?
i ?
4Figure 3: Ambiguity grammar.
viand w5.
.
.
w49occur in a-b and c-d contexts only, respectively.w0.
.
.
w4are ambiguous and occur in both con-texts.bution and they can leverage second-order effectsfor learning improved representations.
In our ex-ample, the second order effect is that the modelfirst learns representations for the ai, bi, ciand diand then uses these as a basis for inferring the sim-ilarity of uito viand of xito wi.4.3 Robustness against ambiguityGrammar.
The grammar in Figure 3 generatestwo types of contexts that we interpret as two dif-ferent meanings: a-b contexts (lines 1,4) and c-dcontexts (lines 2, 3).
vioccur only in a-b contexts(line 1), w5.
.
.
w49occur only in c-d contexts (line2); thus, they are unambiguous.
w0.
.
.
w4are am-biguous and occur with probability ?
in c-d con-texts (line 3) and with probability (1 ?
?)
in a-bcontexts (lines 3, 4).
The parameter ?
controls theskewedness of the sense distribution; e.g., the twosenses are equiprobable for ?
= 0.5 and the sec-ond sense (line 4) is three times as probable as thefirst sense (line 3) for ?
= 0.25.Dataset.
The grammar specified in Figure 3was used to generate a training corpus of 100,000sentences.
Label criterion: A word is labeled posi-tive if it can occur in a c-d context, as negative oth-erwise.
The test set consists of the five ambiguouswords w0.
.
.
w4.
All other words are assigned tothe training set.Linear SVMs were trained for the binary clas-sification task on the train set.
50 trials of thisexperiment were run for each of eleven values of?
: ?
= 2?
?where ?
?
{1.0, 1.1, 1.2, .
.
.
, 2.0}.Thus, for the smallest value of ?, ?
= 1.0, the twosenses have the same frequency; for the largestvalue of ?, ?
= 2.0, the dominant sense is threetimes as frequent as the less frequent sense.Results.
Figure 4 shows accuracy of the classi-2401.0 1.2 1.4 1.6 1.8 2.00.00.20.40.60.81.0alphaaccuracypmilblcbowskipcwinsskipFigure 4: SVM classification results for the am-biguity dataset.
X-axis: ?
= ?
log2?.
Y-axis:classification accuracy:fication on the test set: the proportion of correctlyclassified words out of a total of 250 (five wordseach in 50 trials).All models perform well for balanced sense fre-quencies; e.g., for ?
= 1.0, ?
= 0.5, the SVMswere all close to 100% accurate in predicting thatthe wican occur in a c-d context.
PPMI accuracyfalls steeply when ?
is increased from 1.4 to 1.5.
Ithas a 100% error rate for ?
?
1.5.
Learning-basedmodels perform better in the order CBOW (leastrobust), LBL, SSKIP, SKIP, CWIN (most robust).Even for ?
= 2.0, CWIN and SKIP are still closeto 100% accurate.Discussion.
The evaluation criterion we haveused here is a classification task.
The classifier at-tempts to answer a question that may occur in anapplication ?
can this word be used in this con-text?
Thus, the evaluation criterion is: does theword representation contain a specific type of in-formation that is needed for the application.Another approach to ambiguity is to computemultiple representations for a word, one for eachsense.
We generally do not yet know what thesense of a word is when we want to use itsword representation, so data-driven approacheslike clustering have been used to create represen-tations for different usage clusters of words thatmay capture some of its senses.
For example,Reisinger and Mooney (2010) and Huang et al(2012) cluster the contexts of each word and thenlearn a different representation for each cluster.The main motivation for this approach is the as-sumption that single-word distributional represen-tations cannot represent all senses of a word well(Huang et al, 2012).
However, Li and Jurafsky(2015) show that simply increasing the dimension-1 P (NFn|S) =1/42 P (AFa|S) =1/43 P (NMn|S) =1/44 P (AMf|S) =1/45 P (ni|N) =1/5 0 ?
i ?
46 P (ai|A) =1/5 0 ?
i ?
47 P (xnfiUnfi|Fn) =1/5 0 ?
i ?
48 P (f |Unfi) =1/29 P (?
(Unfi)|Unfi) =1/210 P (xafiUafi|Fa) =1/5 0 ?
i ?
411 P (f |Uafi) =1/212 P (?
(Uafi)|Uafi) =1/213 P (xnmiUnmi|Mn) =1/5 0 ?
i ?
414 P (m|Unmi) =1/215 P (?
(Unmi)|Unmi)=1/216 P (xamiUami|Mf) =1/5 0 ?
i ?
417 P (m|Uami) =1/218 P (?
(Uami)|Uami) =1/2Figure 5: This grammar generates nouns (xn.i) andadjectives (xa.i) with masculine (x.mi) and feminine(x.fi) gender as well as paradigm features ui.
?maps each U to one of {u0.
.
.
u4}.
?
is randomlyinitialized and then kept fixed.ality of single-representation gets comparable re-sults to using multiple-representation.
Our resultsconfirm that a single embedding can be robustagainst ambiguity, but also show the main chal-lenge: skewness of sense distribution.4.4 Accurate and consistent representation ofmultifacetednessGrammar.
The grammar shown in Figure 5 mod-els two syntactic categories, nouns and adjectives,whose left context is highly predictable: it is oneof five left context words ni(resp.
ai) for nouns,see lines 1, 3, 5 (resp.
for adjectives, see lines 2, 4,6).
There are two grammatical genders: feminine(corresponding to the two symbols Fnand Fa)and masculine (corresponding to the two symbolsMnand Ma).
The four combinations of syntac-tic category and gender are equally probable (lines1?4).
In addition to gender, nouns and adjec-tives are distinguished with respect to morpholog-ical paradigm.
Line 7 generates one of five fem-inine nouns (xnfi) and the corresponding paradigmmarkerUnfi.
A noun has two equally probable rightcontexts: a context indicating its gender (line 8)and a context indicating its paradigm (line 9).
?is a function that maps each U to one of five mor-phological paradigms {u0.
.
.
u4}.
?
is randomlyinitialized before a corpus is generated and keptfixed.The function ?
models the assignment of241paradigms to nouns and adjectives.
Nounsand adjectives can have different (or the same)paradigms, but for a given noun or adjective theparadigm is fixed and does not change.
Lines 7?9 generate gender and paradigm markers for fem-inine nouns, for which we use the symbols xnfi.Lines 10?18 cover the three other cases: mas-culine nouns (xnmi), feminine adjectives (xafi) andmasculine adjectives (xami).Dataset.
We perform 10 trials.
In each trial,?
is initialized randomly and a corpus of 100,000sentences is generated.
The train set consists ofthe feminine nouns (xnfi, line 7) and the masculinenouns (xnmi, line 13).
The test set consists of thefeminine (xafi) and masculine (xami) adjectives.Results.
Embeddings have been learned, SVMsare trained on the binary classification task femi-nine vs. masculine and evaluated on test.
Therewas not a single error: accuracy of classificationsis 100% for all embedding models.Discussion.
The facet gender is indicated di-rectly by the distribution and easy to learn.
Fora noun or adjective x, we simply have to checkwhether f or m occurs to its right anywhere in thecorpus.
PPMI stores this information in two di-mensions of the vectors and the SVM learns thisfact perfectly.
The encoding of ?f or m occurs tothe right?
is less direct in the learning-based rep-resentation of x, but the experiment demonstratesthat they also reliably encode it and the SVM reli-ably picks it up.It would be possible to encode the facet in justone bit in a manually designed representation.While all representations are less compact than aone-bit representation ?
PPMI uses two real di-mensions, learning-based models use an activationpattern over several dimensions ?
it is still true thatmost of the capacity of the embeddings is used forencoding facets other than gender: syntactic cat-egories and paradigms.
Note that there are fivedifferent instances each of feminine/masculine ad-jectives, feminine/masculine nouns and uiwords,but only two gender indicators: f and m. Thisis a typical scenario across languages: words aredistinguished on a large number of morphological,grammatical, semantic and other dimensions andeach of these dimensions corresponds to a smallfraction of the overall knowledge we have about agiven word.Point-based tests do not directly evaluate spe-cific facets of words.
In similarity datasets,there is no individual test on facets ?
only full-space similarity is considered.
There are testcases in analogy that hypothetically evaluate spe-cific facets like gender of words, as in king-man+woman=queen.
However, it does not con-sider the impact of other facets and assumes theonly difference of ?king?
and ?queen?
is gen-der.
A clear example that words usually differ onmany facets, not just one, is the analogy: Lon-don:England ?
Ankara:Turkey.
political-capital-of applies to both, cultural-capital-of only to Lon-don:England since Istanbul is the cultural capitalof Turkey.To make our argument more clear, we designedan additional experiment that tries to evaluate gen-der in our dataset based on similarity and anal-ogy methods.
In the similarity evaluation, wesearch for the nearest neighbor of each word andaccuracy is the proportion of nearest neighborsthat have the same gender as the search word.In the analogy evaluation, we randomly selecttriples of the form <xc1g1i,xc1g2j,xc2g2k> where(c1, c2) ?
{(noun, adjective), (adjective, noun)}and (g1, g2) ?
{(masculine, feminine), (feminine,masculine) }.
We then compute ~s = ~xc1g1i?~xc1g2j+ ~xc2g2kand identify the word whose vec-tor is closest to ~s where the three vectors ~xc1g1i,~xc1g2j, ~xc2g2kare excluded.
If the nearest neighborof ~s is of type ~xc2g1l, then the search is successful;e.g., for ~s = ~xnfi?
~xnmj+ ~xamk, the search is suc-cessful if the nearest neighbor is feminine.
We didthis evaluation on the same test set for PPMI andLBL embedding models.
Error rates were 29% forPPMI and 25% for LBL (similarity) and 16% forPPMI and 14% for LBL (analogy).
This high er-ror, compared to 0% error for SVM classification,indicates it is not possible to determine the pres-ence of a low entropy facet accurately and consis-tently when full-space similarity and analogy areused as test criteria.5 AnalysisIn this section, we first summarize and analyze thelessons we learned through experiments in Sec-tion 4.
After that, we show how these lessons aresupported by a real natural-language corpus.5.1 Learned lessons(i) Two words with clearly different context dis-tributions should receive different representations.Aggregation models fail to do so by calculating242all entities head entities tail entitiesMLP 1NN MLP 1NN MLP 1NNPPMI 61.6 44.0 69.2 63.8 43.0 28.5LBL 63.5 51.7 72.7 66.4 44.1 32.8CBOW 63.0 53.5 71.7 69.4 39.1 29.9CWIN 66.1 53.0 73.5 68.6 46.8 31.4SKIP 64.5 57.1 69.9 71.5 49.8 34.0SSKIP 66.2 52.8 73.9 68.5 45.5 31.4Table 1: Entity typing results using embeddingslearned with different models.global statistics.
(ii) Embedding learning can have different ef-fectiveness for sparse vs. non-sparse events.
Thus,models of representations should be evaluatedwith respect to their ability to deal with sparse-ness; evaluation data sets should include rare aswell as frequent words.
(iii) Our results in Section 4.3 suggest thatsingle-representation approaches can indeed rep-resent different senses of a word.
We did a classi-fication task that roughly corresponds to the ques-tion: does this word have a particular meaning?A representation can fail on similarity judgementcomputations because less frequent senses occupya small part of the capacity of the representa-tion and therefore have little impact on full-spacesimilarity values.
Such a failure does not neces-sarily mean that a particular sense is not presentin the representation and it does not necessarilymean that single-representation approaches per-form poor on real-world tasks.
However, we sawthat even though single-representations do well onbalanced senses, they can pose a challenge for am-biguous words with skewed senses.
(iv) Lexical information is complex and multi-faceted.
In point-based tests, all dimensions areconsidered together and their ability to evaluatespecific facets or properties of a word is limited.The full-space similarity of a word may be high-est to a word that has a different value on a low-entropy facet.
Any good or bad result on thesetasks is not sufficient to conclude that the repre-sentation is weak.
The valid criterion of quality iswhether information about the facet is consistentlyand accurately stored.5.2 Extrinsic evaluation: entity typingTo support the case for sub-space evaluation andalso to introduce a new extrinsic task that uses theembeddings directly in supervised classification,we address a fine-grained entity typing task.Learning taxonomic properties or types ofwords has been used as an evaluation methodfor word embeddings (Rubinstein et al, 2015).Since available word typing datasets are quitesmall (cf.
Baroni et al (2014), Rubinstein et al(2015)), entity typing can be a promising alter-native, which enables to do supervised classifi-cation instead of unsupervised clustering.
Enti-ties, like other words, have many properties andtherefore belong to several semantic types, e.g.,?Barack Obama?
is a POLITICIAN, AUTHOR andAWARD WINNER.
We perform entity typing bylearning types of knowledge base entities fromtheir embeddings; this requires looking at sub-spaces because each entity can belong to multipletypes.We adopt the setup of Yaghoobzadeh andSch?utze (2015) who present a dataset of Freebaseentities;5there are 102 types (e.g., POLITICIANFOOD, LOCATION-CEMETERY) and most entitieshave several.
More specifically, we use a multi-layer-perceptron (MLP) with one hidden layer toclassify entity embeddings to 102 FIGER types.To show the limit of point-based evaluation, wealso experimentally test an entity typing modelbased on cosine similarity of entity embeddings.To each test entity, we assign all types of the entityclosest to it in the train set.
We call this approach1NN (kNN for k = 1).6We take part of ClueWeb, which is annotatedwith Freebase entities using automatic annota-tion of FACC17(Gabrilovich et al, 2013), asour corpus.
We then replace all mentions ofentities with their Freebase identifier and learnembeddings of words and entities in the samespace.
Our corpus has around 6 million sen-tences with at least one annotated entity.
Wecalculate embeddings using our different models.Our hyperparameters: for learning-based mod-els: dim=100, neg=10, iterations=20, window=1,sub=10?3; for PPMI: SVD-dim=100, neg=1, win-dow=1, cds=0.75, sub=10?3, eig=0.5.
See (Levyet al, 2015) for more information about the mean-ing of hyperparameters.Table 1 gives results on test for all (about 60,000entities), head (freq > 100; about 12,200 enti-ties) and tail (freq < 5; about 10,000 entities).The MLP models consistently outperform 1NN on5cistern.cis.lmu.de/figment6We tried other values of k, but results were not better.7lemurproject.org/clueweb12/FACC1243all and tail entities.
This supports our hypothe-sis that only part of the information about typesthat is present in the vectors can be determined bysimilarity-based methods that use the overall di-rection of vectors, i.e., full-space similarity.There is little correlation between results ofMLP and 1NN in all and head entities, and thecorrelation between their results in tail entities ishigh.8For example, for all entities, using 1NN,SKIP is 4.3% (4.1%) better, and using MLP is1.7% (1.6%) worse than SSKIP (CWIN).
Thegood performance of SKIP on 1NN using cosinesimilarity can be related to its objective function,which maximizes the cosine similarity of cooccur-ing token embeddings.The important question is not similarity, butwhether the information about a specific type ex-ists in the entity embeddings or not.
Our resultsconfirm our previous observation that a classifica-tion by looking at subspaces is needed to answerthis question.
In contrast, based on full-space sim-ilarity, one can infer little about the quality of em-beddings.
Based on our results, SSKIP and CWINembeddings contain more accurate and consistentinformation because MLP classifier gives betterresults for them.
However, if we considered 1NNfor comparison, SKIP and CBOW would be supe-rior.6 Conclusion and future workWe have introduced a new way of evaluating dis-tributional representation models.
As an alterna-tive to the common evaluation tasks, we proposedto identify generic criteria that are important for anembedding model to represent properties of wordsaccurately and consistently.
We suggested fourcriteria based on fundamental characteristics ofnatural language and designed tests that evaluatemodels on the criteria.
We developed this evalua-tion methodology using PCFG-generated corporaand applied it on a case study to compare differentmodels of learning distributional representations.While we showed important differences of theembedding models, the goal was not to do a com-prehensive comparison of them.
We proposed aninnovative way of doing intrinsic evaluation ofembeddings.
Our evaluation method gave directinsight about the quality of embeddings.
Addi-tionally, while most intrinsic evaluations consider8The spearman correlation between MLP and 1NN forall=0.31, head=0.03, tail=0.75.word vectors as points, we used classifiers thatidentify different small subspaces of the full space.This is an important desideratum when designingevaluation methods because of the multifaceted-ness of natural language words: they have a largenumber of properties, each of which only occupiesa small proportion of the full-space capacity of theembedding.Based on this paper, there are serveral lines ofinvestigation we plan to conduct in the future.
(i)We will attempt to support our results on arti-ficially generated corpora by conducting experi-ments on real natural language data.
(ii) We willstudy the coverage of our four criteria in evalu-ating word representations.
(iii) We modeled thefour criteria using separate PCFGs, but they couldalso be modeled by one single unified PCFG.
Onequestion that arises is then to what extent the fourcriteria are orthogonal and to what extent interde-pendent.
A single unified grammar may make itharder to interpret the results, but may give addi-tional and more fine-grained insights as to how theperformance of embedding models is influencedby different fundamental properties of natural lan-guage and their interactions.Finally, we have made the simplifying assump-tion in this paper that the best conceptual frame-work for thinking about embeddings is that theembedding space can be decomposed into sub-spaces: either into completely orthogonal sub-spaces or ?
less radically ?
into partially ?over-lapping?
subspaces.
Furthermore, we have madethe assumption that the smoothness and robustnessproperties that are the main reasons why embed-dings are used in NLP can be reduced to similar-ities in subspaces.
See Rothe et al (2016) andRothe and Sch?utze (2016) for work that makessimilar assumptions.The fundamental assumptions here are decom-posability and linearity.
The smoothness proper-ties could be much more complicated.
Howevereven if this was the case, then much of the gen-eral framework of what we have presented in thispaper would still apply; e.g., the criterion that aparticular facet be fully and correctly representedis as important as before.
But the validity of theassumption that embedding spaces can be decom-posed into ?linear?
subspaces should be investi-gated in the future.Acknowledgments.
This work was supportedby DFG (SCHU 2246/8-2).244ReferencesEneko Agirre, Enrique Alfonseca, Keith B.
Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.A study on similarity and relatedness using distri-butional and wordnet-based approaches.
In HumanLanguage Technologies: Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, Proceedings, May 31 - June 5,2009, Boulder, Colorado, USA, pages 19?27.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
Asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof the 52nd Annual Meeting of the Associationfor Computational Linguistics, ACL 2014, pages238?247.Bernard Comrie.
1989.
Language universals and lin-guistic typology: Syntax and morphology.
Black-well, 2nd edition.D.
A. Cruse.
1986.
Lexical Semantics.
CambridgeUniversity Press, Cambridge, MA.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Evgeniy Gabrilovich, Michael Ringgaard, and Amar-nag Subramanya.
2013.
Facc1: Freebase annotationof clueweb corpora.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In The 50th Annual Meeting of the Asso-ciation for Computational Linguistics, Proceedingsof the Conference, July 8-14, 2012, Jeju Island, Ko-rea - Volume 1: Long Papers, pages 873?882.Arne K?ohn.
2015.
What?s in an embedding?
ana-lyzing word embeddings through multilingual eval-uation.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Process-ing, pages 2067?2073, Lisbon, Portugal, September.Siwei Lai, Kang Liu, Liheng Xu, and Jun Zhao.
2015.How to generate a good word embedding?
CoRR,abs/1507.05523.Omer Levy and Yoav Goldberg.
2014a.
Linguistic reg-ularities in sparse and explicit word representations.In CoNLL.Omer Levy and Yoav Goldberg.
2014b.
Neural wordembedding as implicit matrix factorization.
In Ad-vances in Neural Information Processing Systems27: Annual Conference on Neural Information Pro-cessing Systems 2014, December 8-13 2014, Mon-treal, Quebec, Canada, pages 2177?2185.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
TACL, 3:211?225.Jiwei Li and Dan Jurafsky.
2015.
Do multi-sense em-beddings improve natural language understanding?In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages1722?1732, Lisbon, Portugal, September.Wang Ling, Chris Dyer, Alan W. Black, and IsabelTrancoso.
2015.
Two/too simple adaptations ofword2vec for syntax problems.
In NAACL HLT2015, The 2015 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Denver,Colorado, USA, May 31 - June 5, 2015, pages 1299?1304.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Proceedings of ICLR.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In NIPS, pages 2265?2273.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
Glove: Global vectors forword representation.
In EMNLP, pages 1532?1543.Joseph Reisinger and Raymond J Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 109?117.
Association for Computational Lin-guistics.Sascha Rothe and Hinrich Sch?utze.
2016.
Wordembedding calculus in meaningful ultradense sub-spaces.
In ACL.Sascha Rothe, Sebastian Ebert, and Hinrich Sch?utze.2016.
Ultradense embeddings by orthogonal trans-formation.
In NAACL.Dana Rubinstein, Effi Levi, Roy Schwartz, and AriRappoport.
2015.
How well do distributional mod-els capture different types of semantic knowledge?In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing of the Asian Federation of NaturalLanguage Processing, ACL 2015, July 26-31, 2015,Beijing, China, Volume 2: Short Papers, pages 726?730.245Magnus Sahlgren.
2006.
The Word-Space Model.Ph.D.
thesis, Stockholm University.Tobias Schnabel, Igor Labutov, David Mimno, andThorsten Joachims.
2015.
Evaluation methods forunsupervised word embeddings.
In Proceedings ofthe 2015 Conference on Empirical Methods in Nat-ural Language Processing, pages 298?307, Lisbon,Portugal, September.Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-laume Lample, and Chris Dyer.
2015.
Evaluation ofword vector representations by subspace alignment.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages2049?2054, Lisbon, Portugal, September.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
J. Artif.
Intell.
Res.
(JAIR), 37:141?188.Yadollah Yaghoobzadeh and Hinrich Sch?utze.
2015.Corpus-level fine-grained entity typing using con-textual information.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 715?725, Lisbon, Portugal,September.Zhi Zhong and Hwee Tou Ng.
2010.
It makes sense:A wide-coverage word sense disambiguation sys-tem for free text.
In ACL 2010, Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, July 11-16, 2010, Uppsala,Sweden, System Demonstrations, pages 78?83.246
