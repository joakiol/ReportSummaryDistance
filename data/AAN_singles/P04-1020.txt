Learning Noun Phrase Anaphoricity to Improve Coreference Resolution:Issues in Representation and OptimizationVincent NgDepartment of Computer ScienceCornell UniversityIthaca, NY 14853-7501yung@cs.cornell.eduAbstractKnowledge of the anaphoricity of a noun phrasemight be profitably exploited by a coreference sys-tem to bypass the resolution of non-anaphoric nounphrases.
Perhaps surprisingly, recent attempts toincorporate automatically acquired anaphoricity in-formation into coreference systems, however, haveled to the degradation in resolution performance.This paper examines several key issues in com-puting and using anaphoricity information to im-prove learning-based coreference systems.
In par-ticular, we present a new corpus-based approach toanaphoricity determination.
Experiments on threestandard coreference data sets demonstrate the ef-fectiveness of our approach.1 IntroductionNoun phrase coreference resolution, the task of de-termining which noun phrases (NPs) in a text referto the same real-world entity, has long been con-sidered an important and difficult problem in nat-ural language processing.
Identifying the linguis-tic constraints on when two NPs can co-refer re-mains an active area of research in the commu-nity.
One significant constraint on coreference, thenon-anaphoricity constraint, specifies that a non-anaphoric NP cannot be coreferent with any of itspreceding NPs in a given text.Given the potential usefulness of knowledgeof (non-)anaphoricity for coreference resolution,anaphoricity determination has been studied fairlyextensively.
One common approach involves thedesign of heuristic rules to identify specific typesof (non-)anaphoric NPs such as pleonastic pro-nouns (e.g., Paice and Husk (1987), Lappin and Le-ass (1994), Kennedy and Boguraev (1996), Den-ber (1998)) and definite descriptions (e.g., Vieiraand Poesio (2000)).
More recently, the problemhas been tackled using unsupervised (e.g., Bean andRiloff (1999)) and supervised (e.g., Evans (2001),Ng and Cardie (2002a)) approaches.Interestingly, existing machine learning ap-proaches to coreference resolution have performedreasonably well without anaphoricity determination(e.g., Soon et al (2001), Ng and Cardie (2002b),Strube and Mu?ller (2003), Yang et al (2003)).
Nev-ertheless, there is empirical evidence that resolutionsystems might further be improved with anaphoric-ity information.
For instance, our coreference sys-tem mistakenly identifies an antecedent for manynon-anaphoric common nouns in the absence ofanaphoricity information (Ng and Cardie, 2002a).Our goal in this paper is to improve learning-based coreference systems using automaticallycomputed anaphoricity information.
In particular,we examine two important, yet largely unexplored,issues in anaphoricity determination for coreferenceresolution: representation and optimization.Constraint-based vs. feature-based representa-tion.
How should the computed anaphoricityinformation be used by a coreference system?From a linguistic perspective, knowledge of non-anaphoricity is most naturally represented as ?by-passing?
constraints, with which the coreferencesystem bypasses the resolution of NPs that are deter-mined to be non-anaphoric.
But for learning-basedcoreference systems, anaphoricity information canbe simply and naturally accommodated into the ma-chine learning framework by including it as a fea-ture in the instance representation.Local vs. global optimization.
Should theanaphoricity determination procedure be developedindependently of the coreference system that usesthe computed anaphoricity information (local opti-mization), or should it be optimized with respectto coreference performance (global optimization)?The principle of software modularity calls for localoptimization.
However, if the primary goal is to im-prove coreference performance, global optimizationappears to be the preferred choice.Existing work on anaphoricity determinationfor anaphora/coreference resolution can be char-acterized along these two dimensions.
Inter-estingly, most existing work employs constraint-based, locally-optimized methods (e.g., Mitkov etal.
(2002) and Ng and Cardie (2002a)), leavingthe remaining three possibilities largely unexplored.In particular, to our knowledge, there have beenno attempts to (1) globally optimize an anaphoric-ity determination procedure for coreference perfor-mance and (2) incorporate anaphoricity into corefer-ence systems as a feature.
Consequently, as part ofour investigation, we propose a new corpus-basedmethod for achieving global optimization and ex-periment with representing anaphoricity as a featurein the coreference system.In particular, we systematically evaluate all fourcombinations of local vs. global optimization andconstraint-based vs. feature-based representation ofanaphoricity information in terms of their effec-tiveness in improving a learning-based coreferencesystem.
Results on three standard coreferencedata sets are somewhat surprising: our proposedglobally-optimized method, when used in conjunc-tion with the constraint-based representation, out-performs not only the commonly-adopted locally-optimized approach but also its seemingly more nat-ural feature-based counterparts.The rest of the paper is structured as follows.Section 2 focuses on optimization issues, dis-cussing locally- and globally-optimized approachesto anaphoricity determination.
In Section 3, wegive an overview of the standard machine learningframework for coreference resolution.
Sections 4and 5 present the experimental setup and evaluationresults, respectively.
We examine the features thatare important to anaphoricity determination in Sec-tion 6 and conclude in Section 7.2 The Anaphoricity DeterminationSystem: Local vs.
Global OptimizationIn this section, we will show how to build a modelof anaphoricity determination.
We will first presentthe standard, locally-optimized approach and thenintroduce our globally-optimized approach.2.1 The Locally-Optimized ApproachIn this approach, the anaphoricity model is sim-ply a classifier that is trained and optimized inde-pendently of the coreference system (e.g., Evans(2001), Ng and Cardie (2002a)).Building a classifier for anaphoricity determina-tion.
A learning algorithm is used to train a classi-fier that, given a description of an NP in a document,decides whether or not the NP is anaphoric.
Eachtraining instance represents a single NP and consistsof a set of features that are potentially useful for dis-tinguishing anaphoric and non-anaphoric NPs.
Theclassification associated with a training instance ?one of ANAPHORIC or NOT ANAPHORIC ?
is de-rived from coreference chains in the training doc-uments.
Specifically, a positive instance is createdfor each NP that is involved in a coreference chainbut is not the head of the chain.
A negative instanceis created for each of the remaining NPs.Applying the classifier.
To determine theanaphoricity of an NP in a test document, aninstance is created for it as during training and pre-sented to the anaphoricity classifier, which returnsa value of ANAPHORIC or NOT ANAPHORIC.2.2 The Globally-Optimized ApproachTo achieve global optimization, we construct a para-metric anaphoricity model with which we optimizethe parameter1 for coreference accuracy on held-out development data.
In other words, we tightenthe connection between anaphoricity determinationand coreference resolution by using the parameterto generate a set of anaphoricity models from whichwe select the one that yields the best coreferenceperformance on held-out data.Global optimization for a constraint-based rep-resentation.
We view anaphoricity determinationas a problem of determining how conservative ananaphoricity model should be in classifying an NPas (non-)anaphoric.
Given a constraint-based repre-sentation of anaphoricity information for the coref-erence system, if the model is too liberal in classi-fying an NP as non-anaphoric, then many anaphoricNPs will be misclassified, ultimately leading to a de-terioration of recall and of the overall performanceof the coreference system.
On the other hand, if themodel is too conservative, then only a small fractionof the truly non-anaphoric NPs will be identified,and so the resulting anaphoricity information maynot be effective in improving the coreference sys-tem.
The challenge then is to determine a ?good?degree of conservativeness.
As a result, we can de-sign a parametric anaphoricity model whose con-servativeness can be adjusted via a conservativenessparameter.
To achieve global optimization, we cansimply tune this parameter to optimize for corefer-ence performance on held-out development data.Now, to implement this conservativeness-basedanaphoricity determination model, we propose twomethods, each of which is built upon a different def-inition of conservativeness.Method 1: Varying the Cost RatioOur first method exploits a parameter present inmany off-the-shelf machine learning algorithms for1We can introduce multiple parameters for this purpose,but to simply the optimization process, we will only considersingle-parameter models in this paper.training a classifier ?
the cost ratio (cr), which isdefined as follows.cr := cost of misclassifying a positive instancecost of misclassifying a negative instanceInspection of this definition shows that cr providesa means of adjusting the relative misclassificationpenalties placed on training instances of differentclasses.
In particular, the larger cr is, the more con-servative the classifier is in classifying an instanceas negative (i.e., non-anaphoric).
Given this obser-vation, we can naturally define the conservativenessof an anaphoricity classifier as follows.
We say thatclassifier A is more conservative than classifier B indetermining an NP as non-anaphoric if A is trainedwith a higher cost ratio than B.Based on this definition of conservativeness, wecan construct an anaphoricity model parameterizedby cr.
Specifically, the parametric model mapsa given value of cr to the anaphoricity classifiertrained with this cost ratio.
(For the purpose of train-ing anaphoricity classifiers with different values ofcr, we use RIPPER (Cohen, 1995), a propositionalrule learning algorithm.)
It should be easy to seethat increasing cr makes the model more conserva-tive in classifying an NP as non-anaphoric.
Withthis parametric model, we can tune cr to optimizefor coreference performance on held-out data.Method 2: Varying the Classification ThresholdWe can also define conservativeness in terms of thenumber of NPs classified as non-anaphoric for agiven set of NPs.
Specifically, given two anaphoric-ity models A and B and a set of instances I to beclassified, we say that A is more conservative thanB in determining an NP as non-anaphoric if A clas-sifies fewer instances in I as non-anaphoric than B.Again, this definition is consistent with our intuitionregarding conservativeness.We can now design a parametric anaphoricitymodel based on this definition.
First, we trainin a supervised fashion a probablistic model ofanaphoricity PA(c | i), where i is an instance rep-resenting an NP and c is one of the two possibleanaphoricity values.
(In our experiments, we usemaximum entropy classification (MaxEnt) (Bergeret al, 1996) to train this probability model.)
Then,we can construct a parametric model making bi-nary anaphoricity decisions from PA by introduc-ing a threshold parameter t as follows.
Given aspecific t (0 ?
t ?
1) and a new instance i, wedefine an anaphoricity model M tA in which M tA(i)= NOT ANAPHORIC if and only if PA(c = NOTANAPHORIC | i) ?
t. It should be easy to see thatincreasing t yields progressively more conservativeanaphoricity models.
Again, t can be tuned usingheld-out development data.Global optimization for a feature-based repre-sentation.
We can similarly optimize our pro-posed conservativeness-based anaphoricity modelfor coreference performance when anaphoricity in-formation is represented as a feature for the corefer-ence system.
Unlike in a constraint-based represen-tation, however, we cannot expect that the recall ofthe coreference system would increase with the con-servativeness parameter.
The reason is that we haveno control over whether or how the anaphoricityfeature is used by the coreference learner.
In otherwords, the behavior of the coreference system is lesspredictable in comparison to a constraint-based rep-resentation.
Other than that, the conservativeness-based anaphoricity model is as good to use forglobal optimization with a feature-based represen-tation as with a constraint-based representation.We conclude this section by pointing out that thelocally-optimized approach to anaphoricity deter-mination is indeed a special case of the global one.Unlike the global approach in which the conserva-tiveness parameter values are tuned based on la-beled data, the local approach uses ?default?
param-eter values.
For instance, when RIPPER is used totrain an anaphoricity classifier in the local approach,cr is set to the default value of one.
Similarly, whenprobabilistic anaphoricity decisions generated via aMaxEnt model are converted to binary anaphoricitydecisions for subsequent use by a coreference sys-tem, t is set to the default value of 0.5.3 The Machine Learning Framework forCoreference ResolutionThe coreference system to which our automaticallycomputed anaphoricity information will be appliedimplements the standard machine learning approachto coreference resolution combining classificationand clustering.
Below we will give a brief overviewof this standard approach.
Details can be found inSoon et al (2001) or Ng and Cardie (2002b).Training an NP coreference classifier.
After apre-processing step in which the NPs in a documentare automatically identified, a learning algorithm isused to train a classifier that, given a description oftwo NPs in the document, decides whether they areCOREFERENT or NOT COREFERENT.Applying the classifier to create coreferencechains.
Test texts are processed from left to right.Each NP encountered, NPj , is compared in turn toeach preceding NP, NPi.
For each pair, a test in-stance is created as during training and is presentedto the learned coreference classifier, which returnsa number between 0 and 1 that indicates the likeli-hood that the two NPs are coreferent.
The NP withthe highest coreference likelihood value among thepreceding NPs with coreference class values above0.5 is selected as the antecedent of NPj ; otherwise,no antecedent is selected for NPj .4 Experimental SetupIn Section 2, we examined how to construct locally-and globally-optimized anaphoricity models.
Re-call that, for each of these two types of models,the resulting (non-)anaphoricity information can beused by a learning-based coreference system eitheras hard bypassing constraints or as a feature.
Hence,given a coreference system that implements the two-step learning approach shown above, we will be ableto evaluate the four different combinations of com-puting and using anaphoricity information for im-proving the coreference system described in the in-troduction.
Before presenting evaluation details, wewill describe the experimental setup.Coreference system.
In all of our experiments,we use our learning-based coreference system (Ngand Cardie, 2002b).Features for anaphoricity determination.
Inboth the locally-optimized and the globally-optimized approaches to anaphoricity determinationdescribed in Section 2, an instance is represented by37 features that are specifically designed for distin-guishing anaphoric and non-anaphoric NPs.
Spacelimitations preclude a description of these features;see Ng and Cardie (2002a) for details.Learning algorithms.
For training coreferenceclassifiers and locally-optimized anaphoricity mod-els, we use both RIPPER and MaxEnt as the un-derlying learning algorithms.
However, for trainingglobally-optimized anaphoricity models, RIPPER isalways used in conjunction with Method 1 and Max-Ent with Method 2, as described in Section 2.2.In terms of setting learner-specific parameters,we use default values for all RIPPER parametersunless otherwise stated.
For MaxEnt, we alwaystrain the feature-weight parameters with 100 iter-ations of the improved iterative scaling algorithm(Della Pietra et al, 1997), using a Gaussian priorto prevent overfitting (Chen and Rosenfeld, 2000).Data sets.
We use the Automatic Content Ex-traction (ACE) Phase II data sets.2 We chooseACE rather than the more widely-used MUC cor-pus (MUC-6, 1995; MUC-7, 1998) simply because2See http://www.itl.nist.gov/iad/894.01/tests/ace for details on the ACE research program.BNEWS NPAPER NWIRENumber of training texts 216 76 130Number of test texts 51 17 29Number of training insts(for anaphoricity)20567 21970 27338Number of training insts(for coreference)97036 148850 122168Table 1: Statistics of the three ACE data setsACE provides much more labeled data for bothtraining and testing.
However, our system was setup to perform coreference resolution according tothe MUC rules, which are fairly different from theACE guidelines in terms of the identification ofmarkables as well as evaluation schemes.
Since ourgoal is to evaluate the effect of anaphoricity infor-mation on coreference resolution, we make no at-tempt to modify our system to adhere to the rulesspecifically designed for ACE.The coreference corpus is composed of three datasets made up of three different news sources: Broad-cast News (BNEWS), Newspaper (NPAPER), andNewswire (NWIRE).
Statistics collected from thesedata sets are shown in Table 1.
For each data set,we train an anaphoricity classifier and a coreferenceclassifier on the (same) set of training texts and eval-uate the coreference system on the test texts.5 EvaluationIn this section, we will compare the effectiveness offour approaches to anaphoricity determination (seethe introduction) in improving our baseline corefer-ence system.5.1 Coreference Without AnaphoricityAs mentioned above, we use our coreference systemas the baseline system where no explicit anaphoric-ity determination system is employed.
Results us-ing RIPPER and MaxEnt as the underlying learnersare shown in rows 1 and 2 of Table 2 where perfor-mance is reported in terms of recall, precision, andF-measure using the model-theoretic MUC scoringprogram (Vilain et al, 1995).
With RIPPER, thesystem achieves an F-measure of 56.3 for BNEWS,61.8 for NPAPER, and 51.7 for NWIRE.
The per-formance of MaxEnt is comparable to that of RIP-PER for the BNEWS and NPAPER data sets butslightly worse for the NWIRE data set.5.2 Coreference With AnaphoricityThe Constraint-Based, Locally-Optimized(CBLO) Approach.
As mentioned before, inconstraint-based approaches, the automaticallycomputed non-anaphoricity information is used asSystem Variation BNEWS NPAPER NWIREExperiments L R P F C R P F C R P F C1 No RIP 57.4 55.3 56.3 - 60.0 63.6 61.8 - 53.2 50.3 51.7 -2 Anaphoricity ME 60.9 52.1 56.2 - 65.4 58.6 61.8 - 54.9 46.7 50.4 -3 Constraint- RIP 42.5 77.2 54.8 cr=1 46.7 79.3 58.8?
cr=1 42.1 64.2 50.9 cr=14 Based, RIP 45.4 72.8 55.9 t=0.5 52.2 75.9 61.9 t=0.5 36.9 61.5 46.1?
t=0.55 Locally- ME 44.4 76.9 56.3 cr=1 50.1 75.7 60.3 cr=1 43.9 63.0 51.7 cr=16 Optimized ME 47.3 70.8 56.7 t=0.5 57.1 70.6 63.1?
t=0.5 38.1 60.0 46.6?
t=0.57 Feature- RIP 53.5 61.3 57.2 cr=1 58.7 69.7 63.7?
cr=1 54.2 46.8 50.2?
cr=18 Based, RIP 58.3 58.3 58.3?
t=0.5 63.5 57.0 60.1?
t=0.5 63.4 35.3 45.3?
t=0.59 Locally- ME 59.6 51.6 55.3?
cr=1 65.6 57.9 61.5 cr=1 55.1 46.2 50.3 cr=110 Optimized ME 59.6 51.6 55.3?
t=0.5 66.0 57.7 61.6 t=0.5 54.9 46.7 50.4 t=0.511 Constraint- RIP 54.5 68.6 60.8?
cr=5 58.4 68.8 63.2?
cr=4 50.5 56.7 53.4?
cr=312 Based, RIP 54.1 67.1 59.9?
t=0.7 56.5 68.1 61.7 t=0.65 50.3 53.8 52.0 t=0.713 Globally- ME 54.8 62.9 58.5?
cr=5 62.4 65.6 64.0?
cr=3 52.2 57.0 54.5?
cr=314 Optimized ME 54.1 60.6 57.2 t=0.7 61.7 64.0 62.8?
t=0.7 52.0 52.8 52.4?
t=0.715 Feature- RIP 60.8 56.1 58.4?
cr=8 62.2 61.3 61.7 cr=6 54.6 49.4 51.9 cr=816 Based, RIP 59.7 57.0 58.3?
t=0.6 63.6 59.1 61.3 t=0.8 56.7 48.4 52.3 t=0.717 Globally- ME 59.9 51.0 55.1?
cr=9 66.5 57.1 61.4 cr=1 56.3 46.9 51.2?
cr=1018 Optimized ME 59.6 51.6 55.3?
t=0.95 65.9 57.5 61.4 t=0.95 56.5 46.7 51.1?
t=0.5Table 2: Results of the coreference systems using different approaches to anaphoricity determination on thethree ACE test data sets.
Information on which Learner (RIPPER or MaxEnt) is used to train the coreference clas-sifier, as well as performance results in terms of Recall, Precision, F-measure and the corresponding Conservativenessparameter are provided whenever appropriate.
The strongest result obtained for each data set is boldfaced.
In addition,results that represent statistically significant gains and drops with respect to the baseline are marked with an asterisk(*) and a dagger (?
), respectively.hard bypassing constraints, with which the corefer-ence system attempts to resolve only NPs that theanaphoricity classifier determines to be anaphoric.As a result, we hypothesized that precision wouldincrease in comparison to the baseline system.
Inaddition, we expect that recall will drop owing tothe anaphoricity classifier?s misclassifications oftruly anaphoric NPs.
Consequently, overall per-formance is not easily predictable: F-measure willimprove only if gains in precision can compensatefor the loss in recall.Results are shown in rows 3-6 of Table 2.
Eachrow corresponds to a different combination oflearners employed in training the coreference andanaphoricity classifiers.3 As mentioned in Section2.2, locally-optimized approaches are a special caseof their globally-optimized counterparts, with theconservativeness parameter set to the default valueof one for RIPPER and 0.5 for MaxEnt.In comparison to the baseline, we see large gainsin precision at the expense of recall.
Moreover,CBLO does not seem to be very effective in improv-ing the baseline, in part due to the dramatic loss inrecall.
In particular, although we see improvementsin F-measure in five of the 12 experiments in thisgroup, only one of them is statistically significant.43Bear in mind that different learners employed in train-ing anaphoricity classifiers correspond to different parametricmethods.
For ease of exposition, however, we will refer to themethod simply by the learner it employs.4The Approximate Randomization test described in NoreenWorse still, F-measure drops significantly in threecases.The Feature-Based, Locally-Optimized (FBLO)Approach.
The experimental setting employedhere is essentially the same as that in CBLO, ex-cept that anaphoricity information is incorporatedinto the coreference system as a feature rather thanas constraints.
Specifically, each training/test coref-erence instance i(NPi,NPj) (created from NPj anda preceding NP NPi) is augmented with a featurewhose value is the anaphoricity of NPj as computedby the anaphoricity classifier.In general, we hypothesized that FBLO wouldperform better than the baseline: the addition of ananaphoricity feature to the coreference instance rep-resentation might give the learner additional flexi-bility in creating coreference rules.
Similarly, weexpect FBLO to outperform its constraint-basedcounterpart: since anaphoricity information is rep-resented as a feature in FBLO, the coreferencelearner can incorporate the information selectivelyrather than as universal hard constraints.Results using the FBLO approach are shown inrows 7-10 of Table 2.
Somewhat unexpectedly, thisapproach is not effective in improving the baseline:F-measure increases significantly in only two of the12 cases.
Perhaps more surprisingly, we see signif-icant drops in F-measure in five cases.
To get a bet-(1989) is applied to determine if the differences in the F-measure scores between two coreference systems are statisti-cally significant at the 0.05 level or higher.System Variation BNEWS (dev) NPAPER (dev) NWIRE (dev)Experiments L R P F C R P F C R P F C1 Constraint- RIP 62.6 76.3 68.8 cr=5 65.5 73.0 69.1 cr=4 56.1 58.9 57.4 cr=32 Based, RIP 62.5 75.5 68.4 t=0.7 63.0 71.7 67.1 t=0.65 56.7 54.8 55.7 t=0.73 Globally- ME 63.1 71.3 66.9 cr=5 66.2 71.8 68.9 cr=3 57.9 59.7 58.8 cr=34 Optimized ME 62.9 70.8 66.6 t=0.7 61.4 74.3 67.3 t=0.65 58.4 55.3 56.8 t=0.7Table 3: Results of the coreference systems using a constraint-based, globally-optimized approach toanaphoricity determination on the three ACE held-out development data sets.
Information on which Learner(RIPPER or MaxEnt) is used to train the coreference classifier as well as performance results in terms of Recall,Precision, F-measure and the corresponding Conservativeness parameter are provided whenever appropriate.
Thestrongest result obtained for each data set is boldfaced.ter idea of why F-measure decreases, we examinethe relevant coreference classifiers induced by RIP-PER.
We find that the anaphoricity feature is used ina somewhat counter-intuitive manner: some of theinduced rules posit a coreference relationship be-tween NPj and a preceding NP NPi even though NPjis classified as non-anaphoric.
These results seem tosuggest that the anaphoricity feature is an irrelevantfeature from a machine learning point of view.In comparison to CBLO, the results are mixed:there does not appear to be a clear winner in any ofthe three data sets.
Nevertheless, it is worth noticingthat the CBLO systems can be characterized as hav-ing high precision/low recall, whereas the reverse istrue for FBLO systems in general.
As a result, eventhough CBLO and FBLO systems achieve similarperformance, the former is the preferred choice inapplications where precision is critical.Finally, we note that there are other ways toencode anaphoricity information in a coreferencesystem.
For instance, it is possible to representanaphoricity as a real-valued feature indicating theprobability of an NP being anaphoric rather than asa binary-valued feature.
Future work will examinealternative encodings of anaphoricity.The Constraint-Based, Globally-Optimized(CBGO) Approach.
As discussed above, weoptimize the anaphoricity model for coreferenceperformance via the conservativeness parameter.
Inparticular, we will use this parameter to maximizethe F-measure score for a particular data set andlearner combination using held-out developmentdata.
To ensure a fair comparison between globaland local approaches, we do not rely on additionaldevelopment data in the former; instead we use23 of the original training texts for acquiring theanaphoricity and coreference classifiers and theremaining 13 for development for each of the datasets.
As far as parameter tuning is concerned,we tested values of 1, 2, .
.
.
, 10 as well as theirreciprocals for cr and 0.05, 0.1, .
.
.
, 1.0 for t.In general, we hypothesized that CBGO wouldoutperform both the baseline and the locally-optimized approaches, since coreference perfor-mance is being explicitly maximized.
Results usingCBGO, which are shown in rows 11-14 of Table 2,are largely consistent with our hypothesis.
The bestresults on all of the three data sets are achieved us-ing this approach.
In comparison to the baseline,we see statistically significant gains in F-measure innine of the 12 experiments in this group.
Improve-ments stem primarily from large gains in precisionaccompanied by smaller drops in recall.
Perhapsmore importantly, CBGO never produces resultsthat are significantly worse than those of the base-line systems on these data sets, unlike CBLO andFBLO.
Overall, these results suggest that CBGO ismore robust than the locally-optimized approachesin improving the baseline system.As can be seen, CBGO fails to produce statisti-cally significant improvements over the baseline inthree cases.
The relatively poorer performance inthese cases can potentially be attributed to the un-derlying learner combination.
Fortunately, we canuse the development data not only for parametertuning but also in predicting the best learner com-bination.
Table 3 shows the performance of thecoreference system using CBGO on the develop-ment data, along with the value of the conservative-ness parameter used to achieve the results in eachcase.
Using the notation Learner1/Learner2 todenote the fact that Learner1 and Learner2 areused to train the underlying coreference classifierand anaphoricity classifier respectively, we can seethat the RIPPER/RIPPER combination achieves thebest performance on the BNEWS development set,whereas MaxEnt/RIPPER works best for the othertwo.
Hence, if we rely on the development data topick the best learner combination for use in testing,the resulting coreference system will outperform thebaseline in all three data sets and yield the best-performing system on all but the NPAPER data sets,achieving an F-measure of 60.8 (row 11), 63.2 (row11), and 54.5 (row 13) for the BNEWS, NPAPER,1 2 3 4 5 6 7 8 9 105055606570758085crScoreRecallPrecisionF?measureFigure 1: Effect of cr on the performance of thecoreference system for the NPAPER developmentdata using RIPPER/RIPPERand NWIRE data sets, respectively.
Moreover, thehigh correlation between the relative coreferenceperformance achieved by different learner combina-tions on the development data and that on the testdata also reflects the stability of CBGO.In comparison to the locally-optimized ap-proaches, CBGO achieves better F-measure scoresin almost all cases.
Moreover, the learned conser-vativeness parameter in CBGO always has a largervalue than the default value employed by CBLO.This provides empirical evidence that the CBLOanaphoricity classifiers are too liberal in classifyingNPs as non-anaphoric.To examine the effect of the conservativeness pa-rameter on the performance of the coreference sys-tem, we plot in Figure 1 the recall, precision, F-measure curves against cr for the NPAPER develop-ment data using the RIPPER/RIPPER learner com-bination.
As cr increases, recall rises and precisiondrops.
This should not be surprising, since (1) in-creasing cr causes fewer anaphoric NPs to be mis-classified and allows the coreference system to finda correct antecedent for some of them, and (2) de-creasing cr causes more truly non-anaphoric NPs tobe correctly classified and prevents the coreferencesystem from attempting to resolve them.
The bestF-measure in this case is achieved when cr=4.The Feature-Based, Globally-Optimized(FBGO) Approach.
The experimental set-ting employed here is essentially the same as thatin the CBGO setting, except that anaphoricityinformation is incorporated into the coreferencesystem as a feature rather than as constraints.Specifically, each training/test instance i(NPi,NPj)is augmented with a feature whose value is thecomputed anaphoricity of NPj .
The developmentdata is used to select the anaphoricity model(and hence the parameter value) that yields thebest-performing coreference system.
This modelis then used to compute the anaphoricity value forthe test instances.
As mentioned before, we use thesame parametric anaphoricity model as in CBGOfor achieving global optimization.Since the parametric model is designed with aconstraint-based representation in mind, we hypoth-esized that global optimization in this case wouldnot be as effective as in CBGO.
Nevertheless, weexpect that this approach is still more effective inimproving the baseline than the locally-optimizedapproaches.Results using FBGO are shown in rows 15-18of Table 2.
As expected, FBGO is less effectivethan CBGO in improving the baseline, underper-forming its constraint-based counterpart in 11 of the12 cases.
In fact, FBGO is able to significantly im-prove the corresponding baseline in only four cases.Somewhat surprisingly, FBGO is by no means su-perior to the locally-optimized approaches with re-spect to improving the baseline.
These results seemto suggest that global optimization is effective onlyif we have a ?good?
parameterization that is able totake into account how anaphoricity information willbe exploited by the coreference system.
Neverthe-less, as discussed before, effective global optimiza-tion with a feature-based representation is not easyto accomplish.6 Analyzing Anaphoricity FeaturesSo far we have focused on computing and us-ing anaphoricity information to improve the perfor-mance of a coreference system.
In this section, weexamine which anaphoricity features are importantin order to gain linguistic insights into the problem.Specifically, we measure the informativeness ofa feature by computing its information gain (seep.22 of Quinlan (1993) for details) on our threedata sets for training anaphoricity classifiers.
Over-all, the most informative features are HEAD MATCH(whether the NP under consideration has the samehead as one of its preceding NPs), STR MATCH(whether the NP under consideration is the samestring as one of its preceding NPs), and PRONOUN(whether the NP under consideration is a pronoun).The high discriminating power of HEAD MATCHand STR MATCH is a probable consequence of thefact that an NP is likely to be anaphoric if there isa lexically similar noun phrase preceding it in thetext.
The informativeness of PRONOUN can also beexpected: most pronominal NPs are anaphoric.Features that determine whether the NP underconsideration is a PROPER NOUN, whether it is aBARE SINGULAR or a BARE PLURAL, and whetherit begins with an ?a?
or a ?the?
(ARTICLE) are alsohighly informative.
This is consistent with our in-tuition that the (in)definiteness of an NP plays animportant role in determining its anaphoricity.7 ConclusionsWe have examined two largely unexplored issuesin computing and using anaphoricity informationfor improving learning-based coreference systems:representation and optimization.
In particular, wehave systematically evaluated all four combinationsof local vs. global optimization and constraint-basedvs.
feature-based representation of anaphoricity in-formation in terms of their effectiveness in improv-ing a learning-based coreference system.Extensive experiments on the three ACE corefer-ence data sets using a symbolic learner (RIPPER)and a statistical learner (MaxEnt) for training coref-erence classifiers demonstrate the effectiveness ofthe constraint-based, globally-optimized approachto anaphoricity determination, which employs ourconservativeness-based anaphoricity model.
Notonly does this approach improve a ?no anaphoric-ity?
baseline coreference system, it is more effec-tive than the commonly-adopted locally-optimizedapproach without relying on additional labeled data.AcknowledgmentsWe thank Regina Barzilay, Claire Cardie, Bo Pang,and the anonymous reviewers for their invaluablecomments on earlier drafts of the paper.
This workwas supported in part by NSF Grant IIS?0208028.ReferencesDavid Bean and Ellen Riloff.
1999.
Corpus-based iden-tification of non-anaphoric noun phrases.
In Proceed-ings of the ACL, pages 373?380.Adam L. Berger, Stephen A. Della Pietra, and Vincent J.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1):39?71.Stanley Chen and Ronald Rosenfeld.
2000.
A survey ofsmoothing techniques for ME models.
IEEE Transac-tions on Speech on Audio Processing, 8(1):37?50.William Cohen.
1995.
Fast effective rule induction.
InProceedings of ICML.Stephen Della Pietra, Vincent Della Pietra, and John Laf-ferty.
1997.
Inducing features of random fields.
IEEETransactions on Pattern Analysis and Machine Intel-ligence, 19(4):380?393.Michel Denber.
1998.
Automatic resolution of anaphorain English.
Technical report, Eastman Kodak Co.Richard Evans.
2001.
Applying machine learning to-ward an automatic classification of it.
Literary andLinguistic Computing, 16(1):45?57.Christopher Kennedy and Branimir Boguraev.
1996.Anaphor for everyone: Pronominal anaphora resolu-tion without a parser.
In Proceedings of COLING,pages 113?118.Shalom Lappin and Herbert Leass.
1994.
An algorithmfor pronominal anaphora resolution.
ComputationalLinguistics, 20(4):535?562.Ruslan Mitkov, Richard Evans, and Constantin Orasan.2002.
A new, fully automatic version of Mitkov?sknowledge-poor pronoun resolution method.
In Al.Gelbukh, editor, Computational Linguistics and Intel-ligent Text Processing, pages 169?187.MUC-6.
1995.
Proceedings of the Sixth Message Un-derstanding Conference (MUC-6).MUC-7.
1998.
Proceedings of the Seventh Message Un-derstanding Conference (MUC-7).Vincent Ng and Claire Cardie.
2002a.
Identifyinganaphoric and non-anaphoric noun phrases to improvecoreference resolution.
In Proceedings of COLING,pages 730?736.Vincent Ng and Claire Cardie.
2002b.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the ACL, pages 104?111.Eric W. Noreen.
1989.
Computer Intensive Methods forTesting Hypothesis: An Introduction.
John Wiley &Sons.Chris Paice and Gareth Husk.
1987.
Towards the au-tomatic recognition of anaphoric features in Englishtext: the impersonal pronoun ?it?.
Computer Speechand Language, 2.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
San Mateo, CA: Morgan Kaufmann.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Michael Strube and Christoph Mu?ller.
2003.
A machinelearning approach to pronoun resolution in spoken di-alogue.
In Proceedings of the ACL, pages 168?175.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4):539?593.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the Sixth Message Understanding Conference(MUC-6), pages 45?52.Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew LimTan.
2003.
Coreference resolution using competitivelearning approach.
In Proceedings of the ACL, pages176?183.
