Incremental Parsing and Reason MaintenanceMats  Wi%nDepar tment  of Computer  and  In fo rmat ion  ScienceL inkSping Un ivers i tyS-581 83 L inkSping,  Swedenmgw@ida.
l iu .
seAbstractThe purpose of this paper is to compare differentways of adopting reason-maintenance techniques inincremental parsing (and interpretation).
A reason-maintenance system supports incremental tbrmationand revision of beliefs.
By viewing the constructionof partial analyses of a text as analogous to form-ing beliefs about the meanings of its parts, a rela-tion between parsing and reason maintenance can beconceived.
In line with this, reason maintenance canbc used for realizing a strong notion of incrementalparsing, allowing for revisions of previous analyses.Moreover, an assumption-based reason-maintenancesystem (ATMS) can be used to support eftieicnt com-parisons of (competing) interpretations.
The paperargues for an approach which is an extension of chartparsing, but which also can be seen as a system con-sisting of an inference ngine (the parser proper) cou-pled with a simplified ATMS.Backgroundand IntroductionThis paper focuses on the problem of incrementalparsing (and to some extent interpretation); in par-ticular, how reason-maintenance techniques can beused to achieve a strong notion of incrementalityallowing for piecemeal construction, revision, andcomp;~rison of partial analyses.Human language understanding is apparently in-cremental in the sense of proceeding in a piece-meal fashion, (ideally) carried out in small, grad=ual steps as each word is encountered (Tyler andMarslen-Wilson 1977, Marslen-Wilson and Tyler1980).
Work on incremental parsing and interpre-tation is typically motivated by a desire to model,or mimic, (aspects of) this behaviour, for example,Bobrow and Webber (1980), Ades and Steedman(1982), Mellish (1985), Pulman (1985), and Haddock(1987~ 1988~ 1990).tlowever, there are also clear-cut computationalreasons for trying to attain incrernentality.
SparkedThis research as been supported by the National SwedishBoard for Technical Development.off by the rapid development of increasingly power-ful, distributed computer hardware, a paradigm of"immediate computation" is gaining popularity ininteractive applications like WYSIWYG word pro-cessing, spreadsheet programs, and programming-language ditors (Reps and Teitelbaum 1984, 1987).It is interesting to consider similar systems appliedto interactive natural-language processing.
Thepoint is that incrementality is a prerequisite of thereactiw~, real~time-based behaviour of such systems.Furthermore, systems that mix for example deicticand natural-language input require that linguisticstatus be given to sentence fragments, thus demand-ing incremental analysis (Kobsa et al 1986).One body of work which appears to be usefifl inincremental parsing and interpretation is re ,on  (ortruth) maintenanceJ A reason-maintenance system(RMS) supports incremental formation and revisionof beliefs.
By viewing the construction of partialanalyses of a text as analogous to forming beliefsabout the meanings of its parts, a relation betweenparsing and reason maintenance can be conceived.An RMS is coupled with an inference ngine (forexample, a parser) which makes inferences withinthe problem domain, and the overall, combined sys-tem can be seen as an inferential problem solver.The RMS makes use of two data structures, nodesand justifications.
A node represents a datum pro-vided by the inference ngine, such as an assumptionor an inferred proposition.
Whenever a datum is in-ferred from a conjunction of other data, the RMSrecords this dependency ms a justification which re-lates the respective nodes.
The RMS thus keepstrack of what data are believed and disbelieved, andwhy, given the inferences made so far.The traditional approach, justification-based rea-son maintenance, JTMS (Doyle 1979), is to main-tain a (global) belief by associating with each nodea status of in (indicating belief in the correspondingdatum) or out (indicating lack of belief in the da-tum) such that every justification is satisfied.
2 Theentire set of (consistent) in data make up the cur-1 For an excellent introduction to reason maintenance, seeReinfrank (1989).2I prefer the term "reason maintenance" but use the stan-dard abbreviations "ATMS" and "JTMS" (where "T" standsfor "truth" ).1 287rent context (belief, interpretation3).
In case of acontradiction, dependency-directed backtracking isinvoked to identify the inconsistent assumptions andenable retraction of some of them.A more recent approach is assumption-based rea-son maintenance, ATMS (de Kleer 1986), which sup-ports problem solving in multiple contexts simul-taneously .... there is no need to keep the over-all database consistent as in the JTMS.
Thus,the ATMS is oriented towards finding all solutions,whereas a JTMS is oriented towards finding onlyone solution.
In this and other respects, an ATMSresembles chart parsing (Kay 1980, Thompson andRitchie 1984), something which will be further dealtwith below.
Each ATMS node has a label with in-formation about the minimal sets of assumptions onwhich it ultimately depends.
Thus, rather than as-sociating explicit in/out information with nodes, thestatus of a node with respect o a context can be de-termined by comparing the label of the node withthe assumptions that underlie the context.
There isno dependency-directed backtracking in an ATMSsince contradictions do not pose any problem andthe assumptions underlying a contradiction are di-rectly identifiable.The rest of this paper is organized as follows: Sec-tions 2 and 3 review and compare various ways ofadopting JTMS and ATMS techniques in parsingand interpretation.
Section 4 carries on with dis-cussing specifically how an ATMS-style system canbe used in various applications of incremental pars-ing and interpretation, and, finally, section 5 smn-marizes the conclusions.2 3TMS-Style Approaches2.1 Overv iewAn early example of adopting reason maintenance inparsing (more precisely, story analysis) is the systemRESUND (O'Rorke 1983).
O'Rorke considers theproblem of correcting inferences 4 that conflict withsubsequent information in the story.
He combinesthe story processor with a JTMS, using dependency-directed backtracking to determine incompatible as-sumptions, and a collection of preference heuristicsto choose from among candidate solutions.
53Sense-semantic interpretation; possibly also contextualinterpretation.4 An inference here corresponds to a selection (assumption)of a schema, a script-like knowledge structure used for deriv-ing information ot explicitly mentioned in the text.~Other work, t tmugh not adopting downright JTMSs,makes use of somewhat similar techniques for the purposeof recovering from erroneous inferences.
For example, Jacobs(1988), in dealing with the problem of "concretion" - -  devel-oping a most  specific or metaphorical  interpretation - - lets hissystem, TRUMP,  treat each interpretation as an assumption.If an assumed interpretation results in a contradiction, depen-dencies are used to discard (chains of) assumpt ions that con-flict with the preferred interpretation.
Conflicting informationis simply thrown away, ~hus in a sense making the system evenMore recently, Zernik and Brown (1988) suggesta coupling of a DCG parser with a JTMS, both ofwhich are embedded in a Prolog system.
The DCGis extended with default rules to enable nonmono-tonic reasoning.
These defaults are used to guidethe entire parsing and interpretation process.
Thedivision of labour is as follows: Given a new pieceof input, the parser outputs an (extended) analysisin the form of a dependency network.
This, in turn,is fed to the JTMS, resulting in an updated in~outlabelling of the network corresponding to the cur-rently believed interpretation.
The purpose of theJTMS is to obtain a system which avoids (chronolog-ical) backtracking and instead handles inconsistentinformation (ambiguities) by choosing to believe adifferent portion of the previous inferences.
For ex-ample, in parsing a sentence like "The child sold byhis parents was found alive" (cf.
Zernik and Brown1988:802), the system initially assumes "the child"to be the agent since the "default voice" is consideredto be "active".
When later the word "by" is parsed,a nonmonotonic supporter of "active" becomes in,thus making "active" become out and "the child" beconsidered irect object.2.2 P rob lemsThis section dis'cusses ome general problems ofJTMSs and JTMS-style approaches to incrementMparsing with special reference to the framework ofZernik and Brown (1988) whose notion of parsingcomes closest o the one considered here.Perhaps the most important characteristic of aJTMS is that it insists on global consistency; in otherwords, it is limited to one single solution (one con-text) at a time:"At each point \[presumably after each newword\], the parser must deposit a hypothesisbased on a partial set of clues, a hypothesiswhich might later be retracted.
"(Zernik and Brown 1986:802.
)Unfortunately, in a domain like natural-languageanalysis where local ambiguity constantly plaguesthe parser with inconsistent information, this be-comes problematic.
First of all, when the set of as-sumptions admits multiple solutions, these cannotbe compared: since a JTMS only allows one con-text, there is simply no way to examine two sets ofbeliefs simultaneously in order to gauge their rela-tive strengths.Furthermore, upon each incremental change (i.e.,parsing of a new word), new JTMS labellings havemore insistent on consistency than  a JTMS.
Story processorslike ARTHUR (Granger 1980), FAUSTUS (Norvig 1983), andATLAST (Eiselt 1987) keep track of successive (candidate)inferences and reconsider ejected ones when faced with con-flicting information.
It could finally be ment ioned that in thepostscript o his book, Mellish (1985:114) suggests a combina-tion of chart parsing and JTMS, but  does not further developtiffs.288 2to be comlmted tbr the network.
If, in the com'seof this, a contradiction arises - tile probability ofwhich increases with the size of the grammar, i.e.,with the number of (l)otentially competing) defaultrules dependency-directed backtracking h~s tobe invoked to identify the sources of the contradic-tion and resolve the conllict.
This requires exten-sive search and often results in new contradictions.Until all conflicts are resolved, the status of somenodes may have changed between in and out sev-eral times.
'l?hus, "tile machinery is cumbersome"(de Kleer 1986:139)fiA further problern is that the irfl'erence engineworks on only one part of the search space at a time.For example, if a word has two senses, only one ofthem will be worked on by the parser.
But all thatis known in such a ca,se is that both senses cannotbe part of the same final solution; it may still be im-portant o draw int~rences Dora them independently.For further discussion of these and other problems inconnection with JTMSs, see de Kleer (1986:138 ft.).3 ATMS-Style Approaches3.1  Overv iewCharniak and Gohlman (1988) make use of an NI)MSfor keeping track of alternatives arising in sense-semantic and co,ltexl;ual interpretation, for example,wil.h respect to word sense, case, and noun-phrasereference.
Each alIernative is treated as an ATMSassun~l)tion and is fnr~hermore assigned a pro/m-bility.
By comparing segs of assumpl.ioi~s underly-ing various potential interpreta.lions, the system canchoose the "best" alternative, i.e., the one with thehighest probability.Nagao (1989) provides an approach where ditfer-ent assumptions about sentence interpretations con-stitute different "worlds" of formulae in a way whichresembles an ATMS representation.
A character-istic of these fra.meworks i  that they only handlesemantic-interpretation alternatiw',s and do not (at-tempt to) integrate this with parsing.A different kind of Ni'MS--style approach, and onethat is grounded in parsing, can be obtained byextending a chart parser with dependencies; morespecifically, by recording for each edge its immedi-ate and ultimate source edges (Wirdn 1989).
Thenext section develops this.6IncidentMly, the sole explicit example provkted by Zernikand Brown (eited above) ouly involves one default rule.
Sincethe original as well as the revised interpretation representcoherent, sets of justified beliefs, both c~m be arrived at bystraightforward (re)labelling, titus avoiding the more cumbm~some process of dependency-directed backtracking - -  in fact,~ernik and Brown do not mention dependency-directed back-tracking at all.
They also do no| state any systematic pref-erence policy, so it is not clear how they generally gauge therelative strengths of incompa/.ible assumptions when trying toresolve contradictions.3.2 An  ATMS-Sty le  Char t  ParsertIow can a chart parser extended with edge depen-.dencies be viewed as consisting of an inference nginealld a (simplified) ATMS?
This section develops aninfonnal answer to this question.
'llb begin with, an ATMS, just like a chart parser,can be seen as a tool for organizing efficient searchthrough a space of alternatives by providing a"cache" of partial results and by sharing these resultsacross different branches of the search space.
Boththe (I);-usic) A'I?MS and a chart parser are inonotonicin the sense of only providing monotonic derivability(and, in case of the ATMS, monotonic justifications).Both frameworks are incremental, performing piece-meal updates in response to a constant stream ofnew assumptions, nodes, and justifications.
Furfller-more, the order in which updates are made does notaffect tile final outcome of the process.
In particular,the following correspondences hold:?
A chart edge corresponds to an NI'MS 7~ode.. A preterminal (lexical) edge corresponds to anassumption ode)'?
The immediate source information of an edgecorresponds t.o a justification.
* Information about the set (of sets) of ultimatesource edges of an edge corresponds toits ATMSlabel..
The chart corresponds to an ATMS network.. An analysis (or interpretation) of a phr~e, sen-tence, etc.
corresponds to an ATMS context, i.e.,the theory of an environment, where the latteris a set of assumptions.?
The (standard) chart-parsing algorithm corre-sponds to the inference ngine.More precisely, information about so,tee edges (i.e.,justifications) can be derived as follows: An edgeformed through a combination depends on theactive--inactive edge pair that generated it.
An edgeformed through a prediction depends on the (one)edge that triggered i t .
(Alternatively, predictededges can be left out of the dependency trails al-together: since they only represent inferential hy-potheses and do not carry any analysis structure,they could be seen ~s belonging with the inferenceengine rather than with the ATMS.
On this view,a prediction has neither dependants nor sources, )A scanned edge does not depend upon any other7Thus, each word sense corresponds to an assumption.
Ina system whidl haaldles noisy input (e.g., ill-formed or spo-ken input), one might instead let hypothesized word formscorrespond to assmnptions.8This would also soNe the.
problenl with top-down parsingpointed out in Wirdn (1989:245 f.).
--- Note that, if we wantto introduce gm'bage collect.ion of useless predictions, it wouldstill be necessary to Imep a record of their dependencies.3 289edge (but on an instance of a word or a lexicalizedphrase).Labels are likely to be simple in an ATMS-stylechart parser.
Normally, each edge has a unique, sta-ble set of sources ~- zero, one, or two edges whichare determined once and for all when the edge iscreated) A potential exception to this uniquenessof source is the case of the parser attempting to re-generate an (existing) edge, something which is pro-hibited by a redundancy test.
This attempted regen-eration actually corresponds to introducing an addi-tional justification for the edge.
Allowing this wouldrequire a more elaborate machinery for computingATMS labellings in accordance with de Kleer (1986).Whether this capability is needed or not would haveto be decided with respect to the demands of theparticular application of the ATMS-style parser (of.section 4).It could finally be noted that a chart parser, ~snormally conceived of, does not record "nogoods"(inconsistent combinations of assumptions) aus doesthe ATMS.
Instead, the chart parser by itself en-sures that inconsistent combinations of edges donot get further worked on (through predictions, theagenda, etc.
).4 Applications4.1 Incrementa l  Pars ingand  In terpretat ionA strong definition of incremental parsing would re-quire that text can be added or deleted in a piece-meal fashion and, furthermore, that the system be-haves monotonically in processing such updates; i.e.,as words are added/deleted, the set of possible analy-ses increases/decreases monotonically.
An attractiveproperty which follows from this is that the amountof processing of an update is roughly proportionalto the size of the update (eft Wirdn 19.89:2,i2, Ear-ley and Caizergues 1.972:1040).
An ordinary chartparser as well as an ATMS-style chart parser as putforward above are incremental in this sense, whereasthe nonlnonotonic model of Zernik attd Brown (1988)attd, say, an ATN parser are not.
In the latter frame-works, a previously determined analysis can be mod-ified by a subsequent step in the parsing process, t?As for interpretation with respect o a model ora context, in order for a system to be incremen-tal in the above sense, it should be compositional(and again monotonic) such that the interpretation9In bidirectionM chart paa'sing (Satta and Stock 1989) anedge might have three so lwces .l?One might ask which granunatical formalisms enable in-cremental processing.
Cai, egorial gratmnoa- in its variousincantations i an obvious alternative (for example, Adesand Steedman 1982).
Unification-based grammar formalisms(Shieber 1986) provide another alternative given that the rati-fication component is capable of incremental processing (Bres-nml and Kaplan 1982:xliv ft., Stee~:hnan 1985).of a phrase is a flmction of the interpretations ofits syntactic constituents and their associated con-texts.
One computationally-oriented model whichflflfils this requirement, and which is indeed tayloredfor incremental interpretation, is that of Haddock(1987, 1988, 199(I).
Haddock's model, which canbe seen as a continuation and refinement of Mel-lish (1985), incrementally interprets ingular noun-phrases which refer to known contextual entities.Translated into a chart-parsing framework, uponscanning a word, the corresponding set of predicatesand potential referents (obtained from the context)is associated with the new edge, and upon combin-ing edges, a constraint-satisfaction algorithm is runto narrow down the set of possible referents.The possibility of using dependencies also to incre-mentally handle deletions of words (assumptions) isinvestigated in Wirdn (1989).
Actually, a machineryis developed to handle arbitrary sgutactic changes ,which can be thought of as edit operations (insertion,deletion, replacement), and which can be fl'eely com-bined and applied to arbitrary portions of the text- -  for example, input can be entered in any order.Edge dependencies are used to propagate the effectsof a change precisely to those edges that are affectedby the change.
II This has potential computationalapplications in interactive na.turaldanguage systemssuch as language-sensitive text editing.
In psycholin-guistic terms, an edit operation might correspond tocorrection of a misread or misheard passage.Since IIaddock's model for incremental interpreta-tion is not in any way limited to left-to-right incre-mentality, it is possible to adopt it also within thesystem of Wir~;.n (1989).
Furthermore, it is possibleto conceive of a semantic analogue to this process-ing of syntactic changes.
Consider a dynamic con-text, for example a database representing the real-time state of some world.
By maintaining depen-dencies between entities and relations in the contex-tual model and their counterparts in the linguisticanalysis, a machinery for incrementally reevaluatingpreviously made interpretations with respect o thechanging context could be attained.4 .2  Compar i son  o f  In terpretat ionsThe chart allows comparison of (competing) inter-pretations ill the sense that any analyses can be si-multaneously examined.
WhaZ one cannot do in or-dinary chart parsing is to ~k  which particular edges(assumptions, etc.)
underlie a given analysis sinceedges are not labelled with justifications or assump-tions (and all information from lower-level edges maynot have been percolated upwards).
Put differently,the chart cannot "explain" its analyses.
Of course,extending a chart parser to record dependencies ia simple thing.
The point is that, in doing so, onehas in effect obtained a simple KI'MS-style problem11 This could also be achieved in a JTMS-style parser.2904solver.
Charniak and Goldman (1988) provide anexample of how ATMS techniques could be used forcomparisons (eft section a.1).4.3 Rev is ion  o f  In terpretat ionsThe basic ATMS does not provide nonmonotonicjustifications and hence no machinery for actuallyrevising a previously held interpretation through de-fault reasoning, etc.
(Similar effects are insteadachieved by maintaining multil)le contexts and byswitching between these.)
However, certain seman-tic and pragmatic phenomena, like anaphoric refer-ence, ,~eem to require a capability tbr default reason-ing; processing such phenomena by gradually devel-oping all possible analyses wouht lead to combina~torial explosion (Asher 1984).
'\]~hus, although non-monotonic devices destroy the strong notion of incre-mentality discussed above (the effects of an updatecannot hr general be kept local, the amount of com-pntation needed is not bounded by the size of theupdate, etc.
), they are sometimes needed.
A recentexample of this is Pollack and Pereira (1988) whouse a strict compositional semantics but.
with non-monotonic interpretation rules; other examples areAsher (1984), Dunin-Keplicz (1984), and Appelt andKonolige (1988).Dre,~sler (1989) shows how to extend the basicArMS with a new type of node which, in particular,allows the encoding of nonmonotonic justificationsand default rules.
Given the relationship betweenchart parsing and ATMS, it seems like there shouldbe a way of translating such a default machineryto an ATMS-style parsing framework.
Furthermore,given a framework which integrates parsing and in-terpretation by performing interpretation on-line tothe parser, it might be advantageous to allow encod-ing of default reasoning at the level of the parser, asindeed Zernik artd Brown do, but to use it in a nmchmore restricted way.5 Conc lus ionThis paper compares JTMS- and ATMS-based ap-proaches to adopting reason-maintenance techniquesin incremental parsing (arid interpretation).
A ma-jor problem with the JTMS-based approach is that itis fundamentally imited to representing one context(one hypothesized interpretation).
One consequenceof this is that competing interpretations cannot becompared.
Another consequence is that the systemis constantly forced to commit rash choices and tospend a large part of its time revising these choiceswhen faced with couflicging information.
This sit-uation appears even worse in light of the relativeinefficiency of JTMSs.
Zernik arm Brown (1988) ina sen,m take this approach to its extreme, using de~fault reasoning and dependency-directed backtrack-ing to guide the entire parsing and interpretationprocess.
It is however not clear why one would wantto confine oneself to a single hypothesi,; at each stepof this process, lp" \[t is also not clear that defaultchoices and dependency-directed backtracking is thebest way to guide se~rch in natural-language analysiswith respect o phenomena like syntactic alnbiguity.From a computational point of view, there are well-known and ell'icient teehniques, such as chart pars-ing, that work by developing syntactic alter,retiresin parallel.
Dora a psycholinguistic point of view, al-though the issue is under discussion, there are boththeoretical arguments and empirical evidence sup-porting the claim thai, alternative interpretations areexplored in parallel (Crain and Steedman 1985).
In-cidentally, one of the rationales for this c\]aim is thatit is a prerequisite for eoml)eting interpretations l.obe compared.In contr~st o this, an ATMS solves or circum-vents the major problems posed by a J'I'MS.
TheATMS supports eificient development and coml)ari-son of all legal (possibly contradictory) analyses; anATMS-style parser thus seems more in accordancewith computational practice as well as with psy-cholinguistic evidence.
Furthermore, the relation-ship between NI'MSs and chart parsing is in itself anadvantage, because it facilitates cross-fertilization fthe respective subtlelds.
In particular, it might bepossible to make use of recertt advances within rea-son maintenance in encoding defatflt reasoning forthe purpose of handling certain I)roblems in seman-tic and pragnmtic interpretation.
Ultimately, thismight provide for a more unitbrm handling of syl>tax, semantics, and l)ragmatics.ReferencesAdes, Anthony E. and Mark J. Steedman (1982).
On theOrder of Words.
Linguistics and Philosophy 4:517-558.Appelt, Douglas and Kurt Konolige (1988).
A Practical No~l-monotonic Theory for Reasoning about Speech Acts.
lZ,'oc.26th.
Annum Meeting o\] the Association \]or ComputationalLinguistic.s, Buffalo, New York: 170-178.Asher, Nicholas (1984).
Linguistic Understanding and Non-Monotonic Heasoning.
P'roc.
Non-\]~,Ionotonic tteaaonin9Workshop, New Pahz, New York: 1-20.Bobrow, Hobert J. and Bomfie Lyre1 Webbcr (1980).Knowledge Representation for Syntactic/Semantic Process-ing.
Proc.
First Annual National Conference on ArtificialIntelligence, Stanford, California: 316- 323.Bresnan, .Ioaa~ and HonMd M. Kaplan (1982).
Introduction:Gramma,~ as Mental Bepresentatlons of Language.
In: JoanBresnan, ed., The Mental Representation o\] Grammatical l~e-lations.
MIT Press, Cambridge, Massachusetts: xvii-lii.Cha*zfiak, Eugene and B.obert Goldman (1988).
A Logicfor Semmltic Interpretation.
Proe.
26th Annual Meeting ofthe Association \]or Computational Linguistics, Buffalo, NewYork: 87-94.Crain, Stephen and Mark Steedmml (1985).
On Not Be-ing Led up the G;u'den Path: The Use of Context by thePsychological Syntax Processor.
In: David R. Dowty, Lam'i12Note that Zernik and Brown also do not make use of look-ahead or delayed processing to facilitate intelligent choices.5 291Karttunen, and Arnold M. Zwicky, eds., Natural LanguageParsing.
Psychological, Computational, and Theoretical Per-spectives.
Cambridge Univel~ity Press, Cambridge, England:320-358.de Kleer, Johan (1986).
An Assumptlon-based TMS.
Artifi-cial Intelligence 28(2):127-162.Doyle, ion (1979).
A Truth Malntcnance System.
ArtificialIntelligence 12(3):231--272.Dressier, Oskar (1989).
An Extended Basic ATMS.
In:Michael Reinfrank, Johan de Kleer, Matthew L. Ginsberg,and Erik Sandewall, eds., Proc.
2nd International Workshopon Non.Monotonlc Reasoning.
Springer Lecture Notes inComputer Science 346, Heidelberg, FRG: 143-163.Dunin-Kepllcz, Barbara (1984).
Default Reasoning inAnaphora Resolution.
Proc.
Sixth European Conference onArtificial Intelligence, Pisa, Italy: 157-166.Earley, .lay and Paul Caizergues (1972).
A Method for Incre-mentally Compiling Languages with Nested Statement Struc-ture.
Communications of the A CM 15(12):1040--1044.Eisclt, Kurt P. (1987).
Recovering from Erroneous Inferences.Proe.
Sixth National Conference on Artificial Intelligence,Seattle, Washington: 540-544.Granger, Richaxd H., Jr. (1980).
When Expectation Fails:Towards a Self-Correcting Inference System.
Proe.
First An-nual National Conference on Artificial Intelligence, Stanford,California: 301-305.Haddock, Nicholas J.
(I987).
InerementM Interpretationand Combinatory Categorial Grammar.
Proc.
Tenth Inter-national Joint Conference on Artificial Intelligence, Milan,Italy: 661-663.Haddock, Nicholas J.
(1988).
Incremental Semaaltics and In-tcractlve Syntactic Processing.
Ph.D. thesis, Department ofArtificial Intelligence, University of Edinburgh, Edinburgh,Scotland.Haddock, Nicholas J.
(1990).
Computational Models of \[n-crementM Sem~ltic Interpretation.
Language and CognitiveProcesses 4(3-4):337--368.Jacobs, Paul S. (1988).
Concretion: Assumption-Based Un-derstanding.
Proc.
12th International Conference on Com-putational Linguistics, Budapest, Itungaxy: 270-274.Kay, Martin (1980).
Algorittm~ Schemata nd Data Struc-tures in Syntactic Processing.
I~eport CSL-80-12, XeroxI)ARC, Polo Alto, California.
Also in: Sture All~n, ed.
(1982), Text Processing.
Proceedings of Nobel Symposium 51.Almqvist & Wiksell International, Stockhohn, Sweden: 327-358.Kobsa, Alfred, Jfirgen Allgayer, Carola Reddig, Norbert Re-ithinger, Dagmar Schmauks, Karin Harbusch, and WoffgangWatdster (1986).
Combining Deictic Gestures and Nattu'alLanguage for Referent Identification.
Proc.
11th Interna-tional Conference on Computational Linguistics, Bonn, Fed-eral Republic of Germany: 356-361.Marslen-Wilson, William and Lorraine Tyler (1980).
TheTemporal Structure of Spoken Language Understanding.Cognition 8(1):1-74.Mellish, Christopher S. (1985).
Computer Interpretation ofNatural Language Descriptions.
Ellis tlorwood, Chichester,England.Natal ,  Katashi (1989).
Semantic Interpretation Based on theMulti-World Model.
Proc.
Eleventh International Joint Con-ference on Artificial Intelligence, Detroit, Michigan: 1467-1473.Norvig, Peter (1983).
Six Problems for Story Understanders.Proe.
Third National Conference on Artificial Intelligence,Washington, D.C.: 284-287.O'Rorke, Paul (1983).
Reaaons for Beliefs in Understanding:Applications of Non-Monotonic Dependencies to Story Pro-cessing.
Proe.
Third National Conference on Artificial Intel-ligence, Washington, D.C.: 306-309.Pulman, Steven G. (1985).
A Pal~er That Doesn't.
Proc.Second Conference of the European Chapter of the Associ-ation for Computational Linguistics, Geneva, Switzerland:128-135.Pollack, Martha E. and Feruando C. N. Pereira (1988).
AnIntegrated Framework for Semantic and Pragmatic Interpre-tation.
Proc.
26th Annual Meeting of the Association forComputational Linguistics, Buffalo, New York: 75-86.Reinfrank, Michael (1989).
Lecture Notes on the Fundamen-tals of Truth Maintenance.
Siemens Report INF 2 ARM-5-88,Version 2.
Siemens AG, Munich, FRG.Reps, Thomas and Tim Teitelbaum (1984).
The Synthe-sizer Generator.
Proc.
A CM SIGSoft/SIGPlan Symposiumon Practical Programming Environments, Pittsburgh, Penn-sylvania: 42-48.Reps, Thomas and Tim Teitelbaum (1987).
Language Pro-cessing in Program Editors.
Computer 20(11):29-40.Satta, Giorgio and Oliviero Stock (1989).
Formal Propertiesand Implementation f Bidirectional Charts.
Proc.
EleventhInternational Joint Conference on Artificial Intelligence, De-trolt, Michigan: 1480-1485.Shieber, Stuart M. (1986).
An Introduction to Unification.Based Approaches to Grammar.
CSLI Lecture Notes No.
4.University of Chicago Press, Chicago, Illinois.Steedman, Mark (1985).
LFG and Psychological Explanation.Linguistics and Philosophy 8:359~385.Thompson, Henry and Graeme Ritchie (1984).
ImplementingNatm'al Language Parsers.
In: Tim O'Shea and Marc Eisen-stadt, Artificial Intelligence: Tools, Techniques, and Applica-tions.
Harper & Row, New York, New York: 245-300.Tyler, LolTaine and William Marslen-Wilson (1977).
TheOn-line Effects of Semantic Context on Syntactic Processiug.Journal of Verbal Learning and Verbal Behavior 16:683-692.Wirdn, Mats (1989).
Interactive Incremental Chart Parsing.Proc.
Fourth Conference of the European Chapter of the As-sociation for Computational Linguistics, Manchester, Eng-land: 241-248.
Also: Research report LiTH-IDA-R-89-24,Department of Computer and Infomaaation Science, Link~SplngUnivemity, Link&ping, Sweden.Zernik, Uri and Allen Brown (1988).
Default Reasolfing inNatural Language Processing.
Proc.
l~th International Con-\]erence on Computational Linguistics, Budapest, Hungary:801-805.2926
