Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 194?204,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsIdiom Token Classification using Sentential Distributed SemanticsGiancarlo D. Salton and Robert J. Ross and John D. KelleherApplied Intelligence Research CentreSchool of ComputingDublin Institute of TechnologyIrelandgiancarlo.salton@mydit.ie {robert.ross,john.d.kelleher}@dit.ieAbstractIdiom token classification is the task ofdeciding for a set of potentially idiomaticphrases whether each occurrence of aphrase is a literal or idiomatic usage ofthe phrase.
In this work we explore theuse of Skip-Thought Vectors to create dis-tributed representations that encode fea-tures that are predictive with respect to id-iom token classification.
We show thatclassifiers using these representations havecompetitive performance compared withthe state of the art in idiom token classifi-cation.
Importantly, however, our modelsuse only the sentence containing the tar-get phrase as input and are thus less de-pendent on a potentially inaccurate or in-complete model of discourse context.
Wefurther demonstrate the feasibility of usingthese representations to train a competitivegeneral idiom token classifier.1 IntroductionIdioms are a class of multiword expressions(MWEs) whose meaning cannot be derived fromtheir individual constituents (Sporleder et al,2010).
Idioms often present idiosyncratic be-haviour such as violating selection restrictions orchanging the default semantic roles of syntac-tic categories (Sporleder and Li, 2009).
Conse-quently, they present many challenges for Natu-ral Language Processing (NLP) systems.
For ex-ample, in Statistical Machine Translation (SMT)it has been shown that translations of sentencescontaining idioms receive lower scores than trans-lations of sentences that do not contain idioms(Salton et al, 2014).Idioms are pervasive across almost all lan-guages and text genres and as a result broad cov-erage NLP systems must explicitly handle idioms(Villavicencio et al, 2005).
A complicating factor,however, is that many idiomatic expressions canbe used both literally or figuratively.
In general,idiomatic usages are more frequent, but for someexpressions the literal meaning may be more com-mon (Li and Sporleder, 2010a).
As a result, thereare two fundamental tasks in NLP idiom process-ing: idiom type classification is the task of identi-fying expressions that have possible idiomatic in-terpretations and idiom token classification is thetask of distinguishing between idiomatic and lit-eral usages of potentially idiomatic phrases (Fazlyet al, 2009).
In this paper we focus on this secondtask, idiom token classification.Previous work on idiom token classification,such as (Sporleder and Li, 2009) and (Peng etal., 2014), often frame the problem in terms ofmodelling the global lexical context.
For exam-ple, these models try to capture the fact that the id-iomatic expression break the ice is likely to have aliteral meaning in a context containing words suchas cold, frozen or water and an idiomatic meaningin a context containing words such as meet or dis-cuss (Li and Sporleder, 2010a).
Frequently theseglobal lexical models create a different idiom to-ken classifier for each phrase.
However, a numberof papers on idiom type and token classificationhave pointed to a range of other features that couldbe useful for idiom token classification; includinglocal syntactic and lexical patterns (Fazly et al,2009) and cue words (Li and Sporleder, 2010a).However, in most cases these non-global featuresare specific to a particular phrase.
So a key chal-lenge is to identify from a range of features whichfeatures are the correct features to use for idiomtoken classification for a specific expression.Meanwhile, in recent years there has been anexplosion in the use of neural networks for learn-ing distributed representations for language (e.g.,194Socher et al (2013), Kalchbrenner et al (2014)and Kim (2014)).
These representations are au-tomatically trained from data and can simultane-ously encode multiple linguistics features.
For ex-ample, word embeddings can encode gender dis-tinctions and plural-singular distinctions (Mikolovet al, 2013b) and the representations generated insequence to sequence mappings have been shownto be sensitive to word order (Sutskever et al,2014).
The recent development of Skip-ThoughtVectors (or Sent2Vec) (Kiros et al, 2015) has pro-vided an approach to learn distributed representa-tions of sentences in an unsupervised manner.In this paper we explore whether the repre-sentations generated by Sent2Vec encodes fea-tures that are useful for idiom token classification.This question is particularly interesting becausethe Sent2Vec based models only use the sentencecontaining the phrase as input whereas the base-lines systems use full the paragraph surroundingthe sentence.
We further investigate the construc-tion of a ?general?
classifier that can predict if asentence contains literal or idiomatic language (in-dependent of the expression) using just the dis-tributed representation of the sentence.
This ap-proach contrasts with previous work that has pri-marily adopted a ?per expression?
classifier ap-proach and has been based on more elaborate con-text features, such as discourse and lexical cohe-sion between and sentence and the larger context.We show that our method needs less contextualinformation than the state-of-the-art method andachieves competitive results, making it an impor-tant contribution to a range of applications that donot have access to a full discourse context.
Weproceed by introducing that previous work in moredetail.2 Previous WorkOne of the earliest works on idiom token classi-fication was on Japanese idioms (Hashimoto andKawahara, 2008).
This work used a set of features,commonly used in Word Sense Disambiguation(WSD) research, that were defined over the textsurrounding a phrase, as well as a number of idiomspecific features, which were in turn used to trainan SVM classifier based on a corpus of sentencestagged as either containing an idiomatic usage ora literal usage of a phrase.
Their results indicatedthat the WSD features worked well on idiom tokenclassification but that their idioms specific featuresdid not help on the task.Focusing on idiom token classification in En-glish, Fazly et al (2009) developed the concept ofa canonical form (defined in terms of local syn-tactic and lexical patterns) and argued that foreach idiom there is a distinct canonical form (orsmall set of forms) that mark idiomatic usagesof a phrase.
Meanwhile Sporleder and Li (2009)proposed a model based on how strongly an ex-pression is linked to the overall cohesive structureof the discourse.
Strong links result in a literalclassification, otherwise an idiomatic classifica-tion is returned.
In related work, Li and Sporleder(2010a) experimented with a range of featuresfor idiom token classification models, including:global lexical context, discourse cohesion, syntac-tic structures based on dependency parsing, andlocal lexical features such as cue words, occurringjust before or after a phrase.
An example of a locallexical feature is when the word between occursdirectly after break the ice; here this could markan idiomatic usage of the phrase: it helped to breakthe ice between Joe and Olivia.
The results of thiswork indicated that features based on global lex-ical context and discourse cohesion were the bestfeatures to use for idiom token classification.
Theinclusion of syntactic structures in the feature setprovided a boost to the performance of the modeltrained on global lexical context and discourse co-hesion.
Interestingly, unlike the majority of pre-vious work on idiom token classification Li andSporleder (2010a) also investigated building gen-eral models that could work across multiple ex-pressions.
Again they found that global lexicalcontext and discourse cohesion were the best fea-tures in their experiments.Continuing work on this topic, Li and Sporleder(2010b) present research based on the assumptionthat literal and figurative language are generatedby two different Gaussians.
The model represen-tation is based on semantic relatedness featuressimilar to those used earlier in (Sporleder and Li,2009).
A Gaussian Mixture Model was trained us-ing an Expectation Maximization method with theclassification of instances performed by choosingthe category which maximises the probability offitting either of the Gaussian components.
Li andSporleder (2010b)?s results confirmed the findingsfrom previous work that figurative language ex-hibits less cohesion with the surrounding contextthen literal language.195More recently, Feldman and Peng (2013) de-scribes an approach to idiom token identificationthat frames the problem as one of outlier detec-tion.
The intuition behind this work is that be-cause idiomatic usages of phrases have weak co-hesion with the surrounding context they are se-mantically distant from local topics.
As a result,phrases that are semantic outliers with respect tothe context are likely to be idioms.
Feldman andPeng (2013) explore two different approaches tooutlier detection based on principle componentanalysis (PCA) and linear discriminant analysis(LDA) respectively.
Building on this work, Penget al (2014) assume that phrases within a giventext segment (e.g., a paragraph) that are seman-tically similar to the main topic of discussion inthe segment are likely to be literal usages.
Theyuse Latent Dirichlet Allocation (LDA) (Blei et al,2003) to extract a topic representation, defined as atopic term document matrix, of each text segmentwithin a corpus.
They then trained a number ofmodels that classify a phrase in a given text seg-ment as a literal or idiomatic usage by using thetopic term document matrix to project the phraseinto a topic space representation and label outlierswithin the topic space as idiomatic.
To the best ofour knowledge, Peng et al (2014) is currently thebest performing approach to idiom token classifi-cation and we use their models as our baseline1.3 Skip-Thought VectorsWhile idiom token classification based on longrange contexts, such as is explored in a numberof the models outlined in the previous section,generally achieve good performance, an NLP sys-tem may not always have access to the surround-ing context, or may indeed find it challenging toconstruct a reliable interpretation of that context.Moreover, the construction of classifiers for eachindividual idiom case is resource intensive, and weargue fails to easily scale to under-resourced lan-guages.
In light of this, in our work we are ex-ploring the potential of distributed compositionalsemantic models to produce reliable estimates ofidiom token classification.Skip-Thought Vectors (Sent2Vec) (Kiros et al,1However, it is not possible for us to reproduce their re-sults directly as they ?apply the (modified) Google stop listbefore extracting the topics?
(Peng et al, 2014, p. 2023) and,to date, we do not have access to the modified list.
So inour experiments we compare our results with the results theyreport on the same data.2015) are a recent prominent example of suchdistributed models.
Skip-Thought Vectors are anapplication of the Encoder/Decoder framework(Sutskever et al, 2014), a popular architecture forNMT (Bahdanau et al, 2015) based on recurrentneural networks (RNN).
The encoder takes an in-put sentence and maps it into a distributed repre-sentation (a vector of real numbers).
The decoderis a language model that is conditioned on the dis-tributed representation and, in Sent2Vec, is used to?predict?
the sentences surrounding the input sen-tence.
Consequently, the Sent2Vec encoder learns(among other things) to encode information aboutthe context of an input sentence without the needof explicit access to it.
Figure 1 presents the archi-tecture of Sent2Vec.More formally, assume a given tuple (si?1, si,si+1) where siis the input sentence, si?1is theprevious sentence to siand si+1is the next sen-tence to si.
Let wtidenote the t-th word for siandxtidenote its word embedding.
We follow Kiroset al (2015) and describe the model in three parts:encoder, decoder and objective function.Encoder.
Given the sentence siof length N ,let w1i, .
.
.
, wNidenote the words in si.
At eachtimestep t, the encoder (in this case an RNN withGated Recurrent Units - GRUs (Cho et al, 2014))produces a hidden state htithat represents the se-quence w1i, .
.
.
, wti.
Therefore, hNirepresents thefull sentence.
Each hNiis produced by iteratingthe following equations (without the subscript i):rt= ?
(Werxt+Uerht?1) (1)zt= ?
(Wezxt+Uezht?1) (2)?ht= tanh(Wext+Ue(rtht?1)) (3)ht= (1?
zt) ht?1+ zt?ht(4)where rtis the reset gate, ztis the update gate,?htis the proposed update state at time t anddenotes a component-wise product.Decoder.
The decoder is essentially a neurallanguage model conditioned on the input sentencerepresentation hNi.
However, two RNNs are used(one for the sentence si?1and the other for thesentence si+1) with different parameters exceptthe embedding matrix (E), and a new set of ma-trices (Cr, Czand C) are introduced to conditionthe GRU on hNi.
Let hti+1denote the hidden stateof the decoder of the sentence si+1at time t. De-196Figure 1: Picture representing the Encoder/Decoder architecture used in the Sent2Vec as shown in Kiroset al (2015).
The gray circles represent the Encoder unfolded in time, the red and the green circlesrepresent the Decoder for the previous and the next sentences respectively also unfolded in time.
In thisexample, the input sentence presented to the Encoder is I could see the cat on the steps.
The previoussentence is I got back home and the next sentence is This was strange.
Unattached arrows are connectedto the encoder output (which is the last gray circle).coding si+1requires iterating the following equa-tions:rt= ?
(Wdrxt+Udrht?1+CrhNi) (5)zt= ?
(Wdzxt+Udzht?1+CzhNi) (6)?ht= tanh(Wdxt+Ud(rtht?1) +ChNi)(7)hti+1= (1?
zt) ht?1+ zt?ht(8)where rtis the reset gate, ztis the update gate,?htis the proposed update state at time t anddenotes a component-wise product.
An analogouscomputation is required to decode si?1.Given hti+1, the probability of the word wti+1conditioned on the previous w<ti+1words and theencoded representation produced by the encoder(hNi) is:P (wti+1|w<ti+1, hNi) ?
exp(Ewti+1hti+1) (9)where Ewti+1denotes the embedding for the wordwti+1.
An analogous computation is performed tofind the probability of si?1.Objective.
Given the tuple (si?1, si, si+1),the objective is to optimize the sum of the log-probabilities of the next (si+1) and previous(si?1) sentences given the distributed representa-tion (hNi) of si:?logP (wti+1|w<ti+1, hNi) + P (wti?1|w<ti?1, hNi)(10)where the total objective is summed over all train-ing tuples (si?1, si, si+1).The utility of Sent2Vec is that it is possible toinfer properties of the surrounding context onlyfrom the input sentence.
Therefore, we can as-sume that the Sent2Vec distributed representationis also carrying information regarding its context(without the need to explicitly access it).
Follow-ing that intuition, we can train a supervised clas-sifier only using the labelled sentences containingexamples of idiomatic or literal language use with-out modelling long windows of context or usingmethods to extract topic representations.4 ExperimentsIn the following we describe a study that eval-uates the predictiveness of the distributed repre-sentations generated by Sent2Vec for idiom tokenclassifier.
We first evaluate these representationsusing a ?per expression?
study design (i.e., oneclassifier per expression) and compare our resultsto those of Peng et al (2014) who applied multi-paragraphs contexts to generate best results.
Wealso experiment with a ?general?
classifier trainedand tested on a set of mixed expressions.4.1 DatasetIn order to make our results comparable with(Peng et al, 2014) we used the same VNC-Tokensdataset (Cook et al, 2008) that they used in theirexperiments.
The dataset used is a collection ofsentences containing 53 different Verb Noun Con-structions2(VNCs) extracted from the British Na-tional Corpus (BNC) (Burnard, 2007).
In total,the VNC-Token dataset has 2984 sentences whereeach sample sentence is labelled with one of threelabels: I (idiomatic); L (literal); or Q (unknown).2This verb-noun constructions can be used either idiomat-ically or literally.197Of the 56 VNCs in the dataset 28 of these expres-sions have a reasonably balanced representation(with similar numbers of idiomatic and literal oc-currences in the corpus) and the other 28 expres-sions have a skewed representation (with one classmuch more common then the other).
Followingthe approach taken by (Peng et al, 2014), in thisstudy we use the ?balanced?
part of the datasetand considered only those sentences labelled as?I?
and ?L?
(1205 sentences - 749 labelled as ?I?and 456 labelled as ?L?
).Peng et al (2014) reported the precision,recall and f1-score of their models on 4 ofthe expressions from the balanced section ofdataset: BlowWhistle; MakeScene; LoseHead;and TakeHeart.
So, our first experiment is de-signed to compare our models with these baselinesystems on a ?per-expression?
basis.
For this ex-periment we built a training and test set for each ofthese expressions by randomly sampling expres-sions following the same distributions presentedin Peng et al (2014).
In Table 1 we present thosedistribution and the split into training and test sets.The numbers in parentheses denote the number ofsamples labelled as ?I?.Expression Samples Train Size Test SizeBlowWhistle 78 (27) 40 (20) 38 (7)LoseHead 40 (21) 30 (15) 10 (6)MakeScene 50 (30) 30 (15) 20 (15)TakeHeart 81 (61) 30 (15) 51 (46)Table 1: The sizes of the samples for each expres-sion and the split into training and test set.
Thenumbers in parentheses indicates the number of id-iomatic labels within the set.
We follow the samesplit as described in Peng et al (2014).While we wish to base our comparison on thework of Peng et al (2014) as it is the currentstate of the art, this is not without its own chal-lenges.
In particular we see the choice of these4 expression as a somewhat random decision asother expressions could also be selected for theevaluation with similar ratios to those describedin Table 1.
Moreover, the choosen expressions areall semi-compositional and do not consider fullynon-compositional expressions (although we be-lieve the task of classifying non-compositional ex-pressions would be easier for any method aimedat idiom token classification as these expressionsare high-fixed) .A better evaluation would con-sider all the 28 expressions of the balanced partof the VNC-tokens dataset.
In addition, we alsosee this choice of training and test splits as some-what arbitrary.
For two of the expressions thetest set contain samples in a way that one of theclasses outnumber the other by a great amount: forBlowWhistle, the literal class contains roughly 4times more samples than the idiomatic class; andfor TakeHeart the idiomatic class contains roughly9 times more samples than the literal class.
Ourconcerns with these very skewed test set ratios isthat it is very easy when applying a per expres-sion approach (i.e., a separate model for each ex-pression) for a model to achieve good performance(in terms of precision, recall, ad f1) if the positiveclass is the majority class in the test set.
However,despite these concerns, in our first experiment inorder to facilitate comparison with the prior art wefollow the expression selections and training/testsplits described in Peng et al (2014).Studies on the characteristics of distributed se-mantic representations of words have shown thatsimilar words tend to be represented by points thatare close to each other in the semantic featurespace (e.g.
Mikolov et al (2013a)).
Inspired bythese results we designed a second experiment totest whether the Sent2Vec representations wouldcluster idiomatic sentences in one part of the fea-ture space and literal sentences in another part ofthe space.
For this experiment we used the entire?balanced?
part of the VNC-tokens dataset to trainand test our ?general?
(multi-expression) models.In this experiment we wanted the data to reflect,as much as possible, the real distribution of the id-iomatic and literal usages of each expression.
So,in constructing our training and test set we triedto maintain for each expression the same ratio ofidiomatic and literal examples across the trainingand test set.
To create the training and test sets, wesplit the dataset into roughly 75% for training (917samples) and 25% for testing (288 samples).
Werandomly sample the expressions ensuring that theratio of idiomatic to literal expressions of each ex-pression were maintained across both sets.
In Ta-ble 2 we show the expressions used and their splitinto training and testing.
The numbers in paren-theses are the number of samples labelled as ?I?.4.2 Sent2Vec ModelsTo encode the sentences into their distributed rep-resentations we used the code and models madeavailable3by Kiros et al (2015).
Using their3https://github.com/ryankiros/skip-thoughts198Expression Samples Train Size Test SizeBlowTop 28 (23) 21 (18) 7 (5)BlowTrumpet 29 (19) 21 (14) 8 (5)BlowWhistle 78 (27) 59 (20) 19 (7)CutFigure 43 (36) 33 (28) 10 (8)FindFoot 53 (48) 39 (36) 14 (12)GetNod 26 (23) 19 (17) 7 (6)GetSack 50 (43) 40 (34) 10 (9)GetWind 28 (13) 20 (9) 8 (4)HaveWord 91 (80) 69 (61) 22 (19)HitRoad 32 (25) 24 (19) 8 (6)HitRoof 18 (11) 14 (9) 4 (2)HitWall 63 (7) 50 (6) 13 (1)HoldFire 23 (7) 19 (5) 4 (2)KickHeel 39 (31) 30 (23) 9 (8)LoseHead 40 (21) 29 (15) 11 (6)LoseThread 20 (18) 16 (15) 4 (3)MakeFace 41 (27) 31 (21) 10 (6)MakeHay 17 (9) 12 (6) 5 (3)MakeHit 14 (5) 9 (3) 5 (2)MakeMark 85 (72) 66 (56) 19 (16)MakePile 25 (8) 18 (6) 7 (2)MakeScene 50 (30) 37 (22) 13 (8)PullLeg 51 (11) 40 (8) 11 (3)PullPlug 64 (44) 49 (33) 15 (11)PullPunch 22 (18) 18 (15) 4 (3)PullWeight 33 (27) 24 (20) 9 (7)SeeStar 61 (5) 49 (3) 12 (2)TakeHeart 81 (61) 61 (45) 20 (16)Table 2: The sizes of the samples for each expres-sion and the split into training and test set.
Thenumbers in parentheses indicates the number ofidiomatic labels within the set.models it is possible to encode the sentences intothree different formats: uni-skip (which uses aregular RNN to encode the sentence into a 2400-dimensional vector); bi-skip (that uses a bidirec-tional RNN to encode the sentence also into a2400-dimensional vector); and the comb-skip (aconcatenation of uni-skip and bi-skip which has4800 dimensions).
Their models were trained us-ing the BookCorpus dataset (Zhu et al, 2015)and has been tested in several different NLP tasksas semantic relatedness, paraphrase detection andimage-sentence ranking.
Although we experi-mented with all the three models, in this paperwe only report the results of classifiers trained andtested using the comb-skip features.4.3 Classifiers4.3.1 ?Per-expression?
modelsThe idea behind Sent2Vec is similar to those ofword embeddings experiments: sentences contain-ing similar meanings should be represented bypoints close to each other in the feature space.
Fol-lowing this intuition we experiment first with asimilarity based classifier, the K-Nearest Neigh-bours (k-NN).
For the k-NNs we experimentedwith k = {2, 3, 5, 10}.We also experimented with a more advancedalgorithm, namely the Support Vector Machine(SVM) (Vapnik, 1995).
We trained the SVM un-der three different configurations:Linear-SVM-PE4.
This model used a ?linear?kernel with C = 1.0 on all the classification se-tups.Grid-SVM-PE.
For this model we performed agrid search for the best parameters for each expres-sion.
The parameters are: BlowWhiste = { ker-nel: ?rbf?, C = 100}; LoseHead = { kernel: ?rbf?,C = 1 }; MakeSene = { kernel: ?rbf?, C = 100 };TakeHeart = { kernel: ?rbf?, C = 1000 }.SGD-SVM-PE.
This model is a SVM with lin-ear kernel but trained using stochastic gradient de-scent (Bottou, 2010).
We set the SGD?s learningrates (?)
using a grid search: BlowWhiste = {?
=0.001 }; LoseHead = {?
= 0.01 }; MakeSene ={?
= 0.0001 }; TakeHeart = {?
= 0.0001 };FullDataset = {?
= 0.0001 }.
We trained theseclassifiers for 15 epochs.4.3.2 ?General?
modelsWe consider the task of creating a ?general?
clas-sifier that takes an example of any potential idiomand classifying it into idiomatic or literal usagemore difficult than the ?per-expression?
classifi-cation task.
Hence we executed this part of thestudy with the SVM models only.
We trained thesame three types of SVM models used in the ?per-expression?
approach but with the following pa-rameters:Linear-SVM-GE5.
This model used a linearkernel with C = 1.0 for all the classification sets.Grid-SVM-GE.
For this model we also per-formed a grid search and set the kernel to ?poly-nomial kernel?
of degree = 2 with C = 1000.SGD-SVM-GE.
We also experimented with aSVM with linear kernel trained using stochasticgradient descent.
We set the SGD?s learning rate?
= 0.0001 after performing a grid search.
Wetrained this classifier for 15 epochs.5 Results and DiscussionWe first present the results for the per expressioncomparison with Peng et al (2014) and then in4PE stands for ?per-expression?5GE stands for ?general?.199Models BlowWhistle LoseHead MakeScene TakeHeartP.
R. F1 P. R. F1 P. R. F1 P. R. F1Peng et.
al (2014)FDA-Topics 0.62 0.60 0.61 0.76 0.97 0.85 0.79 0.95 0.86 0.93 0.99 0.96FDA-Topics+A 0.47 0.44 0.45 0.74 0.93 0.82 0.82 0.69 0.75 0.92 0.98 0.95FDA-Text 0.65 0.43 0.52 0.72 0.73 0.72 0.79 0.95 0.86 0.46 0.40 0.43FDA-Text+A 0.45 0.49 0.47 0.67 0.88 0.76 0.80 0.99 0.88 0.47 0.29 0.36SVMs-Topics 0.07 0.40 0.12 0.60 0.83 0.70 0.46 0.57 0.51 0.90 1.00 0.95SVMs-Topics+A 0.21 0.54 0.30 0.66 0.77 0.71 0.42 0.29 0.34 0.91 1.00 0.95SVMs-Text 0.17 0.90 0.29 0.30 0.50 0.38 0.10 0.01 0.02 0.65 0.21 0.32SVMs-Text+A 0.24 0.87 0.38 0.66 0.85 0.74 0.07 0.01 0.02 0.74 0.13 0.22Distributed RepresentationsKNN-2 0.61 0.41 0.49 0.30 0.64 0.41 0.55 0.89 0.68 0.46 0.96 0.62KNN-3 0.84 0.32 0.46 0.58 0.65 0.61 0.88 0.88 0.88 0.72 0.94 0.81KNN-5 0.79 0.28 0.41 0.57 0.65 0.61 0.87 0.83 0.85 0.73 0.94 0.82KNN-10 0.83 0.30 0.44 0.28 0.68 0.40 0.85 0.83 0.84 0.78 0.94 0.85Linear SVM 0.77 0.50 0.60 0.72 0.84 0.77 0.81 0.91 0.86 0.73 0.96 0.83Grid SVM 0.80 0.51 0.62 0.83 0.89 0.85 0.80 0.91 0.85 0.72 0.96 0.82SGD SVM 0.70 0.40 0.51 0.73 0.79 0.76 0.85 0.91 0.88 0.61 0.95 0.74Table 3: Results in terms of precision (P.), recall (R.) and f1-score (F1) on the four chosen expressions.The results of (Peng et al, 2014) are those of the multi-paragraphs method.
The bold values indicatesthe best results for that expression in terms of f1-score.Section 5.2 we present the results for the ?general?classifier approach.5.1 Per-Expression ClassificationThe averaged results over 10 runs in terms of pre-cision, recall and f1-score are presented in Table3.
When calculating these metrics, we consid-ered the positive class to be the ?I?
(idiomatic) la-bel.
We used McNemar?s test (McNemar, 1947)to check the statistical significance of our models?results and found all our results to be significant atp < 0.05.We can see in Table 3 that some of our mod-els outperform the baselines on 1 expression(BlowWhistle) and achieved the same f1-scores on2 expressions (LoseHead and MakeScene).
Fortheses 3 expressions, our best models generallyhad higher precision than the baselines, findingmore idioms on the test sets.
In addition, forMakeScene, 2 of our models achieved the same f1-scores (KNN-3 and SGD-SVM-PE), although theyhave different precision and recall.The only expression on which a baseline modeloutperformed all our models was TakeHeart whereit achieved higher precision, recall and f1-scores.Nevertheless, this expression had the most imbal-anced test set, with roughly 9 times more idiomsthan literal samples.
Therefore, if the baseline la-bel all the test set samples as idiomatic (includingthe literal examples), it would still have the best re-sults.
It is thus worth emphasizing that the choicesof distributions for training and test sets in Penget als work seems arbitrary and does not reflectthe real distribution of the data in a balanced cor-pus.
Also, Peng et al (2014) did not provide theconfusion matrices for their models so we cannotanalyse their model behaviour across the classes.That aside, while our best models share thesame f1-score with the baseline on 2 of the expres-sions, we believe that our method is more powerfulif we take into account that we do not explicitly ac-cess the context surrounding our input sentences.We can also consider that our method is cheaperthan the baseline in the sense that we do not needto process words other than the words in the inputsentence.In addition, we note that the SVMs generallyoutperform the KNNs, although no single modelperform best across all expressions.
Regardlessof the fact that the KNN-3 achieved the same f1-score as SGD-SVM on MakeScene, the SVM con-sistently scored higher than the KNNs on all ex-pressions.
This is an interesting finding if we con-sider that our feature vector is 4800-dimensionaland the SVMs are projecting these features into aspace that has much more than 4800 dimensionsand not incurring into the ?curse of dimension-ality?.
Furthermore, other work using Sent2vechave shown the capabilities of the Sent2Vec rep-resentations to capture features that are suited tovarious NLP tasks where semantics is involved(e.g., paraphrase detection and semantic related-ness (Kiros et al, 2015)).
These results togetherwith our findings suggests that the factors in-200Expressions Linear-SVM-GE Grid-SVM-GE SGD-SVM-GEP.
R. F1 P. R. F1 P. R. F1BlowTop 0.91 0.96 0.94 0.91 0.93 0.94 0.80 0.98 0.88BlowTrumpet 0.98 0.88 0.93 0.98 0.88 0.93 0.89 0.93 0.90BlowWhistle* 0.84 0.67 0.75 0.84 0.68 0.75 0.67 0.59 0.63CutFigure 0.91 0.85 0.88 0.89 0.85 0.87 0.86 0.85 0.86FindFoot 0.96 0.93 0.94 0.97 0.93 0.95 0.85 0.90 0.87GetNod 0.98 0.91 0.95 0.98 0.91 0.95 0.91 0.91 0.91GetSack 0.87 0.89 0.88 0.86 0.88 0.87 0.81 0.89 0.84GetWind 0.86 0.82 0.84 0.92 0.85 0.88 0.69 0.81 0.75HaveWord 0.99 0.89 0.94 0.99 0.89 0.94 0.95 0.91 0.93HitRoad 0.86 0.98 0.92 0.89 0.98 0.93 0.83 0.98 0.90HitRoof 0.88 0.88 0.88 0.92 0.88 0.90 0.80 0.83 0.82HitWall 0.74 0.58 0.65 0.74 0.58 0.65 0.74 0.45 0.56HoldFire 1.00 0.63 0.77 1.00 0.63 0.77 0.82 0.67 0.74KickHeel 0.92 0.96 0.94 0.92 0.99 0.95 0.89 0.92 0.91LoseHead* 0.78 0.66 0.72 0.75 0.64 0.69 0.75 0.67 0.71LoseThread 1.00 0.88 0.93 1.00 0.86 0.92 0.81 0.85 0.83MakeFace 0.70 0.83 0.76 0.69 0.76 0.72 0.62 0.81 0.70MakeHay 0.81 0.78 0.79 0.81 0.84 0.82 0.73 0.76 0.75MakeHit 0.10 0.54 0.70 0.10 0.54 0.70 0.85 0.55 0.67MakeMark 0.99 0.92 0.95 0.98 0.91 0.94 0.93 0.93 0.93MakePile 0.84 0.67 0.74 0.84 0.70 0.76 0.74 0.70 0.72MakeScene* 0.92 0.84 0.88 0.92 0.81 0.86 0.78 0.81 0.79PullLeg 0.79 0.71 0.75 0.82 0.72 0.77 0.75 0.70 0.72PullPlug 0.91 0.91 0.91 0.91 0.91 0.91 0.90 0.92 0.91PullPunch 0.85 0.87 0.86 0.87 0.87 0.87 0.70 0.85 0.77PullWeight 1.00 0.96 0.98 1.00 0.96 0.98 0.89 0.93 0.93SeeStar 0.17 0.13 0.15 0.17 0.13 0.15 0.17 0.17 0.17TakeHeart* 0.94 0.79 0.86 0.94 0.80 0.86 0.86 0.80 0.83Total 0.84 0.80 0.83 0.84 0.80 0.83 0.79 0.79 0.78Table 4: Precision (P.), recall (R.) and f1-scores (F1) calculated on the expressions of the balanced partof the VNC-Tokens dataset.
The expressions marked with * indicate the expressions also evaluated withthe ?per-expression?
classifiers.volved in distinguishing between the semanticsof idiomatic and literal language are deeply en-trenched in language generation and only a high-dimensional representation can enable a classifierto make that distinction.
This observation also im-plies that the contribution of each feature (gen-erated by the distributed representation) is verysmall, given the fact that we need that many di-mensions and the space needed to unpack the com-ponents of literal and idiomatic language has manymore dimensions than the input space.
Therefore,the current manually engineered features (i.e., thefeatures used in previous idiom token classifica-tion) are only capturing a small portion of thesedimensions and assigning more weight to these di-mensions while other dimensions (not captured)are not considered (i.e., as they are not considered,the features represented by these dimensions havetheir weight equal to 0)Another point for consideration is the fact thatthe combination of our model with the work ofPeng et al (2014) may result in a stronger modelon this ?per-expression?
setting.
Nevertheless, aspreviously highlighted, it was not possible for usto directly re-implement their work.5.2 General ClassificationMoving on to the general classification case, wepresent the average results (in terms of precision,recall and f1-score) over 10 runs to our ?general?classifiers on the balanced part of the VNC-Tokensdataset.
Once again, the positive class is assumedto be the ?I?
(idiomatic) label and we split the out-comes per expression.
It should be noted that the?per-expression?
evaluation was performed usinga balanced set to train the classifiers while in thisexperiment we maintained the ratio of idiomatic toliteral usages for each expression across the train-ing and test sets.
Our motivation for maintainingthis ratio was to simulate the real distribution ofthe classes in the corpus.We present results for the four individualMWEs used in the per-sentence based evaluationas well as a set of averages made over all 28 ex-pression in the ?balanced?
portion of the dataset.Referring to the results we first of all note theoverall performance of the ?general?
classifiers is201fairly high with 2 classifiers (Linear-SVM-GE andGrid-SVM-GE) sharing the same precision, recalland f1-scores.
While averages here are the sameacross the two classifiers, it is worth noting thatdeviations occured across individual MWE types,though these deviations balanced out across thedata set.
Although not displayed in this table dueto space limitations, it should be noted that all the3 classifier had a extremely low performance onSeeStar (f1 = 0.15, 0.15 and 0.17 respectively).If we compare the performance of the 4 ex-pressions analysed in the ?per-expression?
exper-iment we can observe that all the ?general?
clas-sifiers had a better performance over BlowWhis-tle and the Linear-SVM-GE also performed bet-ter on MakeScene.
Nevertheless we should em-phasize that the ?general?
classifier?s evaluation iscloser to what we would expect in a real data dis-tribution than the evaluation presented on the ?per-expression?
section.
This does not invalidate theevaluation of the latter but when we have access toa real data distribution it should also be taken intoaccount when performing a ML evaluation.In general, the results look promising.
It is in-teresting to see how the classifiers trained on a setof mixed expressions (?general?
classifiers) had aperformance close to the ?per-expression?
classi-fiers, even though the latter were trained and testedon ?artificial?
training and test sets that do not re-flect the real data distributions.
We believe thatthese results indicate that the distributed represen-tations generated by Sent2Vec are indeed cluster-ing together sentences within the same class (id-iomatic or literal) in feature space.6 Conclusions and Future WorkIn this paper we have investigated the use of dis-tributed compositional semantics in literal and id-iomatic language classification, more specificallyusing Skip-Thought Vectors (Sent2Vec).
We fol-lowed the intuition that the distributed representa-tions generated by Sent2Vec also include informa-tion regarding the context where the potential id-iomatic expression is inserted and therefore is suf-ficient for distinguishing between idiomatic andliteral language use.We tested this approach with different MachineLearning (ML) algorithms (K-Nearest Neighboursand Support Vector Machines) and compared ourwork against a topic model representation that in-clude the full paragraph or the surrounding para-graphs where the potential idiom is inserted.
Wehave shown that using the Sent2Vec representa-tions our classifiers achieve better results in 3 outof 4 expressions tested.
We have also shownthat our models generally present better precisionand/or recall than the baselines.We also investigated the capability of Sent2Vecclustering representations of sentences within thesame class in feature space.
We followed theintuition presented by previous experiments withdistributed representations that words with simi-lar meaning are clustered together in feature spaceand experimented with a ?general?
classifier thatis trained on a dataset of mixed expressions.
Wehave shown that the ?general?
classifier is feasiblebut the traditional ?per-expression?
does achievebetter results in some cases.In future work we plan to investigate the use ofSent2Vec to encode larger samples of text - notonly the sentence containing idioms.
We also planto further analyse the errors made by our ?general?model and investigate the ?general?
approach onthe skewed part of the VNC-tokens dataset.
Wealso plan to investigate an end-to-end approachbased on deep learning-based representations toclassify literal and idiomatic language use.In addition, we also plan to compare our workto the method of Sporleder et al (2010) as wellapply our work on the IDX Corpus (Sporleder etal., 2010) and to other languages.
The focus ofthese future experiments will be to test how our ap-proach which is relatively less dependent on NLPresources compares with these other methods foridiom token classification.AcknowledgmentsWe would like to thank the anonymous reviewersfor their valuable comments and feedback.
Gi-ancarlo D. Salton would like to thank CAPES(?Coordenac?
?ao de Aperfeic?oamento de Pessoal deN?
?vel Superior?)
for his Science Without Bordersscholarship, proc n. 9050-13-2.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In ICLR.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.202L?eon Bottou.
2010.
Large-scale machine learningwith stochastic gradient descent.
In Proceedings ofthe 19th International Conference on ComputationalStatistics (COMPSTAT?2010), pages 177?187.Lou Burnard.
2007.
Reference guide for the britishnational corpus (xml edition).
Technical report,http://www.natcorp.ox.ac.uk/.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learningphrase representations using rnn encoder?decoderfor statistical machine translation.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1724?1734, Doha, Qatar, October.
Association for Com-putational Linguistics.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2008.
The VNC-Tokens Dataset.
In Proceedingsof the LREC Workshop: Towards a Shared Task forMultiword Expressions (MWE 2008), Marrakech,Morocco.Afsanesh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identification ofidiomatic expressions.
In Computational Linguis-tics, volume 35, pages 61?103.Anna Feldman and Jing Peng.
2013.
Automaticdetection of idiomatic clauses.
In Proceedings ofthe 14th International Conference on ComputationalLinguistics and Intelligent Text Processing - VolumePart I, CICLing?13, pages 435?446.Chikara Hashimoto and Daisuke Kawahara.
2008.Construction of an idiom corpus and its applicationto idiom identification based on wsd incorporatingidiom-specific features.
In Proceedings of the con-ference on empirical methods in natural languageprocessing, pages 992?1001.
Association for Com-putational Linguistics.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
June.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the 2014Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2014, October 25-29,2014, Doha, Qatar, A meeting of SIGDAT, a SpecialInterest Group of the ACL, pages 1746?1751.Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,Richard Zemel, Raquel Urtasun, Antonio Torralba,and Sanja Fidler.
2015.
Skip-thought vectors.
InAdvances in Neural Information Processing Systems28, pages 3276?3284.Linlin Li and Caroline Sporleder.
2010a.
Linguisticcues for distinguishing literal and non-literal usages.In Proceedings of the 23rd International Conferenceon Computational Linguistics: Posters, pages 683?691.Linlin Li and Caroline Sporleder.
2010b.
Using gaus-sian mixture models to detect figurative language incontext.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 297?300, Stroudsburg, PA,USA.
Association for Computational Linguistics.Quinn McNemar.
1947.
Note on the sampling errorof the difference between correlated proportions orpercentages.
Psychometrika, 12(2):153?157.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In The 2013 Conference ofthe North Americal Chapter of the Association forComputational Linguistics: Human Language Tech-nologies (NAACL-HLT), pages 746?751.Jing Peng, Anna Feldman, and Ekaterina Vylomova.2014.
Classifying idiomatic and literal expres-sions using topic models and intensity of emotions.In Proceedings of the 2014 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 2019?2027, October.Giancarlo D. Salton, Robert J. Ross, and John D.Kelleher.
2014.
An Empirical Study of the Im-pact of Idioms on Phrase Based Statistical MachineTranslation of English to Brazilian-Portuguese.
InThird Workshop on Hybrid Approaches to Transla-tion (HyTra) at 14th Conference of the EuropeanChapter of the Association for Computational Lin-guistics.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 1631?1642.Caroline Sporleder and Linlin Li.
2009.
Unsupervisedrecognition of literal and non-literal use of idiomaticexpressions.
In Proceedings of the 12th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 754?762.Caroline Sporleder, Linlin Li, Philip Gorinski, andXaver Koch.
2010.
Idioms in context: The idixcorpus.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC-2010), pages 639?646.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural net-works.
In Z. Ghahramani, M. Welling, C. Cortes,N.
D. Lawrence, and K. Q. Weinberger, editors, Ad-vances in Neural Information Processing Systems27, pages 3104?3112.203Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Aline Villavicencio, Francis Bond, Anna Korhonen,and Diana McCarthy.
2005.
Editorial: Introductionto the special issue on multiword expressions: Hav-ing a crack at a hard nut.
Comput.
Speech Lang.,19(4):365?377.Yukun Zhu, Ryan Kiros, Richard Zemel, RuslanSalakhutdinov, Raquel Urtasun, Antonio Torralba,and Sanja Fidler.
2015.
Aligning books andmovies: Towards story-like visual explanations bywatching movies and reading books.
arXiv preprintarXiv:1506.06724.204
