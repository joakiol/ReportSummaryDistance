Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,pages 110?114, Dublin, Ireland, August 23-29 2014.Distributional Semantics in R with the wordspace PackageStefan EvertProfessur f?ur KorpuslinguistikFriedrich-Alexander-Universit?at Erlangen-N?urnbergBismarckstr.
6, 91054 Erlangen, Germanystefan.evert@fau.deAbstractThis paper introduces the wordspace package, which turns Gnu R into an interactive laboratoryfor research in distributional semantics.
The package includes highly efficient implementationsof a carefully chosen set of key functions, allowing it to scale up to real-life data sets.1 IntroductionDistributional semantic models (DSMs) represent the meaning of a target term (which can be a wordform, lemma, morpheme, word pair, etc.)
in the form of a feature vector that records either co-occurrencefrequencies of the target term with a set of feature terms (term-term model) or its distribution acrosstext units (term-context model).
Such DSMs have become an indispensable ingredient in many NLPapplications that require flexible broad-coverage lexical semantics (Turney and Pantel, 2010).Distributional modelling is an empirical science.
DSM representations are determined by a widerange of parameters such as size and type of the co-occurrence context, feature selection, weighting ofco-occurrence frequencies (often with statistical association measures), distance metric, dimensionalityreduction method and the number of latent dimensions used.
Despite recent efforts to carry out systematicevaluation studies (Bullinaria and Levy, 2007; Bullinaria and Levy, 2012), the precise effects of theseparameters and their relevance for different application settings are still poorly understood.The wordspace package for Gnu R (R Development Core Team, 2010) aims to provide a flexible,powerful and easy to use ?interactive laboratory?
that enables its users to build DSMs and experimentwith them, but that also scales up to the large models required by real-life applications.2 Related workOne reason for the popularity of distributional approaches is that even large-scale models can be im-plemented with relative ease.
In the geometric interpretation, most operations involved in building andusing a DSM can be expressed concisely in terms of matrix and vector algebra: matrix multiplication,inner and outer products, matrix decomposition, and vector norms and metrics.In order to make DSMs accessible to a large group of users, several dedicated software packages havebeen developed.
Most of these packages either implement a particular model, limiting their flexibility, orimpose a complex framework of classes, making it hard for users to carry out operations not envisionedby the package developers.
Examples of the first category are HiDeX (Shaoul and Westbury, 2010), amodern reimplementation of the HAL model, and Semantic Vectors (Widdows and Cohen, 2010), whichenforces a random indexing representation in order to improve scalability.A typical example of the second category is the S-Space package (Jurgens and Stevens, 2010), whichdefines a complete pipeline for building and evaluating a DSM; researchers wishing e.g.
to evaluate themodel on a different task need to implement the evaluation procedure in the form of a suitable Javaclass.
Two Python-based software packages in this category are Gensim (?Reh?u?rek and Sojka, 2010) andDISSECT (Dinu et al., 2013), which has a particular focus on learning compositional models.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/110In order to avoid locking its users into a particular framework or class of distributional model, thewordspace package takes a different approach.
It builds on the statistical software environment GnuR, which provides efficient dense and sparse matrix algebra, sophisticated statistical analysis and visual-ization, as well as numerous machine learning methods as part of its core library and through a wealth ofadd-on packages.
R itself is already an excellent environment for the interactive exploration of DSMs.Like many other R packages, wordspace does not define its own complex framework.
It extendsfunctionality that is already available (and easy to use) with a small set of carefully designed functionsthat (i) encapsulate non-trivial R operations in a convenient and user-friendly way and (ii) provide highlyefficient and memory-friendly C implementations of key operations in order to improve scalability.
Ofthe other DSM software packages available, DISSECT seems closest in spirit to wordspace.
There-fore, it is used as a point of reference for the performance comparison in Sec.
5.3 The wordspace packageThe wordspace package is an open-source project distributed under the GNU General Public License.Since the pacakge is linked to the R interpreter and its functions are always invoked from interpretedcode, this does not preclude commercial applications involving closed-source components.
The packagesource code is hosted on R-Forge.1It can easily be installed from the Comprehensive R Archive Network(CRAN), which provides pre-compiled binaries for Windows and Mac OS X.3.1 Input formatsThe most general representation of a distributional model takes the form of a sparse matrix, with entriesspecified as a triplet of row label (target term), column label (feature term) and co-occurrence frequency(cf.
left panel of Fig.
1).
The wordspace package creates DSM objects from such triplet representa-tions, which can easily be imported into R from a wide range of file and database formats.
Ready-madeimport functions are provided for TAB-delimited text files (as used by DISSECT), which may be com-pressed to save disk space, and for term-document models from the text-mining framework tm for R.The native input format is a pre-compiled sparse matrix representation generated by the UCS toolkit.2In this way, UCS serves as a hub for the preparation of co-occurrence data, which can be collected fromdependency pairs, extracted from a corpus indexed with the IMS Corpus Workbench,3or imported fromvarious other formats such as the Ngram Statistics Package (NSP).43.2 FeaturesThe wordspace package offers flexible convenience functions to filter DSMs by properties of theirrows (targets) and columns (features), combine multiple co-occurrence matrices by rows or by columns,and merge data obtained from different corpora.
Co-occurrence frequencies can be weighted by a tf.idfscheme or one of various statistical association measures, rescaled e.g.
by a logarithmic transformation,standardized and row-normalized.Efficient implementations are provided for dimensionality reduction by randomized SVD (Halko etal., 2009) or random indexing, for computing a distance matrix between a set of row vectors, and forthe identification of the nearest neighbours of a given target term.
Additional functions compute centroidrepresentations for sentence contexts and support the evaluation of DSMs in standard classification, clus-tering and regression tasks.
Several freely available gold standard data sets are included in the package.Due to its philosophy, wordspace only provides essential functionality that cannot easily beachieved with basic R functions or does not scale well in the standard implementation.
Many furtheranalyses and operations (e.g.
partial least-squares regression for learning compositional DSMs) can beperformed with standard R functions or one of more than 5000 add-on packages available from CRAN.1http://wordspace.r-forge.r-project.org/2http://www.collocations.de/software.html3http://cwb.sourceforge.net/4http://ngram.sourceforge.net/111dog-n walk-v 6343dog-n walk-n 2461dog-n bite-v 1732cat-n tail-n 1285cat-n jump-v 541. .
.
.
.
.
.
.
.noun rel verb f modedog subj bite 3 spokendog subj bite 12 writtendog obj bite 4 writtendog obj stroke 3 written.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 1: Typical input data: triplet representation of a sparse co-occurrence matrix (left panel) andverb-noun cooccurrences from the BNC used as example data in Sec.
4 (right panel).4 Example sessionFig.
3 shows the full set of R commands required to compile and use a DSM with the wordspacepackage, based on a built-in table containing co-occurrence counts of verbs and their subject/objectnouns in the British National Corpus (BNC), an excerpt of which is shown in the right panel of Fig.
1.After loading the package (line 1), we use a standard R function to extract data for the written part ofthe BNC (line 3).
The dsm() function constructs a basic DSM object from co-occurrence data in varioussparse matrix representations (line 4).
Note that multiple entries for the same noun-verb combination areautomatically aggregated, resulting in a 19831 ?
4854 noun-verb co-occurrence matrix.
Highly sparsevectors are unreliable as indicators of word meaning, so we filter out rows and columns with less than3 nonzero entries using the subset() method for DSM objects.
The required nonzero counts haveautomatically been added by the dsm() constructor.
Since deleting rows and columns changes thenonzero counts, we apply the process recursively until both constraints are satisfied (line 9).Co-occurrence counts are then weighted by the log-likelihood association measure, log-transformedto deskew their distribution, and row vectors are normalized to Euclidean unit length.
This is achievedwith a single function call and minimal memory overhead (line 13).
For dimensionality reduction, theefficient randomized SVD algorithm is used (line 15), resulting in a plain R matrix VObj300.Typical applications of a DSM are to compute distances or cosine similarities between pairs of targetterms (line 17) and to find the nearest neighbours of a given term (line 20).
The final DSM can beevaluated e.g.
by comparison with the RG65 data set of semantic similarity ratings (Rubenstein andGoodenough, 1965).
Here, we obtain a Pearson correlation of r = 0.521 (with 95% confidence interval0.317 .
.
.
0.679) and a Spearman rank correlation of ?
= 0.511 (line 26).
The correlation can also bevisualized with a built-in plot method (line 29), shown in the left panel of Fig.
2.The commands in lines 31?37 illustrate the use of standard R functions for further analysis and vi-sualization.
Here, an implementation of non-metric multidimensional scaling in the R package MASSproduces a semantic map of the nearest neighbours of book.
A slightly more polished version of thisplot is shown in the right panel of Fig.
2.
The only step that takes a non-negligible amount of time isdimensionality reduction with randomized SVD (approx.
14 seconds on the reference system, see Sec.
5).5 Performance comparisonIn order to determine the usefulness of the wordspace package for realistically sized data sets, abenchmark was carried out using the W ?W projection of the Distributional Memory tensor (Baroniand Lenci, 2010), resulting in a sparse 30686?
30686 matrix with 60 million nonzero entries (6.4% fillrate).
Execution times of key operations and file sizes of the native serialization format are shown inTable 1 and compared against DISSECT v0.1.0 as the closest ?competitor?.
Tests were carried out on a2012 MacBook Pro with a 2.6 GHz 4-core Intel Core i7 CPU, 16 GiB RAM and a 768 GB SSD.Runtimes for nearest neighbours (NN) are averaged over 198 nouns, and those for cosine similarity areaveraged over 351 noun pairs.
Both sample sets were taken from the WordSim-353 data set.
DISSECTrequires about 2.5 GiB RAM to carry out the complete process, while R requires slightly above 4 GiB.Most of the RAM is needed to load the non-native input format (which happens to be the native formatof DISSECT).
For the remaining steps, memory usage remains well below 3 GiB.112wordspace DISSECT size (.rda / .pkl)build model from triples file 186.0 s 503.3 ssave model 57.6 s 1.5 s 228.9 MB / 725.7 MBnormalize row vectors 0.5 s 1.3 sSVD projection to 300 latent dimensions 353.6 s 296.6 ssave latent vectors 10.4 s 0.4 s 71.5 MB / 185.0 MB20 nearest neighbours (full matrix) 119 ms 1269 ms20 nearest neighbours (300 dims) 10 ms 92 mscosine similarity (full matrix) 4 ms < 1 mscosine similarity (300 dims) < 1 ms < 1 msTable 1: Performance comparison: wordspace vs. DISSECT on Distributional Memory.6 Further developmentThe wordspace package is under very active development.
Main objectives for the near future are (i)sparse SVD using SVDLIBC (which is more efficient than randomized SVD in certain cases), (ii) sparsenon-negative matrix factorization (NMF), and (iii) built-in support for a wider range of file and databaseformats.
In addition, new weighting functions and distance metrics are continuously being added.ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distributional Memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?712.John A. Bullinaria and Joseph P. Levy.
2007.
Extracting semantic representations from word co-occurrencestatistics: A computational study.
Behavior Research Methods, 39(3):510?526.John A. Bullinaria and Joseph P. Levy.
2012.
Extracting semantic representations from word co-occurrencestatistics: Stop-lists, stemming and SVD.
Behavior Research Methods, 44(3):890?907.Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013.
DISSECT ?
distributional semantics compositiontoolkit.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: SystemDemonstrations, pages 31?36, Sofia, Bulgaria, August.N.
Halko, P. G. Martinsson, and J[oel] A. Tropp.
2009.
Finding structure with randomness: Stochastic algorithmsfor constructing approximate matrix decompositions.
Technical Report 2009-05, ACM, California Institute ofTechnology, September.David Jurgens and Keith Stevens.
2010.
The S-Space package: An open source package for word space models.In Proceedings of the ACL 2010 System Demonstrations, pages 30?35, Uppsala, Sweden, July.R Development Core Team, 2010.
R: A Language and Environment for Statistical Computing.
R Foundation forStatistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
See also http://www.r-project.org/.Radim?Reh?u?rek and Petr Sojka.
2010.
Software framework for topic modelling with large corpora.
In Proceedingsof the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.ELRA.Herbert Rubenstein and John B. Goodenough.
1965.
Contextual correlates of synonymy.
Communications of theACM, 8(10):627?633.Cyrus Shaoul and Chris Westbury.
2010.
Exploring lexical co-occurrence space using HiDEx.
Behavior ResearchMethods, 42(2):393?413.Peter D. Turney and Patrick Pantel.
2010.
From frequency to meaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.Dominic Widdows and Trevor Cohen.
2010.
The Semantic Vectors package: New algorithms and public tools fordistributional semantics.
In IEEE Fourth International Conference on Semantic Computing (ICSC 2010), pages9?15.113llllllllllllllllllllllllllllllll lll llllllllllllllllllllll0 1 2 3 4405060708090100Correlation with RG65 ratingshuman ratingdistributional model|rho| = 0.511, p = 0.0000, |r| = 0.317 .. 0.679 (8 pairs not found)llllllllllllllll?30 ?20 ?10 0 10 20 30?40?30?20?1001020Neighbourhood graph for BOOKpapernovelmagazineworksarticletextbookguidepoemessay leafleteditiontextpamphletbookletcataloguebookFigure 2: Two visualizations created by the sample code in Fig.
2.1 library(wordspace)23 Triples <- subset(DSM_VerbNounTriples_BNC, mode == "written")4 VObj <- dsm(target=Triples$noun, feature=Triples$verb, score=Triples$f,5 raw.freq=TRUE, sort=TRUE)6 dim(VObj)7 [1] 19831 485489 VObj <- subset(VObj, nnzero >= 3, nnzero >= 3, recursive=TRUE)10 dim(VObj)11 [1] 12428 37351213 VObj <- dsm.score(VObj, score="simple-ll", transform="log", normalize=TRUE)1415 VObj300 <- dsm.projection(VObj, method="rsvd", n=300, oversampling=4)1617 pair.distances("book", "paper", VObj300, method="cosine", convert=FALSE)18 book/paper19 0.732298220 nearest.neighbours(VObj300, "book", n=15) # defaults to angular distance21 paper novel magazine works article textbook guide poem22 42.92059 48.03492 49.10742 49.33028 49.54836 49.82660 50.29588 50.3711123 essay leaflet edition text pamphlet booklet catalogue24 50.45991 50.53009 50.78630 50.95731 51.12786 51.21351 52.438242526 eval.similarity.correlation(RG65, VObj300, format="HW")27 rho p.value missing r r.lower r.upper28 RG65 0.5113531 1.342741e-05 8 0.520874 0.3172827 0.678567429 plot(eval.similarity.correlation(RG65, VObj300, format="HW", details=TRUE))3031 nn <- nearest.neighbours(VObj300, "book", n=15)32 nn.terms <- c("book", names(nn)) # nn = distances labelled with the neighbour terms33 nn.dist <- dist.matrix(VObj300, terms=nn.terms, method="cosine")34 library(MASS) # a standard R package that includes two MDS implementations35 mds <- isoMDS(nn.dist, p=2)36 plot(mds$points, pch=20, col="red")37 text(mds$points, labels=nn.terms, pos=3)Figure 3: Complete example code for building, using and evaluating a DSM with wordspace.114
