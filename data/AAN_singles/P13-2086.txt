Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 484?488,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Novel Text Classifier Based on Quantum ComputationDing Liu,  Xiaofang Yang,  Minghu JiangLaboratory of Computational Linguistics, School of Humanities,Tsinghua University, Beijing , ChinaDingliu_thu@126.com  xfyang.thu@gmail.comjiang.mh@mail.tsinghua.edu.cnAbstractIn this article, we propose a novel classifierbased on quantum computation theory.
Differ-ent from existing methods, we consider theclassification as an evolutionary process of aphysical system and build the classifier by us-ing the basic quantum mechanics equation.The performance of the experiments on twodatasets indicates feasibility and potentiality ofthe quantum classifier.1 IntroductionTaking modern natural science into account, thequantum mechanics theory (QM) is one of themost famous and profound theory which brings aworld-shaking revolution for physics.
Since QMwas born, it has been considered as a significantpart of theoretic physics and has shown its powerin explaining experimental results.
Furthermore,some scientists believe that QM is the final prin-ciple of physics even the whole natural science.Thus, more and more researchers have expandedthe study of QM in other fields of science, and ithas affected almost every aspect of natural sci-ence and technology deeply, such as quantumcomputation.The principle of quantum computation has al-so affected a lot of scientific researches in com-puter science, specifically in computational mod-eling, cryptography theory as well as informationtheory.
Some researchers have employed theprinciple and technology of quantum computa-tion to improve the studies on Machine Learning(ML) (A?meur et al, 2006; A?meur et al, 2007;Chen et al, 2008; Gambs, 2008; Horn andGottlieb, 2001; Nasios and Bors, 2007), a fieldwhich studies theories and constructions of sys-tems that can learn from data, among which clas-sification is a typical task.
Thus, we attempted tobuild a computational model based on quantumcomputation theory to handle classification tasksin order to prove the feasibility of applying theQM model to machine learning.In this article, we present a method that con-siders the classifier as a physical system amena-ble to QM and treat the entire process of classifi-cation as the evolutionary process of a closedquantum system.
According to QM, the evolu-tion of quantum system can be described by aunitary operator.
Therefore, the primary problemof building a quantum classifier (QC) is to findthe correct or optimal unitary operator.
We ap-plied classical optimization algorithms to dealwith the problem, and the experimental resultshave confirmed our theory.The outline of this paper is as follows.
First,the basic principle and structure of QC is intro-duced in section 2.
Then, two different experi-ments are described in section 3.
Finally, section4 concludes with a discussion.2  Basic principle of quantum classifierAs we mentioned in the introduction, the majorprinciple of quantum classifier (QC) is to consid-er the classifier as a physical system and thewhole process of classification as the evolution-ary process of a closed quantum system.
Thus,the evolution of the quantum system can be de-scribed by a unitary operator (unitary matrix),and the remaining job is to find the correct oroptimal unitary operator.2.1 Architecture of quantum classifierThe architecture and the whole procedure of dataprocessing of QC are illustrated in Figure 1.
Asis shown, the key aspect of QC is the optimiza-tion part where we employ the optimization algo-rithm to find an optimal unitary operator ?
?.484Figure 1.
Architecture of quantum classifierThe detailed information about each phase of theprocess will be explained thoroughly in the fol-lowing sections.2.2 Encode input state and target stateIn quantum mechanics theory, the state of aphysical system can be described as a superposi-tion of the so called eigenstates which are or-thogonal.
Any state, including the eigenstate, canbe represented by a complex number vector.
Weuse Dirac?s braket notation to formalize the dataas equation 1:|??
=???|????
(1)where |??
denotes a state and ??
?
?
is a com-plex number with ??
= ??|???
being the projec-tion of |??
on the eigenstate |???.
According toquantum theory, ??
denotes the probability am-plitude.
Furthermore, the probability of |??
col-lapsing on |???
is P(??)
=|??|??
|??|?
?.Based on the hypothesis that QC can be con-sidered as a quantum system, the input datashould be transformed to an available format inquantum theory ?
the complex number vector.According to Euler?s formula, a complex numberz can be denoted as ?
= ????
with r?
?, ?
?
?.Equation 1, thus, can be written as:|??
=???|????????
(2)where ??
and ??
denote the module and thephase of the complex coefficient respectively.For different applications, we employ differentapproaches to determine the value of ??
and ?
?.Specifically, in our experiment, we assigned theterm frequency, a feature frequently used in textclassification to ??
, and treated the phase ??
asa constant, since we found the phase makes littlecontribution to the classification.For each data sample ??????
?, we calculatethe corresponding input complex number vectorby equation 3, which is illustrated in detail inFigure 2.|???
=????
?
??|????????
(3)Figure 2.
Process of calculating the input stateEach eigenstate |???
denotes the correspond-ing ???????
?, resulting in m eigenstates for  allthe samples.As is mentioned above, the evolutionary pro-cess of a closed physical system can be describedby a unitary operator, depicted by a matrix as inequation 4:|???
= ??|?
4))where |???
and |??
denote the final state and theinitial state respectively.
The approach to deter-mine the unitary operator will be discussed in485section 2.3.
We encode the target state in thesimilar way.
Like the Vector Space Model(VSM),we use a label matrix to represent each class as inFigure 3.Figure 3.
Label matrixFor each input sample ??????
?, we generatethe corresponding target complex number vectoraccording to equation 5:|???
=????
?
??|????????
(5)where each eigenstate |???
represents the corre-sponding ????
??
, resulting in w eigenstates forall the labels.
Totally, we need ?+?
eigen-states, including features and labels.2.3 Finding the Hamiltonian matrix and theUnitary operatorAs is mentioned in the first section, finding aunitary operator to describe the evolutionary pro-cess is the vital step in building a QC.
As a basicquantum mechanics theory, a unitary operatorcan be represented by a unitary matrix with theproperty ??
= ???
, and a unitary operator canalso be written as equation 6:?
= ?????
?
6))where H is the Hamiltonian matrix and ?
is thereduced Planck constant.
Moreover, the Hamil-tonian H is a Hermitian matrix with the property??
= (??)?
= ?.
The remaining job, therefore,is to find an optimal Hamiltonian matrix.Since H is a Hermitian matrix, we only needto determine (?
+?)?
free real parameters,provided that the dimension of H is (m+w).
Thus,the problem of determining H can be regarded asa classical optimization problem, which can beresolved by various optimization algorithms(Chen and Kudlek, 2001).
An error function isdefined as equation 7:?)???)
=??
???????????(??(??,??
(7)where T is a set of training pairs with ??
,??
, ???
??
denoting the target, input, and outputstate respectively, and ??
is determined by ??
asequation 8:|???
= ????
?8)                      ???|?
)In the optimization phase, we employed sever-al optimization algorithm, including BFGS, Ge-neric Algorithm, and a multi-objective optimiza-tion algorithm SQP (sequential quadratic pro-gramming) to optimize the error function.
In ourexperiment, the SQP method performed best out-performed the others.3 ExperimentWe tested the performance of QC on two differ-ent datasets.
In section 3.1, the Reuters-21578dataset was used to train a binary QC.
We com-pared the performance of QC with several classi-cal classification methods, including SupportVector Machine (SVM) and K-nearest neighbor(KNN).
In section 3.2, we evaluated the perfor-mance on multi-class classification using an oralconversation datasets and analyzed the results.3.1 Reuters-21578The Reuters dataset we tested contains 3,964texts belonging to ?earnings?
category and 8,938texts belonging to ?others?
categories.
In thisclassification task, we selected the features bycalculating the ??
score of each term from the?earnings?
category (Manning and Sch?tze,2002).For the convenience of counting, we adopted3,900 ?earnings?
documents and 8,900 ?others?documents and divided them into two groups: thetraining pool and the testing sets.
Since we fo-cused on the performance of QC trained bysmall-scale training sets in our experiment, weeach selected 1,000 samples from the ?earnings?and the ?others?
category as our training pooland took the rest of the samples (2,900 ?earnings?and 7,900 ?others?
documents) as our testing sets.We randomly selected training samples from thetraining pool ten times to train QC, SVM, andKNN classifier respectively and then verified thethree trained classifiers on the testing sets, theresults of which are illustrated in Figure 4.
Wenoted that the QC performed better than bothKNN and SVM on small-scale training sets,when the number of training samples is less than50.486Figure 4.
Classification accuracy for Reuters-21578 datasetsGenerally speaking, the QC trained by a largetraining set may not always has an ideal perfor-mance.
Whereas some single training samplepair led to a favorable result when we used onlyone sample from each category to train the QC.Actually, some single samples could lead to anaccuracy of more than 90%, while some othersmay produce an accuracy lower than 30%.Therefore, the most significant factor for QC isthe quality of the training samples rather than thequantity.3.2 Oral conversation datasetsBesides the binary QC, we also built a multi-class version and tested its performance on anoral conversation dataset which was collected bythe Laboratory of Computational Linguistics ofTsinghua university.
The dataset consisted of1,000 texts and were categorized into 5 classes,each containing 200 texts.
We still took the termfrequency as the feature, the dimension of whichexceeded 1,000.
We, therefore, utilized the pri-mary component analysis (PCA) to reduce thehigh dimension of the features in order to de-crease the computational complexity.
In this ex-periment, we chose the top 10 primary compo-nents of the outcome of PCA, which containednearly 60% information of the original data.Again, we focused on the performance of QCtrained by small-scale training sets.
We selected100 samples from each class to construct thetraining pool and took the rest of the data as thetesting sets.
Same to the experiment in section3.1, we randomly selected the training samplesfrom the training pool ten times to train QC,SVM, and KNN classifier respectively and verified the models on the testing sets, the results ofwhich are shown in Figure 5.Figure 5.
Classification accuracy for oralconversation datasets4 DiscussionWe present here our model of text classificationand compare it with SVM and KNN on two da-tasets.
We find that it is feasible to build a super-vised learning model based on quantum mechan-ics theory.
Previous studies focus on combiningquantum method with existing classificationmodels such as neural network (Chen et al, 2008)and kernel function (Nasios and Bors, 2007) aim-ing to improve existing models to work fasterand more efficiently.
Our work, however, focus-es on developing a novel method which exploresthe relationship between machine learning modelwith physical world, in order to investigate thesemodels by physical rule which describe our uni-verse.
Moreover, the QC performs well in textclassification compared with SVM and KNN andoutperforms them on small-scale training sets.Additionally, the time complexity of QC dependson the optimization algorithm and the amounts offeatures we adopt.
Generally speaking, simulat-ing quantum computing on classical computeralways requires more computation resources, andwe believe that quantum computer will tackle thedifficulty in the forthcoming future.
Actually,Google and NASA have launched a quantumcomputing AI lab this year, and we regard theproject as an exciting beginning.Future studies include: We hope to find amore suitable optimization algorithm for QC anda more reasonable physical explanation towardsthe ?quantum nature?
of the QC.
We hope ourattempt will shed some light upon the applicationof quantum theory into the field of machinelearning.487AcknowledgmentsThis work was supported by the National NaturalScience Foundation in China (61171114), StateKey Lab of Pattern Recognition open foundation,CAS.
Tsinghua University Self-determinationResearch Project (20111081023 & 20111081010)and Human & liberal arts development founda-tion (2010WKHQ009)ReferencesEsma A?meur, Gilles Brassard, and S?bastien Gambs.2006.
Machine Learning in a Quantum World.
Ca-nadian AI 2006Esma A?meur, Gilles Brassard and S?bastien Gambs.2007.
Quantum Clustering Algorithms.
Proceed-ings of the 24 th International Conference on Ma-chine LearningJoseph C.H.
Chen and Manfred Kudlek.
2001.
Dualityof Syntex and Semantics ?
From the View Point ofBrain as a Quantum Computer.
Proceedings of Re-cent Advances in NLPJoseph C.H.
Chen.
2001.
Quantum Computation andNatural Language Processing.
University of Ham-burg, Germany.
Ph.D. thesisJoseph C.H.
Chen.
2001.
A Quantum MechanicalApproach to Cognition and Representation.
Con-sciousness and its Place in Nature,Toward a Sci-ence of Consciousness.Cheng-Hung Chen, Cheng-Jian Lin and Chin-TengLin.
2008.
An efficient quantum neuro-fuzzy clas-sifier based on fuzzy entropy and compensatoryoperation.
Soft Comput, 12:567?583.Fumiyo Fukumoto and Yoshimi Suzuki.
2002.
Ma-nipulating Large Corpora for Text Classification.Proceedings of the Conference on Empirical Meth-ods in Natural Language ProcessingS?bastien Gambs.
2008.
Quantum classification,arXiv:0809.0444Lov K. Grover.
1997.
Quantum Mechanics Helps inSearching for a Needle in a Haystack.
Physical Review Letters, 79,325?328David Horn and Assaf Gottlieb.
2001.
The Method ofQuantum Clustering.
Proceedings of Advances inNeural Information Processing Systems .Christopher D. Manning and Hinrich Sch?tze.
2002.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.
Cambridge, Massachu-setts,USA.Nikolaos Nasios and Adrian G. Bors.
2007.
Kernel-based classification using quantum mechanics.
Pat-tern Recognition, 40:875?889Hartmut Neven and Vasil S. Denchev.
2009.
Traininga Large Scale Classifier with the Quantum Adia-batic Algorithm.
arXiv:0912.0779v1Michael A. Nielsen and Isasc L. Chuang.
2000.
Quan-tum Computation and Quantum Information, Cam-bridge University Press, Cambridge, UK.Masahide Sasaki and and Alberto Carlini.
2002.Quantum learning and universal quantum matchingmachine.
Physical Review, A 66, 022303Dan Ventura.
2002.
Pattern classification using aquantum system.
Proceedings of the Joint Confer-ence on Information Sciences.488
