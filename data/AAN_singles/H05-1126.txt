Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 1003?1010, Vancouver, October 2005. c?2005 Association for Computational LinguisticsSpeech-based Information Retrieval Systemwith Clarification Dialogue StrategyTeruhisa Misu Tatsuya KawaharaSchool of informaticsKyoto UniversitySakyo-ku, Kyoto, Japanmisu@ar.media.kyoto-u.ac.jpAbstractThis paper addresses a dialogue strategyto clarify and constrain the queries forspeech-driven document retrieval systems.In spoken dialogue interfaces, users oftenmake utterances before the query is com-pletely generated in their mind; thus inputqueries are often vague or fragmental.
Asa result, usually many items are matched.We propose an efficient dialogue frame-work, where the system dynamically se-lects an optimal question based on infor-mation gain (IG), which represents reduc-tion of matched items.
A set of possiblequestions is prepared using various knowl-edge sources.
As a bottom-up knowl-edge source, we extract a list of wordsthat can take a number of objects and po-tentially causes ambiguity, using a depen-dency structure analysis of the documenttexts.
This is complemented by top-downknowledge sources of metadata and hand-crafted questions.
An experimental evalu-ation showed that the method significantlyimproved the success rate of retrieval, andall categories of the prepared questionscontributed to the improvement.1 IntroductionThe target of spoken dialogue systems is being ex-tended from simple databases such as flight informa-tion (Levin et al, 2000; Potamianos et al, 2000) togeneral documents (Fujii and Itou, 2003) includingnewspaper articles (Chang et al, 2002; Hori et al,2003).
In such systems, the automatic speech recog-nition (ASR) result of the user utterance is matchedagainst a set of target documents using the vectorspace model, and documents with high matchingscores are presented to the user.In this kind of document retrieval systems, userqueries must include sufficient information to iden-tify the desired documents.
In conventional doc-ument query tasks with typed-text input, such asTREC QA Track (NIST and DARPA, 2003), queriesare (supposed to be) definite and specific.
However,this is not the case when speech input is adopted.The speech interface makes input easier.
However,this also means that users can start utterances beforequeries are thoroughly formed in their mind.
There-fore, input queries are often vague or fragmental,and sentences may be ill-formed or ungrammatical.Moreover, important information may be lost due toASR errors.
In such cases, an enormous list of possi-ble relevant documents is usually obtained becausethere is very limited information that can be usedas clues for retrieval.
Therefore, it is necessary tonarrow down the documents by clarifying the user?sintention through a dialogue.There have been several studies on the follow-updialogue, and most of these studies assume that thetarget knowledge base has a well-defined structure.For example, Denecke (Denecke and Waibel, 1997)addressed a method to generate guiding questionsbased on a tree structure constructed by unifyingpre-defined keywords and semantic slots.
However,these approaches are not applicable to general docu-1003Query utteranceSystem UserAutomatic speechrecognition (ASR)Confirmationfor robustretrievalConfirmationReplyMatchingwithKnowledgebase (KB)Knowledgebase(KB)Dialogue tonarrow downretrieveddocumentsQuestionReplyFinal resultpresented in this paperFigure 1: System overviewment sets without such structures.In this paper, we propose a dialogue strategy toclarify the user?s query and constrain the retrievalfor a large-scale text knowledge base, which doesnot have a structure nor any semantic slots.
In theproposed scheme, the system dynamically selects anoptimal question, which can reduce the number ofmatched items most efficiently.
As a criterion ofefficiency of the questions, information gain (IG)is defined.
A set of possible questions is preparedusing bottom-up and top-down knowledge sources.As a bottom-up knowledge source, we conduct de-pendency structure analysis of the document texts,and extract a list of words that can take a numberof objects, thus potentially causing ambiguity.
Thisis combined with top-down knowledge sources ofmetadata and hand-crafted questions.
The systemthen updates the query sentence using the user?s re-ply to the question, so as to generate a confirmationto the user.2 Document retrieval system forlarge-scale knowledge base2.1 System overviewWe have studied a dialogue framework to overcomethe problems in speech-based document retrievalsystems.
In the framework, the system can han-dle three types of problems caused by speech input:ASR errors, redundancy in spoken language expres-sion, and vagueness of queries.
First, the system re-alizes robust retrieval against ASR errors and redun-Table 1: Document set (Knowledge Base: KB)Text collection # documents text size(byte)glossary 4,707 1.4MFAQ 11,306 12MDB of support articles 23,323 44Mdancies by detecting and confirming them.
Then, thesystem makes questions to clarify the user?s queryand narrow down the retrieved documents.The system flow of these processes is summarizedbelow and also shown in Figure 1.1.
Recognize the user?s query utterance.2.
Make confirmation for phrases which may in-clude critical ASR errors.3.
Retrieve from knowledge base (KB).4.
Ask possible questions to the user and narrowdown the matched documents.5.
Output the retrieval results.In this paper, we focus on the latter stage of theproposed framework, and present a clarification dia-logue strategy to narrow down documents.2.2 Task and back-end retrieval systemOur task involves text retrieval from a large-scaleknowledge base.
For the target domain, we adopt asoftware support knowledge base (KB) provided byMicrosoft Corporation.
The knowledge base con-sists of the following three kinds: glossary, fre-quently asked questions (FAQ), and support articles.The specification is listed in Table 1, and there areabout 40K documents in total.
An example of sup-port article is shown in Figure 2.Dialog Navigator (Kiyota et al, 2002) has beendeveloped at University of Tokyo as a retrieval sys-tem for this KB.
The system accepts a typed-text in-put from users and outputs a result of the retrieval.The system interprets an input sentence by takingsyntactic dependency and synonymous expressioninto consideration for matching it with the KB.
Thetarget of the matching is the summaries and detailinformation in the support articles, and the titles ofthe Glossary and FAQ.
The retrieved result is dis-played to the user as the list of documents like Web1004?
?HOWTO:Use Speech Recognition in Windows XPThe information in this article applies to:?
Microsoft Windows XP Professional?
Microsoft Windows XP Home EditionSummary: This article describes how to use speechrecognition in Windows XP.
If you installed speechrecognition with Microsoft Office XP, or if you pur-chased a new computer that has Office XP installed,you can use speech recognition in all Office pro-grams as well as other programs for which it is en-abled.Detail information: Speech recognition enables the op-erating system to convert spoken words to writtentext.
An internal driver, called a speech recognitionengine, recognizes words and converts them to text.The speech recognition engine ...?
?Figure 2: Example of software support articlesearch engines.
Since the user has to read detailinformation of the retrieved documents by clickingtheir icons one by one, the number of items in thefinal result is restricted to about 15.In this work, we adopt Dialog Navigator as aback-end system and construct a spoken dialogue in-terface.3 Dialogue strategy to clarify user?s vaguequeries3.1 Dialogue strategy based on informationgain (IG)In the proposed clarification dialogue strategy, thesystem asks optimal questions to constrain the givenretrieval results and help users find the intendedones.
Questions are dynamically generated by se-lecting from a pool of possible candidates that sat-isfy the precondition.
The information gain (IG)is defined as a criterion for the selection.
The IGrepresents a reduction of entropy, or how many re-trieved documents can be eliminated by incorpo-rating additional information (a reply to a questionin this case).
Its computation is straightforward ifthe question classifies the document set in a com-pletely disjointed manner.
However, the retrieveddocuments may belong to two or more categories forsome questions, or may not belong to any category.For example, some documents in our KB are relatedwith multiple versions of MS-Office, but others maybe irrelevant to any of them.
Moreover, the match-ing score of the retrieved documents should be takeninto account in this computation.
Therefore, we de-fine IG H(S) for a candidate question S by the fol-lowing equations.H(S) = ?n?i=0P (i) ?
log P (i)P (i) =|Ci|?ni=0|Ci||Ci| =?Dk?iCM(Dk)Here, Dkdenotes the k-th retrieved document bymatching the query to the KB, and CM(D) denotesthe matching score of document D. Thus, Cirep-resents the number of documents classified into cat-egory i by candidate question S, which is weightedwith the matching score.
The documents that are notrelated to any category are classified as category 0.The system flow incorporating this strategy issummarized below and also shown in Figure 3.1.
For a query sentence, retrieve from KB.2.
Calculate IG for all possible candidate ques-tions which satisfy precondition.3.
Select the question with the largest IG (largerthan a threshold), and ask the question to theuser.
Otherwise, output the current retrieval re-sult.4.
Update the query sentence using the user?s re-ply to the question.5.
Return to 1.This procedure is explained in detail in the fol-lowing sections.3.2 Question generation based on bottom-upand top-down knowledge sourcesWe prepare a pool of questions using three methodsbased on bottom-up knowledge together with top-down knowledge of KB.
For a bottom-up knowledge1005System UserMatching withknowledge base (KB)Knowledge base(KB)Any questionwith large IG?
NORetrieval resultYESSelect question withlargest IG for clarificationQuestionUpdatequery sentence ReplyQuestion poolFigure 3: Overview of query clarificationTable 2: Examples of candidate questions (Dependency structure analysis: method 1)Question Precondition Ratio of IGapplicable doc.What did you delete?
Query sentence includes ?delete?
2.15 (%) 7.44What did you install?
Query sentence includes ?install?
3.17 (%) 6.00What did you insert?
Query sentence includes ?insert?
1.12 (%) 7.12What did you save?
Query sentence includes ?save?
1.81 (%) 6.89What is the file type?
Query sentence includes ?file?
0.94 (%) 6.00What did you setup?
Query sentence includes ?setup?
0.69 (%) 6.45source, we conducted a dependency structure anal-ysis on KB.
As for top-down knowledge, we makeuse of metadata included in KB and human knowl-edge.3.2.1 Questions based on dependency structureanalysis (method 1)This type of question is intended to clarify themodifier or object of some words, based on de-pendency structure analysis, when they are uncer-tain.
For instance, the verb ?delete?
can have var-ious objects such as ?application program?
or ?ad-dress book?.
Therefore, the query can be clarified byidentifying such objects if they are missing.
How-ever, not all words need to be confirmed because themodifier or object can be identified almost uniquelyfor some words.
For instance, the object of theword ?shutdown?
is ?computer?
in most cases in thistask domain.
It is tedious to identify the object ofsuch words.
We therefore determine the words to beconfirmed by calculating entropy for modifier-headpairs from the text corpus.
The procedure is as fol-lows.1.
Extract all modifier-head pairs from the text ofKB and query sentences (typed input) to an-other retrieval system1 provided by MicrosoftJapan.2.
Calculate entropy H(m) for every word basedon probability P (i).
This P (i) is calculatedwith the occurrence count N(m) of word mthat appears in the text corpus and the countN(i, m) of word m whose modifier is i.H(m) = ?
?iP (i) ?
log P (i)P (i) =N(i, m)N(m)1http://www.microsoft.com/japan/enable/nlsearch/1006Table 3: Examples of candidate questions (Metadata: method 2)Question Precondition Ratio of IGapplicable doc.What is the version None 30.03 (%) 2.63of your Windows?What is your application?
None 30.28 (%) 2.31What is the version Query sentence includes ?Word?
3.76 (%) 2.71of your Word?What is the version Query sentence includes ?Excel?
4.13 (%) 2.44of your Excel?Table 4: List of candidate questions (Human knowledge: method 3)Question Precondition Ratio of IGapplicable doc.When did the symptom occur?
None 15.40 (%) 8.08Tell me the error message.
Query sentence includes ?error?
2.63 (%) 8.61What do you concretely None 6.98 (%) 8.04want to do?As a result, we selected 40 words that have a largevalue of entropy.
Question sentences for these wordswere generated with a template of ?What did you...??
and unnatural ones were corrected manually.Categories for IG calculation are defined by objectsof these words included in matched documents.
Thesystem can make question using this method whenthese words are included in the user?s query.
Ta-ble 2 lists examples of candidate questions using thismethod.
In this table, ratio of applicable documentcorresponds to the ratio of documents that includethe words selected above, and IG is calculated usingapplicable documents.3.2.2 Questions based on metadata included inKB (method 2)We also prepare candidate questions using themetadata attached to the KB.
In general large-scaleKBs, metadata is usually attached to manage themefficiently.
For example, category information is at-tached to newspaper articles and books in libraries.In our target KB, a number of documents includemetadata of product names to which the documentapplies.
The system can generate question to whichthe user?s query corresponds using this metadata.However, some documents are related with multipleversions, or may not belong to any category.
There-fore, the performance of these questions greatly de-pends on the characteristics of the metadata.Fourteen candidate questions are prepared usingthis method.
Example of candidate questions arelisted in Table 3.
Ratio of applicable document cor-responds to the ratio of documents that have meta-data of target products.3.2.3 Questions based on human knowledge(method 3)Software support is conventionally provided byoperators at call centers.
We therefore prepare can-didate questions based on the human knowledge thathas been accumulated there.
This time, three kindsof questions are hand-crafted.
For instance, thequestion ?When did the symptom occur??
tries tocapture key information to identify relevant docu-ments.
The categories for IG caluclation are definedusing hand-crafted rules by focusing on key-phrasessuch as ?after ...?
or ?during ...?.
Candidate ques-tions are listed in Table 4.An example dialogue where the system asks ques-tions based on IG is in Figure 4.3.3 Update of retrieval query sentenceThrough the dialogue to clarify the user?s query,the system updates the query sentence using theuser?s reply to the question.
Our backend informa-tion retrieval system does not adopt simple ?bag-1007S1: What is your problem?U1: Too garbled to read.
(Retrieval results):?
?1.
Close button and maximize button are garbled.2.
Characters are garbled in Outlook Today.3.
Characters are garbled while inserting Japanesetext.4.
VB application is garbled to read.?
?
??
?
(Calculate IG)?
Candidate question 1:What is garbled to read?
?
IG 5.27?
Candidate question 2:What is the version of your Windows?
?
IG 1.43?
Candidate question 3:When did the symptom occur?
?
IG 2.47?
?
?S2: (Select question with largest IG)What is garbled to read?U2: Characters on window button.S3: (Update query sentence)Retrieving with ?Characters on window button are too garbledto read?.Figure 4: Example dialogueof-words?
model, but conducts a more precise de-pendency structure analysis for matching; thereforeforming an appropriate query sentence is desirablerather than simply adding keywords.
Moreover, it ismore comprehensible to the user to present the up-dated query sentence than to show the sequence ofASR results.
Here, the update rules of the query sen-tence are prepared as follows.1.
Questions based on dependency structure anal-ysisThe user?s reply is added immediately beforeof after the word that is the reply?s modifyinghead.
For instance, the reply to the question?What did you delete??
is inserted right afterthe word ?delete?
in the query sentence.2.
Questions based on metadata of KBPhrases ?In {Product name} {version name}?are added to the query sentence.3.
Questions based on human knowledgeThe position where the user?s reply is insertedis specified beforehand for each question can-didate.
For instance, the reply to the question?Tell me the error message.?
is inserted rightafter the word ?error?
in the query sentence.A dialogue example where the system updates theuser?s query is shown in Figure 5.
In the exam-ple, the systemmakes confirmation ?Retrieving with?When I try to open it in explorer, I cannot open Ex-cel 2002 file?
?
at the end of the dialogue before pre-senting the actual retrieval result.3.4 Experimental evaluationWe implemented and evaluated the proposedmethod.
We collected a test data by 14 subjects whohad not used our system Each subject was requestedto retrieve support articles for 14 tasks, which con-sisted of prepared scenarios (query sentences werenot given).
The subjects were allowed to utter aquery again up to twice per task if they thought anadequate retrieval result was not obtained.
As a re-sult, we collected 238 utterances for 196 (=14 ?
14)tasks in total.
An example of scenario and user ut-terances are shown in Figure 6.
The average wordaccuracy of ASR was 82.9%.
The threshold value inIG that the system makes a question is set to 1.0 ini-1008S1: What is your problem?U1: I cannot open the file.S2: What is the file type?
(method 1)U2: Excel file.
(Update query sentence): ?I cannot open Excel file.
?S3: What is the version of your Excel?
(method 2)U3: My Excel is version 2002.
(Update query sentence): ?I cannot open Excel 2002 file.
?S4: When did the symptom occur?
(method 3)U4: Tried to open it in explorer.S5: Retrieving with ?When I try to open it in explorer, I cannotopen Excel 2002 file?.Figure 5: Query sentence update using user?s reply?
??
An example of scenarioYou are looking for restaurant in Kyoto usingWWW.
You have found a nice restaurant and triedto print out an image of the map showing the restau-rant.
However, it is not printed out.
(Your browseris IE 6.0)?
Examples of users?
utterance?
I want to print an image of map.?
I can?t print out.?
I failed to print a picture in homepage usingIE.?
Please tell me how to print out an image.?
?Figure 6: Example of scenario and user utterancestially, and incremented by 0.3 every time the systemgenerates a question through a dialogue session.First, we evaluated the success rate of retrieval.We regarded a retrieval as successful when the re-trieval result contained a correct document entry forthe scenario.
We compared the following cases.1.
Transcript: A correct transcript of the user ut-terance, prepared manually, was used as an in-put.2.
ASR result (baseline): The ASR result wasused as an input.3.
Proposed method (log data): The system gener-ated questions based on the proposed method,and the user replied to them as he/she thoughtappropriate.We also evaluated the proposed method by simu-lation in order to confirm its theoretical effect.
Var-ious factors of the entire system might influence theperformance in real dialogue which is evaluated bythe log data.
Specifically, the users might not haveanswered the questions appropriately, or the repliesmight not have been correctly recognized.
There-fore, we also evaluated with the following condition.4.
Proposed method (simulation): The systemgenerated questions based on the proposedmethod, and appropriate answers were givenmanually.Table 5 lists the retrieval success rate and the rank ofthe correct document in the retrieval result, by thesecases.
The proposed method achieved a better suc-cess rate than when the ASR result was used.
Animprovement of 12.6% was achieved in the simula-tion case, and 7.7% by the log data.
These figuresdemonstrate the effectiveness of the proposed ap-proach.
The success rate of the retrieval was about5% higher in the simulation case than the log data.This difference is considered to be caused by follow-ing factors.1.
ASR errors in user?s uttered repliesIn the proposed strategy, the retrieval sentenceis updated using the user?s reply to the questionregardless of ASR errors.
Even when the usernotices the ASR errors, he/she cannot correctthem.
Although it is possible to confirm themusing ASR confidence measures, it makes di-alogue more complicated.
Hence, it was notimplemented this time.2.
User?s misunderstanding of the system?s ques-tionsUsers sometimes misunderstood the system?squestions.
For instance, to the system question?When did the symptom occur?
?, some user1009Table 5: Success rate and average rank of correctdocument in retrievalSuccess Rank ofrate correct doc.Transcript 76.1% 7.20ASR result (baseline) 70.7% 7.45Proposed method 78.4% 4.40(log data)Proposed method 83.3% 3.85(simulation)Table 6: Comparison of question methodsSuccess # generatedrate questions(per dialogue)ASR result (baseline) 70.7% ?Dependency structure 74.5% 0.38analysis (method 1)Metadata (method 2) 75.7% 0.89Human knowledge 74.5% 0.97(method 3)All methods 83.3% 2.24(method 1-3)replied simply ?just now?
instead of key infor-mation for the retrieval.
To this problem, it maybe necessary to make more specific questionsor to display reply examples.We also evaluated the efficiency of the individualmethods.
In this experiment, each of the three meth-ods was used to generate questions.
The results arein Table 6.
The improvement rate by the three meth-ods did not differ very much, and most significantimprovement was obtained by using the three meth-ods together.
While the questions based on humanknowledge are rather general and were used moreoften, the questions based on the dependency struc-ture analysis are specific, and thus more effectivewhen applicable.
Hence, the questions based on thedependency structure analysis (method 1) obtaineda relatively high improvement rate per question.4 ConclusionWe proposed a dialogue strategy to clarify user?queries for document retrieval tasks.
Candidatequestions are prepared based on the dependencystructure analysis of the KB together with KB meta-data and human knowledge.
The system selects anoptimal question based on information gain (IG).Then, the query sentence is updated using the user?sreply.
An experimental evaluation showed that theproposed method significantly improved the successrate of retrieval, and all categories of the preparedquestions contributed to the improvement.The proposed approach is intended for restricteddomains, where all KB documents and severalknowledge sources are available, and it is not ap-plicable to open-domain information retrieval suchas Web search.
We believe, however, that there aremany targets of information retrieval in restricteddomains, for example, manuals of electric appli-ances and medical documents for expert systems.The methodology proposed here is not so dependenton the domains, thus applicable to many other tasksof this category.5 AcknowledgementsThe authors are grateful to Prof. Kurohashi and Dr.Kiyota at University of Tokyo and Dr. Komatani atKyoto University for their helpful advice.ReferencesE.
Chang, F. Seide, H. M. Meng, Z. Chen, Y. Shi, and Y. C. Li.2002.
A system for spoken query information retrieval onmobile devices.
IEEE Trans.
on Speech and Audio Process-ing, 10(8):531?541.M.
Denecke and A. Waibel.
1997.
Dialogue strategies guid-ing users to their communicative goals.
In Proc.
EU-ROSPEECH.A.
Fujii and K. Itou.
2003.
Building a test collection forspeech-driven Web retrieval.
In Proc.
EUROSPEECH.C.
Hori, T. Hori, H. Isozaki, E. Maeda, S. Katagiri, and S. Furui.2003.
Deriving disambiguous queries in a spoken interactiveODQA system.
In Proc.
IEEE-ICASSP.Y.
Kiyota, S. Kurohashi, and F. Kido.
2002.
?Dialog Nav-igator?
: A question answering system based on large textknowledge base.
In Proc.
COLING, pages 460?466.E.
Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Bocchieri,G.
Di Fabbrizio, W. Eckert, S. Lee, A. Pokrovsky, M. Rahim,P.
Ruscitti, and M. Walker.
2000.
The AT&T-DARPA Com-municator mixed-initiative spoken dialogue system.
In Proc.ICSLP.NIST and DARPA.
2003.
The twelfth Text REtrieval Confer-ence (TREC 2003).
In NIST Special Publication SP 500?255.A.
Potamianos, E. Ammicht, and H.-K. J. Kuo.
2000.
Dia-logue management in the Bell labs Communicator system.In Proc.
ICSLP.1010
