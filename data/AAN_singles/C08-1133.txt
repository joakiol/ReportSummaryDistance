Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1057?1064Manchester, August 2008OntoNotes: Corpus Cleanup of Mistaken Agreement UsingWord Sense DisambiguationLiang-Chih Yu and Chung-Hsien WuDept.
of Computer Science and Information EngineeringNational Cheng Kung UniversityTainan, Taiwan, R.O.C.
{lcyu,chwu}@csie.ncku.edu.twEduard HovyInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292, USAhovy@isi.eduAbstractAnnotated corpora are only useful if theirannotations are consistent.
Most large-scaleannotation efforts take special measures toreconcile inter-annotator disagreement.
Todate, however, no-one has investigated howto automatically determine exemplars inwhich the annotators agree but are wrong.
Inthis paper, we use OntoNotes, a large-scalecorpus of semantic annotations, includingword senses, predicate-argument structure,ontology linking, and coreference.
To de-termine the mistaken agreements in wordsense annotation, we employ word sensedisambiguation (WSD) to select a set ofsuspicious candidates for human evaluation.Experiments are conducted from three as-pects (precision, cost-effectiveness ratio, andentropy) to examine the performance ofWSD.
The experimental results show thatWSD is most effective on identifying erro-neous annotations for highly-ambiguouswords, while a baseline is better for othercases.
The two methods can be combined toimprove the cleanup process.
This procedureallows us to find approximately 2% remain-ing erroneous agreements in the OntoNotescorpus.
A similar procedure can be easilydefined to check other annotated corpora.1 IntroductionWord sense annotated corpora are useful re-sources for many natural language applications.Various machine learning algorithms can then betrained on these corpora to improve the applica-tions?
effectiveness.
Lately, many such corporahave been developed in different languages, in-cluding SemCor (Miller et al, 1993), LDC-DSO(Ng and Lee, 1996), Hinoki (Kasahara et al,2004), and the sense annotated corpora with thehelp of Web users (Chklovski and Mihalcea,2002).
The SENSEVAL1 (Kilgarriff and Palmer,2000; Kilgarriff, 2001; Mihalcea and Edmonds,2004) and SemEval-20072 evaluations have alsocreated large amounts of sense tagged data forword sense disambiguation (WSD) competitions.The OntoNotes (Pradhan et al, 2007a; Hovy etal., 2006) project has created a multilingual cor-pus of large-scale semantic annotations, includ-ing word senses, predicate-argument structure,ontology linking, and coreference3.
In word sensecreation, sense creators generate sense definitionsby grouping fine-grained sense distinctions ob-tained from WordNet and dictionaries into morecoarse-grained senses.
There are two reasons forthis grouping instead of using WordNet sensesdirectly.
First, people have trouble distinguishingmany of the WordNet-level distinctions in realtext, and make inconsistent choices; thus the useof coarse-grained senses can improve inter-annotator agreement (ITA) (Palmer et al, 2004;2006).
Second, improved ITA enables machinesto more accurately learn to perform sense taggingautomatically.
Sense grouping in OntoNotes hasbeen calibrated to ensure that ITA averages atleast 90%.
Table 1 shows the OntoNotes sense1 http://www.senseval.org2 http://nlp.cs.swarthmore.edu/semeval3 Year 1 of the OntoNotes corpus has been re-leased by Linguistic Data Consortium (LDC)(http://www.ldc.upenn.edu) in early 2007.
TheYear 2 corpus will be released in early 2008.?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Un-ported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.1057tags and definitions for the word arm (nounsense).
The OntoNotes sense tags have been usedfor many applications, including the SemEval-2007 evaluation (Pradhan et al, 2007b), sensemerging (Snow et al, 2007), sense pool verifica-tion (Yu et al, 2007), and class imbalance prob-lems (Zhu and Hovy, 2007).In creating OntoNotes, each word sense anno-tation involves two annotators and an adjudicator.First, all sentences containing the target wordalong with its sense distinctions are presentedindependently to two annotators for sense annota-tion.
If the two annotators agree on the samesense for the target word in a given sentence,then their selection is stored in the corpus.
Oth-erwise, this sentence is double-checked by theadjudicator for the final decision.
The majorproblem of the above annotation scheme is thatonly the instances where the two annotators dis-agreed are double-checked, while those showingagreement are stored directly without any adjudi-cation.
Therefore, if the annotators happen toagree but are both wrong, then the corpus be-comes polluted by the erroneous annotations.Table 2 shows an actual occurrence of an errone-ous instance (sentence) for the target word man-agement.
In this example sentence, the actualsense of the target word is management.01, butboth of our annotators made a decision of man-agement.02.
(Note that there is no difficulty inmaking this decision; the joint error might haveoccurred due to annotator fatigue, habituationafter a long sequence of management.02 deci-sions, etc.
)Although most annotations in OntoNotes arecorrect, there is still a small (but unknown) frac-tion of erroneous annotations in the corpus.Therefore, some cleanup procedure is necessaryto produce a high-quality corpus.
However, it isimpractical for human experts to evaluate thewhole corpus for cleanup.
Given that we are fo-cusing on word senses, this study proposes theuse of WSD to facilitate the corpus cleanup proc-ess.
WSD has shown promising accuracy in re-cent SENSEVAL and SemEval-2007 evaluations.The rest of this work is organized as follows.Section 2 describes the corpus cleanup procedure.Section 3 presents the features for WSD.
Section4 summarizes the experimental results.
Conclu-sions are drawn in Section 5.2 Corpus Cleanup ProcedureFigure 1 shows the cleanup procedure (dashedlines) for the OntoNotes corpus.
As mentionedearlier, each word along with its sentence in-stances is annotated by two annotators.
The anno-Sense Tag Sense Definition WordNet sensearm.01 The forelimb of an animal WN.1arm.02 A weapon WN.2arm.03 A subdivision or branch of an organization WN.3arm.04 A projection, a narrow extension of a structure WN.4 WN.5Table 1.
OntoNotes sense tags and definitions.
The WordNet version is 2.1.Example sentence:The 45-year-old Mr. Kuehn, who has a background in crisis management, succeeds Alan D. Rubendall, 45.management.01: Overseeing or directing.
Refers to the act of managing something.He was given overall management of the program.I'm a specialist in risk management.The economy crashed because of poor management.management.02: The people in charge.
The ones actually doing the managing.Management wants to start downsizing.John was promoted to Management.I spoke to their management, and they're ready to make a deal.Table 2.
Example sentence for the target word management along with its sense definitions.1058tated corpus can thus be divided into two partsaccording to the annotation results.
The first partincludes the annotation with disagreement amongthe two annotators, which is then double-checkedby the adjudicator.
The final decisions made bythe adjudicator are stored into the corpus.
Sincethis part is double-checked by the adjudicator, itwill not be evaluated by the cleanup procedure.The second part of the corpus is the focus ofthe cleanup procedure.
The WSD system evalu-ates each instance in the second part.
If the outputof the WSD system disagrees with the two anno-tators, the instance is considered to be a suspi-cious candidate, otherwise it is considered to beclean and stored into the corpus.
The set of sus-picious candidates is collected and subsequentlyevaluated by the adjudicator to identify erroneousannotations.3 Word Sense DisambiguationThis study takes a supervised learning approachto build a WSD system from the OntoNotes cor-pus.
The feature set used herein is similar to sev-eral state-of-the-art WSD systems (Lee and Ng.,2002; Ando, 2006; Tratz et al, 2007; Cai et al,2007; Agirre and Lopez de Lacalle, 2007; Speciaet al, 2007), which is further integrated into aNa?ve Bayes classifier (Lee and Ng., 2002; Mi-halcea, 2007).
In addition, a new feature, predi-cate-argument structure, provided by theOntoNotes corpus is also integrated.
The featureset includes:Part-of-Speech (POS) tags: This feature in-cludes the POS tags in the positions (P-3, P-2, P-1,P0, P1, P2, P3), relative to the POS tag of the tar-get word.Local Collocations: This feature includes singlewords and multi-word n-grams.
The single wordsinclude (W-3, W-2, W-1, W0, W1, W2, W3), relativeto the target word W0.
Similarly, the multi-wordn-grams include (W-2,-1, W-1,1, W1,2, W-3,-2,-1, W-2,-1,1,W-1,1,2, W1,2,3).Bag-of-Words: This feature can be considered asa global feature, consisting of 5 words prior toand after the target word, without regard to posi-tion.Predicate-Argument Structure: The predicate-argument structure captures the semantic rela-tions between the predicates and their argumentswithin a sentence, as shown in Figure 2.
Theserelations can be either direct or indirect.
A directrelation is used to model a verb-noun (VN) ornoun-verb (NV) relation, whereas an indirect re-lation is used to model a noun-noun (NN) rela-tion.
Additionally, an NN-relation can be builtfrom the combination of an NV-relation and VN-relation.
For instance, in Figure 2, the NN-relation (R3) can be built by combining the NV-relation (R1) the VN-relation (R2).
Therefore, thetwo features, R1 and R3, can be used to disam-biguate the noun arm 4.4 Experimental Results4.1 Experiment setupThe experiment data used herein was the 35nouns from the SemEval-2007 English LexicalSample Task (Pradhan et al, 2007b).
All sen-tences containing the 35 nouns were selectedfrom the OntoNotes corpus, resulting in a set of16,329 sentences.
This data set was randomlysplit into training and test sets using differentproportions (1:9 to 9:1, 10% increments).
TheWSD systems (described in Section 3) were then4 Our WSD system does not include the senseidentifier (except for the target word) for word-level training and testing.The New York arm.03  ...  auctioned.01 off the estate.01ARG0-INV ARG1ARG0-INV-ARG1NV-relation: (arm.03, ARG0-INV, auction.01)VN-relation: (auction.01, ARG1, estate.01)NN-relation: (arm.03, ARG0-INV-ARG1, estate.01)Figure 2.
Example of predicate-argument struc-ture.
The label ?-INV?
denotes an inverse direc-tion (i.e.,  from a noun to a verb).final decisionAnnotation withagreementAnnotation withdisagreementAdjudicatorWSDagree withannotatorsdisagree with annotatorsAnnotatedCorpusFigure 1.
Corpus cleanup procedure.1059built from the different portions of the trainingset, called WSD_1 to WSD_9, respectively, andapplied to their corresponding test sets.
In eachtest set, the instances with disagreement amongthe annotators were excluded, since they havealready been double-checked by the adjudicator.A baseline system was also implemented usingthe principle of most frequent sense (MFS),where each word sense distribution was retrievedfrom the OntoNotes corpus.
Table 3 shows theaccuracy of the baseline and WSD systems.The output of WSD may agree or disagreewith the annotators.
The instances with dis-agreement were selected from each WSD systemas suspicious candidates.
This experiment ran-domly selected at most 20 suspicious instancesfor each noun to form a suspicious set of 687 in-stances.
An adjudicator who is a linguistic expertthen evaluated the suspicious set, and agreed in42 instances with the WSD systems, indicatingabout 6% (42/687) truly erroneous annotations.This corresponds to 2.6% (42/16329) erroneousannotations in the corpus as a whole, which weverified by an independent random spot check.In the following sections, we examine the per-formance of WSD from three aspects: precision,cost-effectiveness ratio, and entropy, and finallysummarize a general cleanup procedure for othersense annotated corpora.4.2 Cleanup precision analysisThe cleanup precision for a single WSD systemcan be defined as the number of erroneous in-stances identified by the WSD system, divided bythe number of suspicious candidates selected bythe WSD system.
An erroneous instance refers toan instance where the annotators agree with eachother but disagree with the adjudicator.
Table 4lists the cleanup precision of the baseline andWSD systems.
The experimental results showthat WSD_7 (trained on 70% training data) iden-tified 17 erroneous instances, out of 120 selectedsuspicious candidates, thus yielding the highestprecision of 0.142.
Another observation is thatthe upper bound of WSD_7 was 0.35 (42/120)under the assumption that it identified all errone-ous instances.
This low precision discourages theuse of WSD to automatically correct erroneousannotations.4.3 Cleanup cost-effectiveness analysisThe cleanup procedure used herein is a semi-automatic process; that is, WSD is applied in thefirst stage to select suspicious candidates for hu-man evaluation in the later stage.
Obviously, wewould like to minimize the number of candidatesthe adjudicator has to examine.
Thus we define ametric, the cost-effectiveness (CE) ratio, tomeasure the performance of WSD.
The cost rateis defined as the number of suspicious instancesselected by a single WSD system, divided by thetotal number of suspicious instances in the suspi-cious set.
The effectiveness rate is defined as thenumber of erroneous instances identified by asingle WSD system, divided by the total numberof erroneous instances in the suspicious set.
Inthis experiment, the baseline value of the cost-effectiveness ratio is 1, which means that humanexpert needs to evaluate all 687 instances in thesuspicious set to identify 42 erroneous instances.Figure 3 illustrates the CE ratio of the WSD sys-tems.
The most cost-effective WSD system wasWSD_7.
The CE ratios of the baseline andWSD_7 are listed in Table 5.
The experimentalresults indicate that 17.5% of suspicious in-stances were required to be evaluated to identifyabout 40% erroneous annotations when usingWSD_7.WSD  Baseline(MFS) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Accuracy 0.696 0.751 0.798 0.809 0.819 0.822 0.824 0.831 0.836 0.832Table 3.
Accuracy of the baseline and WSD systems with different training portions.WSD  Baseline (MFS) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Prec 0.090 (17/188)0.113(20/177)0.112(16/143)0.113(17/150)0.124(16/129)0.123(15/122)0.127(16/126)0.142(17/120)0.130(14/108)0.125(14/112)Table 4.
Cleanup precision of the baseline and WSD systems with different training portions.10604.4 Entropy analysisSo far, the experimental results show that the bestWSD system can help human experts identifyabout 40% erroneous annotations, but it stillmissed the other 60%.
To improve performance,we conducted experiments to analyze the effectof word entropy with respect to WSD perform-ance on identifying erroneous annotations.For the SemEval 35 nouns used in this experi-ment, some words are very ambiguous and somewords are not.
This property of ambiguity mayaffect the performance of WSD systems on iden-tifying erroneous annotation.
To this end, thisexperiment used entropy to measure the ambigu-ity of words (Melamed, 1997).
The entropy ofword can be computed by the word sense distri-bution, defined as2( ) ( )log ( ),ii iws WH W P ws P ws?= ?
?
(1)where ( )H W  denotes the entropy of a word W,and iws  denotes a word sense.
A high entropyvalue indicates a high ambiguity level.
For in-stance, the noun defense has 7 senses (see Table7) in the OntoNotes corpus, occurring with thedistribution {.14, .18, .19, .08, .04, .28, .09}, thusyielding a relative high entropy value (2.599).Conversely, the entropy of the noun rate is low(0.388), since it has only two senses with veryskewed distribution {.92, .08}.Consider the two groups of the SemEval nouns:the nouns for which at least one (Group 1) ornone (Group 2) of their erroneous instances canbe identified by the machine.
The average en-tropy of these two groups of nouns was computed,as shown in Table 6.
An independent t-test wasthen used to determine whether or not the differ-ence of the average entropy among these twogroups was statistically significant.
The experi-mental results show that WSD_7 was more effec-tive on identifying erroneous annotationsoccurring in highly-ambiguous words (p<0.05),while the baseline system has no such tendency(p=0.368).Table 7 shows the detail analysis of WSD per-formance on different words.
As indicated,WSD_7 identified the erroneous instances (7/7)occurring in the two top-ranked highly-ambiguous nouns, i.e., defense and position, butmissed all those (0/12) occurring in the two mostunambiguous words, i.e., move and rate.
The ma-jor reason is that the sense distribution of unam-biguous words is often skew, thus WSD systemsbuilt from such imbalanced data tend to sufferfrom the over-fitting problem; that is, tend toover-fit the predominant sense class and ignoresmall sense classes (Zhu and Hovy, 2007).
Fortu-nately, the over-fitting problem can be greatlyreduced when the entropy of words exceeds acertain threshold (e.g., the dashed line in Table 7),since the word sense has become more evenlydistributed.4.5 Combination of WSD and MFSAnother observation from Table 7 is that WSD_7identified more erroneous instances when theword entropy exceeded the cut-point, since theover-fitting problem was reduced.
Conversely,MFS identified more ones when the word entropyis below the cut-point.
This finding encouragesthe use of a combination of WSD_7 and MFS forcorpus cleanup; that is, different strategies can beused with different entropy intervals.
For thisexperiment data, MFS and WSD_7 can be ap-plied below and above the cut-point, respectively,to select the suspicious instances for humanevaluation.
As illustrated in Figure 4, when theentropy of words increased, the accumulated ef-fectiveness rates of both WSD_7 and MFS in-creased accordingly, since more erroneousinstances were identified.
Additionally, the dif-ference of the accumulated effect rate of MFSCost Effect CE RatioBaseline(MFS)0.274(188/687)0.405(17/42) 1.48WSD_7 0.175 (120/687)0.405(17/42) 2.31Table 5.
CE ratio of the baseline and WSD_7.Figure 3.
CE ratio of WSD systems with differ-ent training portions.1061and WSD_7 increased gradually from the begin-ning until the cut-point, since MFS identifiedmore erroneous instances than WSD_7 did in thisstage.
When the entropy exceeded the cut-point,WSD_7 was more effective and thus its effec-tiveness rate kept increasing, while that of MFSincreased slowly, thus their difference was de-creased with the rise of the entropy.
For the com-bination of MFS and WSD_7, its effectivenessrate before the cut-point was the same as that ofMFS, since MFS was used in this stage to selectthe suspicious set.
When WSD was used after thecut-point, the effectiveness rate of the combina-tion system increased continuously, and finallyreached 0.5 (21/42).Based on the above experimental results, themost cost-effective way for corpus cleanup is touse the combination method and begin with themost ambiguous words, since the WSD system inthe combination method is more effective onidentifying erroneous instances occurring inhighly-ambiguous words and these words arealso more important for many applications.
Fig-ure 5 shows the curve of the CE ratios of thecombination method by starting with the mostambiguous word.
The results indicate that the CEratio of the combination method decreasedgradually after more words with lower entropywere involved in the cleanup procedure.
Addi-tionally, the CE ratio of the combination methodwas improved by using MFS after the cut-pointand finally reached 2.50, indicating that 50%(21/42) erroneous instances can be identified bydouble-checking 20% (137/687) of the suspiciousset.
This CE ratio was better than 2.31 and 1.48,reached by WSD_7 and MFS respectively.The proposed cleanup procedure can be ap-plied to other sense annotated corpora by the fol-lowing steps:Noun #sense Major Sense Entropy#err.instances WSD_7 MFSWSD_7+MFSdefense 7 0.28 2.599 5 5 4 5position 7 0.30 2.264 2 2 2 2base 6 0.35 2.023 1 1 0 1system 6 0.54 1.525 2 1 0 1chance 4 0.49 1.361 1 1 1 1order 8 0.72 1.348 4 1 0 1part 5 0.70 1.288 1 1 1 1power 3 0.51 1.233 3 1 3 3area 3 0.72 1.008 2 1 2 2management 2 0.62 0.959 2 1 0 0condition 3 0.71 0.906 1 0 1 1job 3 0.78 0.888 1 0 0 0state 4 0.83 0.822 1 0 0 0hour 4 0.85 0.652 1 1 1 1value 3 0.90 0.571 2 1 1 1plant 3 0.88 0.556 1 0 0 0move 4 0.93 0.447 6 0 0 0rate 2 0.92 0.388 6 0 1 1Total ?
?
?
42 17 17 21Nouns without erroneous instances: authority, bill, capital, carrier, development, drug,effect, exchange, future, network, people, point, policy, president, share, source, spaceTable 7.
Entropy of words versus WSD performance.
The dashed line denotes a cut-point for the com-bination of the baseline and WSD_7.Group 1 Group 2 Difference p-valueBaseline (MFS) 1.226 1.040 0.186 0.368WSD_7 1.401 0.932 0.469* 0.013*p<0.05Table 6.
Average entropy of two groups of nouns for the baseline and WSD_7.1062z Build the baseline (MFS) and WSD systemsfrom the corpus.z Create a suspicious set from the WSD systems.z Calculate the entropy for each word in termsof it sense distribution in the corpus.z Choose a cut-point value.
Select a small por-tion of words with entropy within a certain in-terval (e.g., 1.0 ~ 1.5 in Table 7) for humanevaluation to decide an appropriate cut-pointvalue.
The cut-point value should not be toolow or too high, since WSD systems may suf-fer from the over-fitting problem if it is toolow, and the performance would be dominatedby the baseline system if it is too high.z Combine the baseline and best single WSDsystem through the cut-point.z Start the cleanup procedure in the descendingorder of word entropy until the CE ratio is be-low a predefined threshold.5 ConclusionThis study has presented a cleanup procedure toidentify incorrect sense annotation in a corpus.The cleanup procedure incorporates WSD sys-tems to select a set of suspicious instances forhuman evaluation.
The experiments are con-ducted from three aspects: precision, cost-effectiveness ratio, and entropy, to examine theperformance of WSD.
The experimental resultsshow that the WSD systems are more effectiveon highly-ambiguous words.
Additionally, themost cost-effective cleanup strategy is to use thecombination method and begin with the mostambiguous words.
The incorrect sense annota-tions found in this study can be used for SemE-val-2007 to improve the accuracy of WSDevaluation.The absence of related work on (semi-) auto-matically determining cases of erroneous agree-ment among annotators in a corpus is rathersurprising.
Variants of the method described here,replacing WSD for whatever procedure is appro-priate for the phenomenon annotated in the cor-pus (sentiment recognition for a sentiment corpus,etc.
), are easy to implement and may produceuseful results for corpora in current use.
Futurework will focus on devising an algorithm to per-form the cleanup procedure iteratively on thewhole corpus.ReferencesE.
Agirre and O. Lopez de Lacalle.
2007.
UBC-ALM:Combining k-NN with SVD for WSD.
In Proc.
ofthe 4th International Workshop on SemanticEvaluations (SemEval-2007) at ACL-07, pages 342-345.R.K.
Ando.
2006.
Applying Alternating Structure Op-timization to Word Sense Disambiguation.
In Proc.of CoNLL, pages 77-84.J.F.
Cai, W.S.
Lee, and Y.W.
Teh.
2007.
ImprovingWord Sense Disambiguation Using Topic Features.In Proc.
of EMNLP-CoNLL, pages 1015-1023.T.
Chklovski and R. Mihalcea.
2002.
Building a SenseTagged Corpus with Open Mind Word Expert.
InProc.
of the Workshop on Word Sense Disambigua-tion: Recent Successes and Future Directions atACL-02, pages 116-122.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
Cambridge, MA: MIT Press.E.H.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
OntoNotes: The 90% Solu-tion.
In Proc.
of HLT/NAACL-06, pages 57-60.K.
Kasahara, H. Sato, F. Bond, T. Tanaka, S. Fujita, T.Kanasugi, and S. Amano.
2004.
Construction of aFigure 4.
Effectiveness rate against word entropy.Figure 5.
CE ratio against word entropy.1063apanese Semantic Lexicon: Lexeed.
In IPSG SIG:2004-NLC-159, Tokyo, pages 75-82.A.
Kilgarriff.
2001.
English Lexical Sample Task De-scription.
In Proc.
of the SENSEVAL-2 Workshop,pages 17-20.A.
Kilgarriff and M. Palmer, editors.
2000.SENSEVAL: Evaluating Word Sense Disambigua-tion Programs, Computer and the Humanities,34(1-2):1-13.Y.K.
Lee and H.T.
Ng.
2002.
An Empirical Evaluationof Knowledge Sources and Learning Algorithmsfor Word Sense Disambiguation.
In Proc.
ofEMNLP, pages 41-48.I.D.
Melamed.
1997.
Measuring Semantic Entropy.
InProc.
of ACL-SIGLEX Workshop, pages 41-46.R.
Mihalcea.
2007.
Using Wikipedia for Automatic-Word Sense Disambiguation.
In Proc.
ofNAACL/HLT-07, pages 196-203.R.
Mihalcea and P. Edmonds, editors.
2004.
In Proc.of SENSEVAL-3.G.
Miller, C. Leacock, R. Tengi, and R. Bunker.
1993.A Semantic Concordance.
In Proc.
of the 3rdDARPA Workshop on Human Language Technol-ogy, pages 303?308.H.T.
Ng and H.B.
Lee.
1996.
Integrating MultipleKnowledge Sources to Disambiguate Word Sense:An Exemplar-based Approach.
In Proc.
of the 34thMeeting of the Association for Computational Lin-guistics (ACL-96), pages 40-47.M.
Palmer, O. Babko-Malaya, and H.T.
Dang.
2004.Different Sense Granularities for Different Applica-tions.
In Proc.
of the 2nd International Workshopon Scalable Natural Language Understanding atHLT/NAACL-04.M.
Palmer, H.T.
Dang, and C. Fellbaum.
2006.
Mak-ing Fine-grained and Coarse-grained Sense Distinc-tions, Both Manually and Automatically.
Journal ofNatural Language Engineering, 13:137?163.S.
Pradhan, E.H. Hovy, M. Marcus, M. Palmer, L.Ramshaw, and R. Weischedel.
2007a.
OntoNotes:A Unified Relational Semantic Representation.
InProc.
of the First IEEE International Conferenceon Semantic Computing (ICSC-07), pages 517-524.S.
Pradhan, E. Loper, D. Dligach, and M. Palmer.2007b.
SemEval-2007 Task 17: English LexicalSample, SRL and All Words.
In Proc.
of the 4th In-ternational Workshop on Semantic Evaluations(SemEval-2007) at ACL-07, pages 87-92.R.
Snow, S. Prakash, D. Jurafsky, and A.Y.
Ng.
2007.Learning to Merge Word Senses.
In Proc.
ofEMNLP-CoNLL, pages 1005-1014.L.
Specia, M. Stevenson, and M. das Gracas V. Nunes.2007.
Learning Expressive Models for Word SenseDisambiguation.
In Proc.
of the 45th Annual Meet-ing of the Association of Computational Linguistics(ACL-07), pages 41?48.S.
Tratz, A. Sanfilippo, M. Gregory, A. Chappell, C.Posse, and P. Whitney.
2007.
PNNL: A SupervisedMaximum Entropy Approach to Word Sense Dis-ambiguation.
In Proc.
of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-2007)at ACL-07, pages 264-267.L.C.
Yu, C.H.
Wu, A. Philpot, and E.H. Hovy.
2007.OntoNotes: Sense Pool Verification Using GoogleN-gram and Statistical Tests.
In Proc.
of the On-toLex Workshop at the 6th International SemanticWeb Conference (ISWC 2007).J.
Zhu and E.H. Hovy.
2007.
Active Learning forWord Sense Disambiguation with Methods for Ad-dressing the Class Imbalance Problem, In Proc.
ofEMNLP-CoNLL, pages 783-790.1064
