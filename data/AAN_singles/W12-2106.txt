Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 46?55,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsRe-tweeting from a Linguistic PerspectiveAobo Wang Tao ChenWeb IR / NLP Group (WING)National University of Singapore13 Computing Link, Singapore 117590{wangaobo,taochen,kanmy}@comp.nus.edu.sgMin-Yen KanAbstractWhat makes a tweet worth sharing?
Westudy the content of tweets to uncover linguis-tic tendencies of shared microblog posts (re-tweets), by examining surface linguistic fea-tures, deeper parse-based features and Twitter-specific conventions in tweet content.
Weshow how these features correlate with a func-tional classification of tweets, thereby catego-rizing people?s writing styles based on theirdifferent intentions on Twitter.
We find thatboth linguistic features and functional classi-fication contribute to re-tweeting.
Our workshows that opinion tweets favor originalityand pithiness and that update tweets favor di-rect statements of a tweeter?s current activity.Judicious use of #hashtags also helps to en-courage retweeting.1 IntroductionTweeting1 is a modern phenomenon.
Complement-ing short message texting, instant messaging, andemail, tweeting is a public outlet for netizens tobroadcast themselves.
The short, informal nature oftweets allows users to post often and quickly react toothers?
posts, making Twitter an important form ofclose-to-real-time communication.Perhaps as a consequence of its usability, form,and public nature, tweets are becoming an im-portant source of data for mining emerging trendsThis research is supported by the Singapore National Re-search Foundation under its International Research Centre Sin-gapore Funding Initiative and administered by the IDM Pro-gramme Office, under grant 252-002-372-490.1More generally known as microblogging, in which the postis termed a microblog.and opinion analysis.
Of particular interest areretweets, tweets that share previous tweets from oth-ers.
Tweets with a high retweet count can be takenas a first cut towards trend detection.It is known that social network effects exertmarked influence on re-tweeting (Wu et al, 2011;Recuero et al, 2011).
But what about the contentof the post?
To the best of our knowledge, little isknown about what properties of tweet content moti-vate people to share.
Are there content signals thatmark a tweet as important and worthy of sharing?To answer these questions, we delve into the data,analyzing tweets to better understand posting behav-ior.
Using a classification scheme informed by pre-vious work, we annotate 860 tweets and propagatethe labeling to a large 9M corpus (Section 2).
Onthis corpus, we observe regularities in emoticon use,sentiment analysis, verb tense, named entities andhashtags (Section 3), that enable us to specify fea-ture classes for re-tweet prediction.
Importantly, theoutcome of our analysis is that a single holistic treat-ment of tweets is suboptimal, and that re-tweeting isbetter understood with respect to the specific func-tion of the individual tweet.
These building blocksallow us to build a per-function based re-tweet pre-dictor (Section 4) that outperforms a baseline.2 Linguistically Motivated TweetClassificationBefore we can label tweets for more detailed classi-fication, we must decide on a classification scheme.We first study prior work on tweet classification be-fore setting off on creating our own classification forlinguistic analysis.46Early ethnographic work on tweets manually cre-ated classification schemes based on personal, di-rect observation (Java et al, 2009; Kelly, 2009).Other work is more focused, aiming to use theirconstructed classification scheme for specific sub-sequent analysis (Naaman et al, 2010; Sriram etal., 2010; Ramage et al, 2010; Chen et al, 2010).All schemes included a range of 5?9 categories, andwere meant to be exhaustive.
They exhibit some reg-ularity: all schemes included categories for informa-tion sharing, opinions and updates.
They vary ontheir classification?s level of detail and the intent ofthe classification in the subsequent analysis.Most closely related to our work, Naaman etal.
(2010) focused on distinguishing salient user ac-tivity, finding significant differences in posts aboutthe tweeting party or about others that were reportedby manually classifying tweets into nine categories,sampled from selected users.
However, while theirpaper gave a useful classification scheme, they didnot attempt to operationalize their work into an au-tomated classifier.Other works have pursued automated classifica-tion.
Most pertinent is the work by Sriram etal.
(2010), who applied a Na?
?ve Bayes learningmodel with a set of 8 features (author ID, presence ofshortened words, ?@username?
replies, opinionatedwords, emphasized words, currency and percentagesigns and time phrases) to perform hard classifica-tion into five categories.
To identify trending top-ics, Zubiaga et al (2011) performed a similar clas-sification, but at the topic level (as opposed to theindividual tweet level) using aggregated language-independent features from individual tweets.
Ram-age et al (2010) introduced four salient dimensionsof tweets ?
style, status, social, substance.
Individ-ual terms and users were characterized by these di-mensions, via labeled LDA, in which multiple di-mensions could be applied to both types of objects.While the previous work provides a goodoverview of the genre and topic classification oftweets, their analysis of tweets have been linguis-tically shallow, largely confined to word identityand Twitter-specific orthography.
There has been nowork that examines the discoursal patterns and con-tent regularities of tweets.
Understanding microblogposts from a deeper linguistic perspective may yieldinsight into the latent structure of these posts, and beuseful for trend prediction.
This is the aim of ourwork.2.1 Classification SchemeWe hypothesize that people?s intentions in postingtweets determine their writing styles, and such in-tentions can be characterized by the content and lin-guistic features of tweets.
To test this hypothesis, wefirst collect a corpus of manually annotated tweetsand then analyze their regularities.
In construct-ing our classification annotation scheme, we are in-formed by the literature and adopt a two-level ap-proach.
Our coarser-grained Level-1 classificationgeneralization is an amalgam of the schemes in Naa-man et al and Sriram et al?s work; while our finer-grained, Level-2 classification further breaks downthe Update and Opinion classes, to distinguish lin-guistic regularities among the subclasses.
The lefttwo columns of Table 1 list the categories in ourscheme, accompanied by examples.2.2 Dataset CollectionWe collected three months of public tweets (fromJuly to September in 2011) through Twitter?sstreaming API2.
Non-English tweets were removedusing regular expressions, incurring occasional er-rors.
We note that tweets containing URLs areoften spam tweets or tweets from automated ser-vices (e.g., Foursquare location check-ins) (Thomaset al, 2011), and that any retweet analysis of suchtweets would need to focus much more on thelinked content rather than the tweet?s content.
Wethus removed tweets containing URLs from ourstudy.
While this limits the scope of our study,we wanted to focus on the (linguistic quality of)content alone.
The final dataset explicitly iden-tifies 1,558,996 retweets (hereafter, RT-data) and7,989,009 non-retweets.
To perform further analy-sis on Twitter hashtags (i.e., ?#thankyousteve?
), webreak them into separate words using the MicrosoftData-Driven Word-Breaking API3.
This also ben-efits the classification task in terms of convertinghashtags to known words.2http://dev.twitter.com/docs/streaming-api3http://web-ngram.research.microsoft.com/info/break.html47Table 1: Our two-level classification with example tweets.Level-1 Level-2 Motivation Example retweets Corpus count (%)OpinionAbstract Present opinions towards ab-stract objects.God will lead us all to the right person for our lives.
Havepatience and trust him.291 (33.8%)Concrete Present opinions towards con-crete objects.i feel so bad for nolan.
Cause that poor kid gets blamed foreverything, and he?s never even there.99 (11.5%)Joke Tell jokes for fun.
Hi.
I?m a teenager & I speak 3 languages: English, Sar-casm, & Swearing (; #TeenThings86 (10.0%)UpdateMyself Update my current status.
first taping day for #growingup tomorrow!
So excited.
:) 168 (19.6%)Someone Update others?
current status.
My little sister still sleep ... 66 (7.7%)Interaction Seek interactions with others.
#Retweet If you?re #TeamFollowBack 81 (9.4%)Fact Transfer information.
Learnt yesterday: Roman Empire spent 75% of GDP oninfrastructure.
Roads, aqueducts, etc.23 (2.7%)Deals Make deals.
Everybody hurry!
Get to Subway before they stop servingLIMITED TIME ONLY item ?avocados?.29 (3.4%)Others Other motivations.
Ctfu Lmfao At Kevin Hart ;) 17 (2.0%)We employed U.S.-based workers on Amazon?sMechanical Turk to annotate a random subset of thepreprocessed tweets.
We collected annotations for860 tweets (520 retweets; 340 non-retweets) ran-domly sampled from the final dataset, paying 10cents per block of 10 tweets labeled.
Each tweet waslabeled by 3 different workers who annotated usingthe Level-2 scheme.
Gold standard labels were in-ferred by majority.
Inter-annotator agreement viaFleiss?
?
showed strong (0.79) and modest (0.43)agreement at Level-1 and Level-2, respectively.Table 1?s rightmost columns illustrate the distri-bution of the annotated tweets on each category.From our Level-1 classification, Opinion, Updateand Interaction, make up the bulk of the tweets inthe annotated sample set.
The remaining categoriesof Facts, Deals and Others make up only 8.1% intotal.
We thus focus only on the three major groups.2.3 Labeled LDA ClassificationGiven the labeled data, we first observed that tweetsin different classes have different content and lan-guage usage patterns.
For example, tweets belong-ing to Opinion display more of an argumentativenature, exhibiting a higher use of second personpronouns (e.g., ?you?, ?your?
), modal verbs (e.g.,?can?, ?could?, ?will?, ?must?
), and particular ad-verbs (e.g., ?almost?, ?nearly?)
than the other twogroups.
These observations lead us to employ theclassifier that make use of words?
co-occurrence fea-ture to categorize tweets.Hence, we adopt Labeled LDA, which extendsLatent Dirichlet Allocation (LDA) (Blei et al, 2003)by incorporating supervision at the document level(here, tweet-level), enabling explicit models of textcontent associated with linguistic features.
In adopt-ing this methodology, we follow (Ramage et al,2009) previous work on tweet classification.
Fea-tures are encoded as special tokens to not overlapthe tokens from the tweet content.Tweets arguing in one style tend to share similarlinguistic features.
For example in Table 1, Updatetalks about ongoing events using present tense; andOpinion uses conjunctions to compose and connectideas.
To discover how people talk differently acrossgenres of tweets, we extract five sets of linguisticfeatures from each tweet, namely Tense4, DiscourseRelations5, Hashtags, Named Entities6, and Interac-tion Lexical Patterns7.We use default parameter settings for LabeledLDA.
All the combinations of features were tested tofind the best performing feature set.
Table 2 quanti-fies the contribution of each feature and demonstratethe result from the best combination, as measured byWeighted Average F-Measure (WAFM).
Comparedto the performance of using baseline feature set us-ing tweet content alone, the use of linguistic featuresimprove the performance accordingly, with the ex-ception of the use of named entities which reducedperformance slightly, and hence was removed fromthe final classifier?s feature set.4Using the OpenNLP toolkit.5Using (Lin et al, 2010)?s parser.6Using the UW Twitter NLP tools (Ritter et al, 2011).7Defined as Boolean matches to the following regularexpressions: ?RT @[username]...?, ?...via @[username]...?,?Retweeting @[username]...?,?Follow me if...?, ?retweet@[username]...?, ?...RT if...?
and ?Retweet if...?48Scheme C CI CT CD CH CE CITDHLevel-1 .625 .642 .635 .637 .629 .611 .670Level-2 .413 .422 .427 .432 .415 .409 .451Table 2: Weighted average F-measure results forthe labeled LDA classification.
Legend: C: tweetcontext; I: Interaction; T: Tense; D: Discourse Rela-tions; H: Hashtags; E: Named Entities.Require: Training set L; Test collection C; Evaluation set E;Iteration count Ifunction incrementalTraining(L,C,E,)M ?
labeledLDATraining(L)e?
evaluate(M ,E)for ciC and i < I dori ?
predictLabel(ci,M )rselected ?pickItemsWithHighConfidence(ri);L?
?
add(rselected) into LM ?
?
retrainLDAModel(L?)e?
?
evaluate(M ?,E)if e?
is better than e then M ?
m?
; e?
e?
;else return Mi?
i+ 1keepLog(e?
)return MFigure 1: Pseudocode for incremental training.2.4 Automated ClassificationStarting with the best performing model trained onthe Level-1 schema (the CITDH feature set), we au-tomatically classified the remaining tweets, usingthe incremental training algorithm described in Fig-ure 1.
The 860 annotated tweets were randomly splitinto a training set L and evaluation set E with a5:1 ratio.
The 9M unannotated tweets form the testcollection C. ci is assigned by randomly selecting1000 tweets from C. I is computed as the size ofC divided by the size of ci.
Note that retrainingbecomes more expensive as the dataset L?
grows.Thus, we greedily generate a locally-optimal model,which completes after 6 iterations.From the result of automatically labeled dataset,we see that the Opinion dominates the collectionin count (44.6%), followed by Interaction (28.4%)and Update (20.5%).
This result partially agreeswith the manual classification results in Naaman etal.
(2010), but differs in their Information Sharingcategory, which is broken down here as Facts, Dealsand Others.
We believe the discrepancies are due tothe differences between the two datasets used.
Theirretweets were sampled from selected users who areactive participants, and did not include tweets fromorganizations, marketers and dealers; in our case, thetweets are generally sampled without constraints.3 Analysis of Linguistic FeaturesWe now dissect retweets using the 1.5M RT-data de-fined in Section 2.2.
We do this from a linguisticperspective, based on observations on the values andcorrelations among the features used for the auto-matic classification.3.1 Emoticons and SentimentEmoticons such as smilies ?
:) ?
and frownies ?
:( ?
and their typographical variants, are prevalentin tweets.
Looking at the distribution of emoticons,we find that 2.88% of retweets contain smilies and0.26% contain frownies.
In other words, smileys areused more often than frownies.To give an overall picture of how sentiment isdistributed among retweets, we employed the Twit-ter Sentiment Web API service (Go et al, 2009) toobtain polarity.
Figure 2 shows that while neutraltweets dominate in all three classes, there are morenegative tweets in the Interaction than in the othertwo.
Such negative interactive comments usuallyfind their use in sharing negative experiences in adialogue or with their followers.
?Yeah I hate talk-ing IN my phone.
RT @Jadon Don?t you guys hatetalking in the phone?
is a representative example.Figure 2: Sentiment distribution of retweets.Previous works have leveraged emoticons to au-tomatically build corpora for the sentiment detectiontask, through labeling tweets with smilies (frownies)as true positive (negative) instances (Read, 2005;Alexander and Patrick, 2010; Cui et al, 2011), andtraining statistical classification models on the re-sult.
We wish to verify the veracity of this hy-pothesis.
Do emoticons actually reflect sentiment in49Table 3: Manual sentiment annotation results andconfusion matrix.
Bolded numbers highlight the er-ror caused by neutral posts.Positive Neutral NegativeRetweets with smilies 55 (27.5%) 140 (70%) 5 (2.5%)Retweets with frownies 9 (4.5%) 118 (59%) 73(36.5%)Predicted Positive 43 30 0Predicted Neutral 11 206 12Predicted Negative 7 29 62retweets?
To answer the question, we randomly sub-selected 200 retweets with smilies and another 200with frownies from RT-data, and then manually la-beled their sentiment class after removing the emoti-cons.
Table 3?s top half shows the result.While our experiment is only indicative, neutralposts are still clearly the majority, as indicated bybold numbers.
Simply labeling the sentiment basedon emoticons may mistake neutral posts for emo-tional ones, thus introducing noise into training data.
?Fishers people have no idea how lawrence kids are,guess they do now :)?
is such an example.To demonstrate this effect, we evaluated Go etal.
(2009)?s API on our annotated corpus.
Wepresent the confusion matrix in bottom half of Ta-ble 3.
A common error is in mistaking neutral tweetsas positive or negative ones, as indicated by the boldnumbers.
Given that the detector is trained on thecorpus, in which neutral tweets with smiles (frown-ies) are labeled as positive (negative) ones, the detec-tor may prefer to label neutral tweets as sentiment-bearing.
This observation leads us to believe thatmore careful use of emoticons could improve senti-ment prediction for tweets and microblog posts.3.2 Verb TenseWe analyze the tense of the verbs in retweets, us-ing a simplified inventory of tenses.
We assign twotenses to verbs: past and present.
Tense is assignedper-sentence; tweets that consist of multiple sen-tences may be assigned multiple tenses.
Based onour statistics, one notable finding is that Update hasa higher proportion of past tense use (33.70%) thanOpinion (14.9%) and Interaction (24.2%).
This val-idates that updates often report past events and verbtense is a more crucial feature for Updates.Building on the previous section, we ask our-selves whether sentiment is correlated with verbFigure 3: Tenses (l) and specific times (r) and theirsentiment.tense use.
Interestingly, the results are not uniform.Figure 3 shows our analysis of positive and negative(omitting neutral) sentiments as they co-occur withverb tense in our corpus.
It shows that people tendto view the past negatively (e.g., ?I dont regret mypast, I just regret the times I spent with the wrongpeople?
), whereas emotions towards current eventdo not have any obvious tendency.
A case in point isin the use of ?today?
and ?yesterday?
as time mark-ers related to present and past use.
Figure 3 showsthe number of tweets exhibiting these two words andtheir sentiment.
The results are quite marked: tweetsmay be used to complain about past events, but lookoptimistically about things happening now.3.3 Named EntitiesTo study the diversity of named entities (NEs) inretweets, we used UW Twitter NLP Tools (Ritter etal., 2011) to extract NEs from RT-data.
15.9% ofretweets contain at least one NE, indicating that NEsdo play a large role in retweets.So what types of NEs do people mention in theirtweets?
From each of our primary Level-1 classes,we selected the top 100 correctly recognized NEs,in descending order of frequency.
We then standard-ized variants (i.e.
?fb?
as a variant of ?Facebook?
),and manually categorized them against the 10-classschema defined by Ritter et al (2011).Table 4: The distribution of top 100 named entities8.Class Opinion Update InteractionPERSON 41.2% 44.7% 38.8%GEO-LOC 7.8% 28.9% 25.4%COMPANY 15.7% 6.6% 10.4%PRODUCT 5.9% 5.3% 6.0%SPORTS-TEAM 2.0% 5.3% 1.5%MOVIE 7.8% 5.3% 7.5%TV-SHOW 3.9% 0.0% 3.0%OTHER 15.7% 3.9% 7.5%50Table 4 displays the distribution of the differentclasses of NEs, by frequency.
People?s names rep-resent the largest portion in each class, of whichthe majority are celebrities.
Geographical locations?
either countries or cities ?
make up the secondlargest class for Update and Interaction, account-ing for 28.9% and 25.4%, respectively, whereas theytake only 7.8% of Opinion.
A possible reason is thatpeople prefer to broadcast about events (with loca-tions mentioned) or discuss them through Updateand Interaction classes, respectively.
?California,I?m coming home.?
is a typical example.3.4 HashtagsPrevious work (Cunha et al, 2011) showed that pop-ular hashtags do share common characteristics, suchas being short and simple.
We want to push more inour analysis of this phenomenon.
We organize ourhashtag analysis around three questions: (a) Do peo-ple have any positional preference for embeddinghashtags?
(b) Are there any patterns to how peo-ple form hashtags?
and (c) Is there any relationshipbetween such patterns and their placement?To answer these questions, as shown in Table 5,we extracted the hashtags from RT-data and catego-rized them by the position of their appearance (atthe beginning, middle, or end) of tweet.
69.1% ofhashtags occur at the end, 27.0% are embedded inthe middle, and 8.9% occur at the beginning.
In Fig-ure 4, we plot the frequency and length (in charac-ters) of the hashtags with respect to their position,which shows that the three placement choices leadto different distributions.
Beginning hashtags (here-after, beginners) tend to peak around a length of 11,while middlers peaked at around 7.
Enders featurea bimodal distribution, favoring short (3) or longer(11+) lengths.
We found these length distributionsare artifacts of how people generate and (function-ally) use the hashtags.Beginners are usually created by concatenatingthe preceding words of a tweet, therefore, the com-mon patterns are subject+verb (e.g.,?#IConfess?
),subject+verb+object (e.g., ?#ucanthaveme?
), andsimilar variants.
Middlers, often acting as a syn-tactic constituent in a sentence, are usually used8The other two classes, facility and band, are not found inthe top 100 NEs.Table 5: Hashtags and example tweets.Position TweetsBeginning #ihateitwhen random people poke you on facebookMiddle I just saw the #Dodgers listed on Craig?s List.EndSuccess is nothing without someone you love to shareit with.
#TLTGoodmorning Tweethearts....wishing u all blessedand productive day!
#ToyaTuesdayFigure 4: Length distribution of sampled hashtags.to highlight tweet keywords, which are single-wordnouns (e.g.,?#Scorpio?
and ?#Dodgers?).
Endersprovide additional information for the tweets.
Apopular ender pattern is Twitter slang that have beenused enough to merit their own Twitter acronym,such as ?#TFB?
(Team Follow Back), and ?#TLT?
(Thrifty Living Tips).
Another popular form isconcatenating multiple words, indicating the time(?#ToyaTuesday?
), the category (?#Tweetyquote?)
orthe location (?#MeAtSchool?).
Knowing such hash-tag usage can aid downstream applications such ashashtag suggestion and tweet search.3.5 Discourse RelationsIn full text, textual units such as sentences andclauses work together to transmit information andgive the discourse its argumentive structure.
Howimportant is discourse in the microblog genre, givenits length limitation?
To attempt an answer to thisquestion, we utilized the end-to-end discourse parserproposed by Lin et al (2010) to extract PDTB-styleddiscourse relations (Prasad et al, 2008) from RT-data.
Figure 5 shows the proportion of the five mostfrequent relations.
68.0% of retweets had at leastone discourse relation ?
per class, this was 55.2%of Opinion, 44.7% of Interaction, and 21.6% of Up-date.
Within Opinions, we find that negative opin-ions are often expressed using a Synchrony relation(i.e., negative tweet: ?I hate when I get an itch at a51Figure 5: The distribution of five selected discourserelations.place where my hand can?t reach.?
), while positiveand neutral opinions prefer Condition relations (i.e.,positive tweet: ?If I have a girlfriend :) I will tell herbeautiful everyday.?
).3.6 Sentence Similarity?On Twitter people follow those they wish they knew.
OnFacebook people follow those they used to know.
?We round out our analysis by examining the sen-tence structure of retweets.
Sometimes it is notwhat you say but on how you say it.
This adageis especially relevant to the Opinion class, wherewe observed that the craftiness of a saying influ-ences its ?retweetability?.
This can be reflected intweets having parallel syntactic structure, which canbe captured by sentence similarity within a tweet,as illustrated in the quote/tweet above.
We em-ploy the Syntactic Tree Matching model proposedby Wang et al (2009) on tweets to compute thisvalue.
This method computes tree similarity usinga weighted version of tree kernels over the syntacticparse trees of input sentences.
When we set the sim-ilarity threshold to 0.2 (determined by observation),723 retweets are extracted from the Opinion class ofwhich over 500 (70%) are among the top 5% mostretweeted posts (by count).
Examining this set re-veals that they are more polarized (22.6% positive,23.2% negative) than the average Opinion (14.7%and 16.9%, respectively).4 Predicting RetweetsGiven the diversity in function which we have illus-trated in our linguistic analyses in the previous sec-tions, we argue that whether a tweet is shared withothers is best understood by modeling each func-tion (Level-1) class independently.
We validate thisclaim here, by showing how independently build-ing classification models for the Opinion, Updateand Interaction classes outperforms an agglomer-ated retweet predictor.Previous research have found that features rep-resenting the author?s profile (e.g., number of fol-lowers), tweet metadata (time interval betweeninitial posting and current checkpoint, previouslyretweeted) and Twitter-specific features (URL pres-ence) weight heavily in predicting retweets (Suh etal., 2010; Peng et al, 2011; Hong et al, 2011).
Incontrast, our study is strictly about the content andthus asks the question whether retweeting can bepredicted from the content alone.Before we do so, we call attention to a caveatabout retweet prediction that we feel is importantand unaccounted for in previous work: the actualprobability of retweet is heavily dependent on howmany people view the tweet.
Twitter tracks the fol-lower count of the tweet?s author, which we feel isthe best approximation of this.
Thus we do not per-form retweet count prediction, but instead cast ourtask as:Given the content of a tweet, perform a multi-classclassification that predicts its range of retweet per fol-lower (RTpF) ratio.4.1 Experiment and ResultsWe first examine RTpF distribution over the 9Mtweets in the dataset.
Figure 6 plots RTpF rankagainst retweet count on both normal and log-logscales.
While the normal scale seems to show atypical exponential curve, the log-log scale revealsa clear inflection point that corresponds to an RTpFof 0.1.
We use this inflection point to break the pre-dicted RTpF values into three ordinal classes: noretweets (?N?, RTpF = 0), low (?L?, RTpF < 0.1),and high (?H?, RTpF ?
0.1).We use 10-fold cross validation logistic regres-sion in Weka3 (Hall et al, 2009) to learn predic-tion models.
The regression models use both binarypresence-of feature classes (quotation; past, presenttense; 16 types of discourse relations; 10 NE types;3 hashtag positions) as well as normalized numericfeatures (tweet length, hashtag count, sentence sim-ilarity, 3 sentiment polarity strengths).
Note that themodels reported here do not factor the content (lexi-52(a) Normal scale (b) Logarithmic scaleFigure 6: Retweet per follower (RTpF) ratio rankversus retweet count.
The highlighted point showsthe boundary between classes H and L.Class F?1 Salient Features Feature WeightOpinion 0.57Sentence Similarity 10.34Conjunction -21.09Quotation -19.2Update 0.54Sentence Similarity -2.81Past -5.2Present 1.3Interaction 0.53Sentence Similarity -55.33Hashtag Count 5.34All w/ L-1 class 0.52 Sentence Similarity 9.8All w/o L-1 class 0.42 Hashtag Count 22.03Table 6: Logistic regression results.
Salient featuresalso shown with their respective weight, where a +vevalue denotes a +ve contribution to retweet volume.cal items) directly, but represent content through thelens of the feature classes given.We build individual regression models for thethree major Level-1 classes, and aggregate modelsthat predict RTpF for all three classes.
The two ag-gregate models differ in that one is informed of theLevel-1 class of the tweets, while the other is not.We report average F-measure in Table 6 over thethree RTpF classes (?N?, ?L?
and ?H?).
Adding theLevel-1 classification improves the RTpF predictionresult by 10% in terms of average F1.
This resultsvalidate our hypothesis ?
we see that building sepa-rate logistic models for each class improves classifi-cation results uniformly for all three classes.4.2 RemarksWe make a few conjectures based on our observa-tions, in concluding our work:1.
Getting your Opinion retweeted is easier whenyour readership feels a sense of originality, pithinessand wittiness in your post.
?If you obey all the rules,you miss all the fun - Katharine Hepburn?
exempli-fies these factors at conflict: while being witty inexhibiting parallel syntactic structure (high sentencesimilarity), it has a low RTpF.
Perhaps followers areunsurprised when they find such beautiful words arenot originally the poster?s.
Tweets having complexconjoined components and multiple clauses also ex-hibit a negative RTpF tendency ?
find a short andsimple way of getting your message across.2.
Update tweets show the least bias towards anyparticular feature, exhibiting little weight towardsany one convention.
Update tweets prefer simpletenses, eschewing perfect and progressive variants.Perhaps followers are more curious about what youare doing now but not what you have done.3.
Sentence similarity negatively affects retweet-ing among Interaction tweets.
This implies that peo-ple prefer direct sounds to well-designed proverbs inthe daily interaction, which is mostly in the form ofquestion answering or voting.4.
Globally, the presence and count of hashtags iscorrelated with retweeting, but this effect is greatlylessened when Level-1 class features are used.
Thisfurther validates the importance of our functionalclassification of tweets.5 ConclusionPeople tweet for different reasons.
Understandingthe function of the tweet is interesting in its ownright, but also useful in predicting whether it will beshared with others.
We construct a two-level classi-fication informed by prior work and have annotateda corpus of 860 tweets.Employing Labeled LDA, we propagated our an-notations to a large 9M tweet corpus and inves-tigated the linguistic characteristics of the 1.5Mretweets.
We created a model to predict the levelof retweeting per follower given a tweet?s content.Finally, to further encourage investigation onthese topics, we have made the annotated corpus andthe two tools described in this paper ?
the functionalclassifier and the retweet predictor ?
available to thepublic to test and benchmark against9.In future work, we plan to combine the contentanalysis from this study with known social, time andlinked URL features to see whether content featurescan improve a holistic model of retweeting.9http://wing.comp.nus.edu.sg/tweets/53ReferencesPak Alexander and Paroubek Patrick.
2010.
Twitter as acorpus for sentiment analysis and opinion mining.
InProceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10).European Language Resources Association (ELRA).David M. Blei, Andrew Y. Ng, Michael I. Jordan, andJohn Lafferty.
2003.
Latent dirichlet alocation.
Jour-nal of Machine Learning Research, 3:2003.Jilin Chen, Rowan Nairn, Les Nelson, Michael Bernstein,and Ed H. Chi.
2010.
Short and tweet: Experimentson recommending content from information streams.In CHI 2010.Anqi Cui, Min Zhang, Yiqun Liu, and Shaoping Ma.2011.
Emotion tokens: Bridging the gap among mul-tilingual twitter sentiment analysis.
In AIRS, volume7097 of Lecture Notes in Computer Science, pages238?249.
Springer.Evandro Cunha, Gabriel Magno, Giovanni Comarela,Virgilio Almeida, Marcos Andre?
Gonc?alves, andFabricio Benevenuto.
2011.
Analyzing the dynamicevolution of hashtags on twitter: a language-based ap-proach.
In Proceedings of the Workshop on Languagein Social Media (LSM 2011), pages 58?65, Portland,Oregon, June.
ACL.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twittersentiment classication using distant supervision.
Tech-nical report, Stanford University.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Liangjie Hong, Ovidiu Dan, and Brian D. Davison.
2011.Predicting popular messages in twitter.
In Proceed-ings of the 20th international conference companionon World wide web, WWW ?11, pages 57?58, NewYork, NY, USA.
ACM.Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.2009.
Why we twitter: An analysis of a microblog-ging community.
In Proceedings of WebKDD/SNA-KDD 2007, volume 5439 of LNCS, pages 118?138.Ryan Kelly.
2009.
Twitter study august 2009:Twitter study reveals interesting results about us-age.
http://www.pearanalytics.com/blog/wp-content/uploads/2010/05/Twitter-Study-August-2009.pdf, August.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.A pdtb-styled end-to-end discourse parser.
Technicalreport, School of Computing, National University ofSingapore.Mor Naaman, Jeffrey Boase, and Chih-Hui Lai.
2010.Is it really about me?
: message content in socialawareness streams.
In Proceedings of the 2010 ACMconference on Computer supported cooperative work,CSCW ?10, pages 189?192, New York, NY, USA.ACM.Huan-Kai Peng, Jiang Zhu, Dongzhen Piao, Rong Yan,and Ying Zhang.
2011.
Retweet modeling using con-ditional random fields.
In ICDM 2011 Workshop onData Mining Technologies for Computational Collec-tive Intelligence.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InProceedings of LREC.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled lda: a super-vised topic model for credit attribution in multi-labeledcorpora.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?09, pages 248?256, Stroudsburg, PA, USA.ACL.Daniel Ramage, Susan Dumais, and Dan Liebling.
2010.Characterizing microblogs with topic models.
Inter-national AAAI Conference on Weblogs and Social Me-dia, 5(4):130?137.Jonathon Read.
2005.
Using emoticons to reduce de-pendency in machine learning techniques for senti-ment classification.
In Proceedings of the ACL Stu-dent Research Workshop, ACLstudent ?05, pages 43?48, Stroudsburg, PA, USA.
ACL.Raquel Recuero, Ricardo Araujo, and Gabriela Zago.2011.
How does social capital affect retweets?
InLada A. Adamic, Ricardo A. Baeza-Yates, and ScottCounts, editors, ICWSM.
The AAAI Press.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1524?1534, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Fer-hatosmanoglu, and Murat Demirbas.
2010.
Short textclassification in twitter to improve information filter-ing.
In Proceedings of the 33rd international ACMSIGIR conference on Research and development in in-formation retrieval, SIGIR ?10, pages 841?842, NewYork, NY, USA.
ACM.Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H.Chi.
2010.
Want to be retweeted?
large scale ana-lytics on factors impacting retweet in twitter network.In Proceedings of the 2010 IEEE Second InternationalConference on Social Computing, SOCIALCOM ?10,pages 177?184, Washington, DC, USA.
IEEE Com-puter Society.54Kurt Thomas, Chris Grier, Justin Ma, Vern Paxson, andDawn Song.
2011.
Design and evaluation of a real-time url spam filtering service.
In Proceedings ofthe 2011 IEEE Symposium on Security and Privacy,SP ?11, pages 447?462, Washington, DC, USA.
IEEEComputer Society.Kai Wang, Zhaoyan Ming, and Tat-Seng Chua.
2009.A syntactic tree matching approach to finding similarquestions in community-based qa services.
In Pro-ceedings of the 32nd international ACM SIGIR con-ference on Research and development in informationretrieval, SIGIR ?09, pages 187?194, New York, NY,USA.
ACM.Shaomei Wu, Jake M. Hofman, Winter A. Mason, andDuncan J. Watts.
2011. Who says what to whom ontwitter.
In Proceedings of the World Wide Web Confer-ence.Arkaitz Zubiaga, Damiano Spina, V?
?ctor Fresno, andRaquel Mart??nez.
2011.
Classifying trending topics:a typology of conversation triggers on twitter.
In Pro-ceedings of the 20th ACM international conference onInformation and knowledge management, CIKM ?11,pages 2461?2464, New York, NY, USA.
ACM.55
