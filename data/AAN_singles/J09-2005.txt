Applying Computational Models of SpatialPrepositions to Visually Situated DialogJohn D. Kelleher?Dublin Institute of TechnologyFintan J.
Costello?
?University College DublinThis article describes the application of computational models of spatial prepositions to visuallysituated dialog systems.
In these dialogs, spatial prepositions are important because peopleoften use them to refer to entities in the visual context of a dialog.
We first describe a genericarchitecture for a visually situated dialog system and highlight the interactions between thespatial cognition module, which provides the interface to the models of prepositional semantics,and the other components in the architecture.
Following this, we present two new computationalmodels of topological and projective spatial prepositions.
The main novelty within these modelsis the fact that they account for the contextual effect which other distractor objects in a visualscene can have on the region described by a given preposition.
We next present psycholinguistictests evaluating our approach to distractor interference on prepositional semantics, and illustratehow these models are used for both interpretation and generation of prepositional expressions.1.
IntroductionA growing number of computer applications share a visualized (virtual or real) spacewith the user, for example graphic design programs, computer games, navigation aids,robot systems, and so forth.
If these systems are to be equipped with dialog interfaces,they must be able to participate in visually situated dialog.
Visually situated dialog isspoken from a particular point of view within a physical or simulated context.
Fromtheoretical linguistic and cognitive perspectives, visually situated dialog systems areinteresting as they provide ideal testbeds for investigating the interaction betweenlanguage and vision.
From a human?computer interaction (HCI) perspective, visuallysituated dialog systems promise many advantages to users interacting with thesesystems.
In this article we describe computational models for the interpretation andgeneration of visually situated locative expressions involving topological and projectivespatial prepositions.Contributions An inherent aspect of visually situated dialog is reference to objectsin the physical environment in which the dialog occurs.
People often use locative?
School of Computing, Dublin Institute of Technology, Kevin Street, Dublin 8, Ireland.
E-mail:john.kelleher@comp.dit.ie.??
School of Computer Science and Informatics, University College Dublin, Belfield, Dublin 4, Ireland.E-mail: fintan.costello@ucd.ie.Submission received: 31 July 2006; revised submission received: 30 March 2007; accepted for publication:4 July 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 2expressions, in particular spatial prepositions, to pick out objects in the visual envi-ronment.
In this article we present computational models of the semantics of spatialprepositions and illustrate how these models can be used in a visually situated dialogsystem for reference resolution and generation.
These models are designed to handlereference resolution and generation in complex visual environments containing multi-ple objects, and to account for the contextual influence which the presence of multipleobjects has on the semantics of spatial prepositions.
In this our models move beyondother accounts, which typically do not model the contextual influence of other objectson spatial semantics.
Because most real-world visual scenes are complex and containmultiple objects, our models for the semantics of spatial prepositions are important forvisually situated dialog systems intended to operate usefully in the real world.Overview We begin in Section 2 by describing some terminology we use whendiscussing locative expressions.
In Section 3 we present an abstract architecture fora visually situated dialog system and, using this architecture, illustrate how the spa-tial reasoning component of the architecture interacts with the other components ofthe system.
In Section 4 we review psycholinguistic data on the semantics of spatialprepositions.
Section 5 reviews previous computational models of spatial prepositionalsemantics.
Section 6 presents our computational models accounting for the semantics ofspatial prepositions and the influence of visual context on those semantics, and Section 7presents psycholinguistic evaluation of these models.
Section 8 presents applications ofthe models in implemented systems.
Section 8.1 presents an application of our modelsto the interpretation of locative expressions, based on Kelleher, Kruijff, and Costello(2006), and Section 8.2 presents algorithms which use these models to generate locativeexpressions to identify objects in visual scenes from Kelleher and Kruijff (2006).2.
TerminologyOur computational models are designed to interpret and generate locative expressionsinvolving spatial prepositions.
The term locative expression describes ?an expressioninvolving a locative prepositional phrase together with whatever the phrase modifies(noun, clause, etc.)?
(Herskovits 1986, page 7).
In this article we use the term target(T) to refer to the object that is being located by a locative expression and the termlandmark1 (L) to refer to the object relative to which the target?s location is described;see Example (1).
We will use the term distractor to describe any object in the visualcontext that is neither the landmark nor the target.Example 1[The man]T near [the table]L.The English lexicon of spatial prepositions numbers above 80 members (not consid-ering compounds such as right next to) (Landau 1996).
Within this set a distinction canbe made between static and dynamic prepositions: static prepositions primarily2 denote1 There is a wealth of terms used in the literature describing locative expressions.
The terms local object,figure object, and trajector are all equivalent to our term target while the terms reference object, ground,and relatum are equivalent to our term landmark.2 Static prepositions can be used in dynamic contexts, for example, the man ran behind the house, anddynamic prepositions can be used in static ones, for example, the tree lay across the road.272Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 1Architecture of a visually situated dialog system.the location of an object, dynamic prepositions primarily denote the path of an object(Jackendoff 1983; Herskovits 1986), see Examples (2) and (3).Example 2The tree is [behind]static the house.Example 3The man walked [across]dynamic the road.In general, the set of static prepositions can be decomposed into two sets calledtopological and projective.
Topological prepositions are the category of prepositionsreferring to a region that is proximal to the landmark; for example, at, near.
Often, thedistinctions between the semantics of the different topological prepositions is basedon pragmatic constraints, for example the use of at licenses the target to be in contactwith the landmark, while the use of near does not.
Projective prepositions describe aregion projected from the landmark in a particular direction, with the specification ofthe direction dependent on the frame of reference3 being used; for example, to the rightof, to the left of.3.
Visually Situated Dialog System ArchitectureIn this section we present an abstract implementation-independent architecture for avisually situated dialog system and highlight the role played by spatial reasoning in thefunctioning of the system.
In particular, we describe howmodels of spatial prepositionalsemantics are important for reference resolution and generation.The distinguishing characteristic of a visually situated dialog system is that thesystem has the ability to visually perceive the environment in which a dialog is situated.Consequently, these systems use both visual and linguistic contextual information tounderstand user commands and to generate linguistic descriptions of the environment.Figure 1 illustrates the visual dialog system architecture we will describe.
The arrows inthe figure represent data flows through the system; the boxes are the main informationprocessing components.3 In the context of projective prepositions, a frame of reference consists of six half-line axes with a sharedorigin; in English, these axes are usually labelled front, back, right, left, above, below.
In English, threedifferent frames of reference are distinguished: absolute, intrinsic, and viewer-centered.
Interestinglyhowever, although the use of a tripartite system is common in European languages, this is not universal,with many languages taking different approaches here.
We direct the interested reader to Levinson (1996,2003) and Levelt (1996) for further discussion on frames of reference.273Computational Linguistics Volume 35, Number 2Figure 2Example input and output data from a vision subsystem.There are two information inputs into this system: the vision subsystem and thespeech interpretation pipeline.
The vision subsystem directly updates the system?srepresentation of the visual context.
The basic requirements for the vision subsystemare that it is able to detect and categorize the objects in the visual context and canprovide geometric positioning information for each visible object.
Figure 2 illustratesthe analysis that a vision subsystem may generate for a given scene.The speech interpretation pipeline begins with speech recognition.
This moduletakes a speech utterance from the user and creates a string representation of it.
Theparser uses this string to construct a structured representation of the input.
Parsersrange in function from wide-coverage syntactic focused parsers, such as Cahill et al?s(2004) probabilistic Lexical-Functional Grammar (LFG) parser, to narrow coverage se-mantic based parsers, for example the CoSy parser (Kruijff, Kelleher, and Hawes 2006).Figure 3 illustrates the types of analyses produced by these different types of parsers forthe input string is the box near the ball?
The parse tree on the left was generated using aprobabilistic wide-coverage LFG parser.4 The parse tree provides a syntactic analysis ofthe input string.Generally, parsers developed for interactive dialog systems integrate semantic, aswell as syntactic, information in their grammars.
In these parsers the elements in thelexicon and grammar are based on an analysis of the entities and relations of thespecific domain the system is designed for.
These parsers sacrifice coverage for depth ofanalysis.
For a dialog system, the advantage of this deeper analysis is that the semanticinformation in the parser?s output can be used by the dialog manager to relate the inputto the rest of the dialog.
The parse structure on the right of Figure 3 illustrates the type ofsemantically rich representation that an interactive dialog system parser might produce(this particular representation was generated by the CoSy parser).The CoSy parser uses a Combinatory Categorial Grammar that represents linguisticmeaning using an ontologically rich sorted relational structure (Baldridge and Kruijff4 A demo of the parser is available at: http://lfg-demo.computing.dcu.ie/lfgparser.html.
The parseralso provides detailed LFG f-structures for input strings.274Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 3Example parse structures for the string is the box near the ball?2002, 2003).
Within this representation the statement b2:phys-objmeans that the referentb2 is of type phys-obj (i.e., a physical object as defined by the ontology the grammar in-dexes).
The semantic contribution of the prepositional phrase near the ball is representedby the <Location> structure and its subcomponents.
This structure describes a locativeprepositional phrase containing a static preposition that locates the referent b2 in theregion r that is proximal to the landmark described by the <Anchor> subcomponent.
Itshould be noted that the syntactic and semantic representation of prepositions withingrammars is an area of ongoing research (see Gawron 1986; Tseng 2000; Beermann andHellan 2004).
The analysis presented here of the prepositional phrase near the ball isintended to illustrate some of the semantic features that prepositions may introduceinto a grammar and is not intended as a comprehensive account of how prepositionsshould be grammatically represented.The final stage in the interpretation pipeline is to categorize how the utterancerelates to the current dialog context.
This categorization is driven by the dialog managerand involves interpreting an utterance as a dialog act (Bunt 1994; Carletta et al 1997;Klein 1999).
One of the important tasks in this process is resolving the referencesin the input.
Consequently, the dialog manager may invoke the reference resolutioncomponent.
Reference resolution is one of two functions in the architecture wherespatial reasoning plays an important role.
From a computational perspective, referenceresolution involves two main tasks:1.
Creating and maintaining a model of what the system considers asmutual knowledge (this model should contain all the objects that areavailable for reference and their properties)2.
Matching the representation introduced by a given referring expressionto an element (or elements) in the set of possible referentsIn a visually situated dialog a referring expression may be exophoric (i.e., denotean object in the visual context which has not yet been mentioned in the dialog) or itmay be anaphoric (i.e., access a representation of a previous referring expression in thedialog context).
People often use the spatial location of an object, described using spatialprepositions, when making exophoric references.
As a result, in order to interpret thesereferences the system must have access to models of the semantics of the prepositions275Computational Linguistics Volume 35, Number 2Figure 4The mapping performed by the spatial reasoning module from qualitative to geometricrepresentations during the interpretation of a locative expression.used.
In this architecture this access is provided through the spatial reasoning compo-nent.
Figure 4 illustrates the translation between the qualitative, parser-generated, andthe geometric, vision subsystem?generated representations that must be performed inorder to interpret a spatial locative expression.At different stages during a dialog the dialog manager may recognize that thesystem needs to generate a response to the last input utterance.
For example, theutterance may have been a question, such as where is x?
or which x?.
In such cases,the dialog manager informs the content planner of this.
The role of the content planneris to determine the semantic content that should be included in the system?s output,rather than the linguistic realization of this content.
Indeed, the content planner maygenerate a logical representation closer to the parse structure on the right of Figure 3than to a natural language description.Generating referring expressions (GRE) is a key stage in content planning.
GRE isthe second function in the architecture where spatial reasoning plays an important role.The function of the GRE component is to determine the set of properties that distinguisha particular target object from the other objects in the scene.
For example, in responseto a question such as which x?
the GRE component may determine that a color and typedescription is sufficient to distinguish the target object, resulting in an answer such as theblue x being linguistically realized.
However, it may be the case that the location of thetarget in the scene is the only way to distinguish it.
In such cases, the GRE componentneeds access to computational models of the spatial prepositions if it is to determinewhich spatial relation is most suitable.
Figure 5 illustrates the translation from a geo-metric to qualitative representation that is performed during the GRE process by thespatial reasoning module when a locative description is being generated by the system.Once the content planning and GRE processes have been completed, the realizerdetermines a surface linguistic form in which this content can be conveyed.
Finally, thespeech synthesis systems generate the speech output for the linguistic string created bythe realizer.4.
Psycholinguistic Data on Spatial PrepositionsSpatial reasoning is a complex activity that involves at least two levels of process-ing: a geometric level where metric, topological, and projective properties are handled276Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 5The mapping performed by the spatial reasoning module from geometric to qualitative torepresentations during the generation of a locative description.
(Herskovits 1986), and a functional level where the normal function of an entity affectsthe spatial relationships attributed to it in a context (Coventry and Garrod 2004).There has been much experimental work done on spatial reasoning and language.Some of this work has focused on functional aspects of prepositional semantics (e.g.,Hayward and Tarr 1995; Coventry 1998; Garrod, Ferrier, and Campbell 1999), and someon geometric factors (Gapp 1995; Logan and Sadler 1996; Regier and Carlson 2001).
Inthis article we are primarily concerned with the geometric semantics of prepositionsand, consequently, our review will focus on the experimental data that addresses geo-metric factors.
We will begin by reviewing the experimental data describing topologicalspatial prepositions.
Following this, we will then review data relating to projectiveprepositions.Topological prepositions denote a region that is proximal to a landmark.
Subse-quently we discuss previous psycholinguistic experiments, focusing on how contex-tual factors such as distance, size, and salience may affect proximity.
We also presentexamples showing that the location of other objects in a scene may interfere with theacceptability of a proximal description to locate a target relative to a landmark.Logan and Sadler (1996) examined the semantics of several spatial prepositions.
Intheir experiments, a human subject was shown sentences of the form the X is [relation]the O, each with a picture of a spatial configuration of an O in the center of an invisible7 ?
7 cell grid, and an X in one of the 48 surrounding positions.
The subject then hadto rate how well the sentence described the picture, on a scale from 1 (bad) to 9 (good).Figure 6 gives the mean goodness rating for the relation ?near to?
as a function of theposition occupied by X (Logan and Sadler 1996).
It is clear from Figure 6 that ratingsdiminish as the distance between X and O increases, but also that even at the extremesof the grid the ratings were still above 1 (minimum rating).Besides distance there are also other factors that determine the applicability of aproximal relation.
For example, given prototypical size, the region denoted by near thebuilding is larger than that of near the apple.
Moreover, an object?s salience could influencethe determination of the proximal region associatedwith it; as with size, themore salientan object is the larger the proximal region associated with it (Gapp 1994).Finally, the two scenes in Figure 7 show interference as a contextual factor.
For thescene on the left we can use the blue box is near the black box to describe object (c).
Thisseems inappropriate in the scene on the right.
Placing an object (d) beside (b) appears277Computational Linguistics Volume 35, Number 2Figure 6A 7 ?
7 cell grid with mean goodness ratings for the relation the X is near O as a function of theposition occupied by X.to interfere with the appropriateness of using a proximal relation to locate (c) relative to(b), even though the absolute distance between (c) and (b) has not changed.There are several important features that are evident from these data.
First, given acontext, subjects have the ability to grade the applicability of a spatial relation.
Loganand Sadler (1996) introduced the term spatial template to describe the representation ofthe regions of acceptability associated with a preposition.
A spatial template is centeredon the landmark and identifies for each point in its space the acceptability of thespatial relationship between the landmark and the target appearing at that point beingdescribed by the preposition.
Second, there is empirical evidence pointing to the effectsof distance between that landmark and the target, and landmark salience and size on theapplicability of a proximity-based preposition.
Finally, the examples presented point tothe fact that the location of other distractor objects in context may also interfere with theapplicability of a preposition.
(The model of proximity we present in Section 6 capturesall these factors.
)Figure 8 is a representation of the spatial template for the projective prepositionabove described in Logan and Sadler (1996).
The main points of note relating to thesedata are that there are three regions in the spatial template (good, acceptable, andbad) and these regions are symmetric around the canonical direction of the prepositionwith acceptability approaching 0 as the angular deviation from the canonical directionapproaches 90 degrees.
However, it should be noted that these data were gathered dur-ing an interpretation task and that the task may have affected the subjects?
responses.Figure 7Proximity and distractor interference.278Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 8Spatial template for the preposition above (Logan and Sadler 1996), where LM represents thelandmark and the arrow shows the canonical direction associated with the preposition.Although the subjects may have rated some of the areas on the far right and left ofthe landmark as acceptable with respect to interpreting an utterance such as above thelandmark, this does not mean that they would use the word above to describe a targetobject in these regions relative to the landmark.
This highlights the fact that people maybe more accommodating when they are interpreting a locative description (for example,they may extend the allowable angular deviation to 90 degrees) but be more specificwhen generating a locative description.5.
Previous Models of Topological and Projective Spatial PrepositionsThere has been much research on the formal properties and interactions of topologicalrelations, for example Cohn et al (1997) and Kuipers (2000).
However, before thesehigher-level frameworks can be applied to real-world data, a model of proximity thatis capable of segmenting a region at the metric or geometric level is required.
Atthis geometric level previous approaches to modeling topological prepositions haveadopted one of two approaches to defining the region of proximity.
The first is to adopt aVoronoi segmentation of space.
Under this approach the region considered as proximalto an object is the area surrounding it that is closer to it than to any other object in thescene.
The second is to define the proximal region in terms of the size of the landmark.For example, Gapp (1995) defines the area of proximity as the region within ten timesthe size of the landmark object in each direction.
However, neither of these approachesconsider the effect that the locations of other objects in the scene have on the proximity.Consequently, they cannot distinguish between the different context provided by thetwo images in Figure 7.Several models of projective prepositions have been proposed (Yamada 1993;Olivier and Tsujii 1994; Gapp 1995; Fuhr et al 1998; Regier and Carlson 2001; Kelleherand van Genabith 2006).
Yamada (1993) introduced the concept of a potential fieldfunction to capture the gradation of applicability across the region described by thepreposition.
Later work (Olivier and Tsujii 1994; Gapp 1995) highlighted the issue ofdefining the intended frame of reference.
Building on this work and the psycholin-guistic results of Carlson-Radvansky and Logan (1997), Kelleher and van Genabith279Computational Linguistics Volume 35, Number 2(2006) developed a computational model that constructed amodified spatial template insituations where frame of reference ambiguity occurred.
Fuhr et al (1998) used modelsof prepositional semantics in order to interpret natural language commands to a roboticarm.
Fuhr et al segmented the space around an object into different regions basedon the sides and vertices of the object?s bounding box.
One of the drawbacks of thissystem, however, was that it could not distinguish between the position of two or moreobjects that were fully enclosed within a given region.
Finally, Regier and Carlson (2001)developed a vector sum algorithm to compute the applicability of a projective relationbetween a landmark and a target.
However, as with previous topological models, noneof these models consider the influence of other objects in the context of the landmarktarget relationship.
For example, the introduction of the long black object into image2 in Figure 9 affects the interpretability of a reference such as the blue square above thewhite rectangle.
In the next section we describe new models designed to account for theinfluence of other objects in the semantics of spatial prepositions.6.
Models of Visual Context in Topological and Projective Spatial PrepositionsIf a computational model is going to accommodate the gradation of applicability acrossa preposition?s spatial template it must define the semantics of the preposition assome sort of continuum function.
A potential field model is one form of continuummeasure that is widely used (Yamada 1993; Gapp 1994; Olivier and Tsujii 1994; Regierand Carlson 2001).
Using this approach, a model of a preposition?s spatial templateis constructed using a set of normalized equations that, for a given origin and point,computes a value that represents the cost of accepting that point as the interpretation ofthe preposition.Each equation used to construct the potential field representation of a preposition?sspatial template models a different geometric constraint specified by the preposition?ssemantics.
For example, for topological prepositions such as near, an equation inverselyproportional to the distance between a point and a landmark would be used, whilefor projective prepositions such as to the right of, an equation modeling the angular de-viation of a point from the idealized direction denoted by the preposition would be in-cluded in the construction set; Gapp (1995) and Logan and Sadler (1996) both noted thatacceptability of a projective preposition being used to describe a location approaches0 as the angular deviation of that location approaches 90 degrees.
The potential fieldis then constructed by assigning each point in the field an overall potential by inte-grating the results computed for that point by each of the equations in the construc-tion set.Figure 9Projective prepositions and distractor interference.280Kelleher and Costello Computational Models of Spatial Prepositions for VSDThis potential field approach does not, however, account for the influence of otherobjects in the visual scene on the semantics of a topological or projective preposition.The basic idea in our computational models is to extend the potential field approach byoverlaying the potential fields for each object in a visual scene and combining thosefields to produce relative potential fields for topological or projective prepositions.These relative potential fields represent the semantics of those prepositions as modifiedby the presence of other objects in the visual scene.6.1 Computational Model of Topological PrepositionsIn this section we describe a model of relative proximity that uses (1) the distancebetween objects, (2) the size and salience of the landmark object, and (3) the locationof other objects in the scene.
Our model is based on first computing absolute proximitybetween each point and each landmark in a scene, and then combining or overlayingthe resulting absolute proximity fields to compute the relative proximity of each pointto each landmark.6.1.1 Computing Absolute Proximity Fields.
We first compute for each landmark an ab-solute proximity field giving each point?s proximity to that landmark, independent ofproximity to any other landmark.
We compute fields on the projection of the scene ontothe 2D-plane, represented as a 2D-array of points.
At each point P in that array, theabsolute proximity for landmark L isproxabs(L,P) = (1?
distnormalized(L,P)) ?
salience(L) (1)In this equation the absolute proximity for a point P and a landmark L is a function ofboth the distance between the point and the location of the landmark, and the salienceof the landmark.To represent distance we use a normalized distance function distnormalized(L,P),which returns a value between 0 and 1.5 The smaller the distance between L and P,the higher the absolute proximity value returned, that is, the more acceptable it is to saythat P is close to L. In this way, this component of the absolute proximity field capturesthe gradual gradation in applicability evident in Logan and Sadler (1996).We model the influence of visual and discourse salience on absolute proximity asa function salience(L), returning a value between 0 and 1 that represents the relativesalience of the landmark L in the scene (Equation (2)).
For the current purposes weassume that the relative salience of an object is the average of its visual salience (Svis)and discourse salience (Sdisc).6salience(L) = (Svis(L)+ Sdisc(L))/2 (2)5 We normalize by computing the distance between the two points, and then dividing this distance by themaximum distance between point L and any point in the scene.6 There are, of course, many other operators that could be used to combine visual and linguistic salience,such as maximum (MAX(Svis(L),Sdisc(L))) or probabilistic OR (Svis(L)+ Sdisc(L)?
(Svis(L)?
Sdisc(L))).We currently have no way of deciding among these operators.
Fortunately, however, the modular natureof our framework would allow us to change the computation of relative salience without impacting otheraspects of our model, should evidence in favor of one or other operator become available.281Computational Linguistics Volume 35, Number 2Visual salience Svis is computed using the algorithm of Kelleher and van Genabith(2004).
Computing a relative salience for each object in a scene is based on its perceivablesize and its centrality relative to the viewer?s focus of attention.
The algorithm returnsscores in the range of 0 to 1.
As the algorithm captures object size, we can modelthe effect of landmark size on proximity through the salience component of absoluteproximity.
The discourse salience (Sdisc) of an object is computed based on recency ofmention (Hajicova?
1993) except we represent the maximum overall salience in the sceneas 1, and use 0 to indicate that the landmark is not salient in the current context.Figure 10 shows computed absolute proximity with salience values of 1, 0.6, and0.5, for points from the upper-left to the lower-right of a 2D plane, with the landmarkat the center of that plane.
The graph shows how salience influences absolute proximityin our model: For a landmark with high salience, points far from the landmark can stillhave high absolute proximity to it.6.1.2 Computing Relative Proximity Fields.
Once we have constructed absolute proximityfields for the landmarks in a scene, our next step is to overlay these fields to produce ameasure of relative proximity to each landmark at each point.
For this we first selecta landmark, and then iterate over each point in the scene comparing the absoluteproximity of the selected landmark at that point with the absolute proximity of all otherlandmarks at that point.
The relative proximity of a selected landmark at a point isequal to the absolute proximity field for that landmark at that point, minus the highestabsolute proximity field for any other landmark at that point:proxrel(P,L) = proxabs(P,L)?
MAX?LX =Lproxabs(P,LX) (3)The idea here is that the other landmark with the highest absolute proximity is actingin competition with the selected landmark.
If that other landmark?s absolute proximityFigure 10Absolute proximity ratings for landmark L centered in a 2D plane, points ranging from plane?supper-left corner (??3,?3?)
to lower right corner (?3,3?
).282Kelleher and Costello Computational Models of Spatial Prepositions for VSDis higher than the absolute proximity of the selected landmark, the selected landmark?srelative proximity for the point will be negative.
If the competing landmark?s absoluteproximity is slightly lower than the absolute proximity of the selected landmark, theselected landmark?s relative proximity for the point will be positive, but low.
Only whenthe competing landmark?s absolute proximity is significantly lower than the absoluteproximity of the selected landmark will the selected landmark have a high relativeproximity for the point in question.In Equation (3) the proximity of a given point to a selected landmark rises asthat point?s distance from the landmark decreases (the closer the point is to the land-mark, the higher its proximity score for the landmark will be), but falls as that point?sdistance from some other landmark decreases (the closer the point is to some otherlandmark, the lower its proximity score for the selected landmark will be).
Figure 11shows the relative proximity fields of two landmarks, L1 and L2, computed usingEquation (3) in a 1-dimensional (linear) space.
The two landmarks have different de-grees of salience: a salience of 0.5 for L1 and of 0.6 for L2 (represented by the differentsizes of the landmarks).
In this figure, any point where the relative proximity for oneparticular landmark is above the zero line represents a point which is proximal tothat landmark, rather than to the other landmark.
The extent to which that point isabove zero represents its degree of proximity to that landmark.
The overall proximalarea for a given landmark is the overall area for which its relative proximity field isabove zero.
The left and right borders of the figure represent the boundaries (walls) ofthe area.Figure 11 illustrates three main points.
First, the overall size of a landmark?s prox-imal area is a function of the landmark?s position relative to the other landmark andto the boundaries.
For example, landmark L2 has a large open space between it andthe right boundary: Most of this space falls into the proximal area for that landmark.Landmark L1 falls into quite a narrow space between the left boundary and L2.
L1thus has a much smaller proximal area in the figure than L2.
Second, the relativeproximity field for a landmark is a function of that landmark?s salience.
This can beseen in Figure 11 by considering the space between the two landmarks.
In that spacethe width of the proximal area for L2 is greater than that of L1, because L2 is moresalient.Figure 11Graph of relative proximity fields for two landmarks L1 and L2.
Relative proximity fields werecomputed with salience scores of 0.5 for L1 and 0.6 for L2.283Computational Linguistics Volume 35, Number 2Figure 12Example scene.The third point concerns areas of ambiguous proximity in Figure 11: areas in whichneither of the landmarks have a significantly higher relative proximity than the other.There are two such areas in the Figure.
The first is between the two landmarks, inthe region where one relative proximity field line crosses the other.
These points areambiguous in terms of relative proximity because these points are equidistant fromthose two landmarks.
The second ambiguous area is at the extreme right of the spaceshown in Figure 11.
This area is ambiguous because this area is distant from bothlandmarks: Points in this area would not be judged proximal to either landmark.
Thequestion of ambiguity in relative proximity judgments is considered in more detail inSection 8.1.We will illustrate the different stages of the proximity model using the situationillustrated in Figure 12.
The task is to decide whether the target object is proximal tothe landmark object.
Figure 13 illustrates the absolute potential field for the landmarkFigure 13The absolute proximity fields for the landmark.284Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 14The absolute proximity fields for the landmark and the distractor.object.
Figure 14 illustrates the absolute potential fields for the landmark and thedistractor object.
Figure 15 illustrates the relative proximity field that results from theinteraction between the landmark and distractors absolute proximity fields.
Figure 16illustrates the application of the threshold to the landmark?s relative proximity field.
Ifthe target object is located in the region where the landmark?s relative proximity fieldFigure 15The landmark?s relative proximity field.285Computational Linguistics Volume 35, Number 2Figure 16Applying the threshold to the landmark?s relative proximity field.is above the threshold the target is deemed to be proximal to the landmark.
Figure 16demonstrates the contextual influence which the distractor object has on the landmark?srelative proximity field: The field shrinks on the side of the landmark near the distractor,but expands on the side away from the landmark.6.2 Computational Model of Projective PrepositionsThe two main factors that impact on the applicability of a projective preposition de-scribing the spatial relationship between a target object and a landmark are the angulardeviation of the target object?s position from the canonical direction described by thepreposition relative to the landmark and the distance of the target object from thelandmark.The vector originating from the center of the landmark to the viewer?s positiondescribes the canonical search axis for in front of.
We can produce the search vectorsfor the other projective prepositions (behind, left, right) by rotating this front vector on ahorizontal plane.
Once the canonical vectorc for a given projective preposition has beenselected, the angular deviation of a given point P position relative to the landmark L canbe computed using Equation (4):angle( LP,c ) = cos?1?
?LP ?c???LP??
?|c |??
(4)where LP is the vector from landmark L to point P and c is the canonical vector for theprojective preposition in question.
This equation gives the angle between LP and thatcanonical vector.286Kelleher and Costello Computational Models of Spatial Prepositions for VSDUsing this equation, and the normalized distance measure described in Section 6.1,we define an absolute potential field for the acceptability of a projective prepositionwith canonical vector c for landmark L as follows:projabs(L,P,c ) = 0 if (angle( LP,c ) > 90)or(distnormalized(L,P) = 0),= (angle( LP,c )/distnormalized(L,P)) otherwise (5)In this equation, if the angle between a point P and the canonical vector c is greater than90 degrees, or if the distance between the landmark and the point is 0, the acceptabilityof that point for the projective preposition is 0.
Otherwise, the acceptability of thatpoint is equal to the angle between that point and the canonical vector, divided by thenormalized distance between that point and the landmark.We use this absolute potential field for projective prepositions in the same waythat we used the absolute field for proximity in our model of topological prepositions.Once we have computed the absolute potential field for each point relative to thelandmark we then do the same process for each of the distractor landmarks.
Wethen overlay the landmark applicabilities with those of the distractors by subtractingthe maximum applicability of any of the distractors at a point from the landmark?sapplicability at that point, producing a relative potential field for the projective prepo-sition as in Equation (6):projrel(P,L,c ) = projabs(P,Lc )?
MAX?LX =Lprojabs(P,LX,c ) (6)We then apply a threshold, and the region above this threshold is taken to definethe area described by the projective preposition.
Note that we can use Equation (6) tocompute relative potential fields for various different projective prepositions (in front of,behind, left, right, above, below) by selecting the different canonical vectors correspondingto those prepositions.Figures 17 through 20 illustrate the different stages in this process.
In these imagesthe origin is at the front right corner, the x-axis runs from right to left, the y-axisfrom front to back, and the z-axis is the vertical.
The higher the z-axis value the moreapplicability the preposition.
Figure 17 defines the baseline applicability of z = 0.1.
Weuse this baseline because dividing an angular deviation by distance will never result ina zero value; rather applicability will approach 0 asymptotically.
The baseline providesa cut-off point for applicability.
Figure 18 illustrates the potential field computed forright of a landmark positioned at x = 100, y = 200, z = 0 with a search axis of x = 1,y = 0.
Figure 19 illustrates the potential fields computed for right of the landmark anda distractor object positioned at x = 150, y = 400, z = 0.
Finally, Figure 20 illustrates thepotential field that results for right of the landmark when the distractor potential field issubtracted from it.
Figure 20 demonstrates the contextual influence which the distractorobject has on the landmark?s relative potential field for the preposition: the size of thefield is reduced by the presence of the distractor object.7.
Psycholinguistic Evaluations of Our ModelsWe now describe an experiment which tests our approach to relative proximity byexamining the changes in people?s judgments of the appropriateness of the expressionnear being used to describe the relationship between a target and landmark object in287Computational Linguistics Volume 35, Number 2Figure 17A baseline applicability is set to z = 0.1.an image where a distractor object is present.
All objects in these images were coloredshapes: circles, triangles, or squares.7.1 Materials and ProcedureAll images used in this experiment contained a central landmark object and a targetobject, usually with a third distractor object.
The landmark was always placed in theFigure 18The potential field describing the absolute applicability model for right of the landmark.288Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 19The potential fields describing the absolute applicability model for right of the landmark andright of a distractor object.middle of a 7 ?
7 grid.
Images were divided into eight groups of six images each.
Eachimage in a group contained the target object placed in one of six different cells on thegrid, numbered from 1 to 6.
Figure 21 shows how we number these target positionsaccording to their nearness to the landmark.Groups are organized according to the presence and position of a distractor object.In group a the distractor is directly above the landmark, in group b the distractor isFigure 20The resulting potential field for right of the landmark with the baseline applied to it.289Computational Linguistics Volume 35, Number 2Figure 21Relative locations of landmark (L) target positions (1. .
.
6) and distractor landmark positions(a. .
.g) in images used in the experiment.rotated 45 degrees clockwise from the vertical, in group c it is directly to the right ofthe landmark, in d it is rotated 135 degrees clockwise from the vertical, and so on.
Thedistractor object is always the same distance from the central landmark.
In addition tothe distractor groups a,b,c,d,e,f, and g, there is an eighth group, group x, in which nodistractor object occurs.In the experiment, each image was displayed with a sentence of the form Theis near the , with a description of the target and landmark, respectively.
The sentencewas presented under the image.
Twelve participants took part in this experiment.
Allparticipants were native English speakers and all volunteered to take part.
Participantswere not linguists and were naive to the formal interpretation of spatial prepositionsand to the hypotheses being tested in the experiment.
Participants were asked to ratethe acceptability of the sentence as a description of the image using a 10-point scale,with zero denoting not acceptable at all; 4 or 5 denoting moderately acceptable; and 9perfectly acceptable.
Figure 22 illustrates a trial from the experiment.
Each participantrated every image in the experiment.
Images were presented in random order to controlfor learning effects.7.2 Results and DiscussionThere was significant agreement between participants across all 48 images.
The averagepair-wise correlation between participants?
responses was r = 0.68.
There was a signifi-cant correlation of responses between every pair of participants (p < 0.01 for all pairs).We assess participants?
responses by comparing their average proximity judgmentswith those predicted by the absolute proximity equation (Equation (1)), and by therelative proximity equation (Equation (3)).
For both equations we assume that all objectshave a salience score of 1.
With salience equal to 1, the absolute proximity equationrelates proximity between target and landmark objects to the distance between thosetwo objects, so that the closer the target is to the landmark the higher its proximity willbe.
With salience equal to 1, the relative proximity equation relates proximity to bothdistance between target and landmark and distance between target and distractor, so290Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 22An example trial from the proximity experiment.that the proximity of a given target object to a landmark rises as that target?s distancefrom the landmark decreases but falls as the target?s distance from some other distractorobject decreases.
It should be noted that proximity scores in both Equations (1) and(3) are multiplied by a constant salience and that the evaluations we describe below(correlation, multiple regression) factor out multiplication by a constant.
Consequently,choosing a particular value for salience does not affect our evaluation results.In analyzing our results we are comparing our basic equation for absolute proximity(Equation (1), in which proximity falls with increasing distance between target andlandmark) with the ?relative proximity?
extension of this equation (Equation (3), inwhich proximity falls with increasing distance between target and landmark, but riseswith distance to distractor).
Because both equations are quite similar (both are based ontarget?landmark distance, which is obviously the prime factor in proximity judgments),we expect both equations to produce quite similar responses.
We expect, however, thatthe relative proximity equations will produce responses which are reliably closer topeople?s proximity judgments than those produced by the absolute proximity equation.We initially used Spearman?s rank-order correlation to compare people?s averageproximity scores with those produced by Equation (1) (absolute proximity) and Equa-tion (3) (relative proximity) for each group.
For each group this analysis replaces eachproximity score with its rank within that group, and then compares the ranks.
Wherethe ranks returned by an equation and the ranks from participants?
average proximityscores are identical, the correlation will be 1.0; where the ranks differ, the correlationwill drop.
For the absolute proximity equation, the correlation was 1.0 in six of thegroups, and .94 in the two remaining groups (group c and group g).
For the relativeproximity equation, the Spearman?s rank-order correlation with people?s responses was1.0 in each of the eight groups.
The fact that the relative proximity equation has a rank-order correlation of 1.0 in all groups while the absolute proximity equation fails to reach1.0 in two groups (predicting proximity-ranks incorrectly in those two groups) suggests291Computational Linguistics Volume 35, Number 2that the relative-proximity equation is a better model of people?s proximity responses.However, the fact that there are somany correlations of 1.0 means that Spearman?s rank-order correlation is not particularly useful in distinguishing between the two equations.We therefore use Pearson?s product-moment correlation to compare people?s averageproximity scores with those produced by the absolute and relative proximity equations.Rather than comparing ranks, this analysis compares actual proximity values.Figure 23 shows the product-moment correlations between people?s average prox-imity ratings and those produced by Equation (1) (absolute proximity) and by Equa-tion (3) (relative proximity) for the eight groups in the experiment.
In analyzing thesecorrelations we had two concerns: first, to see whether, for each individual group, thecorrelation produced by Equation (3) was reliably different from that produced byEquation (1); and second, to see whether across all the groups, the correlation producedby Equation (3) was reliably higher than that produced by Equation (1).
In regardto the first question, we did not expect there to be particularly large differences incorrelation between the two equations, because both are based on target?landmarkdistance.
Because we know target?landmark distance to be a good predictor of people?sproximity judgments we expected Equation (1) to have a high correlation with people?sproximity judgments, and we expected Equation (3) to improve on that correlation.However, because the correlation from Equation (1) was already high, any improvementin correlation from Equation (3) would be relatively small.
Indeed this is what is seenacross the seven groups of interest: The average correlation from Equation (1) is high(average 0.93), the average correlation from Equation (3) is higher (average 0.99), butthe difference between the two correlations is relatively small.
Using Fisher?s techniquefor comparing correlation coefficients we find no reliable difference between correlationcoefficients in any group.Given that the correlations for both Equations (1) and (3) are high we examinedwhether the results returned by Equation (3) were reliably closer to human judgmentsthan those from Equation (1).
For the 42 images where a distractor object was present werecorded which equation gave a result that was closer to the participants?
normalizedaverage for that image.
In 28 cases Equation (3) was closer, and in 14 Equation (1) wascloser (a 2:1 advantage for Equation (3), significant in a sign test: n+ = 28,n?
= 14,Z =2.2, p < 0.05).
We conclude that proximity judgments for objects in our experimentare best represented by relative proximity as computed in Equation (3).
These resultssupport our ?relative?
model of proximity.7In addition to these analyses, we also carried out a multiple regression analysis ofparticipants?
responses in the experiment, with target?landmark distance and target?distractor distance as the predictor variables, and participant response as the depen-dent variable.
Because our experiment involved repeated-measures data, we followedthe procedure for regression analysis of repeated-measures data described by Lorchand Myers (1990).
This involves computing individual multiple regression for eachparticipant in our experiment, and then using a t-test to analyze the regression coef-ficients produced for target?distractor distance and target?landmark distance in thoseequations, across all participants.
Recall that in our relative proximity equation (Equa-tion (3)) target?landmark distance had a negative coefficient (as target?landmark dis-tance increased, judgments of target?landmark proximity fell) whereas target?distractor7 Note that, in order to display the relationship between proximity values given by participants, computedin Equation (1), and computed in Equation (3), the values displayed in Figure 23 are normalized so thatproximity values have a mean of 0 and a standard deviation of 1.
This normalization simply means thatall values fall in the same region of the scale, and can be easily compared visually.292Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 23Comparison between normalized proximity scores observed and computed for each group.293Computational Linguistics Volume 35, Number 2distance had a positive coefficient (as target?distractor distance increased, judgmentsof target?landmark proximity increased).
Our prediction, therefore, is that across thesemultiple regression analyses of participants?
responses, the target?landmark distancevariable will reliably have a negative coefficient, whereas the target?distractor variablewill reliably have a positive coefficient.Table 1 shows the regression coefficients obtained for the target?landmark distancevariable and the target?distractor distance variable, across the 12 participants in ourexperiment.
As this table shows, the regression coefficient for target?landmark distancewas significantly more likely to be negative (as predicted) whereas the regressioncoefficient for target?distractor distance was significantly more likely to be positive(again, as predicted).
A single-group t-test showed that both target?landmark regres-sion coefficients and target?distractor regression coefficients reliably differed from zero(t(11) = ?8.64, p < 0.01; t(11) = 2.23, p < 0.05) indicating that both of these predictorvariables had a significant and reliable effect on participants?
responses in the exper-iment.
There was no concern about collinearity between predictor variables in theseregression analyses, as the correlation between those variables (r = 0.38, %var = 0.14)was much lower than that between the predictor variables and the dependent variable(r = 0.93 or higher).
Together these regression results, the sign-test results, and thecomparative correlations described earlier all support the model of relative proximityas described in Equation (3).8.
Applications of the ModelsThe model of proximity presented here has been implemented and used as a compo-nent in a human?robot dialog system (Kelleher and Kruijff 2006; Kelleher, Kruijff, andCostello 2006).
The proximity and projective models have also been integrated into theLIVE virtual environment (Kelleher, Costello, and van Genabith 2005).
In this sectionwe will describe how the models are used in these systems to interpret and generatelocative expressions.Table 1Regression coefficients from individual analyses of subjects data in proximity experiment.participant target?landmark distance target?distractor distance1 ?2.02 0.272 ?1.53 0.093 ?3.06 0.034 ?2.45 ?0.025 ?2.23 0.066 ?0.97 0.327 ?3.09 0.428 ?1.78 0.029 ?0.80 0.1610 ?1.48 ?0.2411 ?3.29 0.0112 ?3.29 0.44M ?2.17 0.13SE 0.88 0.20t ?8.64* 2.23***p < 0.01, **p < 0.05.294Kelleher and Costello Computational Models of Spatial Prepositions for VSD8.1 Interpreting Spatial ReferencesWe use the computational models of Section 6 to interpret spatial references to objects.In this section we illustrate how we use our model of relative proximity to ground theinterpretation of a locative expression containing a topological preposition.
Returningto the architecture described in Section 3, the basic steps triggering the interpretationof a locative are: (1) the user utters a command, such as pick up the ball near the red box,(2) the speech recognition module processes the speech signal and passes the resultingstring to the parser, (3) the parser constructs a formal representation of the meaning ofthe utterance, (4) the dialog manager categorizes the utterance to be a command and,also, recognizes the need to resolve the referring expression the ball near the red box.
Atthis point the reference resolution module is triggered.The first stage in reference resolution is to retrieve the context against which thereference is to be resolved.
This involves accessing the context model and retrievingthe set of currently accessible objects.
This set is then subdivided into the set of objectsfulfilling the landmark description, the set of objects fulfilling the description of thetarget object, and the set of objects fulfilling neither description.For each candidate landmark and each object that is neither a candidate landmarknor a candidate target we compute an absolute proximity field.
For each landmark weconvert its absolute proximity field into a relative proximity field by overlaying theabsolute proximity fields of the other landmarks and the other objects in the contextthat are neither candidate landmarks nor target objects.
For this we iterate over eachpoint in the scene, and compare the competing absolute proximity scores at each point.If the primary landmark?s (i.e., the landmark with the highest relative proximity at thepoint) relative proximity exceeds the next highest relative proximity score at a givenpoint by more than a predefined confidence interval, the point is in the proximity regionanchored around the primary landmark.
Otherwise, we take it as ambiguous and not inthe proximal region that is being interpreted.
The motivation for the confidence intervalis to capture situations where the difference in relative proximity scores between theprimary landmark and one or more landmarks at a given point is relatively small.Figure 24 illustrates the parsing of a scene into the regions ?near?
two landmarks.
Therelative proximity fields of the two landmarks are identical to those in Figure 11, using aconfidence interval of 0.1.
Ambiguous points are where the proximity ambiguity seriesis plotted at 0.5.
The regions ?near?
each landmark are those areas of the graph whereeach landmark?s relative proximity series is the highest plot on the graph.Figure 24 illustrates an important aspect of our model: the comparison of relativeproximity fields naturally defines the extent of vague proximal regions.
For example,see the region right of L2 in Figure 24.
The extent of L2?s proximal region in thisdirection is bounded by the interference effect of L1?s relative proximity field.
Becausethe landmarks?
relative proximity scores converge, the area on the far right of the imageis ambiguous with respect to which landmark it is proximal to.
In effect, the modelcaptures the fact that the area is relatively distant from both landmarks.
In Section 8.2we describe a cognitive load hierarchy of prepositions and how we use this to generatelocative expressions.
Following this hierarchy, objects located in the area on the far rightof the image should be described with a projective relation such as to the right of L2rather than a proximal relation like near L2.8.1.1 An Example.
To illustrate the model further we will apply the model to a real scene.Figure 25 shows a real scene on the left-hand side, and a rendering of the scene analysison the right-hand side.
For the shown scene analysis we have assumed all objects to295Computational Linguistics Volume 35, Number 2Figure 24Graph of ambiguous regions overlaid on relative proximity fields for landmarks L1 and L2,with confidence interval = 0.1 and different salience scores for L1 (0.5) and L2 (0.6).
Locationsof landmarks are marked on the x-axis.have an equal salience: on the left, the blue ball; in the middle, the red ball; and onthe right, the green ball.
As the analysis correctly shows, each object has a proximitypotential field (shown in its own color) but, due to interference between potentialfields, we see that proximity is usually ambiguous between at least two landmarks.The regions that are ambiguous between two landmarks are colored using a mixture ofthe colors.
The white area denotes the regions defined as being ambiguous between thethree objects.Imagine we now place a second blue ball in the scene and the user inputs thecommand pick up the blue ball near the red ball.
As explained previously, when the systemstarts interpreting this reference it will split the context into a set of candidate targetobjects, consisting of the two blue balls in the scene, the set of candidate landmarks,consisting of the one red ball, and the set of remaining objects, the green ball.
It willthen compute proximity fields for each of the candidate landmarks and the other objectsin the scene that are not candidate targets.
It will then overlay these proximity fieldsFigure 25Scene analysis.296Kelleher and Costello Computational Models of Spatial Prepositions for VSDto compute the relative proximity fields around each landmark.
Figure 26 illustratesthe resulting proximity fields.
As can be seen from the image the original blue ball isinside the red ball?s proximity field and consequently it will be selected as the ball to bepicked up.This analysis highlights two important aspects of the model.
First, we can observean interference effect between the red ball and the green ball: The potential field repre-senting proximity to the red ball forms an ellipsoid, being inhibited to the right throughinterference with the potential field of the green ball.
Second, the proximity field ofthe red ball is much larger then that of the green ball; this is due to the relatively highlinguistic salience of the red ball compared to the green ball due to it being mentionedin the reference.8.2 Generating ReferencesIn this section we illustrate how we use our models of the semantics of spatial preposi-tions to guide the generation of a locative expression in visual situated contexts.In the architecture described earlier, the GRE component is triggered by the contentmanager.
Similar to reference resolution, GRE will first retrieve the context from thecontext model and generate the reference relative to this context.
If a locative expressionis necessary the GRE component has three things to decide: (1) what properties of thetarget object to include, (2) which object in the scene should be used as a landmark andhow should that be described, and (3) which spatial relation to use (and hence whichpreposition to use).Several GRE algorithms have addressed the issue of generating locative expressions(Dale and Haddock 1991; Horacek 1997; Gardent 2002; Krahmer and Theune 2002;Figure 26Interpreting the blue ball near the red ball.297Computational Linguistics Volume 35, Number 2Varges 2004).
However, all these algorithms assume the GRE component has access toa predefined scene model that defines all the spatial relations between all the entitiesin the scene.
For many visually situated dialog systems, in particular robotic dialogsystems, this assumption is a serious drawback for these algorithms.
If an agent wishesto generate a contextually appropriate reference it cannot assume the availability ofa domain model, rather it must dynamically construct one.
Moreover, constructing amodel containing all the spatial relationships between all the entities in the domainis prone to combinatorial explosion, both in terms of the number of objects in thecontext (the location of each object in the scene must be checked against all the otherobjects in the scene) and number of inter-object spatial relations (as a greater numberof spatial relations will require a greater number of comparisons between each pairof objects).
Furthermore, the context-free a priori construction of such an exhaustivescene model is cognitively implausible.
Psychological research indicates that spatialrelations are not preattentively perceptually available (Treisman and Gormican 1988).Rather, their perception requires attention (Logan 1994, 1995).
These findings point tosubjects constructing contextually dependent reduced relational scene models, ratherthan an exhaustive context-free model.The approach we adopt to generating locative expressions addresses the issue ofcombinatorial explosion inherent in relational scene model construction by incremen-tally creating a series of reduced scene models.
Within each scene model only onespatial relation is considered and only a subset of objects are considered as candidatelandmarks.
This reduces both the number of relations that must be computed over eachobject pair and the number of object pairs.
The decision as to which relations should beincluded in each scene model is guided by a cognitively-motivated hierarchy of spatialrelations.
The set of candidate landmarks in a given scene is dependent on the set ofobjects in the scene that fulfill the description of the target object and the semantic relationthat is being considered.We use Dale and Reiter?s (1995) incremental GRE algorithm as the starting pointfor the generation framework.
The incremental algorithm iterates through the proper-ties of the target object and for each property computes the set of distractor objects forwhich the conjunction of the properties selected so far, and the current property, hold.
Aproperty is added to the list of selected properties if it reduces the size of the distractorobject set.
The algorithm succeeds when all the distractors have been ruled out; it failsif all the properties have been processed and there are still some distractor objects.The algorithm can be refined by ordering the checking of properties according to fixedpreferences; for example, first a taxonomic description of the target, second an absoluteproperty such as color, third a relative property such as size.
Dale and Reiter alsostipulate that the type description of the target should be included in the descriptioneven if its inclusion does not distinguish the target from any of the distractors; seeAlgorithm 1.
Dale and Reiter argue that this algorithm has a polynomial complexityand that the theoretical run time can be characterized as nd ?
nl: the run time dependssolely on the number of distractor objects nd and the number of properties consideredin iterations nl.
If we assume that nd and nl are both proportional to n, the number of ob-jects being considered, then the complexity of the incremental algorithm is of order n2.The incremental algorithm generates a description (in terms of type, color, andsize) which distinguishes a given target object from a set of distractor objects (if sucha description exists).
However, we wish to generate locative expressions which iden-tify objects, rather than simple descriptions.
These locative expressions may containa description of a landmark object (in terms of type, color, or size), of a target object(type, color, or size), and a topological or projective preposition relating those two298Kelleher and Costello Computational Models of Spatial Prepositions for VSDAlgorithm 1 The Basic Incremental AlgorithmRequire: T = target object; D = set of distractor objects.Initialize: P = {type, color, size}; DESC = {}for i = 0 to |P| doif |D|= 0 thenD?
= {x : x ?
D,Pi(x) = Pi(T)}if |D?| < |D| thenDESC = DESC ?
Pi(T)D = {x : x ?
D,Pi(x) = Pi(T)}end ifelseDistinguishing description generatedif type(x)?
DESC thenDESC = DESC ?
type(x)end ifreturn DESCend ifend forFailed to generate distinguishing descriptionreturn DESCobjects.
To generate such locative expressions we repeatedly call the basic incrementalalgorithm for a sequence of different possible spatial relations.
The fact that each call tothe algorithm uses a different spatial relation results in a different set of objects fromthe context being defined as candidate landmarks for each function call.
If a givenspatial relation allows the basic incremental algorithm to generate a description whichdistinguishes the target object from the set of distractor objects, that spatial relation isused to generate an expression identifying that object.
Otherwise we move on and callthe basic incremental algorithm for the next spatial relation in our sequence.When generating a referring expression, we use a sequence of possible forms ofreference ordered by assumed cognitive load (see Figure 27), with simpler forms ofreference (those identifying object type, for example) coming early in the sequenceand more complex forms (those involving projective prepositions, for example) cominglater.
This means that our approach will preferentially produce simpler expressions toidentify an object, and only if no such simple expressions can be found which distin-guish that object successfully will more complex topological or projective prepositionsFigure 27Cognitive load of reference forms.299Computational Linguistics Volume 35, Number 2be produced.
Our sequence of relations can be extended to include relations of ternaryand higher arity such as the ball between the box and the triangle or the ball near the box andthe triangle.We use the models of topological and projective prepositions described in Sec-tions 6.1 and 6.2 to define the regions around a landmark to which a given topological orprojective description applies.
If the target or one of the distractor objects is the only ob-ject within that region around a given landmark, this is taken to represent a contrastiveuse of a preposition relative to that landmark.
If that region contains more than oneobject from the target and distractor object set, then it is a relative use of the preposition.8.2.1 Landmarks and Distinguishing Descriptions.
In order to use a locative expression,an object in the context must be selected to function as the landmark.
An implicitassumption in selecting an object to function as a landmark is that the hearer can easilyidentify and locate the object within the context.
As shown in Example (4), a landmarkcan be the speaker, the hearer, the scene, an object in the scene, or a group of objects inthe scene.8Example 4?
the ball on my right [speaker]?
the ball to your left [hearer]?
the ball on the right [scene]?
the ball to the left of the box [an object in the scene]?
the ball in the middle [group of objects]Clearly, deciding which objects in a given visual context can function as landmarksis a complex process.
Some of the factors effecting this decision are object salienceand the functional relationships between objects.
However, one basic constraint onlandmark selection is that the landmark should be distinguishable from the target.
Forexample, in the context provided by Figure 28 the ball has a relatively high salience,because it is a singleton, despite the fact that it is smaller and geometrically less complexthan the other figures.
Moreover, in this context, the ball is the only object in the scenethat can function as a landmark without recourse to using the scene itself or a groupingof objects in the scene.
Given the context in Figure 28 and all other factors being equal,using a locative such as the man to the left of the man would be much less helpful thanusing the man to the right of the ball.Following this observation, we treat an object as a candidate landmark if thefollowing conditions are met:1.
The object is not the target.2.
The object is not a member of the distractor set.Furthermore, a target landmark is a member of the candidate landmark set that standsin relation to the target under the relation being considered and a distractor landmark8 See Gorniak and Roy (2004) for further discussion on the use of spatial extrema of the scene and groupsof objects in the scene as landmarks.300Kelleher and Costello Computational Models of Spatial Prepositions for VSDFigure 28Visual context used to illustrate the relative semantics of topological and projective prepositions.is a member of the candidate landmark set that stands in relation to a distractor objectunder the relation being considered.Using these categories of landmark we can define a distinguishing locative de-scription as a locative description where there is a target landmark that can be dis-tinguished using the basic incremental algorithm from all the members of the set ofdistractor landmarks which stand under the relation used in the locative expression.Given this, our approach is to try to generate a distinguishing description using thestandard incremental algorithm.
If this fails, we divide the context into three compo-nents: the target, the distractor objects, and the set of candidate landmarks.
We thenbegin to iterate through the hierarchy of relations and for each relation we create acontext model that defines the set of target and distractor landmarks.
Once a contextmodel has been created we iterate through the target landmarks (using a salience order-ing if there is more than one) and try to create a distinguishing locative description.
Adistinguishing locative description is created by using the basic incremental algorithmto distinguish the target landmark from the distractor landmarks.
If we succeed ingenerating a distinguishing locative description we return the description and stopprocessing.Algorithm 2 The Locative Incremental AlgorithmRequire: T = target object; D = set of distractor objects; R = hierarchy of relations.DESC = Basic-Incremental-Algorithm(T,D)if DESC= Distinguishing thencreate CL the set of candidate landmarksCL = {x : x= T,DESC(x) = false}for i = 0 to |R| docreate a context model for relation Ri consisting of TL the set of target landmarks and DL the setof distractor landmarksTL = {y : y ?
CL,Ri(T, y) = true}DL = {z : z ?
CL,Ri(D, z) = true}for j = 0 to |TL| by salience(TL) doLANDDESC = Basic-Incremental-Algorithm(TLj, DL)if LANDDESC = Distinguishing thenDistinguishing locative generatedreturn {DESC,Ri,LANDDESC}end ifend forend forend ifFAIL301Computational Linguistics Volume 35, Number 2If we cannot create a distinguishing locative description we move on to the next,more complex spatial relation in the sequence of spatial relations, and attempt to gener-ate a distinguishing locative description using that relation.
This process continues untileither a distinguishing expression is produced or no possible spatial relations remain.This algorithm runs the basic incremental algorithm a number of times for eachcandidate relation in the list of possible relations.
The length of this list will be aconstant; call it R. For each candidate relation, the number of times the incrementalalgorithm runs is equal to the number of TL objects (the number of objects whichdon?t fulfill the description of the target created by the current run of the incrementalalgorithm, and which the target object stands under the currently selected relation to).Call the number of TL objects nTL and note that nTL must be less than, and proportionalto, n (the total number of objects).
The number of times the basic incremental algorithmcan run, in our system, is then proportional to NTL ?
R; replacing with nTL with n givesn?
R runs of the basic incremental algorithm.
Inserting the complexity of the basicincremental algorithm into this, we get an overall complexity of n2 ?
n?
R = n3 ?
R,which although worse than the basic incremental algorithm?s n2 complexity, is stillpolynomial.This algorithm cannot generate embedded locative descriptions, such as the bag onthe chair near the window, because it does not use spatial relations as properties todescribe the landmark.
However, these descriptions can be generated if needed byreplacing the call to the basic incremental algorithm for the landmark object with acall to the whole locative expression algorithm, using the target landmark as the targetobject and the set of distractor landmarks as the distractors.
A nice consequence ofthis approach to generating embedded locative descriptions is that infinite descriptions(e.g., the bag on the chair supporting the bag on the chair ...) will not be generated as thetarget object is excluded from the context that the landmark?s description is generatedin.
However, the cost of being able to generate these embedded descriptions is a higherexponential complexity.8.2.2 An Example.
We can illustrate the framework using the visual context providedby the scene on the left of Figure 29.
This context consists of two red boxes R1 andFigure 29A visual scene and the topological analysis of R1 and R2.302Kelleher and Costello Computational Models of Spatial Prepositions for VSDR2 and two blue balls B1 and B2.
Imagine that we want to refer to B1.
We begin bycalling the locative incremental algorithm, Algorithm 2.
This in turn calls the basicincremental algorithm, Algorithm 1, which will return the property ball.
However, thisis not sufficient to create a distinguishing description as B2 is also a ball.
In this contextthe set of candidate landmarks equals {R1,R2} and the first relation in the hierarchy istopological proximity, which we model as described in Section 6.1.
The image on theright of Figure 29 illustrates the analysis of the scene using this framework: The greenregion on the left defines the area deemed to be proximal to R1, and the yellow regionon the right defines the area deemed to be proximal to R2.
It is evident that B1 is inthe area proximal to R1; consequently R1 is classified as a target landmark.
As none ofthe distractors (i.e., B2) are located in a region that is proximal to a candidate landmarkthere are no distractor landmarks.
As a result when the basic incremental algorithm iscalled to create a distinguishing description for the target landmark R1 it will returnbox and this will be deemed to be a distinguishing locative description.
The overallalgorithm will then return the vector {ball, proximal, box} which would result in therealizer generating a reference of the form: the ball near the box.9.
Conclusions and Future WorkIn this article we have described the application of computational models of spatialprepositions to visually situated dialog systems.
These computational models allow sys-tems to both interpret and generate expressionswhich refer to topological and projectiverelations between objects in the visual environment.
The computational models of spa-tial prepositions we present are designed to handle reference resolution and generationin complex visual environments containing multiple objects.
In particular, these modelsare designed to account for the contextual influence which the presence of multipleobjects has on the semantics of topological and projective prepositions.
In this respectour computational models move beyond other accounts of the semantics of spatialprepositions, which typically do not model the contextual influence of other objectson spatial semantics.
Because most real-world visual scenes are complex and containmultiple objects, our computational models for the semantics of spatial prepositions areimportant for visually situated dialog systems intended to operate successfully in thereal world.Clearly there are many interesting areas for future work.
To date our research hasfocused on a small number of static topological and projective prepositions.
We feel,however, that our framework will apply usefully to a range of other more complex staticand dynamic prepositions, for example: between, among, within, along, beside, around.These prepositions either involve several objects or multiple areas and, consequently,our account of the effect of distractor objects on the target?landmark relationship couldprovide a worthwhile perspective on their semantics.This leads to another promising area for future work.
Although our current modelwas designed to accommodate multiple distractor objects, our empirical studies havefocused on cases where there is only one distractor.
An important aim for future researchis to extend these studies and test the model in situations with multiple distractors.From a theoretical point of view, we feel that our approach to the semantics of spa-tial prepositions illustrates an important point for researchers working on the semanticsof natural language in general: that it is possible to investigate and model semantics notsolely as a linguistic phenomenon, but also in terms of non-linguistic factors such asthe visual environment in which language is used.
For example, in the psychologicalevaluations described in Section 7 we found that the semantic applicability of ?near?
to303Computational Linguistics Volume 35, Number 2the relationship between a target and a landmark object was reliably influenced by thepresence and location of a third, distractor, object, which was not part of the linguisticcontext.
That the semantics of language is influenced by non-linguistic factors is an oldpoint and an obvious one: however, we think that our research on visually-situateddialog systems makes a useful contribution by showing that these systems provideideal testbeds for investigating the interaction between language and vision, and fordeveloping detailed and useful computational models of how those interactions work.ReferencesBaldridge, J. and G. J. M. Kruijff.
2002.Coupling CCG and hybrid logicdependency semantics.
In Proceedingsof the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL-02),pages 319?326, Philadelphia, PA.Baldridge, J. and G. J. M. Kruijff.
2003.Multi-modal combinatory categorialgrammar.
In Proceedings of the 10thConference of the European Chapter of theAssociation for Computational Linguistics(EACL-03), volume 1, pages 211?218,Budapest.Beermann, D. and L. Hellan.
2004.
Semanticdecomposition in a computational HPSGgrammar: A treatment of aspect andcontext-dependent directionals.
InProceedings of the HPSG04 Conference,pages 357?377, Leuven.Bunt, H. 1994.
Context and dialogue control.Think, 3:19?31.Cahill, A., M. Burke, R. O?Donovan, J. vanGenabith, and A.
Way.
2004.
Long-distancedependency resolution in automaticallyacquired wide-coverage PCFG-basedLFG approximations.
In Proceedings of the42nd Annual Meeting of the Association forComputational Linguistics (ACL-04),pages 320?327, Barcelona.Carletta, J., A. Isard, S. Isard, J. C. Kowtko,G.
Doherty-Sneddon, and A. H. Anderson.1997.
The reliability of a dialogue structurecoding scheme.
Computational Linguistics,23(1):13?32.Carlson-Radvansky, L. A. and G. D. Logan.1997.
The influence of reference frameselection on spatial template construction.Journal of Memory and Language, 37:411?437.Cohn, A. G., B. Bennett, J. M. Gooday, andN.
Gotts.
1997.
RCC: A calculus for regionbased qualitative spatial reasoning.GeoInformatica, 1:275?316.Coventry, K. R. 1998.
Spatial prepositions,functional relations, and lexicalspecification.
In P. Olivier andK.
P. Gapp, editors, Representation andProcessing of Spatial Expressions.
LawrenceErlbaum Associates, Hillsdale, NJ,pages 247?262.Coventry, K. R. and S. Garrod.
2004.
Saying,Seeing and Acting.
The PsychologicalSemantics of Spatial Prepositions.
Essays inCognitive Psychology Series.
LawrenceErlbaum Associates, Hillsdale, NJ.Dale, R. and N. Haddock.
1991.
Generatingreferring expressions involving relations.In Proceedings of the 5th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL-91),pages 161?166, Berlin.Dale, R. and E. Reiter.
1995.
Computatinalinterpretations of the gricean maxims inthe generation of referring expressions.Cognitive Science, 18:233?263.Fuhr, T., G. Socher, C. Scheering, andG.
Sagerer.
1998.
A three-dimensionalspatial model for the interpretationof image data.
In P. Olivier andK.
P. Gapp, editors, Representation andProcessing of Spatial Expressions.
LawrenceErlbaum Associates, Hillsdale, NJ,pages 103?118.Gapp, K. P. 1994.
Basic meanings ofspatial relations: Computation andevaluation in 3D space.
In Proceedingsof the 12th National Conference on ArtificialIntelligence (AAAI-94), Seattle, WA,pages 1393?1398.Gapp, K. P. 1995.
An empirically validatedmodel for computing spatial relations.
InProceedings of the 19th German Conference onArtificial Intelligence (KI-95), Bielefeld,Germany, pages 245?256.Gardent, C. 2002.
Generating minimaldefinite descriptions.
In Proceedings ofthe 40th Annual Meeting of the Associationof Computational Linguistics (ACL-02),pages 96?103, Philadelphia, PA.Garrod, S., G. Ferrier, and S. Campbell.
1999.In and on: Investigating the functionalgeometry of spatial prepositions.Cognition, 72:167?189.Gawron, J. M. 1986.
Situations andprepositions.
Linguistics and Philosophy,9(3):327?382.Gorniak, P. and D. Roy.
2004.
Groundedsemantic composition for visual scenes.Journal of Artificial Intelligence Research,21:429?470.304Kelleher and Costello Computational Models of Spatial Prepositions for VSDHajicova?, E. 1993.
Issues of Sentence Structureand Discourse Patterns, volume 2 ofTheoretical and Computational Linguistics.Charles University Press, Prague.Hayward, W. G. and M. J. Tarr.
1995.
Spatiallanguage and spatial representation.Cognition, 55:39?84.Herskovits, A.
1986.
Language and SpatialCognition: An Interdisciplinary Study ofPrepositions in English.
Studies in NaturalLanguage Processing.
CambridgeUniversity Press, Cambridge, UK.Horacek, H. 1997.
An algorithm forgenerating referential descriptions withflexible interfaces.
In Proceedings of the35th Annual Meeting of the Association forComputational Linguistics, pages 206?213,Madrid.Jackendoff, R. 1983.
Semantics and Cognition.Current Studies in Linguistics.
The MITPress, Cambridge, MA.Kelleher, J., F. Costello, and J. van Genabith.2005.
Dynamically structuring, updatingand interrelating representations of visualand lingusitic discourse context.
ArtificialIntelligence, 167(1?2):62?102.Kelleher, J. and J. van Genabith.
2004.
Visualsalience and reference resolution insimulated 3D environments.
AI Review,21(3-4):253?267.Kelleher, J. and J. van Genabith.
2006.
Acomputational model of the referentialsemantics of projective prepositions.In P. Saint-Dizier, editor, Syntax andSemantics of Prepositions, Speech andLanguage Processing.
Kluwer AcademicPublishers, Dordrecht, The Netherlands,pages 199?216.Kelleher, J. D. and G. J. Kruijff.
2006.Incremental generation of spatialreferring expressions in situated dialog.In Proceedings of the 3rd Joint Conferenceof the International Committee onComputational Linguistics and theAssociation for Computational Linguistics(COLING/ACL-06), pages 1041?1048,Sydney.Kelleher, J. D., G. J. Kruijff, and F. Costello.2006.
Proximity in context: an empiricallygrounded computation model ofproximity for processing topologicalspatial expressions.
In Proceedings of the3rd Joint Conference of the InternationalCommittee on Computational Linguisticsand the Association for ComputationalLinguistics (COLING/ACL-06),pages 745?752, Sydney.Klein, M. 1999.
An overview of the state ofthe art of coding schemes for dialogue actannotation.
In V. Matousek, P. Mautner,J.
Ocelikova?, and P. Sojka, editors, Text,Speech and Dialogue (TSD?99), LectureNotes in Computer Science.
Springer,Berlin/Heidelberg, pages 274?297.Krahmer, E. and M. Theune.
2002.
Efficientcontext-sensitive generation of referringexpressions.
In K. van Deemter andR.
Kibble, editors, Information Sharing:Reference and Presupposition in LanguageGeneration and Interpretation.
CLSIPublications, Stanford, CA, pages 223?263.Kruijff, Geert-Jan, John Kelleher, and NickHawes.
2006.
Information fusion forvisual reference resolution in dynamicsituated dialogue.
In Elisabeth Andre,Laila Dybkjaer, Wolfgang Minker,Heiko Neumann, and Michael Weber,editors, In Proceedings of Perception andInteractive Technologies (PIT06),volume 4021 of Lecture Notes in ComputerScience.
Springer Berlin/Heidelberg,pages 117?128.Kuipers, Benjamin.
2000.
The spatialsemantic hierarchy.
Artificial Intelligence,19:191?233.Landau, B.
1996.
Multiple geometricrepresentations of objects in languageand language learners.
In P. Bloom,M.
Peterson, L. Nadel, and M. Garrett,editors, Language and Space.
MIT Press,Cambridge, MA, pages 317?363.Levelt, W. J. M. 1996.
Perspective taking andellipsis in spatial descriptions.
InM.
Bloom, P. Peterson, L. Nadell, andM.
Garrett, editors, Language and Space.MIT Press, Cambridge, MA, pages 77?108.Levinson, S. 1996.
Frame of reference andMolyneux?s question: Crosslinguisticevidence.
In M. Bloom, P. Peterson,L.
Nadell, and M. Garrett, editors,Language and Space.
MIT Press,Cambridge, MA, pages 109?170.Levinson, S. 2003.
Space in Language andCognition: Explorations in CognitiveDiversity.
Cambridge University Press,Cambridge, UK.Logan, G. D. 1994.
Spatial attention and theapprehension of spatial relations.
Journalof Experimental Psychology: HumanPerception and Performance, 20:1015?1036.Logan, G. D. 1995.
Linguistic and conceptualcontrol of visual spatial attention.Cognitive Psychology, 12:523?533.Logan, G. D. and D. D. Sadler.
1996.A computational analysis of theapprehension of spatial relations.In M. Bloom, P. Peterson, L. Nadell,and M. Garrett, editors, Language and305Computational Linguistics Volume 35, Number 2Space.
MIT Press, Cambridge, MA,pages 493?529.Lorch, R. F. and J. L. Myers.
1990.Regression analyses of repeatedmeasures data in cognitive research.Journal of Experimental Psychology:Learning, Memory, and Cognition,16(1):149?157.Olivier, P. and J. Tsujii.
1994.
Quantitativeperceptual representation of prepositionalsemantics.
Artificial Intelligence Review,8:147?158.Regier, T and L. Carlson.
2001.
Groundingspatial language in perception:An empirical and computationalinvestigation.
Journal of ExperimentalPsychology: General, 130(2):273?298.Treisman, A. and S. Gormican.
1988.
Featureanalysis in early vision: Evidence fromsearch assymetries.
Psychological Review,95:15?48.Tseng, J. L. 2000.
The Representation andSelection of Prepositions.
Ph.D. thesis,University of Edinburgh.Varges, S. 2004.
Overgenerating referringexpressions involving relations andbooleans.
In Proceedings of the 3rdInternational Conference on NaturalLanguage Generation (INLG-04),pages 171?181, Brighton.Yamada, A.
1993.
Studies in SpatialDescriptions Understanding Based onGeometric Constraints Satisfaction.Ph.D.
thesis, University of Kyoto.306This article has been cited by:1.
Timothy Baldwin, Valia Kordoni, Aline Villavicencio.
2009.
Prepositions in Applications:A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey andIntroduction to the Special Issue.
Computational Linguistics 35:2, 119-149.
[Citation] [PDF][PDF Plus]
