R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
1 ?
9, 2005.?
Springer-Verlag Berlin Heidelberg 2005A New Method for Sentiment Classificationin Text RetrievalYi Hu1, Jianyong Duan1, Xiaoming Chen1,2, Bingzhen Pei1,2, and Ruzhan Lu11 Department of Computer Science and Engineering,Shanghai Jiao Tong University, Shanghai, China, 2000302 School of Computer Science and Engineering,Guizhou University, Guiyang, China, 550025{huyi, duan_jy, chen-xm, peibz, rz-lu}@cs.sjtu.edu.cnAbstract.
Traditional text categorization is usually a topic-based task, but asubtle demand on information retrieval is to distinguish between positive andnegative view on text topic.
In this paper, a new method is explored to solvethis problem.
Firstly, a batch of Concerned Concepts in the researched domainis predefined.
Secondly, the special knowledge representing the positive ornegative context of these concepts within sentences is built up.
At last, anevaluating function based on the knowledge is defined for sentiment classifica-tion of free text.
We introduce some linguistic knowledge in these procedures tomake our method effective.
As a result, the new method proves better comparedwith SVM when experimenting on Chinese texts about a certain topic.1   IntroductionClassical technology in text categorization pays much attention to determiningwhether a text is related to a given topic [1], such as sports and finance.
However, asresearch goes on, a subtle problem focuses on how to classify the semantic orientationof the text.
For instance, texts can be for or against ?racism?, and not all the texts arebad.
There exist two possible semantic orientations: positive and negative (the neutralview is not considered in this paper).
Labeling texts by their semantic orientationwould provide readers succinct summaries and be great useful in intelligent retrievalof information system.Traditional text categorization algorithms, including Na?ve Bayes, ANN, SVM, etc,depend on a feature vector representing a text.
They usually utilize words or n-gramsas features and construct the weightiness according to their presence/absence or fre-quencies.
It is a convenient way to formalize the text for calculation.
On the otherhand, employing one vector may be unsuitable for sentiment classification.
See thefollowing simple sentence in English:?
Seen from the history, the great segregation is a pioneering work.Here, ?segregation?
is very helpful to determine that the text is about the topic ofracism, but the terms ?great?
and ?pioneering work?
may just be the important hintsfor semantic orientation (support the racism).
These two terms probably contribute2 Y. Hu et alless to sentiment classification if they are dispersed into the text vector because therelations between them and ?segregation?
are lost.
Intuitively, these terms can providemore contribution if they are considered as a whole within the sentence.
We explore anew idea for sentiment classification by focusing on sentences rather than entire text.?Segregation?
is called as Concerned Concept in our work.
These ConcernedConcepts are always the sensitive nouns or noun phrases in the researched domainsuch as ?race riot?, ?color line?
and ?government?.
If the sentiment classifyingknowledge about how to comment on these concepts can be acquired, it will behelpful for sentiment classification when meeting these concepts in free texts again.In other words, the task of sentiment classification of entire text has changed intorecognizing the semantic orientation of the context of all Concerned Concepts.We attempt to build up this kind of knowledge to describe different sentimentcontext by integrating extended part of speech (EPOS), modified triggered bi-gramsand position information within sentences.
At last, we experiment on Chinese textsabout ?racism?
and draw some conclusions.2    Previous WorkA lot of past work has been done about text categorization besides topic-based clas-sification.
Biber [2] concentrated on sorting texts in terms of their source or sourcestyle with stylistic variation such as author, publisher, and native-languagebackground.Some other related work focused on classifying the semantic orientation of indi-vidual words or phrases by employing linguistic heuristics [3][4].
Hatzivassiloglouet alworked on predicting the semantic orientation of adjectives rather than phrasescontaining adjectives and they noted that there are linguistic constraints on theseorientations of adjectives in conjunctions.Past work on sentiment-based categorization of entire texts often involved usingcognitive linguistics [5][11] or manually constructing discriminated lexicons[7][12].
All these work enlightened us on the research on Concerned Concepts ingiven domain.Turney?s work [9] applied an unsupervised learning algorithm based on the mu-tual information between phrases and the both words ?excellent?
and ?poor?.
Themutual information was computed using statistics gathered by a search engine andsimple to be dealt with, which encourage further work with sentiment classification.Pang et al[10] utilized several prior-knowledge-free supervised machine learningmethods in the sentiment classification task in the domain of movie review, andthey also analyzed the problem to understand better how difficult it is.
They ex-perimented with three standard algorithms: Na?ve Bayes, Maximum Entropy andSupport Vector Machines, then compared the results.
Their work showed that, gen-erally, these algorithms were not able to achieve accuracies on the sentimentclassification problem comparable to those reported for standard topic-basedcategorization.A New Method for Sentiment Classification in Text Retrieval 33   Our Work3.1   Basic IdeaAs mentioned above, terms in a text vector are usually separated from the ConcernedConcepts (CC for short), which means no relations between these terms and CCs.
Toavoid the coarse granularity of text vector to sentiment classification, the context ofeach CC is researched on.
We attempt to determine the semantic orientation of a freetext by evaluating context of CCs contained in sentences.
Our work is based on thetwo following hypothesizes:?
H1.
A sentence holds its own sentiment context and it is the processingunit for sentiment classification.?
H2.
A sentence with obvious semantic orientation contains at least oneConcerned Concept.H1 allows us to research the classification task within sentences and H2 means that asentence with the value of being learnt or evaluated should contain at least one de-scribed CC.
A sentence can be formed as:( 1) 1 1 ( 1)... ...m m i n nword word word CC word word word?
?
?
?
?.
(1)CCi (given as an example in this paper) is a noun or noun phrase occupying the po-sition 0 in sentence that is automatically tagged with extended part of speech (EPOSfor short)(see section 3.2).
A word and its tagged EPOS combine to make a 2-tuple,and all these 2-tuples on both sides of CCi can form a sequence as follows:????????????????????????????????????????????????????nnnnimmmmeposwordeposwordeposwordCCeposwordeposwordeposword)1()1(1111)1()1(.
(2)All the words and corresponding EPOSes are divided into two parts: m 2-tuples onthe left side of CCi (from ?m to -1) and n 2-tuples on the right (from 1 to n).
These 2-tuples construct the context of the Concerned Concept CCi.The sentiment classifying knowledge (see sections 3.3 and 3.4) is the contributionof all the 2-tuples to sentiment classification.
That is to say, if a 2-tuple often co-occurs with CCi in training corpus with positive view, it contributes more to positiveorientation than negative one.
On the other hand, if the 2-tuple often co-occurs withCCi in training corpus with negative view, it contributes more to negative orientation.This kind of knowledge can be acquired by statistic technology from corpus.When judging a free text, the context of CCi met in a sentence is respectively com-pared with the positive and negative sentiment classifying knowledge of the same CCitrained from corpus.
Thus, an evaluating function E (see section 3.5) is defined toevaluate the semantic orientation of the free text.3.2   Extended Part of SpeechUsual part of speech (POS) carries less sentiment information, so it cannot distinguishthe semantic orientation between positive and negative.
For example, ?hearty?
and?felonious?
are both tagged as ?adjective?, but for the sentiment classification, only4 Y. Hu et althe tag ?adjective?
cannot classify their sentiment.
This means different adjective hasdifferent effect on sentiment classification.
So we try to extend words?
POS (EPOS)according to its semantic orientation.Generally speaking, empty words only have structural function without sentimentmeaning.
Therefore, we just consider substantives in context, which mainly includenouns/noun phrases, verbs, adjectives and adverbs.
We give a subtler manner to de-fine EPOS of substantives.
Their EPOSes are classified to be positive orientation(PosO) or negative orientation (NegO).
Thus, ?hearty?
is labeled with ?pos-adj?,which means PosO of adjective; ?felonious?
is labeled with ?neg-adje?, which meansNegO of adjective.
Similarly, nouns, verbs and adverbs tagged with their EPOS con-struct a new word list.
In our work, 12,743  Chinese entries in machine readable dic-tionary are extended by the following principles:?
To nouns, their PosO or NegO is labeled according to their semantic ori-entation to the entities or events they denote (pos-n or neg-n).?
To adjectives, their common syntax structure is {Adj.+Noun*}.
If adjec-tives are favor of or oppose to their headwords (Noun*), they will be de-fined as PosO or NegO (pos-adj or neg-adj).?
To adverbs, their common syntax structure is {Adv.+Verb*/Adj*.
}, andVerb*/Adj*.
is headword.
Their PosO or NegO are analyzed in the sameway of adjective (pos-adv or neg-adv).?
To transitive verb, their common syntax structure is {TVerb+Object*},and Object* is headword.
Their PosO or NegO are analyzed in the sameway of adjective (pos-tv or neg-tv).?
To intransitive verb, their common syntax structure is {Sub-ject*+InTVerb}, and Subject* is headword.
Their PosO or NegO are ana-lyzed in the same way of adjective (pos-iv or neg-iv).3.3   Sentiment Classifying Knowledge FrameworkSentiment classifying knowledge is defined as the importance of all 2-tuples <word,epos> that compose the context of CCi (given as an example) to sentiment classifica-tion and every Concerned Concept like CCi has its own positive and negative senti-ment classifying knowledge thatcan be formalized as a 3-tuple K:: ( , , )pos negK CC S S=  .
(3)To CCi, its Sipos has concrete form that is described as a set of 5-tuples:{ }: ( , , , , , )pos left rightiS word epos wordval eposval?
?
?
?
?
??
?= < >  .
(4)Where Sipos represents the positive sentiment classifying knowledge of CCi, and it is adata set about all 2-tuples <word, epos> appearing in the sentences containing CCi intraining texts with positive view.
In contrast, Sineg is acquired from the training textswith negative view.
In other words, Sipos and Sineg respectively reserve the features forpositive and negative classification to CCi in corpus.In terms of Sipos, the importance of ,word epos?
?< > is divided into wordval?
andeposval?
(see section 4.1) which is estimated by modified triggered bi-grams to fit theA New Method for Sentiment Classification in Text Retrieval 5long distance dependence.
If ,word epos?
?< > appears on the left side of CCi, the?side?
adjusting factor is lefti?
; if it appears on the right, the ?side?
adjusting factor isrighti?
.
We also define another factor ?
(see section 4.3) that denotes dynamic ?posi-tional?
adjusting information during processing a sentence in free text.3.4   Contribution of <word, epos>If a <word, epos> often co-occurs with CCi in sentences in training corpus with posi-tive view, which may means it contribute more to positive orientation than negativeone, and if it often co-occurs with CCi in negative corpus, it may contribute more tonegative orientation.We modify the classical bi-grams language model to introduce long distance trig-gered mechanism of ,iCC word epos?< > .
Generally to describe, the contribution c ofeach 2-tuple in a positive or negative context (denoted by Pos_Neg) is calculated by(5).
This is an analyzing measure of using multi-feature resources.
( )( , | , _ ) : exp Pr( , | , _ ) , 0i ic word epos CC Pos Neg word epos CC Pos Neg??
?
?< > = < > >  .
(5)The value represents the contribution of <word, epos> to sentiment classification inthe sentence containing CCi.
Obviously, when ?
and ?
are fixed, the biggerPr(<word, epos>|CCi, Pos_Neg>) is, the bigger contribution c of the 2-tuple <word,epos> to the semantic orientation Pos_Neg (one of {positive, negative} view) is.It has been mentioned that ?
and ?
are adjusting factor to the sentiment contribu-tion of pair <word, epos>.
?
rectifies the effect of the 2-tuple according to its ap-pearance on which side of CCi, and ?
rectifies the effect of the 2-tuple according toits distance from CCi.
They embody the effect of ?side?
and ?position?.
Thus, it canbe inferred that even the same <word, epos> will contribute differently because of itsside and position.3.5   Evaluation Function EWe propose a function E (equation (6)) to evaluate a free text by comparing the con-text of every appearing CC with the two sorts of sentiment context of the same CCtrained from corpus respectively.
( )' '1(1/ ) ( , ) ( , )Npos negi i i iiE N Sim S S Sim S S== ??
.
(6)N is the number of total Concerned Concepts in the free text, and i denotes certainCCi.
E is the semantic orientation of the whole text.
Obviously, if 0?E , the text is tobe regarded as positive, otherwise, negative.To clearly explain the function E, we just give the similarity between the contextof CCi (Si?)
in free text and the positive sentiment context of the same CCi trainedfrom corpus.
The function Sim is defined as follows:6 Y. Hu et al'1111( , ) exp Pr( , | , )exp Pr( , | , )m mpos left lefti i in nright rightiSim S S word epos CC positiveword epos CC positive?
?
?
????
?
?
????
??
??
?=?=?==?
?
?
?= < >?
?
?
??
??
??
?
?
?+ < >?
?
?
??
??
?????.
(7)11exp Pr( , | , )m mleft leftiword epos CC positive?
?
?
????
??
?=?=??
?
?
?< >?
?
?
??
??
?
?
?is the positive orientation of the leftcontext of CCi, and11exp Pr( , | , )n nright rightiword epos CC positive?
?
?
????
?==?
?
?
?< >?
?
?
??
??
?
?
?is the right one.Equation (7) means that the sentiment contribution c of each <word, epos> calculatedby (5) in the context of CCi within a sentence in free text, which is Si?, construct theoverall semantic orientation of the sentence together.
On the other hand, '( , )negi iSim S Scan be thought about in the same way.4   Parameter Estimation4.1  Estimating Wordval and EposvalIn terms of CCi, its sentiment classifying knowledge is depicted by (3) and (4), andthe parameters wordval and eposval need to be leant from corpus.
Every calculationof Pr(<word, epos>|CCi, Pos_Neg) is divided into two parts like (8) according tostatistic theory:Pr( , | , _ ) Pr( | , _ ) Pr( | , _ , )i i iword epos CC Pos Neg epos CC Pos Neg word CC Pos Neg epos?
?
?
?
?< > = ?
.
(8)eposval := Pr( | , _ )iepos CC Pos Neg?
and wordval := Pr( | , _ , )iword CC Pos Neg epos?
?
.The ?eposval?
is the probability of epos?
appearing on both sides of the CCi and isestimated by Maximum Likelihood Estimation (MLE).
Thus,#( , ) 1Pr( | , _ )#( , )iiieposepos CCepos CC Pos Negepos CC EPOS??
+=+?.
(9)The numerator in (9) is the co-occurring frequency between epos?
and CCi withinsentence in training texts with Pos_Neg (certain one of {positive, negative}) view andthe denominator is the frequency of co-occurrence between all EPOSes appearing inCCi ?s context with Pos_Neg view.The ?wordval?is the conditional probability of ?word  given CCi and epos?
whichcan also be estimated by MLE:#( , , ) 1Pr( , _ , )#( , , ) 1iiiword wordword epos CCword CC Pos Neg eposword epos CC?
??
??+=+?
?
.
(10)A New Method for Sentiment Classification in Text Retrieval 7The numerator in (10) is the frequency of co-occurrence between < ?word , epos?
>and CCi , and the denominator is the frequency of co-occurrence between all possiblewords  corresponding to epos?
appearing in CCi ?s context with Pos_Neg view.For smoothing, we adopt add?one method in (9) and (10).4.2   Estimating ?The ??
is the adjusting factor representing the different effect of the ,word epos?
?< >to CCi in texts with Pos_Neg view according to the side it appears, which means dif-ferent side has different contribution.So, it includes left??
and right??
:ii# of ,  appearing on the left side of CC# of ,  appearing on both sides of CCleft word eposword epos?
???
?
?< >=< >,          (11)ii# of ,  appearing on the right side of CC# of ,  appearing on both sides of CCright word eposword epos?
???
?
?< >=< >.
(12)4.3   Calculating ??
is positional adjusting factor, which means different position to some CC will beassigned different weight.
This is based on the linguistic hypothesis that the further aword get away from a researched word, the looser their relation is.
That is to say, ?ought to satisfy an inverse proportion relationship with position.Unlike wordval, eposval and ?
which are all private knowledge to some CC, ?
isa dynamic positional factor which is independent of semantic orientation of trainingtexts and it is only depend on the position from CC.
To the example CCi, ?
of,word epos?
?< > occupying theth?
position on its left side is left??
, which can be de-fined as:| | 1 1 1(1 2) (2 (1 2) )left m???
?
?
?= ?
1 ~ m?
= ?
?
.
(13)?
of ,word epos?
?< > occupying  the th?
position on the right side of CCi is right??
,which can be defined as:1 1 1(1 2) (2 (1 2) )right n???
?
?
?= ?
1 ~ n?
= .
(14)5   Test and ConclusionsOur research topic is about ?Racism?
in Chinese texts.
The training corpus is built upfrom Chinese web pages and emails.
As mentioned above, all these extracted texts incorpus have obvious semantic orientations to racism: be favor of or oppose to.
There are1137 texts with positive view and 1085 texts with negative view.
All the Chinese textsare segmented and tagged with defined EPOS in advance.
They are also marked posi-8 Y. Hu et altive/negative for supervised learning.
The two sorts of texts with different view arerespectively divided into 10 folds.
9 of them are trained and the left one is used for test.For the special domain, there is no relative result that can be consulted.
So, we com-pare the new method with a traditional classification algorithm, i.e.
the popular SVMthat uses bi-grams as features.
Our experiment includes two parts: a part experiments onthe relatively ?long?
texts that contain more than 15 sentences and the other part ex-periments on the ?short?
texts that contain less than 15 sentences.
We choose ?15?
asthe threshold to distinguish long or short texts because it is the mathematic expectationof ?length?
variable of text in our testing corpus.
The recall, precision and F1-score arelisted in the following Experiment Result Table.Table.
Experiment ResultTexts with Positive View(more than 15 sentences)Texts with Negative View(more than 15 sentences)SVM Our Method SVM Our MethodRecall(%) 80.6 73.2 68.4 76.1Precision(%) 74.1 75.3 75.6 73.8F1-score(%) 77.2 74.2 71.82 74.9Texts with Positive View(less than 15 sentences)Texts with Negative View(less than 15 sentences)SVM Our Method SVM Our MethodRecall(%) 62.1 63.0 62.1 69.5Precision(%) 65.1 70.1 59.0 62.3F1-score(%) 63.6 66.4 60.5 65.7The experiment shows that our method is useful for sentiment classifica-tion?especially for short texts.
Seen from the table, when evaluating texts that havemore than 15 sentences, for enough features, SVM has better result, while ours is aver-agely close to it.
However, when evaluating the texts containing less than 15 sentences,our method is obviously superior to SVM in either positive or negative view.
Thatmeans our method has more potential value to sentiment classification of short texts,such as emails, short news, etc.The better result owes to the fine description within sentences and introducing lin-guistic knowledge to sentiment classification (such as EPOS, ?
and ?
), which provedthe two hypothesizes may be reasonable.
We use modified triggered bi-grams to de-scribe the importance among features ({<word, epos>}) and Concerned Concepts, thenconstruct sentiment classifying knowledge rather than depend on statistic algorithmonly.To sum up, we draw the following conclusions from our work:?
Introducing more linguistic knowledge is helpful for improving statisticsentiment classification.A New Method for Sentiment Classification in Text Retrieval 9?
Sentiment classification is a hard task, and it needs subtly describing capa-bility of language model.
Maybe the intensional logic of words will be help-ful in this field in future.?
Chinese is a language of concept combination and the usage of words ismore flexible than Indo-European language, which makes it more difficultto acquire statistic information than English [10].?
We assume an independent condition among sentences yet.
We should in-troduce a suitable mathematic model to group the close sentences.Our experiment also shows that the algorithm will become weak when no CC ap-pears in sentences, but this method is still deserved to explore further.
In future, wewill integrate more linguistic knowledge and expand our method to a suitable sen-tence group to improve its performance.
Constructing a larger sentiment area maybalance the capability of our method between long and short text sentimentclassification.Acknowledgement.
This work is supported by NSFC Major Research Program60496326: Basic Theory and Core Techniques of Non Canonical Knowledge and alsosupported by National 863 Project (No.
2001AA114210-11).References1.
Hearst, M.A.
: Direction-based text interpretation as an information access refinement.
InP.
Jacobs (Ed.
), Text-Based Intelligent Systems: Current Research and Practice in Infor-mation Extraction and Retrieval.
Mahwah, NJ: Lawrence Erlbaum Associates (1992)2.
Douglas Biber: Variation across Speech and Writing.
Cambridge University Press (1988)3.
Vasileios Hatzivassiloglou and Kathleen McKeown: Predicting the semantic orientation ofadjectives.
In Proc.
of the 35th ACL/8th EACL (1997) 174-1814.
Peter D. Turney and Michael L. Littman: Unsupervised learning of semantic orientationfrom a hundred-billion-word corpus.
Technical Report EGB-1094, National ResearchCouncil Canada (2002)5.
Marti Hearst: Direction-based text interpretation as an information access refinement.
InPaul Jacobs, editor, Text-Based Intelligent Systems.
Lawrence Erlbaum Associates (1992)6.
Bo Pang and Lillian Lee: A Sentimental Education: Sentiment Analysis Using SubjectivitySummarization Based on Minimum Cuts.
Proceedings of the 42nd ACL (2004) 271--2787.
Sanjiv Das and Mike Chen: Yahoo!
for Amazon: Extracting market sentiment from stockmessage boards.
In Proc.
of the 8th Asia Pacific Finance Association Annual Conference(2001)8.
Vasileios Hatzivassiloglou, Janyce Wiebe: Effects of Adjective Orientation and Gradabil-ity on Sentence Subjectivity.
COLING (2000) 299-3059.
Peter Turney: Thumbs up or thumbs down?
Semantic orientation applied to unsupervisedclassication of reviews.
In Proc.
of the ACL (2002)10.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan: Thumbs up?
Sentiment Classifica-tion using Machine Learning Techniques.
In Proc.
Conf.
on EMNLP (2002)11.
Warren Sack: On the computation of point of view.
In Proc.
of the Twelfth AAAI, page1488.
Student abstract (1994)12.
Richard M. Tong: An operational system for detecting and tracking opinions in on-linediscussion.
Workshop note, SIGIR Workshop on Operational Text Classification (2001)
