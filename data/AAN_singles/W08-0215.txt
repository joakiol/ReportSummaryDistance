Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 120?128,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsPsychocomputational Linguistics:A Gateway to the Computational Linguistics CurriculumWilliam Gregory SakasDepartment of Computer Science, Hunter CollegePh.D.
Programs in Linguistics and Computer Science, The Graduate CenterCity University of New York (CUNY)695 Park Avenue, North 1008New York, NY, USA, 10065sakas@hunter.cuny.eduAbstractComputational modeling of human languageprocesses is a small but growing subfield ofcomputational linguistics.
This paperdescribes a course that makes use of recentresearch in psychocomputational modeling asa framework to introduce a number ofmainstream computational linguisticsconcepts to an audience of linguistics,cognitive science and computer sciencedoctoral students.
The emphasis on what Itake to be the largely interdisciplinary natureof computational linguistics is particularlygermane for the computer science students.Since 2002 the course has been taught threetimes under the auspices of the MA/PhDprogram in Linguistics at The City Universityof New York?s Graduate Center.
A briefdescription of some of the students?experiences after having taken the course isalso provided.1 IntroductionA relatively small (but growing) subfield ofcomputational linguistics, psychocomputationalmodeling affords a strong foundation from whichto introduce graduate students in linguistics tovarious computational techniques, and students incomputer science1 (CS) to a variety of topics in1 For rhetorical reasons I will often crudely partition thestudent makeup of the course into linguistics students and CSstudents.
This preempts lengthy illocutions such as ?
?
thestudents with a strong computational background as comparedto students with a strong linguistics background.?
In fact therehave been students from other academic disciplines inattendance bringing with them a range of technical facility inboth CS and linguistics; linguistics students with anpsycholinguistics, though it has rarely beenincorporated into the computational linguisticscurriculum.Psychocomputational modeling involves theconstruction of computer models that embody oneor more psycholinguistic theories of natural(human) language processing and use.
Over thepast decade or so, there's been renewed interestwithin the computational linguistics communityrelated to the possibility of incorporating humanlanguage strategies into computational languagetechnologies.
This is evidenced by the occassionalspecial session at computational linguisticsmeetings (e.g., ACL-1999 Thematic Session onComputational Psycholinguistics), severalworkshops (e.g., COLING-2004, ACL-2005Psychocomputational Models of Human LanguageAcquisition, ACL-2004 Incremental Parsing:Bringing Engineering and Cognition Together),recent conference themes (e.g., CoNLL-2008 "...models that explain natural phenomena relating tohuman language") and regular invitations topsycholinguists to deliver plenary addresses atrecent ACL and COLING meetings.Unfortunately, it is too often the case thatcomputational linguistics programs (primarilythose housed in computer science departments)delay the introduction of cross-disciplinarypsycholinguistic / computational linguisticsapproaches until either late in a student's course ofstudy (usually as an elective) or not at all.
At theCity University of New York (CUNY)?s GraduateCenter (the primary Ph.D.-granting school of theuniversity) I have created a course that presentsundergraduate degree in CS; and CS students with a strongundergraduate background in theoretical linguistics.120research in this cross-disciplinary area relativelyearly in the graduate curriculum.
I have also run anundergraduate version of the course at HunterCollege, CUNY in the interdisciplinary ThomasHunter Honors Program.I contend that a course developed within them?lange of psycholinguistics and computationallinguistics is not only a valuable asset in a student'srepertoire of graduate experience, but caneffectively be used as a springboard to introduce avariety of techniques and topics central to thebroader field of CL/NLP.2 BackgroundThe CUNY Graduate Center (hereafter, GC) hasdoctoral programs in both computer science andlinguistics.
The Linguistics Program also containstwo master's tracks.
Closely linked to bothprograms, but administratively independent ofeither, there exists a Cognitive ScienceConcentration.2 In spring of 2000, I was asked bythe Cognitive Science Coordinator to create aninterdisciplinary course in "computing andlanguage" that would be attractive to linguistics,speech and hearing, computer science, philosophy,mathematics, psychology and anthropologystudents.
One might imagine how a newly-mintedPh.D.
might react to this request.
Well this one, notyet realizing the potential for abuse of juniorfaculty (a slightly more sage me later wondered ifthere was a last minute sabbatical-relatedcancellation of some course that needed to bereplaced ASAP) dove in and developedComputational Mechanisms of Syntax Acquisition.The course was designed to cover generativeand non-generative linguistics and debatessurrounding (child) first language acquisitionprincipally focused on the question of Chomsky?sUniversal Grammar (UG), or not?
Fourcomputational models drawn from diverseparadigms ?
connectionist learning, statisticalformal language induction, principles-and-parameters acquisition and acquisition in anoptimality theory framework3 ?
were presented and2 This is not a state-registered program, but rather an "in-house" means of allowing students to receive recognition ofinterdisciplinary studiy in cognitive science.3 Although the semester ended as we were only just getting tocover optimality theoretic acquisition.discussion of the UG-or-not debate was framed inthe context of these models.Over the past eight years, I've taught threevariations of this course gradually molding thecourse away from a seminar format into aseminar/lecture format, dropping a large chunk ofthe UG-or-not debate, and targeting the courseprimarily for students who are in search of a "taste"of computational linguistics who might very wellgo on to take other CL-related course work.4 Whatfollows is a description of the designconsiderations, student makeup, and course contentfocusing on its last instantiation in Spring 2007.3 The Course: Computational NaturalLanguage LearningMost readers will recognize the most recent title ofthe course which was shamelessly borrowed fromthe ACL?s Special Interest Group on NaturalLanguage Learning?s annual meeting.
Althoughlargely an NLP-oriented meeting, the title andindeed many of the themes of the meeting?s CFPsover the years accurately portray the materialcovered in the course.The course is currently housed in the GC?sLinguistics Program and is primarily designed toserve linguistics doctoral and masters students whowant some exposure to computational linguisticsbut with a decidedly linguistics emphasis.Importantly though, the course often needs to servea high percentage of students from other graduateprograms.The GC Linguistics and Computer SciencePrograms also offer other computational linguistics(CL) courses: a Natural Language Processing(NLP) applications survey course, a corpusanalysis course, a statistical NLP course and a CLmethods sequence (in addition to a small variety ofelectives).
Although (at least until recently, seeSection 7) these courses are not taught within astructured CL curriculum, they effectively serve asthe ?meat-and-potatoes?
CL courses which requireprojects and assignments involving programming,a considerable math component and extensiveexperimentation with existing NLP/CL4 Though the details of the undergraduate version of the courseare beyond the scope of this paper, it is worth noting that it didnot undergo this gradual revision; it was structured much asthe original Cognitive Science version of the course, actuallywith an increased UG-or-not discussion.121applications.
The students taking these classeshave already reached the point where they intendto include a substantial amount of computationallinguistics in their dissertation or master?s thesis.Computational Natural Language Learning issomewhat removed from these other courses andthe design considerations were purposefullydirected at providing an ?appetizer?
that wouldboth entice interested students into taking othercourses, and prepare them with some experience incomputational linguistics techniques.
Over time thecourse has evolved to incorporate the following setof prerequisites and goals.?
No programming prerequisite, no introduction toprogramming Many of the students who take thecourse are first or second year linguistics studentswho have had little or no programmingbackground.
Students are aware that"Programming for Linguists" is part of the CLmethods sequence.
They come to this courselooking for either an overview of CL, or for howCL concepts might be relevant to psycholinguisticor theoretical linguistics research.Often there are students who have had asubstantial programming background ?
includinggraduate students in computer science.
This hasn?tproved to be problematic since the assignmentsand  projects are designed not to involveprogramming.?
Slight math prerequisite, exposure toprobabilities, and information theory Students areexpected to be comfortable with basic algebra.
Idissuade students from taking the course who areintimidated by a one-line formula with a few Greekletters in it.
Students are not expected to knowwhat a conditional probability is, but will leave thecourse with a decent grasp of basic(undergraduate-level) concepts in probability andinformation theory.This lightweight math prerequisite actually doessplit the class for a brief time during the semesteras the probability/information theory lecture andassignment is a cinch for the CS students, andtypically quite difficult for the linguistics students.But this is balanced by the implementation of thedesign consideration expressed in the next bullet.?
No linguistics prerequisite, exposure to syntactictheory Students need to know what a syntax tree is(at least in the formal language sense) but do notneed to know a particular theory of humanlanguage syntax (e.g., X-bar theory or even S ?NP VP).
By the end of the semester students willbe comfortable with elementary syntax beyond thelevel covered by most undergraduate ?Ling 101?courses.?
Preparation for follow-up CL courses Studentsleaving this course should be comfortably preparedto enter the other GC computational linguisticsofferings.5?
Appreciation of the interdisciplinary nature ofCL Not all students move on to othercomputational linguistics courses.
Perhaps themost important goal of the course is to expose CSand linguistics students (and others) to the role thatcomputational linguistics can play in areas oftheoretical linguistics and cognitive scienceresearch, and conversely to the role that cognitivescience and linguistics can play in the field ofcomputational linguistics.3.1 Topical unitsIn this section I present the syllabus of the courseframed in topical units.
They have varied over theyears; what follows is the content of the coursemostly as it was taught in Spring 2007.Janet Dean Fodor and I lead an activepsychocomputational modeling research group atthe City University of New York: CUNY CoLAG ?CUNY Computational Language AcquisitionGroup which is primarily dedicated to the designand evaluation of computational models of firstlanguage acquisition.
Most, though not all, of thetopical units purposefully contain material thatintersects with CUNY CoLAG?s ongoing researchefforts.The depth of coverage of the units is designed togive the students some fluency in computationalissues (e.g., use and ramifications of Markovassumptions), and a basic understanding beyondexposure to the computational mechanisms of CL(e.g., derivation of the standard MLE bigram5 The one exception in the Linguistics Program is CorpusLinguistics which has a programming prerequisite, and theoccasional CL elective course in Computer Science targetedprimarily for their more advanced students.122probability formula), but not designed to allowstudents to bypass a more computationally rigorousNLP survey course.
The same is true of the breadthof coverage; a comprehensive survey is not thegoal.
For example, in the ngram unit, typically nomore than two or at most three smoothingtechniques are covered.Note that the citations in this section are mostlyrequired reading, but some articles are optional.
Ithas been my experience however, that the studentsby and large read most of the material since thereadings were highly directed (i.e., which sectionsand pages are most relevant to the course.
)Supplemental materials that present introductorymathematics and tutorial presentations are notexhaustively listed, but included Jurafsky andMartin (2000), Goldsmith (2007, previouslyonline) and a host of  (other) online resources.History of linguistics and computation [1lecture] The history is framed around the question?Is computational linguistics, well uh, linguistics?
?We conclude with ?It was, then it wasn?t, nowmaybe it is, or at least in part, should be.?
Thematerial is tightly tied to Lee (2004); withadditional discussion along the lines of Sakas(2004).Syntax [1 lecture] This is a crash course in syntaxusing a context-free grammar withtransformational movement.
The more difficulttopics include topicalization (including null-topic),Wh-movement and verb-second phenomena.
Wemake effective use of an in-house database ofabstract though linguistically viable cross-linguistic sentence patterns and tree structures ?the CUNY CoLAG Domain (Sakas, 2003).
Thepoint of this lecture is to introduce non-linguisticsstudents to the intricacies of a linguistically viablegrammatical theory.Language Acquisition [1 lecture] We discusssome of the current debates in L1 (a child?s first)language acquisition surrounding ?no negativeevidence?
(Marcus, 1993), Poverty of the Stimulus(Pullum, 1996), and Chomsky?s conceptualizationof Universal Grammar.
This is the leastcomputational lecture of the semester, although itoften generates some of the most energizeddiscussion.
The language acquisition unit is thecentral arena in which we stage most of the rest ofthe topics in the course.Gold and the Subset Principle [2 lectures]During the presentation of Gold?s (1967) andAngluin's (1980) proofs and discussion of howthey might be used to argue (often incorrectly) fora Universal Grammar (Johnson, 2004) some coreCL topics are introduced including formallanguage classes (the Chomsky Hierarchy) and thenotions of hypothesis space and search space.
Thefirst (toy) probabilistic analyses are also presented(e.g., given a finite enumeration and a probability pthat an arbitrary non-target grammar licenses asentence in the input sample, what is the ?worstcase?
number of sentences required to converge onthe target grammar?
)Next, the Subset Principle and linguisticovergeneralization (Fodor and Sakas, 2005) isintroduced.
An important focus is on how keeping(statistical) track of what?s not encountered mightsupply a ?retreat?
mechanism to pull back from anover-general hypothesis.
Although themathematical details of how the statistics mightwork are omitted, this topic leads neatly into a uniton Bayesian learning later in the semester.This is an important two lectures.
It's the firstunit where students are exposed to the use ofcomputational techniques applied to theoreticalissues in psycholinguistics.
By this point, studentsoften are intellectually engaged in the debatessurrounding L1 acquisition.
To understand thearguments presented in this unit students need toflex their computational muscles for the first time.Connectionism [3 lectures] This unit covers thebasics of Simple Recurrent Network (SRN)learning (Elman, 1990, 1993).
More or less, Elmanargues that language acquisition is not necessarilythe acquisition of rules operating over atomiclinguistic units (e.g., phrase markers) but rather theprocess of capturing the ?dynamics?
of wordpatterns in the input stream.
He demonstrates howthis can be simulated in an SRN paradigm.The mechanics of how an SRN operates and canbe used to model language acquisition phenomenais covered but more importantly core conceptscommon to most all supervised machine learningparadigms are emphasized.
Topics include howtraining and testing corpora are developed andused, cross validation, hill-climbing, learning bias,123linear and non-linear separation of the hypothesisspace, etc.
A critique of SRN learning is alsocovered (Marcus, 1998) which presents theimportant distinction between generalizationperformance and learning within the trainingspace in a way that is approachable by non-CSstudents, but also interesting to CS-students.Information retrieval [1 lecture] Elman (1990)uses hierarchal clustering to analyze some of hisresults.
I use Elman's application of clustering totake a brief digression from the psycholinguisticstheme of the course and present an introduction tovector space models and document clustering.This is the most challenging technical lecture ofthe semester and is included only when there are arelatively high proportion of CS students inattendance.
Most of the linguistics students get adecent feel for the material, but most require asecond exposure it in another course to fullyunderstand the math.
That said, the linguisticsstudents do understand how weight heuristics areused to effectively represent documents in vectors(though most linguistics students have a hard timeswallowing the bag-of-words paradigm at facevalue), and how vectors can be nearer or fartherfrom each other in a hypothesis space.Ngram language models [3 lecture] In this unitwe return to psycholinguistics.
Reali andChristiansen, (2005) present a simple ngramlanguage model of child-directed speech to argueagainst the need for innate UG-providedknowledge of hierarchal syntactic structure.
Basicprobability and information theory is introduced ?conditional probabilities and Markov assumptions,the chain rule, Bayes Rule, maximum likelihoodestimates, entropy, etc.
Although relatively easyfor the CS students (they had their hands full withthe syntax unit), introduction of this material isinvaluable to the linguistics students who need tobe somewhat fluent in it before entering our otherCL offerings.We continue with a presentation of the sparse dataproblem, Zipf?s law, corpus cross-entropy and ahandful of smoothing techniques (Reali &Christiansen use a particularly impoverishedversion of deleted interpolation).
We continue witha discussion of the pros and cons of employingMarkov assumptions in computational linguisticsgenerally, the relationship of Markov assumptionsto incremental learning and psycholinguisticmodeling, and the use of cross-entropy as anevaluation metric, and end with a brief discussionof the descriptional necessity (or not) of traditionalgenerative grammars (Pereira, 2000).."Ideal" learning, Bayesian learning andcomputational resources [1 lecture] Regier andGahl (2004) in response to Lidz et al (2003)present a Bayesian learning model that learns thecorrect structural referent for anaphoric "one" inEnglish from a corpus of child-directed speech.Similarly to Reali & Christiansen (op.
cit.
), theyargue against the need for innate knowledge ofhierarchal structure since their Bayesian modelstarts tabula rasa and learns from linear wordstrings with no readily observable structure.The fundamental mechanics of Bayesianinference is presented.
Since most Bayesianmodels are able to retreat from overgeneralhypotheses in the absence of positive evidence, thecourse returns to overgeneralization errors, theSubset Principle and the alternative of usingstatistics as a possible retreat mechanism.Computationally heavy ("ideal") batch processing,and incremental (psychologically plausible)processing are contrasted here as is the use ofheuristics (psycholinguistically-based or not) tomitigate the potentially huge computational cost ofsearching a large domain.Principle and parameters [2 lectures] As theacademic scheduling has worked out, the course isusually offered during years when the LinguisticsProgram does not offer a linguistics-basedlearnability course.
As a result, there is a unit onacquisition within a principles-and-parameters(P&P) framework (Chomsky, 1981).
Roughly, inthe P&P framework cross-linguistic commonalitiesare considered principles, and language variation isstandardly specified by the settings of a bank ofbinary parameters (i.e., UG = principles +parameters; a specific language = principles +parameters + parameter settings).Although this unit is the furthest away frommainstream CL, it has served as a useful means tointroduce deterministic learning (Sakas and Fodor,2001), versus non-deterministic learning (Yang,2002), the effectiveness of hill-climbing in124linguistically smooth and non-smooth domains,6 aswell as the notion of computational complexity andcombinatorial explosion (n binary parametersyields a search space of 2n possible grammars).Finally, and perhaps most importantly there isextensive discussion of the difficulty of buildingcomputational systems that can efficiently andcorrectly learn to navigate through domains withan enormous amount of ambiguity.In the P&P framework ambiguity stems fromcompetition of cross-linguistic structural analysesof surface word order patterns.
For example, givena (tensed) S V O sentence pattern, is the V situatedunder the phrase maker I (English), or under thephrase marker C (German)?
Although this is asomewhat different form of ambiguity than thewithin-language structural ambiguity that is all toofamiliar to those of use working in CL, it serves asuseful background material for the next unit.Part of speech tagging and statistical parsing [3lectures] In this unit we begin by putting aside thepsycholinguistics umbrella of the course and coverintroductory CL in a more traditional manner.Using Charniak (1997) as the primary reading, wecover rudimentary HMM's, and probabilisticCFG's.
We use supplemental materials to introducelexicalized statistical parsing (e.g., Jurafsky andMartin, 2000 and online materials).
We then turnback to psycholinguistics and after a (somewhatcondensed overview) of human sentenceprocessing, discuss the viability of probabilisticparsing as a model of human sentence processing(Keller, 2005).
This unit, more than some others, islightweight on detailed computational mechanics;the material is presented throughout at a levelsimilar to that of Charniak?s article.
For examplethe specifics of EM algorithms are not coveredalthough what they do, and why they are necessaryare.The Linguistics Program at CUNY is very activein human sentence processing research and thisunit is of interest to many of the linguisticsstudents.
In particular we contrast computationalapproaches that employ nondeterminism andparallelism to mainstream psycholinguisticsmodels which are primarily deterministic, serialand employ a reanalysis strategy when evaluating a6 By "smooth", I mean a correlation between the similarity ofgrammars, and the similarity of languages they generate.parse ?online?
(though of course there is asignificant amount of research that falls outside ofthis mainstream).
We then focus on issues ofcomputational resources that each paradigmrequires.In some ways the last lectures of this unit bestembody the goal of exposing the students to thepotential of interdisciplinary research incomputational linguistics.
The CS students leavewith an appreciation of psycholinguisticapproaches to human sentence processing, and thelinguistics students with a firm grasp of theeffectiveness of computational approaches.4 Assignments and ProjectsAcross the three most recent incarnations of thecourse the number and difficulty of theassignments and projects has varied quite a bit.
Inthe last version, there were three assignments (fiveto ten hours of student effort each) and one project(twenty to thirty hours effort).Due to the typically small size of the course,assignments and projects (beyond weeklyreadings) were often individually tailored andassessed.
The goal of the assignments was toconcretize the CL aspects of the primarilypsycholinguistic readings with either hands-on useof the computer, mathematically-oriented problemsets, or a critical evaluation of the CLmethodologies employed.
A handful of examplesfollow.?
Gold and the Subset Principle (Assignments)All students are asked to formulate a Markov chain(though at this point in the course, not by thatname) of a Gold-style enumeration learneroperating over a small finite domain (e.g., 4grammars, 12 sentences and a sentence to grammarmapping).
The more mathematically inclined areadditionally asked to calculate the expected valueof the number of input sentences consumed by alearner operating over an enumeration of ngrammars and given a generalized mapping ofsentences to grammars, or to formally prove thelearnability of any finite domain of languagesgiven text (positive) presentation of input.?
Connectionism (Assignments) All studentswere asked to pick a language from the CUNYCoLAG domain, develop a training and test set125from that language using existing software and runa predict-the-next-word SRN simulation on either aMatLab or TLearn neural network platform.Linguistics and CS students were paired on thisassignment.
When the assignment is given, arelatively detailed after-class lab tutorial on how torun the software is presented.?
Ngram language models (Projects) One CSstudent implemented a version of Reali andChristiansen?s experiment and was asked toevaluate the effectiveness of different smoothingtechniques on child-directed speech and to design astudy of how to evaluate differences betweenchild-directed speech and adult-to-adult speech interms of language modeling.
A linguistics studentwas asked to write a paper explaining how onecould develop a computational evaluation of how abigram learner might be evaluated longitudinally.
(I.e., to answer the question, how can one measurethe effectiveness of a language model after eachinput sentence?).
Another linguistics student (withstrong CS skills) created an annotation tool thatsemi-automatically mapped child-directed speechin French onto the CoLAG Domain tag set.5 Students: Past and CurrentAs mentioned earlier, the Linguistics DoctoralProgram at CUNY has just recently begun tostructure their computational linguistics offeringsinto a cohesive course of study (described brieflyin Section 7).
During the past several yearsComputational Natural Learning has been offeredon an ad hoc basis primarily in response to studentdemand and demographics of students?computational skills.
Since the course was notoriginally intended to serve any specific functionas part of a larger curriculum, and was notintegrated into a reoccurring schedule there hasbeen little need to carry out a systematic evaluationof the impact of the course on students?
academiccareers.
Still a few anecdotal accounts will helpgive a picture of the course?s effectiveness.After the first Cognitive Science offering of thecourse in 2000, approximately 30 graduatestudents have taken one of the three subsequentincarnations.
Two of the earliest linguisticsstudents went on to take undergraduate CS coursesin programming and statistics, and subsequentlycame back to take graduate level CL courses.7They have obtained their doctorates and arecurrently working in industry as computationallinguists.
One is a lead software engineer for aninformation retrieval startup company in NewYork that does email data mining.
And though I?velost track of the other student, she was at one pointworking for a large software company on the westcoast.I am currently the advisor of one CS student,and two linguistics students who have taken thecourse.
One linguistics student is in the throws ofwriting a dissertation on the plausibility ofexploiting statistical regularities of varioussyntactic structures (contra regularities of wordstrings) in child-directed speech during L1acquisition.
The other is relatively early in heracademic career, but is interested in working oncomputational semantics and discourse analysiswithin a categorial grammar framework.
Herthoughts currently revolve around establishing(and formalizing) relationships between traditionallinguistics-oriented semantics and a computationalsemantics paradigm.
She hopes to makecontributions to both linguistics and CL.
The CSstudent, also early in his career, is interested insemi-productive multi-word expressions and howyoung children can come to acquire them.
His ideais to employ a learning component in a machinetranslation system that can be trained to translateproductive metaphors between a variety oflanguages.These five students have chosen to pursuespecific areas of study and research directly as aresult of having taken Computational NaturalLanguage Learning early in their careers.I am also sitting on two CS students?
secondqualifying exam committees.
One is working onmachine translation of Hebrew and the otherworking on (relatively) mainstream word-sensedisambiguation.
Both of their qualifying exampapers show a sensitivity to psycholinguistics thatI?m frankly quite happy to see, and am surewouldn?t have been present without their havingtaken the course.The parsing unit was just added this past springsemester and I?ve had two recent discussions with7 The CL methods sequence was established only 3 years ago,previously students were encouraged to develop their basiccomputational skills at one of CUNY?s undergraduate schools.126a second year linguistics student aboutincorporating a statistical component into a currentpsycholinguistic model of human sentenceprocessing.
Another second year student hasexpressed interested in doing a comprehensivestudy of neural network models of human sentenceparsing for his first qualifying paper.
It?s not clearthat they will ultimately pursue these directions,but I?m certain they wouldn?t have thought of thepossibilities if they hadn?t taken the ComputationalNatural Language Learning.Finally, most all of the students who have takenthe course have also taken the NLP-survey course(no programming required), slightly less than athird have moved on to the CL methods sequence(includes an introduction to programming), or ifthey have some CS background move directly toCorpus Analysis (programming experiencerequired as a prerequisite).
We hope thateventually, especially in light of the GC?s newcomputational linguistics program, the course willserve as the gateway for many more students tobegin to pursue studies that will lead to researchareas in both psychocomputational modeling andmore mainstream CL.6 Brief DiscussionIt is my view that computational linguistics is bynature a cross-disciplinary endeavor.
Indeed, onecould argue that only after the incorporation oftechniques and strategies gleaned from theoreticaladvances in psychocomputational modeling oflanguage, can we achieve truly transparent (to theuser) human-computer language applications.That argument notwithstanding, a course such asthe one described in this paper can effectivelyserve as an introduction to an assortment ofconcepts in computational linguistics that canbroaden the intellectual horizons of both CS andlinguistics students, as well providing a foundationthat students can build on in the pursuit of moreadvanced studies in the field.7 Postscript: The Future of the CourseThe CUNY Graduate Center has recently created astructured computational linguistics programhoused in the GC?s Linguistics Program.
Theprogram consists of a Computational LinguisticsConcentration in the Linguistics Master?ssubprogram, and particularly relevant to thediscussion in this article, a ComputationalLinguistics Certificate8 (both fall under theacronym CLC).
Any City University of New Yorkdoctoral student can enroll in CLC concurrentlywith enrollment in their primary doctoral program(as one might imagine, we expect a substantialnumber of Linguistics and CS doctoral candidatesto enroll in the CLC program).Due to my newly-acquired duties as director ofthe CLC program and to scheduling constraints onCLC faculty teaching assignments, the coursecannot be offered again until Fall 2009 or Spring2010.
At that time Computational NaturalLanguage Learning will need to morph into a moretechnically advanced elective course in appliedmachine learning techniques in computationallinguistics (or some such) since the CLC course ofstudy currently posits the NLP survey course andthe CL Methods sequence as the first yearintroductory requirements.However, I expect that a course similar to theone described here will supplement the NLPsurvey course as a first year requirement in Fall2010.
The course will be billed as having broadappeal and made available to both CLC studentsand linguistics, CS and other students who mightnot want or require the ?meat-and-potatoes?
thatCLC offers, but who only desire a CL ?appetizer?.Though if the appetizer is tasty enough, studentsmay well hunger for the main course.AcknowledgmentsI would like to thank the three anonymousreviewers for helpful comments, and the manyintellectually diverse and engaging students I?vehad the pleasure to introduce to the field ofcomputational linguistics.ReferencesAngluin, D. (1980).
Inductive inference of formallanguages from positive data.
Information andControl 45:117-135.Charniak, E. (1997).
Statistical Techniques for Naturallanguage Parsing.
AI Magazine 18:33-44.Chomsky, N. (1981).
Lectures on government andbinding: Studies in generative grammar.
Dordrecht:Foris.8 Pending state Department of Education approval, hopefullyto be received in Spring 2009.127Elman, J. L. (1990).
Finding structure in time.
CognitiveScience 14:179-211.Elman, J. L. (1993).
Learning and development inneural networks: The importance of starting small.Cognition 48:71-99.Fodor, J. D., and Sakas, W. G. (2005).
The SubsetPrinciple in syntax: Costs of compliance.
Journal ofLinguistics 41:513-569.Gold, E. M. (1967).
Language identification in the limit.Information and Control 10:447-474.Goldsmith, J.
(2007).
Probability for Linguists.Mathematics and Social Sciences 180:5-40.Johnson, K. (2004).
Gold?s Theorem and CognitiveScience.
Philosophy of Science 71:571-592.Jurafsky, D., and Martin, J. H. (2000).
Speech andLanguage Processing: An Introduction to NaturalLanguage Processing, Computational Linguistics,and Speech RecognitionKeller, F. (2005).
Probabilistic Models of HumanSentence Processing.
Presented at ProbabilisticModels of Cognition: The Mathematics of Mind,IPAM workshop, Los Angeles.Lee, L. (2004).
"I'm sorry Dave, I'm afraid I can't dothat" : Linguistics, statistics, and natural languageprocessing circa 2001.
In Computer Science:Reflections on the Field, 111-118.Washington:National Academies Press.Lidz, J., Waxman, S., and Freedman, J.
(2003).
Whatinfants know about syntax but couldn?t have learned:Experimental evidence for syntactic structure at 18months.
Cognition 89:65-73.Marcus, G. F. (1993).
Negative evidence in languageacquisition.
Cognition 46:53-85.Marcus, G. F. (1998).
Can connectionism saveconstructivism?
Cognition 66:153-182.Pereira, F. (2000) Formal grammar and informationtheory: Together again.
Philosophical Transactionsof the Royal Society A358:1239-1253.Pullum, G. K. (1996).
Learnability, hyperlearning, andthe poverty of the stimulus.
Proceedings of the 22ndAnnual Meeting of the Berkley Linguistics Society:General Session and Parasession on the Role ofLearnability in Grammatical Theory, Berkeley: 498-513.Reali, F., and Christiansen, M. H. (2005).
Uncoveringthe richness of the stimulus: Structural dependenceand indirect statistical evidence.
Cognitive Science29:1007-10018.Regier, T., and Gahl, S. (2004).
Learning theunlearnable: The role of missing evidence.
Cognition93:147-155.Sakas, W. G., and Fodor, J. D. (2001).
The StructuralTriggers Learner.
In Language Acquisition andLearnability, ed.
S. Bertolo, 172-233.
Cambridge:Cambridge University Press.Sakas, W. G. (2003) A Word-Order Database forTesting Computational Models of LanguageAcquisition, Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics,ACL-2003: 415-422.Sakas, W. G. (2004) Introduction.
Proceedings of theFirst Workshop on Psycho-computational Models ofHuman Language Acquisition, COLING-2004.Geneva: iv-vi.Yang, C. D. (2002).
Knowledge and learning in naturallanguage.
New York: Oxford University Press.128
