Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 70?75,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemEval-2010 Task 1: Coreference Resolution in Multiple LanguagesMarta Recasens, Toni Mart?, Mariona Taul?
Llu?s M?rquez, Emili SapenaCentre de Llenguatge i Computaci?
(CLiC) TALP Research Center,University of Barcelona Technical University of CataloniaGran Via de les Corts Catalanes 58508007 BarcelonaJordi Girona Salgado 1-308034 Barcelona{mrecasens, amarti, mtaule}@ub.edu{lluism, esapena}@lsi.upc.eduAbstractThis paper presents the task ?CoreferenceResolution in Multiple Languages?
to be runin SemEval-2010 (5th International Workshopon Semantic Evaluations).
This task aims toevaluate and compare automatic coreferenceresolution systems for three different lan-guages (Catalan, English, and Spanish) bymeans of two alternative evaluation metrics,thus providing an insight into (i) the portabil-ity of coreference resolution systems acrosslanguages, and (ii) the effect of different scor-ing metrics on ranking the output of the par-ticipant systems.1 IntroductionCoreference information has been shown to bebeneficial in many NLP applications such as In-formation Extraction (McCarthy and Lehnert,1995), Text Summarization (Steinberger et al,2007), Question Answering (Morton, 2000), andMachine Translation.
In these systems, there is aneed to identify the different pieces of informationthat refer to the same discourse entity in order toproduce coherent and fluent summaries, disam-biguate the references to an entity, and solve ana-phoric pronouns.Coreference is an inherently complex phenome-non.
Some of the limitations of the traditional rule-based approaches (Mitkov, 1998) could be over-come by machine learning techniques, which allowautomating the acquisition of knowledge from an-notated corpora.This task will promote the development of lin-guistic resources ?annotated corpora1?
and ma-chine-learning techniques oriented to coreferenceresolution.
In particular, we aim to evaluate andcompare coreference resolution systems in a multi-lingual context, including Catalan, English, andSpanish languages, and by means of two differentevaluation metrics.By setting up a multilingual scenario, we canexplore to what extent it is possible to implement ageneral system that is portable to the three lan-guages, how much language-specific tuning is nec-essary, and the significant differences betweenRomance languages and English, as well as thosebetween two closely related languages such asSpanish and Catalan.
Besides, we expect to gainsome useful insight into the development of multi-lingual NLP applications.As far as the evaluation is concerned, by em-ploying B-cubed (Bagga and Baldwin, 1998) andCEAF (Luo, 2005) algorithms we can considerboth the advantages and drawbacks of using one orthe other scoring metric.
For comparison purposes,the MUC score will also be reported.
Among oth-ers, we are interested in the following questions:Which evaluation metric provides a more accuratepicture of the accuracy of the system performance?Is there a strong correlation between them?
Can1 Corpora annotated with coreference are scarce, especially forlanguages other than English.70statistical systems be optimized under both metricsat the same time?The rest of the paper is organized as follows.Section 2 describes the overall task.
The corporaand the annotation scheme are presented in Section3.
Conclusions and final remarks are given in Sec-tion 4.2 Task descriptionThe SemEval-2010 task ?Coreference Resolutionin Multiple Languages?
is concerned with auto-matic coreference resolution for three differentlanguages: Catalan, English, and Spanish.2.1 Specific tasksGiven the complexity of the coreference phenom-ena, we will concentrate only in two tractable as-pects, which lead to the two following subtasks foreach of the languages:i) Detection of full coreference chains, com-posed by named entities, pronouns, and fullnoun phrases (NPs).ii) Pronominal resolution, i.e.
finding the antece-dents of the pronouns in the text.The example in Figure 1 illustrates the two sub-tasks.2 Given a text in which NPs are identified andindexed (including elliptical subjects, representedas ?
), the goal of (i) is to extract all coreferencechains: 1?5?6?30?36, 9?11, and 7?18; while thegoal of (ii) is to identify the antecedents of pro-nouns 5 and 6, which are 1 and 5 (or 1), respec-tively.
Note that (b) is a simpler subtask of (a) andthat for a given pronoun there can be multiple an-tecedents (e.g.
both 1 and 5 are correct antecedentsfor 6).We restrict the task to solving ?identity?
rela-tions between NPs (coreference chains), and be-tween pronouns and antecedents.
Nominalpredicates and appositions as well as NPs with anon-nominal antecedent (discourse deixis) will notbeen taken into consideration in the recognition ofcoreference chains (see Section 3.1 for more in-formation about decisions concerning the annota-tion scheme).Although we target at general systems address-ing the full multilingual task, we will allow takingpart on any subtask of any language in order topromote participation.Figure 1.
NPs in a sample from the Catalan trainingdata (left) and the English translation (right).2 The example in Figure 1 is a simplified version of the anno-tated format.
See Section 2.2 for more details.
[The beneficiaries of [[spouse?s]3 pensions]2]1 willbe able to keep [the payment]4 even if [they]5 re-marry provided that [they]6 fulfill [a series of [con-ditions]8]7, according to [the royal decree approvedyesterday by [the Council of Ministers]10]9.
[The new rule]11 affects [the recipients of [a[spouse?s]13 pension]12 [that]14 get married after[January_1_,_2002]16]17.
[The first of [the conditions]18]19 is being older[than 61 years old]20 or having [an officially rec-ognized permanent disability [that]22 makes onedisabled for [any [profession]24 or [job]25]23]21.
[The second one]26 requires that [the pension]27 be[the main or only source of [the [pensioner?s]30 in-come]29]28, and provided that [the annual amountof [the pension]32]31 represents, at least, [75% of[the total [yearly income of [the pen-sioner]36]35]34]33.
[Els beneficiaris de [pensions de [viudetat]3]2]1 po-dran conservar [la paga]4 encara_que [?
]5 es tornina casar si [?
]6 compleixen [una s?rie de [condi-cions]8]7 , segons [el reial decret aprovat ahir pel[Consell_de_Ministres]10]9 .
[La nova norma]11 afecta [els perceptors d' [unapensi?
de [viudetat]13]12 [que]14 contreguin [matri-moni]15 a_partir_de [l' 1_de_gener_del_2002]16]17 .
[La primera de [les condicions]18]19 ?s tenir [m?sde 61 anys]20 o tenir reconeguda [una incapacitatpermanent [que]22 inhabiliti per a [tota [professi?
]24o [ofici]25]23]21.
[La segona]26 ?s que [la pensi?
]27 sigui [la principalo ?nica font d' [ingressos del [pensionista]30]29]28 , isempre_que [l' import anual de [la mateixa pen-si?
]32]31 representi , com_a_m?nim , [el 75% del[total dels [ingressos anuals del [pensionis-ta]36]35]34]33.712.2 Evaluation2.1.1 Input informationThe input information for the task will consist of:word forms, lemmas, POS, full syntax, and seman-tic role labeling.
Two different scenarios will beconsidered regarding the source of the input infor-mation:i) In the first one, gold standard annotation willbe provided to participants.
This input annota-tion will correctly identify all NPs that are partof coreference chains.
This scenario will beonly available for Catalan and Spanish.ii) In the second, state-of-the-art automatic lin-guistic analyzers for the three languages willbe used to generate the input annotation of thedata.
The matching between the automaticallygenerated structure and the real NPs interven-ing in the chains does not need to be perfect inthis setting.By defining these two experimental settings, wewill be able to check the performance of corefer-ence systems when working with perfect linguistic(syntactic/semantic) information, and the degrada-tion in performance when moving to a more realis-tic scenario with noisy input annotation.2.1.2 Closed/open challengesIn parallel, we will also consider the possibility ofdifferentiating between closed and open chal-lenges, that is, when participants are allowed to usestrictly the information contained in the trainingdata (closed) and when they make use of some ex-ternal resources/tools (open).2.1.3 Scoring measuresRegarding evaluation measures, we will have spe-cific metrics for each of the subtasks, which will becomputed by language and overall.Several metrics have been proposed for the taskof coreference resolution, and each of them pre-sents advantages and drawbacks.
For the purposeof the current task, we have selected two of them ?B-cubed and CEAF ?
as the most appropriate ones.In what follows we justify our choice.The MUC scoring algorithm (Vilain et al, 1995)has been the most widely used for at least two rea-sons.
Firstly, the MUC corpora and the MUCscorer were the first available systems.
Secondly,the MUC scorer is easy to understand and imple-ment.
However, this metric has two major weak-nesses: (i) it does not give any credit to the correctidentification of singleton entities (chains consist-ing of one single mention), and (ii) it intrinsicallyfavors systems that produce fewer coreferencechains, which may result in higher F-measures forworse systems.A second well-known scoring algorithm, theACE value (NIST, 2003), owes its popularity tothe ACE evaluation campaign.
Each error (a miss-ing element, a misclassification of a coreferencechain, a mention in the response not included in thekey) made by the response has an associated cost,which depends on the type of entity (e.g.
person,location, organization) and on the kind of mention(e.g.
name, nominal, pronoun).
The fact that thismetric is entity-type and mention-type dependent,and that it relies on ACE-type entities makes thismeasure inappropriate for the current task.The two measures that we are interested in com-paring are B-cubed (Bagga and Baldwin, 1998)and CEAF (Luo, 2005).
The former does not lookat the links produced by a system as the MUC al-gorithm does, but looks at the presence/absence ofmentions for each entity in the system output.
Pre-cision and recall numbers are computed for eachmention, and the average gives the final precisionand recall numbers.CEAF (Luo, 2005) is a novel metric for evaluat-ing coreference resolution that has already beenused in some published papers (Ng, 2008; Denisand Baldridge, 2008).
It mainly differs from B-cubed in that it finds the best one-to-one entityalignment between the gold and system responsesbefore computing precision and recall.
The bestmapping is that which maximizes the similarityover pairs of chains.
The CEAF measure has twovariants: a mention-based, and an entity-based one.While the former scores the similarity of twochains as the absolute number of common men-tions between them, the latter scores the relativenumber of common mentions.Luo (2005) criticizes the fact that a responsewith all mentions in the same chain obtains 100%B-cubed recall, whereas a response with each men-tion in a different chain obtains 100% B-cubed72precision.
However, precision will be penalized inthe first case, and recall in the second case, eachcaptured by the corresponding F-measure.
Luo?sentity alignment might cause that a correctly iden-tified link between two mentions is ignored by thescoring metric if that entity is not aligned.
Finally,as far as the two CEAF metrics are concerned, theentity-based measure rewards alike a correctlyidentified one-mention entity and a correctly iden-tified five-mention entity, while the mention-basedmeasure takes into account the size of the entity.Given this series of advantages and drawbacks,we opted for including both B-cubed and CEAFmeasures in the final evaluation of the systems.
Inthis way we will be able to perform a meta-evaluation study, i.e.
to evaluate and compare theperformance of metrics with respect to the taskobjectives and system rankings.
It might be inter-esting to break B-cubed and CEAF into partial re-sults across different kinds of mentions in order toget a better understanding of the sources of errorsmade by each system.
Additionally, the MUC met-ric will also be included for comparison purposeswith previous results.Finally, for the setting with automatically gener-ated input information (second scenario in Section2.1.1), it might be desirable to devise metric vari-ants accounting for partial matches of NPs.
In thiscase, capturing the correct NP head would givemost of the credit.
We plan to work in this researchline in the near future.Official scorers will be developed in advanceand made available to participants when postingthe trial datasets.
The period in between the releaseof trial datasets and the start of the full evaluationwill serve as a test for the evaluation metrics.
De-pending on the feedback obtained from the partici-pants we might consider introducing someimprovements in the evaluation setting.3 AnCora-CO corporaThe corpora used in the task are AnCora-CO,which are the result of enriching the AnCora cor-pora (Taul?
et al, 2008) with coreference informa-tion.
AnCora-CO is a multilingual corpusannotated at different linguistic levels consisting of400K words in Catalan3, 400K words in Spanish2,3 Freely available for research purposes from the followingURL: http://clic.ub.edu/ancoraand 120K words in English.
For the purpose of thetask, the corpora are split into a training (85%) andtest (15%) set.
Each file corresponds to one news-paper text.AnCora-CO consists mainly of newspaper andnewswire articles: 200K words from the Spanishand Catalan versions of El Peri?dico newspaper,and 200K words from the EFE newswire agency inthe Spanish corpus, and from the ACN newswireagency in the Catalan corpus.
The source corporafor Spanish and Catalan are the AnCora corpora,which were annotated by hand with full syntax(constituents and functions) as well as with seman-tic information (argument structure with thematicroles, semantic verb classes, named entities, andWordNet nominal senses).
The annotation ofcoreference constitutes an additional layer on topof the previous syntactic-semantic information.The English part of AnCora-CO consists of a se-ries of documents of the Reuters newswire corpus(RCV1 version).4 The RCV1 corpus does not comewith any syntactic nor semantic annotation.
This iswhy we only count with automatic linguistic anno-tation produced by statistical taggers and parserson this corpus.Although the Catalan, English, and Spanish cor-pora used in the task all belong to the domain ofnewspaper texts, they do not form a three-way par-allel corpus.3.1 Coreference annotationThe annotation of a corpus with coreference in-formation is highly complex due to (i) the lack ofinformation in descriptive grammars about thistopic, and (ii) the difficulty in generalizing the in-sights from one language to another.
Regarding (i),a wide range of units and relations occur for whichit is not straightforward to determine whether theyare or not coreferent.
Although there are theoreticalstudies for English, they cannot always be ex-tended to Spanish or Catalan since coreference is avery language-specific phenomenon, which ac-counts for (ii).In the following we present some of the linguis-tic issues more problematic in relation to corefer-ence annotation, and how we decided to deal withthem in AnCora-CO (Recasens, 2008).
Some ofthem are language dependent (1); others concern4 Reuters Corpus RCV1 is distributed by NIST at the follow-ing URL: http://trec.nist.gov/data/reuters/reuters.html73the internal structure of the mentions (2), or thetype of coreference link (3).
Finally, we presentthose NPs that were left out from the annotationfor not being referential (4).1.
Language-specific issues- Since Spanish and Catalan are pro-droplanguages, elliptical subjects were intro-duced in the syntactic annotation, and theyare also annotated with coreference.- Expletive it pronouns, which are frequentin English and to a lesser extent in Spanishand Catalan are not referential, and so theydo not participate in coreference links.- In Spanish, clitic forms for pronouns canmerge into a single word with the verb; inthese cases the whole verbal node is anno-tated for coreference.2.
Issues concerning the mention structure- In possessive NPs, only the reference ofthe thing possessed (not the possessor) istaken into account.
For instance, su libro?his book?
is linked with a previous refer-ence of the same book; the possessive de-terminer su ?his?
does not constitute an NPon its own.- In the case of conjoined NPs, three (ormore) links can be encoded: one betweenthe entire NPs, and additional ones foreach of the constituent NPs.
AnCora-COcaptures links at these different levels.3.
Issues concerning types of coreference links- Plural NPs can refer to two or more ante-cedents that appear separately in the text.In these cases an entity resulting from theaddition of two or more entities is created.- Discourse deixis is kept under a specificlink tag because not all coreference resolu-tion systems can handle such relations.- Metonymy is annotated as a case of iden-tity because both mentions pragmaticallycorefer.4.
Non-referential NPs- In order to be linguistically accurate (vanDeemter and Kibble, 2000), we distinguishbetween referring and attributive NPs:while the first point to an entity, the latterexpress some of its properties.
Thus, at-tributive NPs like apposition and predica-tive phrases are not treated as identitycoreference in AnCora-CO (they are keptdistinct under the ?predicative link?
tag).- Bound anaphora and bridging reference gobeyond coreference and so are left outfrom consideration.The annotation process of the corpora is outlined inthe next section.3.2 Annotation processThe Ancora coreference annotation process in-volves: (a) marking of mentions, and (b) markingof coreference chains (entities).
(a) Referential full NPs (including proper nouns)and pronouns (including elliptical and clitic pro-nouns) are the potential mentions of a coreferencechain.
(b) In the current task only identity relations(coreftype=?ident?)
will be considered, which linkreferential NPs that point to the same discourseentity.
Coreferent mentions are annotated with theattribute entity.
Mentions that point to the sameentity share the same entity number.
In Figure 1,for instance, el reial decret aprovat ahir pel Con-sell_de_Ministres ?the royal decree approved yes-terday by the Council of Ministers?
isentity=?entity9?
and la nova norma ?the new rule?is also entity=?entity9?
because they corefer.Hence, mentions referring to the same discourseentity all share the same entity number.The corpora were annotated by a total of sevenannotators (qualified linguists) using the An-CoraPipe annotation tool (Bertran et al, 2008),which allows different linguistic levels to be anno-tated simultaneously and efficiently.
AnCoraPipesupports XML in-line annotations.An initial reliability study was performed on asmall portion of the Spanish AnCora-CO corpus.In that study, eight linguists annotated the corpusmaterial in parallel.
Inter-annotator agreement wascomputed with Krippendorff?s alpha, achieving aresult above 0.8.
Most of the problems detectedwere attributed either to a lack of training of thecoders or to ambiguities that are left unresolved inthe discourse itself.
After carrying out this reliabil-ity study, we opted for annotating the corpora in atwo-stage process: a first pass in which all mentionattributes and coreference links were coded, and asecond pass in which the already annotated fileswere revised.744 ConclusionsThe SemEval-2010 multilingual coreference reso-lution task has been presented for discussion.Firstly, we aim to promote research on coreferenceresolution from a learning-based perspective in amultilingual scenario in order to: (a) explore port-ability issues; (b) analyze language-specific tuningrequirements; (c) facilitate cross-linguistic com-parisons between two Romance languages and be-tween Romance languages and English; and (d)encourage researchers to develop linguistic re-sources ?
annotated corpora ?
oriented to corefer-ence resolution for other languages.Secondly, given the complexity of the corefer-ence phenomena we split the coreference resolu-tion task into two (full coreference chains andpronominal resolution), and we propose two dif-ferent scenarios (gold standard vs. automaticallygenerated input information) in order to evaluate towhat extent the performance of a coreference reso-lution system varies depending on the quality ofthe other levels of information.Finally, given that the evaluation of coreferenceresolution systems is still an open issue, we areinterested in comparing different coreference reso-lution metrics: B-cubed and CEAF measures.
Inthis way we will be able to evaluate and comparethe performance of these metrics with respect tothe task objectives and system rankings.AcknowledgmentsThis research has been supported by the projectsLang2World (TIN2006-15265-C06), TEXT-MESS(TIN2006-15265-C04), OpenMT (TIN2006-15307-C03-02), AnCora-Nom (FFI2008-02691-E),and the FPU grant (AP2006-00994) from the Span-ish Ministry of Education and Science, and thefunding given by the government of the Generalitatde Catalunya.ReferencesBagga, Amit and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of Lan-guage Resources and Evaluation Conference.Bertran, Manuel, Oriol Borrega, Marta Recasens, andB?rbara Soriano.
2008.
AnCoraPipe: A tool for mul-tilevel annotation, Procesamiento del Lenguaje Natu-ral, n. 41: 291-292, SEPLN.Denis, Pascal and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
Pro-ceedings of the Empirical Methods in Natural Lan-guage Processing (EMNLP 2008).Luo, Xiaoqiang.
2005.
On coreference resolution per-formance metrics.
Proceedings of HLT/NAACL 2005.McCarthy Joseph and Wendy Lehnert.
1995.
Usingdecision trees for coreference resolution.
Proceed-ings of the Fourteenth International Joint Conferenceon Artificial Intelligence.Mitkov, Ruslan.
1998.
Robust pronoun resolution withlimited knowledge.
Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics, and 17th International Conference on Com-putational Linguistics (COLING-ACL98).Morton, Thomas.
2000.
Using coreference for questionanswering.
Proceedings of the 8th Text REtrievalConference (TREC-8).Ng, Vincent.
2008.
Unsupervised models for corefer-ence resolution.
Proceedings of the Empirical Meth-ods in Natural Language Processing (EMNLP 2008).NIST.
2003.
In Proceedings of ACE 2003 workshop.Booklet, Alexandria, VA.Recasens, Marta.
2008.
Towards Coreference Resolu-tion for Catalan and Spanish.
Master Thesis.
Univer-sity of Barcelona.Steinberger, Josef, Massimo Poesio, Mijail Kabadjov,and Karel Jezek.
2007.
Two uses of anaphora resolu-tion in summarization.
Information Processing andManagement, 43:1663?1680.Taul?, Mariona, Ant?nia Mart?, and Marta Recasens.2008.
AnCora: Multilevel corpora with coreferenceinformation for Spanish and Catalan.
In Proceedingsof the Language Resources and Evaluation Confer-ence (LREC 2008).van Deemter, Kees and Rodger Kibble.
2000.
Squibsand Discussions: On coreferring: coreference inMUC and related annotation schemes.
Computa-tional Linguistics, 26(4):629-637.Vilain, Marc, John Burger, John Aberdeen, DennisConnolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof MUC-6.75
