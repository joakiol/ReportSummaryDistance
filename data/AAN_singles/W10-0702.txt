Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 13?20,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCorpus Creation for New Genres:A Crowdsourced Approach to PP AttachmentMukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal and Kathleen McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USA{mj2472,jda2129}@columbia.edu, {kapil,sara,kathy}@cs.columbia.eduAbstractThis paper explores the task of building an ac-curate prepositional phrase attachment corpusfor new genres while avoiding a large invest-ment in terms of time and money by crowd-sourcing judgments.
We develop and presenta system to extract prepositional phrases andtheir potential attachments from ungrammati-cal and informal sentences and pose the subse-quent disambiguation tasks as multiple choicequestions to workers from Amazon?s Mechan-ical Turk service.
Our analysis shows thatthis two-step approach is capable of producingreliable annotations on informal and poten-tially noisy blog text, and this semi-automatedstrategy holds promise for similar annotationprojects in new genres.1 IntroductionRecent decades have seen rapid development in nat-ural language processing tools for parsing, semanticrole-labeling, machine translation, etc., and much ofthis success can be attributed to the study of statisti-cal techniques and the availability of large annotatedcorpora for training.
However, the performance ofthese systems is heavily dependent on the domainand genre of their training data, i.e.
systems trainedon data from a particular domain tend to performpoorly when applied to other domains and adap-tation techniques are not always able to compen-sate (Dredze et al, 2007).
For this reason, achiev-ing high performance on new domains and genresfrequently necessitates the collection of annotatedtraining data from those domains and genres, a time-consuming and frequently expensive process.This paper examines the problem of collectinghigh-quality annotations for new genres with a focuson time and cost efficiency.
We explore the well-studied but non-trivial task of prepositional phrase(PP) attachment and describe a semi-automated sys-tem for identifying accurate attachments in blogdata, which is frequently noisy and difficult to parse.PP attachment disambiguation involves finding acorrect attachment for a prepositional phrase in asentence.
For example, in the sentence ?We went toJohn?s house on Saturday?, the phrase ?on Satur-day?
attaches to the verb ?went?.
In another exam-ple, ?We went to John?s house on 12th Street?, thePP ?on 12th street?
attaches to the noun ?John?shouse?.
This sort of disambiguation requires se-mantic knowledge about sentences that is difficultto glean from their surface form, a problem whichis compounded by the informal nature and irregularvocabulary of blog text.In this work, we investigate whether crowd-sourced human judgments are capable of distin-guishing appropriate attachments.
We present a sys-tem that simplifies the attachment problem and rep-resents it in a format that can be intuitively tackledby humans.Our approach to this task makes use of a heuristic-based system built on a shallow parser that identi-fies the likely words or phrases to which a PP canattach.
To subsequently select the correct attach-ment, we leverage human judgments from multi-ple untrained annotators (referred to here as work-ers) through Amazon?s Mechanical Turk 1, an onlinemarketplace for work.
This two-step approach of-1http://www.mturk.amazon.com13fers distinct advantages: the automated system cutsdown the space of potential attachments effectivelywith little error, and the disambiguation task can bereduced to small multiple choice questions whichcan be tackled quickly and aggregated reliably.The remainder of this paper focuses on the PP at-tachment task over blog text and our analysis of theresulting aggregate annotations.
We note, however,that this type of semi-automated approach is poten-tially applicable to any task which can be reliablydecomposed into independent judgments that un-trained annotators can tackle (e.g., quantifier scop-ing, conjunction scope).
This work is intended asan initial step towards the development of efficienthybrid annotation tools that seamlessly incorporateaggregate human wisdom alongside effective algo-rithms.2 Related WorkIdentifying PP attachments is an essential task forbuilding syntactic parse trees.
While this task hasbeen studied using fully-automated systems, manyof them rely on parse tree output for predicting po-tential attachments (Ratnaparkhi et al, 1994; Yehand Vilain, 1998; Stetina and Nagao, 1997; Zavrelet al, 1997).
However, systems that rely on goodparses are unlikely to perform well on new genressuch as blogs and machine translated texts for whichparse tree training data is not readily available.Furthermore, the predominant dataset for eval-uating PP attachment is the RRR dataset (Ratna-parkhi et al, 1994) which consists of PP attach-ment cases from the Wall Street Journal portion ofthe Penn Treebank.
Instead of complete sentences,this dataset consists of sets of the form {V,N1,P,N2}where {P,N2} is the PP and {V,N1} are the poten-tial attachments.
This simplification of the PP at-tachment task to a choice between two alternativesis unrealistic when considering the potential long-distance attachments encountered in real-world text.While blogs and other web text, such as discus-sion forums and emails, have been studied for a va-riety of tasks such as information extraction (Hongand Davison, 2009), social networking (Gruhl etal., 2004), and sentiment analysis (Leshed andKaye, 2006), we are not aware of any previous ef-forts to gather syntactic data (such as PP attach-ments) in the genre.
Syntactic methods such asPOS tagging, parsing and structural disambiguationare commonly used when analyzing well-structuredtext.
Including the use of syntactic informationhas yielded improvements in accuracy in speechrecognition (Chelba and Jelenik, 1998; Collins etal., 2005) and machine translation (DeNeefe andKnight, 2009; Carreras and Collins, 2009).
We an-ticipate that datasets such as ours could be useful forsuch tasks as well.Amazon?s Mechanical Turk (MTurk) has becomevery popular for manual annotation tasks and hasbeen shown to perform equally well over labelingtasks such as affect recognition, word similarity, rec-ognizing textual entailment, event temporal order-ing and word sense disambiguation, when comparedto annotations from experts (Snow et al, 2008).While these tasks were small in scale and intended todemonstrate the viability of annotation via MTurk,it has also proved effective in large-scale tasks in-cluding the collection of accurate speech transcrip-tions (Gruenstein et al, 2009).
In this paper we ex-plore a method for corpus building on a large scalein order to extend annotation into new domains andgenres.We previously evaluated crowdsourced PP attach-ment annotation by using MTurk workers to repro-duce PP attachments from the Wall Street Journalcorpus (Rosenthal et al, 2010).
The results demon-strated that MTurk workers are capable of identi-fying PP attachments in newswire text, but the ap-proach used to generate attachment options is de-pendent on the existing gold-standard parse treesand cannot be used on corpora where parse trees arenot available.
In this paper, we build on the semi-automated annotation principle while avoiding thedependency on parsers, allowing us to apply thistechnique to the noisy and informal text found inblogs.3 System DescriptionOur system must both identify PPs and generate alist of potential attachments for each PP in this sec-tion.
Figure 1 illustrates the structure of the system.First, the system extracts sentences from scrapedblog data.
Text is preprocessed by stripping HTMLtags, advertisements, non-Latin and non-printable14PPsPPsQuestionBuilderPP IdentifierChunker+PreprocessorsentencesChunkedChunkedsentencespoint predictorAttachmentattachmentsPotentialMechanicalTurkQuestionsforNew domaindata (Blogs)Figure 1: Overview of question generation systemcharacters.
Emoticon symbols are removed using astandard list.
2The cleaned data is then partitioned into sentencesusing the NLTK sentence splitter.
3 In order tocompensate for the common occurrence of informalpunctuation and web-specific symbols in blog text,we replace all punctuation symbols between quo-tation marks and parentheses with placeholder tags(e.g.
?QuestionMark?)
during the sentence splittingprocess and do the same for website names, timemarkers and referring phrases (e.g.
@John).
Ad-ditionally, we attempt to re-split sentences at ellipsisboundaries if they are longer than 80 words and dis-card them if this fails.As parsers trained on news corpora tend to per-form poorly on unstructured texts like blogs, werely on a chunker to partition sentences into phrases.Choosing a good chunker is essential to this ap-proach: around 35% of the cases in which the cor-rect attachment is not predicted by the system aredue to chunker error.
We experimented with differ-ent chunkers over a random sample of 50 sentencesbefore selecting a CRF-based chunker (Phan, 2006)for its robust performance.The chunker output is initially processed by fus-ing together chunks in order to ensure that a singlechunk represents a complete attachment point.
Twoconsecutive NP chunks are fused if the first containsan element with a possessive part of speech tag (e.g.John?s book), while particle chunks (PRT) are fusedwith the VP chunks that precede them (e.g.
packup).
These chunked sentences are then processedto identify PPs and potential attachment points forthem, which can then be used to generate questions2http://www.astro.umd.edu/?marshall/smileys.html3http://www.nltk.orgfor MTurk workers.3.1 PP ExtractionPPs can be classified into two broad categories basedon the number of chunks they contain.
A simplePP consists of only two chunks: a preposition andone noun phrase, while a compound PP has multi-ple simple PPs attached to its primary noun phrase.For example, in the sentence ?I just made some last-minute changes to the latest issue of our newsletter?,the PP with preposition ?to?
can be considered to beeither the simple PP ?to the latest issue?
or the com-pound PP ?to the latest issue of our newsletter?.We handle compound PPs by breaking them downinto multiple simple PPs; compound PPs can be re-covered by identifying the attachments of their con-stituent simple PPs.
Our simple PP extraction al-gorithm identifies PPs as a sequence of chunks thatconsist of one or more prepositions terminating in anoun phrase or gerund.3.2 Attachment Point PredictionA PP usually attaches to the noun or verb phrase pre-ceding it or, in some cases, can modify a followingclause by attaching to the head verb.
We build a setof rules based on this intuition to pick out the poten-tial attachments in the sentence; these rules are de-scribed in Table 1.
The rules are applied separatelyfor each PP in a sentence and in the same sequenceas mentioned in the table (except for rule 4, whichis applied while choosing a chunk using any of theother rules).15Rule Example1 Choose closest NP and VP preceding the PP.
I made modifications to our newsletter.2 Choose next closest VP preceding the PP if the VP selected in (1)contains a VBG.He snatched the disk flying away with one hand.3 Choose first VP following the PP.
On his desk he has a photograph.4 All chunks inside parentheses are skipped, unless the PP falls withinparentheses.Please refer to the new book (second edition) formore notes.5 Choose anything immediately preceding the PP that is not out ofchunk and has not already been picked.She is full of excitement.6 If a selected NP contains the word and, expand it into two options,one with the full expression and one with only the terms followingand.He is president and chairman of the board.7 For PPs in chains of the form P-NP-P-NP (PP-PP), choose all theNPs in the chain preceding the PP and apply all the above rulesconsidering the whole chain as a single PP.They found my pictures of them from the concert.8 If there are fewer than four options after applying the above rules,also select the VP preceding the last VP selected, the NP precedingthe last NP selected, and the VP following the last VP picked.Table 1: List of rules for attachment point predictor.
In the examples, PPs are denoted by boldfaced text and potentialattachment options are underlined.4 ExperimentsAn experimental study was undertaken to test ourhypothesis that we could obtain reliable annotationson informal genres using MTurk workers.
Here wedescribe the dataset and our methods.4.1 Dataset and InterfaceWe used a corpus of blog posts made on LiveJour-nal 4 for system development and evaluation.
Onlyposts from English-speaking countries (i.e.
USA,Canada, UK, Australia and New Zealand) were con-sidered for this study.The interface provided to MTurk workers showedthe sentence on a plain background with the PP high-lighted and a statement prompting them to pick thephrase in the sentence that the given PP modified.The question was followed by a list of options.
Inaddition, we provided MTurk workers the option toindicate problems with the given PP or the listed op-tions.
Workers could write in the correct attachmentif they determined that it wasn?t present in the list ofoptions, or the correct PP if the one they were pre-sented with was malformed.
This allowed them tocorrect errors made by the chunker and automatedattachment point predictor.
In all cases, workerswere forced to pick the best answer among the op-tions regardless of errors.
We also supplied a num-4http://www.livejournal.comber of examples covering both well-formed and er-roneous cases to aid them in identifying appropriateattachments.4.2 Experimental SetupFor our experiment, we randomly selected 1000questions from the output produced by the systemand provided each question to five different MTurkworkers, thereby obtaining five different judgmentsfor each PP attachment case.
Workers were paid fourcents per question and the average completion timeper task was 48 seconds.
In total $225 was spenton the full study with $200 spent on the workers and$25 on MTurk fees.The total time taken for the studywas approximately 16 hours.A pilot study was carried out with 50 sentencesbefore the full study to test the annotation interfaceand experiment with different ways of presenting thePP and attachment options to workers.
During thisstudy, we observed that while workers were will-ing to suggest correct answers or PPs when facedwith erroneous questions, they often opted to notpick any of the options provided unless the questionwas well-formed.
This was problematic because, inmany cases, expert annotators were able to identifythe most appropriate attachment option.
Therefore,in the final study we forced them to pick the mostsuitable option from the given choices before indi-cating errors and writing in alternatives.16Workers in agreement Number of questions Accuracy Coverage5 (unanimity) 389 97.43% 41.33%?
4 (majority) 689 94.63% 73.22%?
3 (majority) 887 88.61% 94.26%?
2 (plurality) 906 87.75% 96.28%Total 941 84.48% 100%Table 2: Accuracy and coverage over agreement thresholds5 Evaluation corpusIn order to determine if the MTurk results were re-liable, worker responses had to be validated by hav-ing expert annotators perform the same task.
Forthis purpose, two of the authors annotated the 1000questions used for the experiment independently andcompared their judgments.
Disagreements were ob-served in 127 cases; these were then resolved by apool of non-author annotators.
If all three annota-tors on a case disagreed with each other the questionwas discarded; this situation occured 43 times.
Anadditional 16 questions were discarded because theydid not have a valid PP.
For example, ?I am paintingwith my blanket on today?.
Here ?on today?
is in-correctly extracted as a PP because the particle ?on?is tagged as a preposition.
The rest of the analysispresented in this section was performed on the re-maining 941 sentences.The annotators?
judgments were compared to theanswers provided by the MTurk workers and, inthe case of disagreement between the experts andthe majority of workers, the sentences were man-ually inspected to determine the reason.
In fivecases, more than one valid attachment was possi-ble; for example, in the sentence ?The video below isof my favourite song on the album - A Real Woman?,the PP ?of my favourite song?
could attach to eitherthe noun phrase ?the video?
or the verb ?is?
and con-veys the same meaning.
In such cases, both the ex-perts and the workers were considered to have cho-sen the correct answer.In 149 cases, the workers also augmented theirchoices by providing corrections to incomplete an-swers and badly constructed PPs.
For example,the PP ?of the Rings and Mikey?
in the sentence?Samwise from Lord of the Rings and Mikey fromThe Goonies are the same actor ??
was corrected to?of the Rings?.
In 34/39 of the cases where the cor-rect answer was not present in the options provided,at least one worker indicated correct attachment forthe PP.5.1 Attachment Prediction EvaluationWe measure the recall for our attachment point pre-dictor as the number of questions for which the cor-rect attachment appeared among the generated op-tions divided by the total number of questions.
Thesystem achieves a recall of 95.85% (902/941 ques-tions).
We observed that in many cases where thecorrect attachment point was not predicted, it wasdue to a chunker error.
For example, in the followingsentence, ?Stop all the clocks , cut off the telephone, Prevent the dog from barking with a juicy bone...?,the PP ?from barking?
attaches to the verb ?Pre-vent?
; however, due to an error in chunking ?Pre-vent?
is tagged as a noun phrase and hence is notpicked by our system.
The correct attachment wasalso occasionally missed when the attachment pointwas too far from the PP.
For example, in the sentence?Fitting as many people as possible on one sofa andunder many many covers and getting intimate?, thecorrect attachment for the PP ?under many manycovers?
is the verb ?Fitting?
but it is not picked byour system.Even though the correct attachment was not al-ways given, the workers could still provide their owncorrect answer.
In the first example above, 3/5 work-ers indicated that the correct attachment was not inthe list of options and wrote it in.6 ResultsTable 2 summarizes the results of the experiment.We assess both the coverage and reliability ofworker predictions at various levels of worker agree-ment.
This serves as an indicator of the effective-ness of the MTurk results: the accuracy can be taken17Figure 2: The number of questions in which exactly xworkers provided the correct answeras a general confidence measure for worker predic-tions; when five workers agree we can be 97.43%confident in the correctness of their prediction, whenat least four workers agree we can be 94.63% con-fident, etc.
Unanimity indicates that all workersagreed on an answer, majority indicates that morethan half of workers agreed on an answer, and plu-rality indicates that two workers agreed on a singleanswer, while the remaining three workers each se-lected different answers.
We observe that at highlevels of worker agreement, we get extremely highaccuracy but limited coverage of the data set; aswe decrease our standard for agreement, coverageincreases rapidly while accuracy remains relativelyhigh.Figure 2 shows the number of workers providingthe correct answer on a per-question basis.
Thisillustrates the distribution of worker agreementsacross questions.
Note that in the majority of cases(69.2%), at least four workers provided the correctanswer; in only 3.6% of cases were no workers ableto select the correct attachment.Figure 3 shows the distribution of worker agree-ments.
Unlike Table 2, these figures are not cumu-lative and include non-plurality two-worker agree-ments.
Note that the number of agreements dis-cussed in this figure is greater than the 941 evaluatedbecause in some cases there were multiple agree-ments on a single question.
As an example, threeworkers may choose one answer while the remain-ing two workers choose another; this question thenproduces both a three-worker agreement as well as atwo-worker agreement.Figure 3: The number of cases in which exactly x work-ers agreed on an answerNo.
of options No.
of cases Accuracy< 4 179 86.59%4 718 84.26%> 4 44 79.55%Table 3: Variation in worker performance with the num-ber of attachment options presentedAll questions on which there is agreement alsoproduce a majority vote, with one exception: the2/2/1 agreement.
Although the correct answer wasselected by one set of two workers in every case of2/2/1 agreement, this is not particularly useful forcorpus-building as we have no way to identify a pri-ori which set is correct.
Fortunately, 2/2/1 agree-ments were also quite rare and occurred in only 3%of cases.Figure 3 appears to indicate that instances ofagreement between two workers are unlikely to pro-duce good attachments; they have a an average ac-curacy of 37.2%.
However, this is due in large partto cases of 3/2 agreement, in which the two workersin the minority are usually wrong, as well as cases of2/2/1 agreement which contain at least one incorrectinstance of two-worker agreement.
However, if weonly consider cases in which the two-worker agree-ment forms a plurality (i.e.
all other workers dis-agree amongst themselves), we observe an averageaccuracy of 64.3% which is similar to that of casesof three-worker agreement (67.7%).We also attempted to study the variation in workerperformance based on the complexity of the task;specifically looking at how response accuracy var-ied depending on the number of options that workerswere presented with.
Although our system aimed to18Figure 4: Variation in accuracy with sentence length.generate four attachment options per case, fewer op-tions were produced for small sentences and openingPPs while additional options were generated in sen-tences containing PP-NP chains (see Table 1 for thecomplete list of rules).
Table 3 shows the variation inaccuracy with the number of options provided to theworkers.
We might expect that an increased numberof options may be correlated with decreased accu-racy and the data does indeed seem to suggest thistrend; however, we do not have enough datapointsfor the cases with fewer or more than four options toverify whether this effect is significant.We also analyzed the relationship between thelength of the sentence (in terms of number of words)and the accuracy.
Figure 4 indicates that as thelength of the sentence increases, the average accu-racy decreases.
This is not entirely unexpected aslengthy sentences tend to be more complicated andtherefore harder for human readers to parse.7 Conclusions and Future WorkWe have shown that by working in conjunctionwith automated attachment point prediction sys-tems, MTurk workers are capable of annotating PPattachment problems with high accuracy, even whenworking with unstructured and informal blog text.This work provides an immediate framework for thebuilding of PP attachment corpora for new genreswithout a dependency on full parsing.More broadly, the semi-automated frameworkoutlined in this paper is not limited to the task ofannotating PP attachments; indeed, it is suitable foralmost any syntactic or semantic annotation taskwhere untrained human workers can be presentedwith a limited number of options for selection.
Bydividing the desired annotation task into smallersub-tasks that can be tackled independently or in apipelined manner, we anticipate that more syntac-tic information can be extracted from unstructuredtext in new domains and genres without the sizableinvestment of time and money normally associatedwith hiring trained linguists to build new corpora.To this end, we intend to further leverage the adventof crowdsourcing resources in order to tackle moresophisticated annotation tasks.AcknowledgementsThe authors would like to thank Kevin Lerman forhis help in formulating the original ideas for thiswork.
This material is based on research supportedin part by the U.S. National Science Foundation(NSF) under IIS-05-34871.
Any opinions, findingsand conclusions or recommendations expressed inthis material are those of the authors and do not nec-essarily reflect the views of the NSF.ReferencesXavier Carreras and Michael Collins.
2009.
Non-projective parsing for statistical machine translation.In Proceedings of EMNLP, pages 200?209.Ciprian Chelba and Frederick Jelenik.
1998.
Structuredlanguage modeling for speech recognition.
In Pro-ceedings of NLDB.Michael Collins, Brian Roark, and Murat Saraclar.2005.
Discriminative syntactic language modeling forspeech recognition.
In Proceedings of ACL, pages507?514.Steve DeNeefe and Kevin Knight.
2009.
Synchronoustree adjoining machine translation.
In Proceedings ofEMNLP, pages 727?736.Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-man Ganchev, Joa?o Graca, and Fernando Pereira.2007.
Frustratingly hard domain adaptation for depen-dency parsing.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL, pages 1051?1055,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Alex Gruenstein, Ian McGraw, and Andrew Sutherland.2009.
A self-transcribing speech corpus: collectingcontinuous speech with an online educational game.In Proceedings of the Speech and Language Technol-ogy in Education (SLaTE) Workshop.19Figure 5: HIT Interface for PP attachment taskDaniel Gruhl, R. Guha, David Liben-Nowell, and An-drew Tomkins.
2004.
Information diffusion throughblogspace.
In Proceedings of WWW, pages 491?501.Liangjie Hong and Brian D. Davison.
2009.
Aclassification-based approach to question answering indiscussion boards.
In Proceedings of SIGIR, pages171?178.Gilly Leshed and Joseph ?Jofish?
Kaye.
2006.
Under-standing how bloggers feel: recognizing affect in blogposts.
In CHI ?06 extended abstracts on Human fac-tors in computing systems, pages 1019?1024.Xuan-Hieu Phan.
2006.
CRFChunker: CRFEnglish phrase chunker.
http://crfchunker.sourceforge.net.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.1994.
A maximum entropy model for prepositionalphrase attachment.
In Proceedings of HLT, pages 250?255.Sara Rosenthal, William J. Lipovsky, Kathleen McKe-own, Kapil Thadani, and Jacob Andreas.
2010.
Semi-automated annotation for prepositional phrase attach-ment.
In Proceedings of LREC, Valletta, Malta.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of EMNLP, pages 254?263.Jiri Stetina and Makoto Nagao.
1997.
Corpus based PPattachment ambiguity resolution with a semantic dic-tionary.
In Proceedings of the Workshop on Very LargeCorpora, pages 66?80.Alexander S. Yeh and Marc B. Vilain.
1998.
Some prop-erties of preposition and subordinate conjunction at-tachments.
In Proceedings of COLING, pages 1436?1442.Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.1997.
Resolving PP attachment ambiguities withmemory-based learning.
In Proceedings of the Work-shop on Computational Language Learning (CoNLL),pages 136?144.Appendix A: Mechanical Turk InterfaceFigure 5 shows a screenshot of the interface pro-vided to the Mechanical Turk workers for the PP at-tachment task.
By default, examples and additionaloptions are hidden but can be viewed using the linksprovided.
The screenshot illustrates a case in whicha worker is confronted with an incorrect PP and usesthe additional options to correct it.20
