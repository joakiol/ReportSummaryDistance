BioNLP 2007: Biological, translational, and clinical language processing, pages 163?170,Prague, June 2007. c?2007 Association for Computational LinguisticsBaseNPs that contain gene names: domain specicity and genericityIan LewinComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FD, UKian.lewin@cl.cam.ac.ukAbstractThe names of named entities very often oc-cur as constituents of larger noun phraseswhich denote different types of entity.
Un-derstanding the structure of the embeddingphrase can be an enormously beneficial firststep to enhancing whatever processing is in-tended to follow the named entity recogni-tion in the first place.
In this paper, weexamine the integration of general purposelinguistic processors together with domainspecific named entity recognition in order tocarry out the task of baseNP detection.
Wereport a best F-score of 87.17% on this task.We also report an inter-annotator agreementscore of 98.8 Kappa on the task of baseNPannotation of a new data set.1 IntroductionBase noun phrases (baseNPs), broadly ?the initialportions of non-recursive noun phrases up to thehead?
(Ramshaw and Marcus, 1995), are valuablepieces of linguistic structure which minimally ex-tend beyond the scope of named entities.
In thispaper, we explore the integration of different tech-niques for detecting baseNPs that contain a namedentity, using a domain-trained named entity recog-nition (NER) system but in combination with otherlinguistic components that are ?general purpose?.The rationale is simply that domain-trained NER isclearly a necessity for the task; but one might expectto be able to secure good coverage at the higher syn-tactic level by intelligent integration of general pur-pose syntactic processing without having to undergoa further round of domain specific annotation andtraining.
We present a number of experiments ex-ploring different ways of integrating NER into gen-eral purpose linguistic processing.
Of course, goodresults can also be used subsequently to help reducethe effort required in data annotation for use in dedi-cated domain-specific machine learning systems forbaseNP detection.First, however, we motivate the task itself.
Enor-mous effort has been directed in recent years to theautomatic tagging of named entities in bio-medicaltexts and with considerable success.
For example,iHOP reports gene name precision as being between87% and 99% (depending on the organism) (Hoff-man and Valencia, 2004).
Named entities are ofcourse only sometimes identical in scope with nounphrases.
Often they are embedded within highlycomplex noun phrases.
Nevertheless, the simple de-tection of a name by itself can be valuable.
Thisdepends in part on the intended application.
Thus,iHOP uses gene and protein names to hyperlinksentences from Medline and this then supports abrowser over those sentences with additional navi-gation facilities.
Clicking on Dpp whilst viewing apage of information about hedgehog leads to a pageof information about Dpp in which sentences thatrelate both Dpp and hedgehog are prioritized.One of the application advantages of iHOP is thatthe discovered gene names are presented to the userin their original context and this enables users tocompensate for problems in reliability and/or con-textual relevance.
In many Information Extraction(IE) systems, relations between entities are detectedand extracted into a table.
In this case, since the im-163mediate surrounding context of the gene name maybe simply lost, the reliability of the original identifi-cation becomes much more important.
In section 2below, we explain our own application backgroundin which our objective is to increase the productiv-ity of human curators whose task is to read partic-ular scientific papers and fill in fields of a databaseof information about genes.
Directing curators?
at-tention to sentences which contain gene names isclearly one step.
Curators additionally report thatan index into the paper that uses the gene name andits embedding baseNP is even more valuable (ref-erence omitted for anonymity).
This often enablesthem to predict the possible relevance of the nameoccurrence to the curation task and thus begin or-dering their exploration of the paper.
Consequently,our technical goal of baseNP detection is linked di-rectly to a valuable application task.
We also use thebaseNP identification in order to type the occurrencesemantically and use this information in an anaphoraresolution process (Gasperin, 2006).The detection of baseNPs that contain a namedentity is a super-task of NER, as well as a sub-taskof NP-chunking.
Given that NER is clearly a domainspecific task, it is an interesting question what per-formance levels are achievable using domain trainedNER in combination with general purpose linguisticprocessing modules.There is a further motivation for the task.
The dis-tinction between a named entity and an embeddingnoun phrase is one with critical importance even forthe sub-task of NER.
Dingare et al(2005) conclude,from their analysis of a multi-feature maximum en-tropy NER module, that increases in performance ofbiomedical NER systems will depend as much uponqualitative improvements in annotated data as in thetechnology underlying the systems.
The claim is thatquality problems are partly due to confusion overwhat lies in the scope of a named entity and whatlies at higher syntactic levels.
Current biomedicalannotations are often inconsistent partly because an-notators are left with little guidance on how to han-dle complexities in noun phrases, especially with re-spect to premodifiers and conjunctions.
For exam-ple, which premodifiers are part of the named entityand which are ?merely?
part of the embedding nounphrase?
Is human part of the named entity in theregulation of human interleukin-2 gene expression,Figure 1: Paper Browser showing baseNP indexor not?By focussing attention instead on the baseNPsthat contain a named entity, one can clearly sidestepthis issue to some extent.
After all, increasing theaccuracy of an NER module with respect to premod-ifier inclusion is unlikely to affect the overall accu-racy of detection of the embedding noun phrases.2 FlyBase curationThe intended application for our work is a soft-ware environment for FlyBase curators that includesan NLP-enhanced Browser for Scientific Papers.FlyBase is the world?s leading genomics databasefor the fruitfly Drosophila (melanogaster and otherspecies) (Crosby et al, 2007).
FlyBase is largelyupdated through a paper-by-paper methodology inwhich research articles likely to contain informa-tion relevant for the FlyBase database are first putin a priority list.
Subsequently, these are read byskilled geneticists (at post-doctoral level) who dis-til gene related information into the database itself.Although this is a paradigm example of IE, our ob-jective is not to fully automate this task itself, sim-ply because the expected accuracy rates are unlikelyto be high enough to provide a genuinely usefultool.
Rather, our task is to enable curators to ex-plore the gene related sections of papers more effi-ciently.
The Browser currently highlights potential164items of interest for curators and provides novel in-dexing and navigation possibilities.
It is in this con-text that the identification of baseNPs that containgene names is carried out.
An individual sentencethat contains a gene name is very often not enough,considered in isolation, for curators to fill in a re-quired database field.
Information often needs tobe gathered from across a paragraph and even thewhole paper.
So extraction of sentences is not an at-tractive option.
Equally, a whole sentence is unfeasi-bly large to serve simply as an indexing term into thepaper.
Noun phrases provide more information thansimply gene names, but post-modification can alsolead to extremely long terms.
BaseNPs are there-fore a useful compromise, these being short enoughto display whole in a window (i.e.
no scrollingis required) and often bearing enough informationfor the user to understand much more of the con-text in which the gene name itself appears.
Fur-thermore, the baseNP is both a natural ?unit?
of in-formation (whereas a window of n tokens around agene name is not) and it supports further processing.BaseNPs are typed according to whether they denotegenes or various gene products and linked togetherin anaphoric chains.In our navigation panel for the Browser, thebaseNPs are sorted according to the gene name thatthey contain (and then by order in which they appearwithin the paper), and hyperlinked to their occur-rence in the paper.
This enables users to explore pa-pers gene-by-gene but also, when considering a par-ticular gene, to understand more about the referenceto the gene - for example whether gene products orpromoters are being referenced.
Figure 1 containsan example screenshot.3 Scope of the DataComplex nominals have long been held to be a com-mon feature in scientific text.
The corpus of Vlachosand Gasperin (2006) contains 80 abstracts (600 sen-tences) annotated with gene names.
In this data-set,noun phrases that contain gene names (excludingpost-modifiers) of 3 words or more comprise morethan 40% of the data and exhibit primarily: strings ofpremodifiers tudor mutant females, zygotic Dnop5expression; genitives: Robo ?s cytoplasmic domain,the rdgB protein ?s amino terminal 281 residues; co-ordination the copia and mdg-1 elements and par-enthetical apposition the female-specic gene Sexlethal ( Sxl ), and the SuUR (suppressor of under-replication) gene.
Only 41% of the baseNPs con-taining a gene name consist of one token only.
16%have two tokens.
The two token baseNPs includelarge numbers of combinations of gene names withmore general words such as Ras activity, vnd mu-tants, Xiro expression, IAP localization and vasaprotein.
In general, the gene name appears in mod-ifier position although species modifiers are com-mon, such as Drosophila Tsg, and there are otherpossibilities: truncated p85.Our intention is to categorize this data using theconcept of ?baseNP?
and build effective computa-tional models for recognizing instances.
AlthoughbaseNP is a reasonably stable linguistic concept,its application to a new data-set is not completelystraightforward.
Ramshaw and Marcus (1995) statethat a baseNP aims ?to identify essentially the ini-tial portions of nonrecursive noun phrases up to thehead, including determiners but not including post-modifying prepositional phrases or clauses?.
How-ever, work on baseNPs has essentially always pro-ceeded via algorithmic extraction from fully parsedcorpora such as the Penn Treebank.
BaseNPs havetherefore depended on particular properties of theannotation framework and this leads to certain as-pects of the class appearing unnatural.The clearest case is single element conjunction,which Penn Treebank policy dictates is annotatedat word-level with a flat structure like this [lpl andxsl] (brackets indicate baseNP boundaries).
As soonas one of the elements is multi-word however, thenseparate structures are to be identified [lpl] and [thesxl gene].
The dependency on numbers of tokensbecomes clearly problematic in the bio-medical do-main.
Quite different structures will be identifiedfor lpl and fasciclin, lpl and fasciclin 1 and possiblylpl and fasciclin-1, depending on how tokenizationtreats hyphens.
Furthermore, nothing here dependson the motivating idea of ?initial segments up to thehead?.
In order to provide a more natural class, ourguidelines are that unless there is a shared modifierto account for (as in [embryonic lgl and sxg]), all co-ordinations are split into separate baseNPs.
All othercases of coordination follow the standard guidelinesof the Penn Treebank.165A second difficult case is possessives.
BaseNP ex-traction algorithms generally split possessives likethis: [fra] [?s ectodomain], corresponding (some-what) to an intuition that there are two NPs whilstassigning each word to some baseNP chunk andnot introducing recursiveness.
This policy howevercauses a sharp division between this case and the fraectodomain following the Penn Treebank bracketingguideline that nominal modifiers are never labelled.Since our interest is ?the smallest larger NP con-taining a gene name?, we find it much more natu-ral to treat fra?s as just another modifier of the headectodomain.
Whether it recursively contains a sin-gle word NP fra (or just a single word NNP) is againnot something that is motivated by the idea of ?ini-tial segments up to the head?.
Similarly, we markone baseNP in the rdgB protein?s amino terminal281 residues, viz.
the rdgB protein.Apposition, as in Sex lethal ( Sxl ) and the gene sexlethal , is a further interesting case.
In the first case,?Sex lethal?
and ?Sxl?
stand in apposition.
Both aregene names.
The former is the head.
In the sec-ond, ?gene?
is the head and ?sex lethal?
is a namethat stands in apposition.
In each case, we have ahead and post-modifiers which are neither clausalnor prepositional.
It is unclear whether the rubric?clausal or prepositional?
in Ramshaw and Marcus?statement of intent is merely illustrative or defini-tive.
On the grounds that a sharp division betweenthe non-parenthetical case the gene sex lethal and thepre-modifier the sex lethal gene is unnatural, our in-tuition is that the baseNP does cover all 4 tokens inthis case.
All (post-head) parentheticals are howeverto be treated more like optional adjuncts and there-fore not included with the head to which they attach.In order to verify the reliability of baseNP an-notation, two computational linguists (re)annotatedthe 600 sentences (6300 tokens) of Vlachos andGasperin (2006) with baseNPs and heads using thepublished guidelines.
We added material concern-ing head annotation.
Vlachos and Gasperin didnot quote agreement scores for baseNP annotation.Their interest was directed at gene name agreementbetween a linguist and a biologist.
Our 2-personinter-annotator Kappa scores were 0.953 and 0.988on head and baseNP annotation respectively repre-senting substantial agreement.1 .4 MethodologyA reasonable and simple baseline system for ex-tracting baseNPs that contain a gene name is to usean off-the-shelf baseNP extractor and simply filterthe results for those that contain a gene name.
Tosimplify analysis of results, except where otherwisenoted this filter and subsequent uses of NER arebased on a gold standard gene name annotation.
Inthis way, the contributions of different componentscan be compared without factoring in relative errorsof NER.
Naturally, in the live system, an automatedNER process is used (Vlachos and Gasperin, 2006).For the baseline we chose an implementation of theRamshaw and Marcus baseNP detector distributedwith GATE2 pipelined with the Stanford maximumentropy part of speech tagger 3.
The Stanford tag-ger is a state of the art tagger incorporating a num-ber of features including use of tag contexts, lexicalfeatures, a sophisticated smoothing technique, andfeatures for unknown words (including 4-gram pre-fixes and suffixes).
Both components of the base-line systems utilize the 48 tag Penn Treebank tagset.Results however showed that poor performance ofthe part of speech tagger could have a disastrous ef-fect on baseNP detection.
A simple extension of thebaseline is to insert a module in between POS tag-ging and NP detection.
This module revises the POStags from the tagger in the light of NER results, es-sentially updating the tags of tokens that are part ofnamed entities.
This is essentially a simple versionof the strategy mooted by Toutanova at el (2003) thatthe traditional order of NER and tagging be reversed.It is simpler because, in a maximum entropy frame-work, NER results can function as one extra fea-ture amongst many in POS detection; whereas hereit functions merely as an override.
Retraining thetagger did not form part of our current exploration.1In fact, although the experiment can be considered a classi-fication of 6300 tokens in IOB format, the counting of classifi-cations is not completely straightforward.
The task was ?anno-tate the baseNP surrounding each gene name?
rather than ?an-notate each token?.
In principle, each token is examined; inpractice a variable number is examined.
If we count all tokensclassified into NPs plus one token of context either side, thenboth annotators annotated over 930 tokens.2http://www.gate.ac.uk3http://nlp.stanford.edu/software/tagger.shtml166We adopted a similar strategy with the domain in-dependent full parsing system RASP (Briscoe et al,2006).
RASP includes a simple 1st order HMM POStagger using 149 of the CLAWS-2 tagset.
The taggeris trained on the manually corrected subsets of the(general English) Susanne, LOB and BNC corpora.The output of the tagger is a distribution of possi-ble tags per token (all tags that are at least 1/50 asprobable as the top tag; but only the top tag if morethan 90% probable).
The tagger also includes an un-known word handling module for guessing the pos-sible tags of unknown words.
The RASP parser isa probabilistic LALR(1) parser over the CLAWS-2tags, or, more precisely, a unification grammar for-malism whose lexical categories are feature baseddescriptions of those tags.
The parser has no accessto lexical information other than that made availableby the part of speech tags.
Although the output ofRASP is a full parse (or a sequence of fragments, ifno connected parse can be found) and baseNPs maynot be constituents of NPs, baseNPs can be extractedalgorithmically from the full parse.Some more interesting pre-parsing integrationstrategies are available with RASP because it doesnot demand a deterministic choice of tag for eachword.
We experimented with both a deterministicre-write strategy (as for the baseline system) andwith various degrees of interpolation; for example,adjusting the probability distribution over tags sothat proper noun tags receive 50% of the probabil-ity mass if the token is recognized by NER, andthe other tags receive the remaining 50% in directproportion to the amount they would receive fromthe POS tagger alone.
In this set-up, the NER re-sults need not function simply as an override, butequally they do not function simply as a feature foruse in part of speech tagging.
Rather, the parser maybe able to select a best parse which makes use ofa sequence of tags which is not itself favoured bythe tagger alone.
This allows some influence to thegrammatical context surrounding the gene name andmay also permit tags within phrasal names such astransforming growth factor to propagate.RASP is also a non-deterministic parser and con-sequently a further possible integration strategy isto examine the output n-best list of parses to findbaseNPs, rather than relying on simply the 1-bestoutput.
The n-best parses are already scored accord-ing to a probabilistic model trained on general text.Our strategy is to re-score them using the additionalknowledge source of domain specific NER.
We ex-plored a number of re-scoring hypotheses.
First, acut-off of 20 on n-best lists was found to be optimal.That is, correct analyses tended to either be in the top20 or else not in the top 100 or even 1000.
Secondly,differences in score between the incorrect 1-best andthe correct nth hypothesis were not a very reliableindicator of ?almost right?.
This is not surprising asthe scores are probabilities calculated over the com-plete analysis, whereas our focus is one small partof it.
Consequently, the re-scoring system uses theprobabilistic model just to generate the top 20 anal-yses; and those analyses are then re-scored using 3features.
Analyses that concur with NER in havinga named entity within an NP receive a reward of +1.Secondly, NP analyses that contain N+1 genes (asin a co-ordination) receive a score of +N, so longas the NP is single headed.
For example, ?gurkenor torpedo females?
will receive a preferred analy-sis in which ?gurken?
and ?torpedo?
are both mod-ifiers of ?females?.
The ?single headedness?
con-straint rules out very unlikely NP analyses that theparser can return as legal possibilities.
Finally, anal-yses receive a score of -1 if the NP contains a deter-miner but the head of the NP is a gene name.
Thetop 20 parses may include analyses in which, for ex-ample, ?the hypothesis that phenylalanine hydroxy-lase?
contains ?that phenylalanine hydroxylase?
asan NP constituent.Finally, we also experimented with using both thefull parsing and shallow baseNP spotter together;here, the idea is simply that when two analyses over-lap, then the analysis from full parsing should bepreferred on the grounds that it has more informa-tion available to it.
However, if the shallow spotterdetects an analysis when full parsing detects nonethen this is most likely because full parsing has beenled astray rather than it has discovered a more likelyanalysis not involving any baseNP.5 Experimental ResultsTable 1 gives the precision, recall and (harmonic) F-score measures for the baseline NP system with andwithout the extra pre-parsing retagging module; andtable 2 gives similar figures for the generic full pars-167ing system.
Scores for the left boundary only, rightboundary only and full extent (?correct?)
are shown.The extra retagging module (i.e.
override tagger re-sults, given NER results) improves results in bothsystems and by similar amounts.
This is nearly al-ways on account of gene names being mis-taggedas verbal which leads to their exclusion from the setof baseNP chunks.
The override mechanism is ofcourse a blunt instrument and only affects the tagsof tokens within gene names and not those in its sur-rounding context.Table 3 shows the results from interpolating thePOS tag distribution P with the NER distributionN linearly using different levels of ?.
For example,?
= 1.00 is the simple retagging approach in whichall the probability is assigned to the NER suggestedtag; whereas ?
= 0.25 means that only 25% is allo-cated by NER.
The figures shown are for one variantof the full parsing system which included n-best se-lection but other variants showed similar behaviour(data not shown).
The results from interpolationshow that the extra information available in the parsedoes not prove valuable overall.
Decreasing valuesof ?
lead to decreases in performance.
These resultscan be interpreted as similar in kind to Charniak etal (1996) who found that a parser using multiplePOS tag inputs could not improve on the tag accu-racy of a tagger outputting single POS tags.
Ourresults differ in that the extra tag possibilities are de-rived from an alternative knowledge source and ourmeasurement is baseNP detection.
Nevertheless theconclusion may be that the best way forward hereis a much tighter integration between NER and POStagging itself.POS tagging errors naturally affect the perfor-mance of both shallow and full parsing systems,though not necessarily equally.
For example, thetagger in the shallow system tags ectopic as a verbin vnd-expression leads to ectopic Nk6 expressionand this is not corrected by the retagging module be-cause ectopic is not part of the gene name.
Conse-quently the baseNP spotter is led into a left bound-ary error.
Nevertheless, the distribution of baseNPsfrom the two systems do appear to be complemen-tary in a rather deeper fashion.
Analysis of the re-sults indicates that parentheticals in pre-modifier po-sitions appears to throw the shallow parser severelyoff course.
For example, it generates the analysisR P Fretag+shallow(correct) 80.21 75.92 78.01(left b) 92.40 87.46 89.86(right b) 90.81 85.95 88.32shallow only(correct) 74.03 76.32 75.16(left b) 84.28 86.89 85.56(right b) 82.69 85.25 83.95Table 1: Generic shallow parsingR P Fretag+full(correct) 80.92 84.81 82.82(left b) 85.69 89.81 87.70(right b) 88.69 92.96 90.78full only(correct) 75.44 85.23 80.04(left b) 80.21 90.62 85.10(right b) 82.51 93.21 87.54Table 2: Generic full parsing[the transforming growth factor-beta] ( [ TGF-beta] ) superfamily.
Also, appositions such as the humanauto antigen La and the homeotic genes abdominalA and abdominal B cause problems.
In these kindsof case, the full parser detects the correct analysis.On the other hand, the extraction of baseNPs fromgrammatical relations relies in part on the parseridentifying a head correctly (for example, via a non-clausal subject relation).
The shallow parser doesnot however rely on this depth of analysis and maysucceed in such cases.
There are also cases wherethe full parser fails to detect any analysis at all.System (correct) (left b) (right b)?=0.25 83.97 88.34 90.71?=0.50 84.16 88.69 91.22?=0.80 85.18 89.67 91.28?=1.00 85.38 89.87 91.66Table 3: F-scores for baseNP detection for various ?Table 4 indicates the advantages to be gained in n-best selection.
The entries for full and retag+full arerepeated from table 2 for convenience.
The entries168System R P Fretag+full 80.92 84.81 82.82retag+full+sel 83.22 87.22 85.17retag+full+oracle 85.87 90.17 87.96full 75.44 85.23 80.04full+sel 78.80 86.60 82.52full+oracle 81.63 89.88 85.56Table 4: Effects of n-best selectionfor full+sel and retag+full+sel show the effect ofadding n-best selection.
The entries for full+oracleand retag+full+oracle show the maximum achiev-able performance by replacing the actual selectionpolicy with an oracle that always chooses the cor-rect hypothesis, if it is available.
The results arethat, regardless of whether a retagging policy isadopted, an oracle which selects the best analysiscan achieve an error reduction of well over 25%.Furthermore, the simple selection policy outlinedbefore succeeds in achieving almost half the pos-sible error reduction available.
This result is par-ticularly interesting because it demonstrates that theextra knowledge source available in this baseNP de-tection task (namely NER) can profitably be broughtto bear at more than one stage in the overall process-ing pipeline.
Even when NER has been used to im-prove the sequence of POS tags given to the parser,it can profitably be exploited again when selectingbetween parses.The complementary nature of the two systems isrevealed in Table 5 which shows the effects of inte-grating the two parsers.
baseNPs from the shallowparser are accepted whenever it hypothesizes oneand there is no competing overlapping baseNP fromthe full parser.
Note that this is rather different fromthe standard method of simply selecting between ananalysis from the one parser and one from another.The success of this policy reflects the fact that thereremain several cases where the full parser fails todeliver ?apparently?
simple baseNPs either becausethe tagger has failed to generate a suitable hypoth-esis, or because parsing itself fails to find a goodenough analysis in the time available to it.Overall, the best results (87.17% F-score) are ob-tained by applying NER results both before parsingthrough the update of POS tags and after it in se-System R P F1-best 85.69 84.35 85.01n-best 87.63 86.71 87.17oracle 90.28 89.49 89.89Table 5: Combining shallow and full parsinglection from n-best lists; and by combining the re-sults of both full parsing in order to improve analy-sis of more complex structures and shallow parsingas a back-off strategy.
The same strategy applied us-ing our automated gene name recognizer results ina F-score of 73.6% F-score, which is considerablyless of course, although the gene name recognizeritself operates at 82.5% F-Score, with similar preci-sion and recall figures.
This naturally limits the pos-sible performance of our baseNP recognition task.Encouragingly, the ?lost?
performance (just under11%) is actually less in this scenario than when genename recognition is perfect.6 Previous WorkThe lack of clarity between noun phrase extents andnamed entity extents and its impact on evaluationand training data for NER has been noted previ-ously, e.g.
for proteins (Mani et al, 2005).
Vla-chos and Gasperin (2006) claim that their name ver-sus mention distinction was helpful in understand-ing disagreements over gene name extents and thisled, through greater clarity of intended coverage, toimproved NER.
BaseNP detectors have also beenused more directly in building NER systems.
Ya-mamoto et al(2003) describe an SVM approach toprotein name recognition, one of whose features isthe output of a baseNP recognizer.
BaseNP recogni-tion supplies a top-down constraint for the search forprotein names within a baseNP.
A similar approachalbeit in a CRF framework is described in Song etal.
(2005).The concept of baseNP has undergone a numberof revisions (Ramshaw and Marcus, 1995; TjongKim Sang and Buchholz, 2000) but has previouslyalways been tied to extraction from a more com-pletely annotated treebank, whose annotations aresubject to other pressures than just ?initial materialup to the head?.
To our knowledge, our figures forinter-annotator agreement on the baseNP task itself169(i.e.
not derived from a larger annotation task) arethe first to be reported.
Quality measures can beindirectly inferred from a treebank complete anno-tation, but baseNP identification is probably a sim-pler task.
Doddington et al(2004) report an ?overallvalue score of 86?
for inter-annotator agreement inACE; but this is a multi-component evaluation usinga complete noun phrase, but much else besides.Improving results through the combination of dif-ferent systems has also been a topic of previouswork in baseNP detection.
For example, Sang et al(2000) applied majority voting to the top five ma-chine learning algorithms from a sample of sevenand achieved a baseNP recognition rate that ex-ceeded the recognition rates of any of the individualmethods.7 ConclusionWe have motivated the task of detecting baseNPsthat contain a given named entity as a task both ofinterest from the standpoint of use within a particu-lar application and on more general grounds, as anintermediate point between the task of general NPchunking and domain specific NER.We have explored a variety of methods for under-taking baseNP detection using only domain specificNER in addition to otherwise general purpose lin-guistic processors.
In particular, we have exploredboth shallow and full parsing general purpose sys-tems and demonstrated that the domain specific re-sults of NER can be applied profitably not only atdifferent stages in the language processing pipelinebut also more than once.
The best overall recogni-tion rates were obtained by a combination of bothshallow and full parsing systems with knowledgefrom NER being applied both before parsing, at thestage of part of speech detection and after parsing,during parse selection.ReferencesE.J.
Briscoe, J. Carroll, and R. Watson.
2006.
The sec-ond release of the rasp system.
Proc.
Coling/ACL2006 Interactive Sessions.E.
Charniak, G. Carroll, J. Adcock, A.R.
Cassandra,Y.
Gotoh, J. Katz, M.L.
Littman, and J. McCann.1996.
Taggers for parsers.
Artificial Intelligence,85(1-2):45?57.M.A.
Crosby, Goodman J.L., Strelets V.B., P. Zhang,W.M.
Gelbart, and the FlyBase Consortium.
2007.Flybase: genomes by the dozen.
Nucleic Acids Re-search, 35:486?491.Shipra Dingare, Malvina Nissim, Jenny Finkel, Christo-pher Manning, and Claire Grover.
2005.
A system foridentifying named entities in biomedical text: how re-sults from two evaluations reflect on both the systemand the evaluations:.
Comp.
Funct.
Genomics, 6(1-2):77?85.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassel, and R. Weischedel.
2004.
Automatic con-tent extraction (ace) program - task definitions and per-formance measures.
In Proceedings of LREC 2004.C.
Gasperin.
2006.
Semi-supervised anaphora resolu-tion in biomedical texts.
In Proceedings of BIONLP inHLT-NAACL06, New York, pages 96?103.R.
Hoffman and A. Valencia.
2004.
A gene network fornavigating the literature.
Nature Genetics, 36:664.I.
Mani, Z. Hu, S.B.
Jang, K. Samuel, M. Krause,J.
Phillips, and C.H.
Wu.
2005.
Protein name taggingguidelines: lessons learned.
Comparative and Func-tional Genomics, 6(1-2):72?76.L.A.
Ramshaw and M.P Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proceedingsof the Third Annual Workshop on Very Large Corpora,pages 82?94.
ACL.Y.
Song, G. Kim, E. ad Lee, and B. Yi.
2005.
Posbiotm-ner: a trainable biomedical named-entity recognitionsystem.
Bioinformatics, 21(11):2794?2796.E.F.
Tjong Kim Sang and S. Buchholz.
2000.
Introduc-tion to the conll-2000 shared task: Chunking.
In Pro-ceedings of CoNLL-2000 and LLL-2000.Erik F. Tjong Kim Sang, Walter Daelemans, Herve?De?jean, Rob Koeling, Yuval Krymolowski, Vasin Pun-yakanok, and Dan Roth.
2000.
Applying system com-bination to base noun phrase identification.
In COL-ING 2000, pages 857?863.
Saarbruecken, Germany.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part of speechtagging with a cyclic dependency network.
In HLT-NAACL, pages 252?259.A.
Vlachos and C. Gasperin.
2006.
Bootstrapping andevaluating named entity recognition in the biomed-ical domain.
In Proceedings of BIONLP in HLT-NAACL06, New York.K.
Yamamoto, T. Kudo, T. Konagaya, and Y. Matsumoto.2003.
Protein name tagging for biomedical anno-tation in text.
In ACL 2003 Workshop on NLP inBiomedicine.170
