Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 176?183,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsParsing and Generation as Datalog QueriesMakoto KanazawaNational Institute of Informatics2?1?2 Hitotsubashi, Chiyoda-ku, Tokyo, 101?8430, Japankanazawa@nii.ac.jpAbstractWe show that the problems of parsing and sur-face realization for grammar formalisms with?context-free?
derivations, coupled with Mon-tague semantics (under a certain restriction) canbe reduced in a uniform way to Datalog queryevaluation.
As well as giving a polynomial-time algorithm for computing all derivation trees(in the form of a shared forest) from an in-put string or input logical form, this reductionhas the following complexity-theoretic conse-quences for all such formalisms: (i) the de-cision problem of recognizing grammaticality(surface realizability) of an input string (logicalform) is in LOGCFL; and (ii) the search prob-lem of finding one logical form (surface string)from an input string (logical form) is in func-tional LOGCFL.
Moreover, the generalized sup-plementary magic-sets rewriting of the Datalogprogram resulting from the reduction yields ef-ficient Earley-style algorithms for both parsingand generation.1 IntroductionThe representation of context-free grammars (aug-mented with features) in terms of definite clause pro-grams is well-known.
In the case of a bare-boneCFG, the corresponding program is in the function-free subset of logic programming, known as Dat-alog.
For example, determining whether a stringJohn found a unicorn belongs to the language of theCFG in Figure 1 is equivalent to deciding whetherthe Datalog program in Figure 2 together with thedatabase in (1) can derive the query ??
?S(0, 4).?
(1) John(0, 1).
found(1, 2).
a(2, 3).
unicorn(3, 4).S ?
NP VPVP ?
V NPV ?
V Conj VNP ?
Det NNP ?
JohnV ?
foundV ?
caughtConj ?
andDet ?
aN ?
unicornFigure 1: A CFG.S(i, j) :?
NP(i, k),VP(k, j).VP(i, j) :?
V(i, k),NP(k, j).V(i, j) :?
V(i, k),Conj(k, l),V(l, j).NP(i, j) :?
Det(i, k),N(k, j).NP(i, j) :?
John(i, j).V(i, j) :?
found(i, j).V(i, j) :?
caught(i, j).Conj(i, j) :?
and(i, j).Det(i, j) :?
a(i, j).N(i, j) :?
unicorn(i, j).Figure 2: The Datalog representation of a CFG.By naive (or seminaive) bottom-up evaluation(see, e.g., Ullman, 1988), the answer to such a querycan be computed in polynomial time in the size ofthe database for any Datalog program.
By recordingrule instances rather than derived facts, a packed rep-resentation of the complete set of Datalog derivationtrees for a given query can also be obtained in poly-nomial time by the same technique.
Since a Data-log derivation tree uniquely determines a grammarderivation tree, this gives a reduction of context-freerecognition and parsing to query evaluation in Data-log.In this paper, we show that a similar reductionto Datalog is possible for more powerful grammarformalisms with ?context-free?
derivations, such as(multi-component) tree-adjoining grammars (Joshiand Schabes, 1997; Weir, 1988), IO macro gram-mars (Fisher, 1968), and (parallel) multiple context-free grammars (Seki et al, 1991).
For instance, theTAG in Figure 3 is represented by the Datalog pro-gram in Figure 4.
Moreover, the method of reduc-176SA?ANAa Ab A?NA cdFigure 3: A TAG with one initial tree (left) and oneauxiliary tree (right)S(p1, p3) :?
A(p1, p3, p2, p2).A(p1, p8, p4, p5) :?
A(p2, p7, p3, p6), a(p1, p2), b(p3, p4),c(p5, p6), d(p7, p8).A(p1, p2, p1, p2).Figure 4: The Datalog representation of a TAG.tion extends to the problem of tactical generation(surface realization) for these grammar formalismscoupled with Montague semantics (under a certainrestriction).
Our method essentially relies on the en-coding of different formalisms in terms of abstractcategorial grammars (de Groote, 2001).The reduction to Datalog makes it possible to ap-ply to parsing and generation sophisticated evalu-ation techniques for Datalog queries; in particular,an application of generalized supplementary magic-sets rewriting (Beeri and Ramakrishnan, 1991) au-tomatically yields Earley-style algorithms for bothparsing and generation.
The reduction can alsobe used to obtain a tight upper bound, namelyLOGCFL, on the computational complexity of theproblem of recognition, both for grammaticality ofinput strings and for surface realizability of inputlogical forms.With regard to parsing and recognition of in-put strings, polynomial-time algorithms and theLOGCFL upper bound on the computational com-plexity are already known for the grammar for-malisms covered by our results (Engelfriet, 1986);nevertheless, we believe that our reduction to Data-log offers valuable insights.
Concerning generation,our results seem to be entirely new.12 Context-free grammars on ?-termsConsider an augmentation of the grammar in Fig-ure 1 with Montague semantics, where the left-hand1We only consider exact generation, not taking into accountthe problem of logical form equivalence, which will most likelyrender the problem of generation computationally intractable(Moore, 2002).S(X1X2) ?
NP(X1) VP(X2)VP(?x.X2(?y.X1yx)) ?
V(X1) NP(X2)V(?yx.X2(X1yx)(X3yx)) ?
V(X1) Conj(X2) V(X3)NP(X1X2) ?
Det(X1) N(X2)NP(?u.u Johne) ?
JohnV(finde?e?t) ?
foundV(catche?e?t) ?
caughtConj(?t?t?t) ?
andDet(?uv.?(e?t)?t(?y.
?t?t?t(uy)(vy))) ?
aN(unicorne?t) ?
unicornFigure 5: A context-free grammar with Montaguesemantics.SNPJohnVPVfoundNPDetaNunicornFigure 6: A derivation tree.side of each rule is annotated with a ?-term that tellshow the meaning of the left-hand side is composedfrom the meanings of the right-hand side nontermi-nals, represented by upper-case variables X1, X2, .
.
.
(Figure 5).2The meaning of a sentence is computed from itsderivation tree.
For example, John found a unicornhas the derivation tree in Figure 6, and the grammarrules assign its root node the ?-term(?u.u John)(?x.(?uv.?(?y.?
(uy)(vy))) unicorn (?y.find y x)),which ?-reduces to the ?-term(2) ?(?y.?
(unicorn y)(find y John))encoding the first-order logic formula representingthe meaning of the sentence (i.e., its logical form).Thus, computing the logical form(s) of a sentenceinvolves parsing and ?-term normalization.
To find asentence expressing a given logical form, it suffices2We follow standard notational conventions in typed ?-calculus.
Thus, an application M1M2M3 (written without paren-theses) associates to the left, ?x.
?y.M is abbreviated to ?xy.M,and ??
??
?
stands for ??
(??
?).
We refer the readerto Hindley, 1997 or S?rensen and Urzyczyn, 2006 for standardnotions used in simply typed ?-calculus.177S(X1X2) :?
NP(X1),VP(X2).VP(?x.X2(?y.X1yx)) :?
V(X1),NP(X2).V(?yx.X2(X1yx)(X3yx)) :?
V(X1),Conj(X2),V(X3).NP(X1X2) :?
Det(X1),N(X2).NP(?u.u Johne).V(finde?e?t).V(catche?e?t).Conj(?t?t?t).Det(?uv.?(e?t)?t(?y.
?t?t?t(uy)(vy))).N(unicorne?t).Figure 7: A CFLG.to find a derivation tree whose root node is associ-ated with a ?-term that ?-reduces to the given log-ical form; the desired sentence can simply be readoff from the derivation tree.
At the heart of bothtasks is the computation of the derivation tree(s) thatyield the input.
In the case of generation, this may beviewed as parsing the input ?-term with a ?context-free?
grammar that generates a set of ?-terms (innormal form) (Figure 7), which is obtained from theoriginal CFG with Montague semantics by strippingoff terminal symbols.
Determining whether a givenlogical form is surface realizable with the originalgrammar is equivalent to recognition with the result-ing context-free ?-term grammar (CFLG).In a CFLG such as in Figure 7, constants appear-ing in the ?-terms have preassigned types indicatedby superscripts.
There is a mapping ?
from nonter-minals to their types (?
= {S 7?
t,NP 7?
(e?
t)?t,VP 7?
e?t,V 7?
e?e?t,Conj 7?
t?t?t,Det 7?(e?t)?
(e?t)?t,N 7?
e?t}).
A rule that has A onthe left-hand side and B1, .
.
.
, Bn as right-hand sidenonterminals has its left-hand side annotated with awell-formed ?-term M that has type ?
(A) under thetype environment X1 :?
(B1), .
.
.
, Xn :?
(Bn) (in sym-bols, X1 : ?
(B1), .
.
.
, Xn : ?
(Bn) ` M : ?
(A)).What we have called a context-free ?-term gram-mar is nothing but an alternative notation for an ab-stract categorial grammar (de Groote, 2001) whoseabstract vocabulary is second-order, with the restric-tion to linear ?-terms removed.3 In the linear case,Salvati (2005) has shown the recognition/parsingcomplexity to be PTIME, and exhibited an algorithmsimilar to Earley parsing for TAGs.
Second-order3A ?-term is a ?I-term if each occurrence of ?
binds at leastone occurrence of a variable.
A ?I-term is linear if no subtermcontains more than one free occurrence of the same variable.S(?y.X1(?z.z)y) :?
A(X1).A(?xy.ao?o(X1(?z.bo?o(x(co?oz)))(do?oy))) :?
A(X1).A(?xy.xy).Figure 8: The CFLG encoding a TAG.linear ACGs are known to be expressive enough toencode well-known mildly context-sensitive gram-mar formalisms in a straightforward way, includ-ing TAGs and multiple context-free grammars (deGroote, 2002; de Groote and Pogodalla, 2004).For example, the linear CFLG in Figure 8 is anencoding of the TAG in Figure 3, where?
(S) = o?oand ?
(A) = (o?
o)?
o?
o (see de Groote, 2002for details of this encoding).
In encoding a string-generating grammar, a CFLG uses o as the type ofstring position and o?
o as the type of string.
Eachterminal symbol is represented by a constant of typeo?o, and a string a1 .
.
.
an is encoded by the ?-term?z.ao?o1 (.
.
.
(ao?on z) .
.
.
), which has type o?
o.A string-generating grammar coupled with Mon-tague semantics may be represented by a syn-chronous CFLG, a pair of CFLGs with matchingrule sets (de Groote 2001).
The transduction be-tween strings and logical forms in either directionconsists of parsing the input ?-term with the source-side grammar and normalizing the ?-term(s) con-structed in accordance with the target-side grammarfrom the derivation tree(s) output by parsing.3 Reduction to DatalogWe show that under a weaker condition than linear-ity, a CFLG can be represented by a Datalog pro-gram, obtaining a tight upper bound (LOGCFL) onthe recognition complexity.
Due to space limitation,our presentation here is kept at an informal level;formal definitions and rigorous proof of correctnesswill appear elsewhere.We use the grammar in Figure 7 as an example,which is represented by the Datalog program in Fig-ure 9.
Note that all ?-terms in this grammar are al-most linear in the sense that they are ?I-terms whereany variable occurring free more than once in anysubterm must have an atomic type.
Our constructionis guaranteed to be correct only when this conditionis met.Each Datalog rule is obtained from the corre-sponding grammar rule in the following way.
Let178S(p1) :?
NP(p1, p2, p3),VP(p2, p3).VP(p1, p4) :?
V(p2, p4, p3),NP(p1, p2, p3).V(p1, p4, p3) :?V(p2, p4, p3),Conj(p1, p5, p2),V(p5, p4, p3).NP(p1, p4, p5) :?
Det(p1, p4, p5, p2, p3),N(p2, p3).NP(p1, p1, p2) :?
John(p2).V(p1, p3, p2) :?
find(p1, p3, p2).V(p1, p3, p2) :?
catch(p1, p3, p2).Conj(p1, p3, p2) :?
?
(p1, p3, p2).Det(p1, p5, p4, p3, p4) :?
?
(p1, p2, p4),?
(p2, p5, p3).N(p1, p2) :?
unicorn(p1, p2).Figure 9: The Datalog representation of a CFLG.M be the ?-term annotating the left-hand side of thegrammar rule.
We first obtain a principal (i.e., mostgeneral) typing of M.4 In the case of the second rule,this isX1 : p3?
p4?
p2, X2 : (p3?
p2)?
p1 `?x.X2(?y.X1yx) : p4?
p1.We then remove ?
and parentheses from the typesin the principal typing and write the resulting se-quences of atomic types in reverse.5 We obtain theDatalog rule by replacing Xi and M in the grammarrule with the sequence coming from the type pairedwith Xi and M, respectively.
Note that atomic typesin the principal typing become variables in the Data-log rule.
When there are constants in the ?-term M,they are treated like free variables.
In the case of thesecond-to-last rule, the principal typing is?
: (p4?
p2)?
p1, ?
: p3?
p5?
p2 `?uv.?(?y.?
(uy)(vy)) : (p4?
p3)?
(p4?
p5)?
p1.If the same constant occurs more than once, distinctoccurrences are treated as distinct free variables.The construction of the database representing theinput ?-term is similar, but slightly more complex.A simple case is the ?-term (2), where each constantoccurs just once.
We compute its principal typing,treating constants as free variables.6?
: (4?
2)?
1, ?
: 3?
5?
2,unicorn : 4?
3, find : 4?
6?
5 , John : 6` ?(?y.?
(unicorn y)(find y John)) : 1.4To be precise, we must first convert M to its ?-long formrelative to the type assigned to it by the grammar.
For example,X1X2 in the first rule is converted to X1(?x.X2x).5The reason for reversing the sequences of atomic types isto reconcile the ?-term encoding of strings with the conventionof listing string positions from left to right in databases like (1).6We assume that the input ?-term is in ?-long normal form.We then obtain the corresponding database (3) andquery (4) from the antecedent and succedent of thisjudgment, respectively.
Note that here we are using1, 2, 3, .
.
.
as atomic types, which become databaseconstants.?
(1, 2, 4).
?
(2, 5, 3).
unicorn(3, 4).find(5, 6, 4).
John(6).(3)??S(1).
(4)When the input ?-term contains more than one oc-currence of the same constant, it is not always cor-rect to simply treat them as distinct free variables,unlike in the case of ?-terms annotating grammarrules.
Consider the ?-term (5) (John found andcaught a unicorn):(5) ?(?y.?
(unicorn y)(?
(find y John)(catch y John))).Here, the two occurrences of John must be treatedas the same variable.
The principal typing is (6) andthe resulting database is (7).?
: (4?
2)?
1, ?1 : 3?
5?
2,unicorn : 4?
3, ?2 : 6?
8?
5,find : 4?
7?
6, John : 7, catch : 4?
7?
8` ?(?y.
?1(unicorn y)(?2(find y John)(catch y John))) : 1.(6)?
(1, 2, 4).
?
(2, 5, 3).
?
(5, 8, 6).
unicron(3, 4).find(6, 7, 4).
John(7).
catch(8, 7, 4).
(7)It is not correct to identify the two occurrences of?
in this example.
The rule is to identify distinctoccurrences of the same constant just in case theyoccur in the same position within ?-equivalent sub-terms of an atomic type.
This is a necessary con-dition for those occurrences to originate as one andthe same occurrence in the non-normal ?-term at theroot of the derivation tree.
(As a preprocessing step,it is also necessary to check that distinct occurrencesof a bound variable satisfy the same condition, sothat the given ?-term is ?-equal to some almost lin-ear ?-term.7)7Note that the way we obtain a database from an input?-term generalizes the standard database representation of astring: from the ?-term encoding ?z.ao?o1 (.
.
.
(ao?on z) .
.
. )
of astring a1 .
.
.
an, we obtain the database {a1(0, 1), .
.
.
, an(n?1, n)}.1794 Correctness of the reductionWe sketch some key points in the proof of cor-rectness of our reduction.
The ?-term N obtainedfrom the input ?-term by replacing occurrences ofconstants by free variables in the manner describedabove is the normal form of some almost linear ?-term N?.
The leftmost reduction from an almost lin-ear ?-term to its normal form must be non-deletingand almost non-duplicating in the sense that whena ?-redex (?x.P)Q is contracted, Q is not deleted,and moreover it is not duplicated unless the typeof x is atomic.
We can show that the Subject Ex-pansion Theorem holds for such ?-reduction, so theprincipal typing of N is also the principal typing ofN?.
By a slight generalization of a result by Aoto(1999), this typing ?
` N?
: ?
must be negativelynon-duplicated in the sense that each atomic typehas at most one negative occurrence in it.
By Aotoand Ono?s (1994) generalization of the CoherenceTheorem (see Mints, 2000), it follows that every ?-term P such that ??
` P : ?
for some ??
?
?
must be?
?-equal to N?
(and consequently to N).Given the one-one correspondence between thegrammar rules and the Datalog rules, a Data-log derivation tree uniquely determines a grammarderivation tree (see Figure 10 as an example).
Thisrelation is not one-one, because a Datalog deriva-tion tree contains database constants from the inputdatabase.
This extra information determines a typ-ing of the ?-term P at the root of the grammar deriva-tion tree (with occurrences of constants in the ?-termcorresponding to distinct facts in the database re-garded as distinct free variables):John : 6, find : 4?
6?
5, ?
: (4?
2)?
1,?
: 3?
5?
2, unicorn : 4?
3 `(?u.u John)(?x.(?uv.?(?y.?
(uy)(vy))) unicorn (?y.find y x)) : 1.The antecedent of this typing must be a subset of theantecedent of the principal typing of the ?-term Nfrom which the input database was obtained.
By theproperty mentioned at the end of the preceding para-graph, it follows that the grammar derivation tree isa derivation tree for the input ?-term.Conversely, consider the ?-term P (with distinctoccurrences of constants regarded as distinct freevariables) at the root of a grammar derivation treefor the input ?-term.
We can show that there is asubstitution ?
which maps the free variables of Pto the free variables of the ?-term N used to buildthe input database such that ?
sends the normal formof P to N. Since P is an almost linear ?-term, theleftmost reduction from P?
to N is non-deleting andalmost non-duplicating.
By the Subject ExpansionTheorem, the principal typing of N is also the prin-cipal typing of P?, and this together with the gram-mar derivation tree determines a Datalog derivationtree.5 Complexity-theoretic consequencesLet us call a rule A(M) :?
B1(X1), .
.
.
, Bn(Xn) in aCFLG an ?-rule if n = 0 and M does not contain anyconstants.
We can eliminate ?-rules from an almostlinear CFLG by the same method that Kanazawa andYoshinaka (2005) used for linear grammars, notingthat for any ?
and ?, there are only finitely manyalmost linear ?-terms M such that ?
` M : ?.
If agrammar has no ?-rule, any derivation tree for theinput ?-term N that has a ?-term P at its root nodecorresponds to a Datalog derivation tree whose num-ber of leaves is equal to the number of occurrencesof constants in P, which cannot exceed the numberof occurrences of constants in N.A Datalog program P is said to have the poly-nomial fringe property relative to a class D ofdatabases if there is a polynomial p(n) such that forevery database D in D of n facts and every query qsuch that P?D derives q, there is a derivation tree forq whose fringe (i.e., sequence of leaves) is of lengthat most p(n).
For such P and D, it is known that{ (D, q) | D ?
D,P ?
D derives q } is in the complex-ity class LOGCFL (Ullman and Van Gelder, 1988;Kanellakis, 1988).We state without proof that the database-querypair (D, q) representing an input ?-term N can becomputed in logspace.
By padding D with extra use-less facts so that the size of D becomes equal to thenumber of occurrences of constants in N, we obtaina logspace reduction from the set of ?-terms gener-ated by an almost linear CFLG to a set of the form{ (D, q) | D ?
D,P ?
D ` q }, where P has the poly-nomial fringe property relative to D. This showsthat the problem of recognition for an almost linearCFLG is in LOGCFL.180S(1)NP(1, 1, 6)John(6)VP(1, 6)V(5, 6, 4)find(5, 6, 4)NP(1, 5, 4)Det(1, 5, 4, 3, 4)?
(1, 2, 4) ?
(2, 5, 3)N(3, 4)unicorn(3, 4)S((?u.u John)(?x.(?uv.?(?y.?
(uy)(vy))) unicorn (?y.find y x)))NP(?u.u John) VP(?x.(?uv.?(?y.?
(uy)(vy))) unicorn (?y.find y x)))V(find) NP((?uv.?(?y.?
(uy)(vy))) unicorn)Det(?uv.?(?y.?
(uy)(vy))) N(unicorn)Figure 10: A Datalog derivation tree (left) and the corresponding grammar derivation tree (right)By the main result of Gottlob et al (2002), the re-lated search problem of finding one derivation treefor the input ?-term is in functional LOGCFL, i.e.,the class of functions that can be computed by alogspace-bounded Turing machine with a LOGCFLoracle.
In the case of a synchronous almost linearCFLG, the derivation tree found from the source ?-term can be used to compute a target ?-term.
Thus,to the extent that transduction back and forth be-tween strings and logical forms can be expressed bya synchronous almost linear CFLG, the search prob-lem of finding one logical form of an input sentenceand that of finding one surface realization of an inputlogical form are both in functional LOGCFL.8 As aconsequence, there are efficient parallel algorithmsfor these problems.6 Regular sets of trees as inputAlmost linear CFLGs can represent a substan-tial fragment of a Montague semantics for En-glish and such ?linear?
grammar formalisms as(multi-component) tree-adjoining grammars (bothas string grammars and as tree grammars) and mul-tiple context-free grammars.
However, IO macrogrammars and parallel multiple context-free gram-mars cannot be directly represented because repre-senting string copying requires multiple occurrencesof a variable of type o ?
o.
This problem can besolved by switching from strings to trees.
We con-vert the input string into the regular set of binarytrees whose yield equals the input string (using c8If the target-side grammar is not linear, the normal form ofthe target ?-term cannot be explicitly computed because its sizemay be exponential in the size of the source ?-term.
Neverthe-less, a typing that serves to uniquely identify the target ?-termcan be computed from the derivation tree in logspace.
Also, ifthe target-side grammar is linear and string-generating, the tar-get string can be explicitly computed from the derivation tree inlogspace (Salvati, 2007).as the sole symbol of rank 2), and turn the gram-mar into a tree grammar, replacing all instances ofstring concatenation in the grammar with the treeoperation t1, t2 7?
c(t1, t2).
This way, a string gram-mar is turned into a tree grammar that generates aset of trees whose image under the yield function isthe language of the string grammar.
(In the case ofan IO macro grammar, the result is an IO context-free tree grammar (Engelfriet, 1977).)
String copy-ing becomes tree copying, and the resulting gram-mar can be represented by an almost linear CFLGand hence by a Datalog program.
The regular setof all binary trees that yield the input string is repre-sented by a database that is constructed from a deter-ministic bottom-up finite tree automaton recogniz-ing it.
Determinism is important for ensuring cor-rectness of this reduction.
Since the database canbe computed from the input string in logspace, thecomplexity-theoretic consequences of the last sec-tion carry over here.7 Magic sets and Earley-style algorithmsMagic-sets rewriting of a Datalog program allowsbottom-up evaluation to avoid deriving useless factsby mimicking top-down evaluation of the originalprogram.
The result of the generalized supplemen-tary magic-sets rewriting of Beeri and Ramakrish-nan (1991) applied to the Datalog program repre-senting a CFG essentially coincides with the deduc-tion system (Shieber et al, 1995) or uninstantiatedparsing system (Sikkel, 1997) for Earley parsing.By applying the same rewriting method to Datalogprograms representing almost linear CFLGs, we canobtain efficient parsing and generation algorithmsfor various grammar formalisms with context-freederivations.We illustrate this approach with the programin Figure 4, following the presentation of Ullman181(1989a; 1989b).
We assume the query to take theform ???
S(0, x).
?, so that the input database can beprocessed incrementally.
The program is first madesafe by eliminating the possibility of deriving non-ground atoms:S(p1, p3) :?
A(p1, p3, p2, p2).A(p1, p8, p4, p5) :?
A(p2, p7, p3, p6), a(p1, p2), b(p3, p4), c(p5, p6), d(p7, p8).A(p1, p8, p4, p5) :?
a(p1, p2), b(p2, p4), c(p5, p6), d(p6, p8).The subgoal rectification removes duplicate argu-ments from subgoals, creating new predicates asneeded:S(p1, p3) :?
B(p1, p3, p2).A(p1, p8, p4, p5) :?
A(p2, p7, p3, p6), a(p1, p2), b(p3, p4), c(p5, p6), d(p7, p8).A(p1, p8, p4, p5) :?
a(p1, p2), b(p2, p4), c(p5, p6), d(p6, p8).B(p1, p8, p4) :?
A(p2, p7, p3, p6), a(p1, p2), b(p3, p4), c(p4, p6), d(p7, p8).B(p1, p8, p4) :?
a(p1, p2), b(p2, p4), c(p4, p6), d(p6, p8).We then attach to predicates adornments indicatingthe free/bound status of arguments in top-down eval-uation, reordering subgoals so that as many argu-ments as possible are marked as bound:Sbf(p1, p3) :?
Bbff(p1, p3, p2).Bbff(p1, p8, p4) :?
abf(p1, p2), Abfff(p2, p7, p3, p6), bbf(p3, p4), cbb(p4, p6),dbf(p7, p8).Bbff(p1, p8, p4) :?
abf(p1, p2), bbf(p2, p4), cbf(p4, p6), dbf(p6, p8).Abfff(p1, p8, p4, p5) :?
abf(p1, p2), Abfff(p2, p7, p3, p6), bbf(p3, p4), cbb(p5, p6),dbf(p7, p8).Abfff(p1, p8, p4, p5) :?
abf(p1, p2), bbf(p2, p4), cff(p5, p6), dbf(p6, p8).The generalized supplementary magic-sets rewritingfinally gives the following rule set:r1 : m B(p1) :?
m S(p1).r2 : S(p1, p3) :?
m B(p1), B(p1, p3, p2).r3 : sup2.1(p1, p2) :?
m B(p1), a(p1, p2).r4 : sup2.2(p1, p7, p3, p6) :?
sup2.1(p1, p2), A(p2, p7, p3, p6).r5 : sup2.3(p1, p7, p6, p4) :?
sup2.2(p1, p7, p3, p6), b(p3, p4).r6 : sup2.4(p1, p7, p4) :?
sup2.3(p1, p7, p6, p4), c(p4, p6).r7 : B(p1, p8, p4) :?
sup2.4(p1, p7, p4), d(p7, p8).r8 : sup3.1(p1, p2) :?
m B(p1), a(p1, p2).r9 : sup3.2(p1, p4) :?
sup3.1(p1, p2), b(p2, p4).r10 : sup3.3(p1, p4, p6) :?
sup3.2(p1, p4), c(p4, p6).r11 : B(p1, p8, p4) :?
sup3.3(p1, p4, p6), d(p6, p8).r12 : m A(p2) :?
sup2.1(p1, p2).r13 : m A(p2) :?
sup4.1(p1, p2).r14 : sup4.1(p1, p2) :?
m A(p1), a(p1, p2).r15 : sup4.2(p1, p7, p3, p6) :?
sup4.1(p1, p2), A(p2, p7, p3, p6).r16 : sup4.3(p1, p7, p6, p4) :?
sup4.2(p1, p7, p3, p6), b(p3, p4).r17 : sup4.4(p1, p7, p4, p5) :?
sup4.3(p1, p7, p6, p4), c(p5, p6).r18 : A(p1, p8, p4, p5) :?
sup4.4(p1, p7, p4, p5), d(p7, p8).r19 : sup5.1(p1, p2) :?
m A(p1), a(p1, p2).r20 : sup5.2(p1, p4) :?
sup5.1(p1, p2), b(p2, p4).r21 : sup5.3(p1, p4, p5, p6) :?
sup5.2(p1, p4), c(p5, p6).r22 : A(p1, p8, p4, p5) :?
sup5.3(p1, p4, p5, p6), d(p6, p8).The following version of chart parsing adds con-trol structure to this deduction system:1.
(????)
Initialize the chart to the empty set, theagenda to the singleton {m S(0)}, and n to 0.2.
Repeat the following steps:(a) Repeat the following steps until theagenda is exhausted:i.
Remove a fact from the agenda, calledthe trigger.ii.
Add the trigger to the chart.iii.
Generate all facts that are immediateconsequences of the trigger togetherwith all facts in the chart, and add tothe agenda those generated facts thatare neither already in the chart nor inthe agenda.
(b) (????)
Remove the next fact from the in-put database and add it to the agenda, in-crementing n. If there is no more fact inthe input database, go to step 3.3.
If S(0, n) is in the chart, accept; otherwise re-ject.The following is the trace of the algorithm on in-put string aabbccdd:1. m S(0) ????2.
m B(0) r1, 13. a(0, 1) ????4.
sup2.1(0, 1) r3, 2, 35. sup3.1(0, 1) r8, 2, 36. m A(1) r12, 47. a(1, 2) ????8.
sup4.1(1, 2) r14, 6, 79. sup5.1(1, 2) r19, 6, 710. m A(2) r13, 811. b(2, 3) ????12.
sup5.2(1, 3) r20, 9, 1113. b(3, 4) ????14.
c(4, 5) ????15.
sup5.3(1, 3, 4, 5) r21, 12, 1416. c(6, 5) ????17.
sup5.3(1, 3, 5, 6) r21, 12, 1618. d(6, 7) ????19.
A(1, 7, 3, 5) r22, 17, 1820. sup2.2(0, 7, 3, 5) r4, 4, 1921. sup2.3(0, 7, 5, 4) r5, 13, 2022. sup2.4(0, 7, 4) r6, 14, 2123. d(7, 8) ????24.
B(0, 8, 4) r7, 22, 2325.
S(0, 8) r2, 2, 24Note that unlike existing Earley-style parsing al-gorithms for TAGs, the present algorithm is an in-stantiation of a general schema that applies to pars-ing with more powerful grammar formalisms as wellas to generation with Montague semantics.8 ConclusionOur reduction to Datalog brings sophisticated tech-niques for Datalog query evaluation to the problems182of parsing and generation, and establishes a tightbound on the computational complexity of recogni-tion for a wide range of grammars.
In particular, itshows that the use of higher-order ?-terms for se-mantic representation need not be avoided for thepurpose of achieving computational tractability.ReferencesAoto, Takahito.
1999.
Uniqueness of normal proofs inimplicational intuitionistic logic.
Journal of Logic,Language and Information 8, 217?242.Aoto, Takahito and Hiroakira Ono.
1994.
Uniqueness ofnormal proofs in {?,?
}-fragment of NJ.
Research Re-port IS-RR-94-0024F.
School of Information Science,Japan Advanced Institute of Science and Technology.Beeri, Catriel and Raghu Ramakrishnan.
1991.
On thepower of magic.
Journal of Logic Programming 10,255?299.Engelfriet, J. and E. M. Schmidt.
1977.
IO and OI, partI.
The Journal of Computer and System Sciences 15,328?353.Engelfriet, Joost.
1986.
The complexity of languagesgenerated by attribute grammars.
SIAM Journal onComputing 15, 70?86.Fisher, Michael J.
1968.
Grammars with Macro-LikeProductions.
Ph.D. dissertation.
Harvard University.Gottlob, Georg, Nicola Lenoe, Francesco Scarcello.2002.
Computing LOGCFL certificates.
TheoreticalComputer Science 270, 761?777.de Groote, Philippe.
2001.
Towards abstract catego-rial grammars.
In Association for Computational Lin-guistics, 39th Annual Meeting and 10th Conference ofthe European Chapter, Proceedings of the Conference,pages 148?155.de Groote, Philippe.
2002.
Tree-adjoining gram-mars as abstract categorial grammars.
In Proceed-ings of the Sixth International Workshop on Tree Ad-joining Grammar and Related Frameworks (TAG+6),pages 145?150.
Universita?
di Venezia.de Groote, Philippe and Sylvain Pogodalla.
2004.
Onthe expressive power of abstract categorial grammars:Representing context-free formalisms.
Journal ofLogic, Language and Information 13, 421?438.Hindley, J. Roger.
1997.
Basic Simple Type Theory.Cambridge: Cambridge University Press.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In Grzegoz Rozenberg and ArtoSalomaa, editors, Handbook of Formal Languages,Vol.
3, pages 69?123.
Berlin: Springer.Kanazawa, Makoto and Ryo Yoshinaka.
2005.
Lexi-calization of second-order ACGs.
NII Technical Re-port.
NII-2005-012E.
National Institute of Informat-ics, Tokyo.Kanellakis, Paris C. 1988.
Logic programming andparallel complexity.
In Jack Minker, editor, Foun-dations of Deductive Databases and Logic Program-ming, pages 547?585.
Los Altos, CA: Morgan Kauf-mann.Mints, Grigori.
2000.
A Short Introduction to Intuitionis-tic Logic.
New York: Kluwer Academic/Plenum Pub-lishers.Moore, Robert C. 2002.
A complete, efficient sentence-realization algorithm for unification grammar.
In Pro-ceedings, International Natural Language GenerationConference, Harriman, New York, pages 41?48.Salvati, Sylvain.
2005.
Proble`mes de filtrageet proble`mes d?analyse pour les grammairescate?gorielles abstraites.
Doctoral dissertation,l?Institut National Polytechnique de Lorraine.Salvati, Sylvain.
2007.
Encoding second order stringACG with deterministic tree walking transducers.
InShuly Wintner, editor, Proceedings of FG 2006: The11th conference on Formal Grammar, pages 143?156.FG Online Proceedings.
Stanford, CA: CSLI Publica-tions.Seki, Hiroyuki, Takashi Matsumura, Mamoru Fujii, andTadao Kasami.
1991.
On multiple context-free gram-mars.
Theoretical Computer Science 88, 191?229.Shieber, Stuart M., Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementations of de-ductive parsing.
Journal of Logic Programming 24,3?36.Sikkel, Klaas.
1997.
Parsing Schemata.
Berlin:Springer.S?rensen, Morten Heine and Pawe?
Urzyczyn.
2006.Lectures on the Curry-Howard Isomorphism.
Ams-terdam: Elsevier.Ullman, Jeffrey D. 1988.
Principles of Database andKnowledge-Base Systems.
Volume I. Rockville, MD.
:Computer Science Press.Ullman, Jeffrey D. 1989a.
Bottom-up beats top-downfor Datalog.
In Proceedings of the Eighth ACMSIGACT-SIGMOD-SIGART Symposium on Principlesof Database Systems, Philadelphia, pages 140?149.Ullman, Jeffrey D. 1989b.
Principles of Database andKnowledge-Base Systems.
Volume II: The New Tech-nologies.
Rockville, MD.
: Computer Science Press.Ullman, Jeffrey D. and Allen Van Gelder.
1988.
Par-allel complexity of logical query programs.
Algorith-mica 3, 5?42.David J. Weir.
1988.
Characterizing Mildly Context-Sensitive Grammar Formalisms.
Ph.D. dissertation.University of Pennsylvania.183
