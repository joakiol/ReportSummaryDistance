Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 21?30,Beijing, August 2010Automatic Classification of Semantic Relationsbetween Facts and OpinionsKoji Murakami?
Eric Nichols?
Junta Mizuno??
Yotaro Watanabe?Hayato Goto?
Megumi Ohki?
Suguru Matsuyoshi?
Kentaro Inui?
Yuji Matsumoto?
?Nara Institute of Science and Technology?Tohoku University{kmurakami,matuyosi,hayato-g,megumi-o,matsu}@is.naist.jp{eric-n,junta-m,inui}@ecei.tohoku.ac.jpAbstractClassifying and identifying semantic re-lations between facts and opinions onthe Web is of utmost importance for or-ganizing information on the Web, how-ever, this requires consideration of abroader set of semantic relations than aretypically handled in Recognizing Tex-tual Entailment (RTE), Cross-documentStructure Theory (CST), and similartasks.
In this paper, we describe the con-struction and evaluation of a system thatidentifies and classifies semantic rela-tions in Internet data.
Our system targetsa set of semantic relations that have beeninspired by CST but that have been gen-eralized and broadened to facilitate ap-plication to mixed fact and opinion datafrom the Internet.
Our system identi-fies these semantic relations in JapaneseWeb texts using a combination of lexical,syntactic, and semantic information andevaluate our system against gold stan-dard data that was manually constructedfor this task.
We will release all goldstandard data used in training and eval-uation of our system this summer.1 IntroductionThe task of organizing the information on the In-ternet to help users find facts and opinions ontheir topics of interest is increasingly importantas more people turn to the Web as a source ofimportant information.
The vast amounts of re-search conducted in NLP on automatic summa-rization, opinion mining, and question answer-ing are illustrative of the great interest in mak-ing relevant information easier to find.
Provid-ing Internet users with thorough information re-quires recognizing semantic relations betweenboth facts and opinions, however the assump-tions made by current approaches are often in-compatible with this goal.
For example, theexisting semantic relations considered in Rec-ognizing Textual Entailment (RTE) (Dagan etal., 2005) are often too narrow in scope to bedirectly applicable to text on the Internet, andtheories like Cross-document Structure Theory(CST) (Radev, 2000) are only applicable to factsor second-hand reporting of opinions rather thanrelations between both.As part of the STATEMENT MAP project weproposed the development of a system to sup-port information credibility analysis on the Web(Murakami et al, 2009b) by automatically sum-marizing facts and opinions on topics of inter-est to users and showing them the evidence andconflicts for each viewpoint.
To facilitate the de-tection of semantic relations in Internet data, wedefined a sentence-like unit of information calledthe statement that encompasses both facts andopinions, started compiling a corpus of state-ments annotated with semantic relations (Mu-rakami et al, 2009a), and begin constructing asystem to automatically identify semantic rela-tions between statements.In this paper, we describe the construction andevaluation of a prototype semantic relation iden-tification system.
We build on the semantic rela-tions proposed in RTE and CST and in our pre-vious work, refining them into a streamlined setof semantic relations that apply across facts andopinions, but that are simple enough to makeautomatic recognition of semantic relations be-tween statements in Internet text possible.Oursemantic relations are [AGREEMENT], [CON-FLICT], [CONFINEMENT], and [EVIDENCE].
[AGREEMENT] and [CONFLICT] are expansionsof the [EQUIVALENCE] and [CONTRADICTION]21relations used in RTE.
[CONFINEMENT] and[EVIDENCE] are new relations between factsand opinions that are essential for understandinghow statements on a topic are inter-related.Our task differs from opinion mining and sen-timent analysis which largely focus on identify-ing the polarity of an opinion for defined param-eters rather than identify how facts and opinionsrelate to each other, and it differs from web doc-ument summarization tasks which focus on ex-tracting information fromweb page structure andcontextual information from hyperlinks ratherthan analyzing the semantics of the language onthe webpage itself.We present a system that automatically iden-tifies semantic relations between statements inJapanese Internet texts.
Our system uses struc-tural alignment to identify statement pairs thatare likely to be related, then classifies seman-tic relations using a combination of lexical, syn-tactic, and semantic information.
We evaluatecross-statement semantic relation classificationon sentence pairs that were taken from JapaneseInternet texts on several topics and manually an-notated with a semantic relation where one ispresent.
In our evaluation, we look closely at theimpact that each of the resources has on semanticrelation classification quality.The rest of this paper is organized as follows.In Section 2, we discuss related work in summa-rization, semantic relation classification, opinionmining, and sentiment analysis, showing howexisting classification schemes are insufficientfor our task.
In Section 3, we introduce a set ofcross-sentential semantic relations for use in theopinion classification needed to support informa-tion credibility analysis on the Web.
In Section4, we present our cross-sentential semantic re-lation recognition system, and discuss the algo-rithms and resources that are employed.
In Sec-tion 5, we evaluate our system in a semantic rela-tion classification task.
In Section 6, we discussour findings and conduct error analysis.
Finally,we conclude the paper in Section 7.2 Related Work2.1 Recognizing Textual EntailmentIdentifying logical relations between texts is thefocus of Recognizing Textual Entailment, thetask of deciding whether the meaning of onetext is entailed from another text.
A majortask in the RTE Challenge (Recognizing Tex-tual Entailment Challenge) is classifying the se-mantic relation between a Text (T) and a Hy-pothesis (H) into [ENTAILMENT], [CONTRA-DICTION], or [UNKNOWN].
Over the last sev-eral years, several corpora annotated with thou-sands of (T,H) pairs have been constructed forthis task.
In these corpora, each pair was taggedindicating its related task (e.g.
Information Ex-traction, Question Answering, Information Re-trieval or Summarization).The RTE Challenge has successfully em-ployed a variety of techniques in order to rec-ognize instances of textual entailment, includingmethods based on: measuring the degree of lex-ical overlap between bag of words (Glickmanet al, 2005; Jijkoun and de Rijke, 2005), thealignment of graphs created from syntactic or se-mantic dependencies (Marsi and Krahmer, 2005;MacCartney et al, 2006), statistical classifierswhich leverage a wide range of features (Hicklet al, 2005), or reference rule generation (Szpek-tor et al, 2007).
These approaches have showngreat promise in RTE for entailment pairs in thecorpus, but more robust models of recognizinglogical relations are still desirable.The definition of contradiction in RTE is thatT contradicts H if it is very unlikely that both Tand H can be true at the same time.
However, inreal documents on the Web, there are many pairsof examples which are contradictory in part, orwhere one statement confines the applicability ofanother, as shown in the examples in Table 1.2.2 Cross-document Structure TheoryCross-document Structure Theory (CST), devel-oped by Radev (2000), is another task of rec-ognizing semantic relations between sentences.CST is an expanded rhetorical structure analy-sis based on Rhetorical Structure Theory (RST:(William and Thompson, 1988)), and attemptsto describe the semantic relations that existbetween two or more sentences from differ-ent source documents that are related to thesame topic, as well as those that come froma single source document.
A corpus of cross-document sentences annotated with CST rela-tions has also been constructed (The CSTBankCorpus: (Radev et al, 2003)).
CSTBank isorganized into clusters of topically-related ar-ticles.
There are 18 kinds of semantic rela-tions in this corpus, not limited to [EQUIVA-LENCE] or [CONTRADICTION], but also includ-ing [JUDGEMENT], [ELABORATION], and [RE-22Query Matching sentences Output???????????????????????????????????????????????????
?
?The cavity-prevention effects are greater the more Xylitol is included.
[AGREEMENT].?????????????????????????????
?
?Xylitol is effective at preventingcavities.Xylitol shows effectiveness at maintaining good oral hygiene and preventing cavities.
[AGREEMENT]?????????????????????????????????????????????????
?There are many opinions about the cavity-prevention effectiveness of Xylitol, but itis not really effective.[CONFLICT]?????????????????????????????????????????
?
?Reduced water, which has weak alkaline ions, supports the health of you and yourfamily.[AGREEMENT]??????????????????????????????????
?
?Reduced water is good for thehealth.Reduced water is said to remove active oxygen from the body, making it effective atpromoting good health.[AGREEMENT]???????????????????????
?
?Even if oxidized water tastes good, it does not help one?s health.
[CONFLICT]??????????????????????????????????????????????????????????????
?Isoflavone is effective atmaintaining good health.Taking too much soy isoflavone as a supplement will have a negative effect on one?shealth[CONFINEMENT]Table 1: Example semantic relation classification.FINEMENT].
Etoh et al (Etoh and Okumura,2005) constructed a Japanese Cross-documentRelation Corpus, and they redefined 14 kinds ofsemantic relations in their corpus.CST was designed for objective expressionsbecause its target data is newspaper articles re-lated to the same topic.
Facts, which can be ex-tracted from newspaper articles, have been usedin conventional NLP research, such as Informa-tion Extraction or Factoid Question Answering.However, there are a lot of opinions on the Web,and it is important to survey opinions in additionto facts to give Internet users a comprehensiveview of the discussions on topics of interest.2.3 Cross-document Summarization Basedon CST Relations between SentencesZhang and Radev (2004) attempted to classifyCST relations between sentence pairs extractedfrom topically related documents.
However, theyused a vector space model and tried multi-classclassification.
The results were not satisfactory.This observation may indicate that the recog-nition methods for each relation should be de-veloped separately.
Miyabe et al (2008) at-tempted to recognize relations that were definedin a Japanese cross-document relation corpus(Etoh and Okumura, 2005).
However, their tar-get relations were limited to [EQUIVALENCE]and [TRANSITION]; other relations were not tar-geted.
Recognizing [EVIDENCE] is indispens-able for organizing information on the Internet.We need to develop satisfactory methods of [EV-IDENCE] recognition.2.4 Opinion Mining and Sentiment AnalysisSubjective statements, such as opinions, haverecently been the focus of much NLP re-search including review analysis, opinion ex-traction, opinion question answering, and senti-ment analysis.
In the corpus constructed in theMulti-Perspective Question Answering (MPQA)Project (Wiebe et al, 2005), individual expres-sions are tagged that correspond to explicit men-tions of private states, speech event, and expres-sive subjective elements.The goal of opinion mining to extract expres-sions with polarity from texts, not to recognizesemantic relations between sentences.
Sentimentanalysis also focus classifying subjective expres-sions in texts into positive/negative classes.
Incomparison, although we deal with sentiment in-formation in text, our objective is to recognizesemantic relations between sentences.
If a user?squery requires positive/negative information, wewill also need to extract sentences including sen-timent expression like in opinion mining, how-ever, our semantic relation, [CONFINEMENT], ismore precise because it identifies the conditionor scope of the polarity.
Queries do not neces-sarily include sentiment information; we also ac-cept queries that are intended to be a statementof fact.
For example, for the query ?Xylitol iseffective at preventing cavities.?
in Table 1, weextract a variety of sentences from the Web andrecognize semantic relations between the queryand many kinds of sentences.233 Semantic Relations betweenStatementsIn this section, we define the semantic relationsthat we will classify in Japanese Internet texts aswell as their corresponding relations in RTE andCST.
Our goal is to define semantic relations thatare applicable over both fact and opinions, mak-ing them more appropriate for handling Internettexts.
See Table 1 for real examples.3.1 [AGREEMENT]A bi-directional relation where statements haveequivalent semantic content on a shared topic.Here we use topic in a narrow sense to mean thatthe semantic contents of both statements are rel-evant to each other.The following is an example of [AGREE-MENT] on the topic of bio-ethanol environmentalimpact.
(1) a. Bio-ethanol is good for the environment.b.
Bio-ethanol is a high-quality fuel, and ithas the power to deal with the environ-ment problems that we are facing.Once relevance has been established,[AGREEMENT] can range from strict logi-cal entailment or identical polarity of opinions.Here is an example of two statements thatshare a broad topic, but that are not classified as[AGREEMENT] because preventing cavities andtooth calcification are not intuitively relevant.
(2) a. Xylitol is effective at preventing cavities.b.
Xylitol advances tooth calcification.3.2 [CONFLICT]A bi-directional relation where statements havenegative or contradicting semantic content on ashared topic.
This can range from strict logicalcontradiction to opposite polarity of opinions.The next pair is a [CONFLICT] example.
(3) a. Bio-ethanol is good for our earth.b.
There is a fact that bio-ethanol further thedestruction of the environment.3.3 [EVIDENCE]A uni-directional relation where one statementprovides justification or supporting evidence forthe other.
Both statements can be either facts oropinions.
The following is a typical example:(4) a. I believe that applying the technology ofcloning must be controlled by law.b.
There is a need to regulate cloning, be-cause it can be open to abuse.The statement containing the evidence con-sists of two parts: one part has a [AGREEMENT]or [CONFLICT] with the other statement, theother part provides support or justification for it.3.4 [CONFINEMENT]A uni-directional relation where one statementprovides more specific information about theother or quantifies the situations in which it ap-plies.
The pair below is an example, in whichone statement gives a condition under which theother can be true.
(5) a. Steroids have side-effects.b.
There is almost no need to worry aboutside-effects when steroids are used for lo-cal treatment.4 Recognizing Semantic RelationsIn order to organize the information on theInternet, we need to identify the [AGREE-MENT], [CONFLICT], [CONFINEMENT], and[EVIDENCE] semantic relations.
Because iden-tification of [AGREEMENT] and [CONFLICT] isa problem of measuring semantic similarity be-tween two statements, it can be cast as a sen-tence alignment problem and solved using anRTE framework.
The two sentences do not needto be from the same source.However, the identification of [CONFINE-MENT] and [EVIDENCE] relations depend oncontextual information in the sentence.
For ex-ample, conditional statements or specific dis-course markers like ?because?
act as importantcues for their identification.
Thus, to identifythese two relations across documents, we mustfirst identify [AGREEMENT] or [CONFLICT] be-tween sentences in different documents and thendetermine if there is a [CONFINEMENT] or [EV-IDENCE] relation in one of the documents.Furthermore, the surrounding text often con-tains contextual information that is important foridentifying these two relations.
Proper handlingof surrounding context requires discourse analy-sis and is an area of future work, but our basicdetection strategy is as follows:1.
Identify a [AGREEMENT] or [CONFLICT] re-lation between the Query and Text2.
Search the Text sentence for cues that iden-tify [CONFINEMENT] or [EVIDENCE]243.
Infer the applicability of the [CONFINE-MENT] or [EVIDENCE] relations in the Textto the Query4.1 System OverviewWe have finished constructing a prototype sys-tem that detects semantic relation between state-ments.
It has a three-stage architecture similar tothe RTE system of MacCartney et al (2006):1.
Linguistic analysis2.
Structural alignment3.
Feature extraction for detecting [EVIDENCE]and [CONFINEMENT]4.
Semantic relation classificationHowever, we differ in the following respects.First, our relation classification is broader thanRTE?s simple distinction between [ENTAIL-MENT], [CONTRADICTION], and [UNKNOWN];in place of [ENTAILMENT] and [CONTRA-DICTION, we use broader [AGREEMENT] and[CONFLICT] relations.
We also consider covergradations of applicability of statements with the[CONFINEMENT] relation.Second, we conduct structural alignment withthe goal of aligning semantic structures.
We dothis by directly incorporating dependency align-ments and predicate-argument structure informa-tion for both the user query and the Web textinto the alignment scoring process.
This allowsus to effectively capture many long-distancealignments that cannot be represented as lexicalalignments.
This contrasts with MacCartney etal.
(2006), who uses dependency structures forthe Hypothesis to reduce the lexical alignmentsearch space but do not produce structural align-ments and do not use the dependencies in detect-ing entailment.Finally, we apply several rich semantic re-sources in alignment and classification: extendedmodality information that helps align and clas-sify structures that are semantically similar butdivergent in tense or polarity; and lexical simi-larity through ontologies like WordNet.4.2 Linguistic AnalysisIn order to identify semantic relations betweenthe user query (Q) and the sentence extractedfrom Web text (T), we first conduct syntactic andsemantic linguistic analysis to provide a basis foralignment and relation classification.For syntactic analysis, we use the Japanesedependency parser CaboCha (Kudo and Mat-!"#$%&'!"#$!%&'()*'+$!
()*!,'-!.-'/'0+1!#$)23#!+,-!$4$50*$!./!%&!6!
7!!"#$%&'!"#$!%&'()*'+$!01234!'&3$'.'-'&%&!567!*)-%'8&!897:;!&85#!)&!9!3-$)3/$+3!<=>?@A/!#)&!:$$+!&#';+!()BC-!
B!
C!?!)!5!:!J!3'!#)*$!!#$)23#!
)..2%5)0'+&!C!J!J!B!B!C!Figure 1: An example of structural alignmentsumoto, 2002) and the predicate-argument struc-ture analyzer ChaPAS (Watanabe et al, 2010).CaboCha splits the Japanese text into phrase-likechunks and represents syntactic dependenciesbetween the chunks as edges in a graph.
Cha-PAS identifies predicate-argument structures inthe dependency graph produced by CaboCha.We also conduct extended modality analysisusing the resources provided by Matsuyoshi etal.
(2010), focusing on tense, modality, and po-larity, because such information provides impor-tant clues for the recognition of semantic rela-tions between statements.4.3 Structural AlignmentIn this section, we describe our approach tostructural alignment.
The structural alignmentprocess is shown in Figure 1.
It consists of thefollowing two phases:1. lexical alignment2.
structural alignmentWe developed a heuristic-based algorithm toalign chunk based on lexical similarity infor-mation.
We incorporate the following informa-tion into an alignment confidence score that hasa range of 0.0-1.0 and align chunk whose scorescross an empirically-determined threshold.?
surface level similarity: identical contentwords or cosine similarity of chunk contents?
semantic similarity of predicate-argumentstructurespredicates we check for matches in predi-cate entailment databases (Hashimoto etal., 2009; Matsuyoshi et al, 2008) consid-ering the default case frames reported byChaPASarguments we check for synonym or hy-pernym matches in the Japanese WordNet(2008) or the Japanese hypernym collec-tion of Sumida et al (2008)25>?@???????????????????AB?C????DEF)!>?'???????????????????AB?C???
?GHF)!I!T :!H :!
(field) (in)!
(agricultural chemicals) (ACC)!
(use)!
(field) (on)!
(agricultural chemicals) (ACC)!
(spray)!Figure 2: Determining the compatibility of se-mantic structuresWe compare the predicate-argument structureof the query to that of the text and determineif the argument structures are compatible.
Thisprocess is illustrated in Figure 2 where the T(ext)?Agricultural chemicals are used in the field.?
isaligned with the H(ypothesis) ?Over the field,agricultural chemicals are sprayed.?
Althoughthe verbs used and sprayed are not directly se-mantically related, they are aligned because theyshare the same argument structures.
This lets upalign predicates for which we lack semantic re-sources.
We use the following information to de-termine predicate-argument alignment:?
the number of aligned children?
the number of aligned case frame arguments?
the number of possible alignments in a win-dow of n chunk?
predicates indicating existence or quantity.E.g.
many, few, to exist, etc.?
polarity of both parent and child chunks us-ing the resources in (Higashiyama et al,2008; Kobayashi et al, 2005)We treat structural alignment as a machinelearning problem and train a Support Vector Ma-chine (SVM) model to decide if lexically alignedchunks are semantically aligned.We train on gold-standard labeled alignmentof 370 sentence pairs.
This data set is describedin more detail in Section 5.1.
As features for ourSVM model, we use the following information:?
the distance in edges in the dependency graphbetween parent and child for both sentences?
the distance in chunks between parent andchild in both sentences?
binary features indicating whether eachchunk is a predicate or argument accordingto ChaPAS?
the parts-of-speech of first and last word ineach chunk?
when the chunk ends with a case marker, thecase of the chunk , otherwise none?
the lexical alignment score of each chunkpair4.4 Feature Extraction for DetectingEvidence and ConfinementOnce the structural alignment system has iden-tified potential [AGREEMENT] or [CONFLICT]relations, we need to extract contextual cues inthe Text as features for detecting [CONFINE-MENT] and [EVIDENCE] relations.
Conditionalstatements, degree adverbs, and partial negation,which play a role in limiting the scope or degreeof a query?s contents in the statement, can be im-portant cues for detecting the these two semanticrelations.
We currently use a set of heuristics toextract a set of expressions to use as features forclassifying these relations using SVM models.4.5 Relation ClassificationOnce the structural alignment is successfullyidentified, the task of semantic relation classi-fication is straightforward.
We also solve thisproblem with machine learning by training anSVM classifier.
As features, we draw on a com-bination of lexical, syntactic, and semantic infor-mation including the syntactic alignments fromthe previous section.
The feature set is:alignments We define two binary function,ALIGNword(qi, tm) for the lexical align-ment and ALIGNstruct((qi, qj), (tm, tk))for the structural alignment to be true if andonly if the node qi, qj ?
Q has been seman-tically and structurally aligned to the nodetm, tk ?
T .
Q and T are the (Q)uery and the(T)ext, respectively.
We also use a separatefeature for a score representing the likelihoodof the alignment.modality We have a feature that encodes all ofthe possible polarities of a predicate nodefrom modality analysis, which indicates theutterance type, and can be assertive, voli-tional, wish, imperative, permissive, or in-terrogative.
Modalities that do not repre-sent opinions (i.e.
imperative, permissive andinterrogative) often indicate [OTHER] rela-tions.antonym We define a binary functionANTONYM(qi, tm) that indicates ifthe pair is identified as an antonym.
Thisinformation helps identify [CONFLICT].26Relation Measure 3-class Cascaded 3-class ?
[AGREEMENT] precision 0.79 (128 / 162) 0.80 (126 / 157) +0.01[AGREEMENT] recall 0.86 (128 / 149) 0.85 (126 / 149) -0.01[AGREEMENT] f-score 0.82 0.82 -[CONFLICT] precision 0 (0 / 5) 0.36 (5 / 14) +0.36[CONFLICT] recall 0 (0 / 12) 0.42 (5 / 12) +0.42[CONFLICT] f-score 0 0.38 +0.38[CONFINEMENT] precision 0.4 (4 / 10) 0.8 (4 / 5) +0.4[CONFINEMENT] recall 0.17 (4 / 23) 0.17 (4 / 23) -[CONFINEMENT] f-score 0.24 0.29 +0.05Table 2: Semantic relation classification results comparing 3-class and cascaded 3-class approachesnegation To identify negations, we primar-ily rely on a predicate?s Actuality value,which represents epistemic modality andexistential negation.
If a predicate pairALIGNword(qi, tm) has mismatching actu-ality labels, the pair is likely a [OTHER].contextual cues This set of features is used tomark the presence of any contextual cuesthat identify of [CONFINEMENT] or [EVI-DENCE] relations in a chunk .
For example,???
(because)?
or ???
(due to)?
are typ-ical contextual cues for [EVIDENCE], and ???
(when)?
or ????
(if)?
are typical for[CONFINEMENT].5 Evaluation5.1 Data PreparationIn order to evaluate our semantic relation clas-sification system on realistic Web data, we con-structed a corpus of sentence pairs gathered froma vast collection of webpages (2009a).
Our basicapproach is as follows:1.
Retrieve documents related to a set numberof topics using the Tsubaki1 search engine2.
Extract real sentences that include major sub-topic words which are detected based onTF/IDF in the document set3.
Reduce noise in data by using heuristics toeliminate advertisements and comment spam4.
Reduce the search space for identifying sen-tence pairs and prepare pairs, which look fea-sible to annotate5.
Annotate corresponding sentences with[AGREEMENT], [CONFLICT], [CONFINE-MENT], or [OTHER]1http://tsubaki.ixnlp.nii.ac.jp/Although our target semantic relations in-clude [EVIDENCE], they difficult annotate con-sistently, so we do not annotate them at thistime.
Expanding our corpus and semantic re-lation classifier to handle [EVIDENCE] remainsand area of future work.The data that composes our corpus comesfrom a diverse number of sources.
A hand sur-vey of a random sample of the types of domainsof 100 document URLs is given below.
Half ofthe URL domains were not readily identifiable,but the known URL domains included govern-mental, corporate, and personal webpages.
Webelieve this distribution is representative of in-formation sources on the Internet.type countacademic 2blogs 23corporate 10governmental 4news 5press releases 4q&a site 1reference 1other 50We have made a partial release of our corpusof sentence pairs manually annotated with thecorrect semantic relations2.
We will fully releaseall the data annotated semantic relations and withgold standard alignments at a future date.5.2 Experiment SettingsIn this section, we present results of empiri-cal evaluation of our proposed semantic rela-tion classification system on the dataset we con-structed in the previous section.
For this experi-ment, we use SVMs as described in Section 4.52http://stmap.naist.jp/corpus/ja/index.html (in Japanese)27to classify semantic relations into one of the fourclasses: [AGREEMENT], [CONFLICT], [CON-FINEMENT], or [OTHER] in the case of no re-lation.
As data we use 370 sentence pairs thathave been manually annotated both with the cor-rect semantic relation and with gold standardalignments.
Annotations are checked by two na-tive speakers of Japanese, and any sentence pairwhere annotation agreement is not reached isdiscarded.
Because we have limited data that isannotated with correct alignments and semanticrelations, we perform five-fold cross validation,training both the structural aligner and semanticrelation classifier on 296 sentence pairs and eval-uating on the held out 74 sentence pairs.
Thefigures presented in the next section are for thecombined results on all 370 sentence pairs.5.3 ResultsWe compare two different approaches to classi-fication using SVMs:3-class semantic relations are directly classifiedinto one of [AGREEMENT], [CONFLICT],and [CONFINEMENT] with all features de-scribed in 4.5cascaded 3-class semantic relations are firstclassified into one of [AGREEMENT], [CON-FLICT] without contextual cue features.
Thenan additional judgement with all features de-termines if [AGREEMENT] and [CONFLICT]should be reclassified as [CONFINEMENT]Initial results using the 3-class classifica-tion model produced high f-scores for [AGREE-MENT] but unfavorable results for [CONFLICT]and [CONFINEMENT].
We significantly im-proved classification of [CONFLICT] and [CON-FINEMENT] by adopting the cascaded 3-classmodel.
We present these results in Table 2 andsuccessfully recognized examples in Table 1.6 Discussion and Error AnalysisWe constructed a prototype semantic relationclassification system by combining the compo-nents described in the previous section.
Whilethe system developed is not domain-specific andcapable of accepting queries on any topic, weevaluate its semantic relation classification onseveral queries that are representative of ourtraining data.Figure 3 shows a snapshot of the semantic re-lation classification system and the various se-mantic relations it recognized for the query.Baseline Structural Upper-boundAlignmentPrecision 0.44 0.52 0.74(56/126) (96/186) (135/183)Recall 0.30 0.52 0.73(56/184) (96/184) (135/184)F1-score 0.36 0.52 0.74Table 3: Comparison of lexical, structural, andupper-bound alignments on semantic relationclassificationIn the example (6), recognized as [CONFINE-MENT] in Figure 3, our system correctly identi-fied negation and analyzed the description ?Xyl-itol alone can not completely?
as playing a roleof requirement.
(6) a.
?????????????????
(Xylitol is effective at preventing cavi-ties.)b.
?????????????????????
(Xylitol alone can not completely preventcavities.
)Our system correctly identifies [AGREE-MENT] relations in other examples about re-duced water from Table 1 by structurally align-ing phrases like ?promoting good health?
and?supports the health?
to ?good for the health.
?These examples show how resources like(Matsuyoshi et al, 2010) and WordNet (Bond etal., 2008) have contributed to the relation clas-sification improvement of structural alignmentover them baseline in Table 3.
Focusing on sim-ilarity of syntactic and semantic structures givesour alignment method greater flexibility.However, there are still various exampleswhich the system cannot recognized correctly.In examples on cavity prevention, the phrase?effective at preventing cavities?
could not bealigned with ?can prevent cavities?
or ?good forcavity prevention,?
nor can ?cavity prevention?and ?cavity-causing bacteria control.
?The above examples illustrate the importanceof the role played by the alignment phase in thewhole system?s performance.Table 3 compares the semantic relation classi-fication performance of using lexical alignmentonly (as the baseline), lexical alignment andstructural alignment, and, to calculate the maxi-mum possible precision, classification using cor-rect alignment data (the upper-bound).
We can28Figure 3: Alignment and classification example for the query ?Xylitol is effective at preventingcavities.
?see that structural alignment makes it possible toalign more words than lexical alignment alone,leading to an improvement in semantic relationclassification.
However, there is still a large gapbetween the performance of structural alignmentand the maximum possible precision.
Error anal-ysis shows that a big cause of incorrect classifi-cation is incorrect lexical alignment.
Improvinglexical alignment is a serious problem that mustbe addressed.
This entails expanding our cur-rent lexical resources and finding more effectivemethods of apply them in alignment.The most serious problem we currently face isthe feature engineering necessary to find the op-timal way of applying structural alignments orother semantic information to semantic relationclassification.
We need to conduct a quantita-tive evaluation of our current classification mod-els and find ways to improve them.7 ConclusionClassifying and identifying semantic relationsbetween facts and opinions on the Web is of ut-most importance to organizing information onthe Web, however, this requires consideration ofa broader set of semantic relations than are typi-cally handled in RTE, CST, and similar tasks.
Inthis paper, we introduced a set of cross-sententialsemantic relations specifically designed for thistask that apply over both facts and opinions.
Wepresented a system that identifies these semanticrelations in Japanese Web texts using a combina-tion of lexical, syntactic, and semantic informa-tion and evaluated our system against data thatwas manually constructed for this task.
Prelimi-nary evaluation showed that we are able to detect[AGREEMENT] with high levels of confidence.Our method also shows promise in [CONFLICT]and [CONFINEMENT] detection.
We also dis-cussed some of the technical issues that need tobe solved in order to identify [CONFLICT] and[CONFINEMENT].AcknowledgmentsThis work is supported by the National Instituteof Information and Communications TechnologyJapan.ReferencesBond, Francis, Hitoshi Isahara, Kyoko Kanzaki, andKiyotaka Uchimoto.
2008.
Boot-strapping awordnet using multiple existing wordnets.
In Proc.of the 6th International Language Resources andEvaluation (LREC?08).Dagan, Ido, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailmentchallenge.
In Proc.
of the PASCAL ChallengesWorkshop on Recognising Textual Entailment.Etoh, Junji and Manabu Okumura.
2005.
Cross-document relationship between sentences corpus.29In Proc.
of the 14th Annual Meeting of the Associa-tion for Natural Language Processing, pages 482?485.
(in Japanese).Glickman, Oren, Ido Dagan, and Moshe Koppel.2005.
Web based textual entailment.
In Proc.
ofthe First PASCAL Recognizing Textual EntailmentWorkshop.Hashimoto, Chikara, Kentaro Torisawa, KowKuroda, Masaki Murata, and Jun?ichi Kazama.2009.
Large-scale verb entailment acquisitionfrom the web.
In Conference on Empiri-cal Methods in Natural Language Processing(EMNLP2009), pages 1172?1181.Hickl, Andrew, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2005.
Recog-nizing textual entailment with lcc?s groundhog sys-tem.
In Proc.
of the Second PASCAL ChallengesWorkshop.Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-sumoto.
2008.
Acquiring noun polarity knowl-edge using selectional preferences.
In Proc.
of the14th Annual Meeting of the Association for Natu-ral Language Processing.Jijkoun, Valentin and Maarten de Rijke.
2005.
Rec-ognizing textual entailment using lexical similar-ity.
In Proc.
of the First PASCAL Challenges Work-shop.Kobayashi, Nozomi, Kentaro Inui, Yuji Matsumoto,Kenji Tateishi, and Toshikazu Fukushima.
2005.Collecting evaluative expressions for opinion ex-traction.
Journal of natural language processing,12(3):203?222.Kudo, Taku and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InProc of CoNLL 2002, pages 63?69.MacCartney, Bill, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D.Manning.
2006.
Learning to recognize fea-tures of valid textual entailments.
In Proc.
ofHLT/NAACL 2006.Marsi, Erwin and Emiel Krahmer.
2005.
Classifi-cation of semantic relations by humans and ma-chines.
In Proc.
of ACL-05 Workshop on Empiri-cal Modeling of Semantic Equivalence and Entail-ment, pages 1?6.Matsuyoshi, Suguru, Koji Murakami, Yuji Mat-sumoto, and Kentaro Inui.
2008.
A database of re-lations between predicate argument structures forrecognizing textual entailment and contradiction.In Proc.
of the Second International Symposiumon Universal Communication, pages 366?373, De-cember.Matsuyoshi, Suguru, Megumi Eguchi, Chitose Sao,Koji Murakami, Kentaro Inui, and Yuji Mat-sumoto.
2010.
Annotating event mentions in textwith modality, focus, and source information.
InProc.
of the 7th International Language Resourcesand Evaluation (LREC?10), pages 1456?1463.Miyabe, Yasunari, Hiroya Takamura, and ManabuOkumura.
2008.
Identifying cross-document re-lations between sentences.
In Proc.
of the 3rd In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP-08), pages 141?148.Murakami, Koji, Shouko Masuda, Suguru Mat-suyoshi, Eric Nichols, Kentaro Inui, and Yuji Mat-sumoto.
2009a.
Annotating semantic relationscombining facts and opinions.
In Proceedings ofthe Third Linguistic Annotation Workshop, pages150?153, Suntec, Singapore, August.
Associationfor Computational Linguistics.Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,Asuka Sumida, Shouko Masuda, Kentaro Inui, andYuji Matsumoto.
2009b.
Statement map: Assist-ing information credibility analysis by visualizingarguments.
In Proc.
of the 3rd ACM Workshopon Information Credibility on the Web (WICOW2009), pages 43?50.Radev, Dragomir, Jahna Otterbacher,and Zhu Zhang.
2003.
CSTBank:Cross-document Structure Theory Bank.http://tangra.si.umich.edu/clair/CSTBank.Radev, Dragomir R. 2000.
Common theory of infor-mation fusion from multiple text sources step one:Cross-document structure.
In Proc.
of the 1st SIG-dial workshop on Discourse and dialogue, pages74?83.Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-sawa.
2008.
Boosting precision and recall of hy-ponymy relation acquisition from hierarchical lay-outs in wikipedia.
In Proc.
of the 6th InternationalLanguage Resources and Evaluation (LREC?08).Szpektor, Idan, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acqui-sition.
In Proc.
of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages456?463.Watanabe, Yotaro, Masayuki Asahara, and Yuji Mat-sumoto.
2010.
A structured model for joint learn-ing of argument roles and predicate senses.
In Pro-ceedings of the 48th Annual Meeting of the Associ-ation of Computational Linguistics (to appear).Wiebe, Janyce, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions andemotions in language.
Language Resources andEvaluation, 39(2-3):165?210.William, Mann and Sandra Thompson.
1988.Rhetorical structure theory: towards a functionaltheory of text organization.
Text, 8(3):243?281.Zhang, Zhu and Dragomir Radev.
2004.
Combin-ing labeled and unlabeled data for learning cross-document structural relationships.
In Proc.
of theProceedings of IJC-NLP.30
