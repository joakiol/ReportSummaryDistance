Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 363?367,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsNULEX: An Open-License Broad Coverage LexiconClifton J. McFate Kenneth D. ForbusNorthwestern University Northwestern UniversityEvanston, IL.
USA.
Evanston, IL.
USAc-mcfate@northwestern.edu forbus@northwestern.eduAbstractBroad coverage lexicons for the Englishlanguage have traditionally been handmade.This approach, while accurate, requires toomuch human labor.
Furthermore, resourcescontain gaps in coverage, contain specifictypes of information, or are incompatible withother resources.
We believe that the state ofopen-license technology is such that acomprehensive syntactic lexicon can beautomatically compiled.
This paper describesthe creation of such a lexicon, NU-LEX, anopen-license feature-based lexicon for generalpurpose parsing that combines WordNet,VerbNet, and Wiktionary and contains over100,000 words.
NU-LEX was integrated into abottom up chart parser.
We ran the parserthrough three sets of sentences, 50 sentencestotal, from the Simple English Wikipedia andcompared its performance to the same parserusing Comlex.
Both parsers performed almostequally with NU-LEX finding all lex-items for50% of the sentences and Comlex succeedingfor 52%.
Furthermore, NULEX?sshortcomings primarily fell into twocategories, suggesting future researchdirections.1 IntroductionWhile there are many types of parsersavailable, all of them rely on a lexicon of words,whether syntactic like Comlex, enriched withsemantics like WordNet, or derived from taggedcorpora like the Penn Treebank (Macleod et al1994; Fellbaum, 1998; Marcus et al 1993).However, many of these resources have gaps thatthe others can fill in.
WordNet, for example, onlycontains open-class words, and it lacks theextensive subcategorization frame and agreementinformation present in Comlex (Miller et al1993; Macleod et al 1994).
Comlex, whilesyntactically deep, doesn?t have tagged usagedata or semantic groupings (Macleod et al1994).
Furthermore, many of these resources donot map to one another or have restrictedlicenses.The goal of our research was to create asyntactic lexicon, like Comlex, that unifiedmultiple existing open-source resourcesincluding Felbaum?s (1998) WordNet, Kipper etal?s (2000) VerbNet, and Wiktionary.Furthermore, we wanted it to have direct links toframe semantic representations via the open-license OpenCyc knowledge base.The result was NU-LEX a lexicon of over100,000 words that has the coverage ofWordNet, is enriched with tense informationfrom automatically screen-scrappingWiktionary1, and contains VerbNetsubcategorization frames.
This lexicon wasincorporated into a bottom-up chart parser,EANLU, that connects the words to Cycrepresentations (Tomai & Forbus 2009).
Eachentry is represented by Cyc assertions andcontains syntactic information as a set of featuresconsistent with previous feature systems (Allen1995; Macleod et al 1994).1http://www.wiktionary.org/3632 Previous WorkComlex is handmade and contains 38,000lemmas.
It represents words in feature value liststhat contain lexical data such as part of speech,agreement information, and syntactic frameparticipation (Macleod et al 1994).
Furthermore,Comlex has extensive mappings to, and usesrepresentations compatible with, multiple lexicalresources (Macleod et al 1994).Attempts to automatically create syntacticlexical resources from tagged corpora have alsobeen successful.
The Penn Treebank is one suchresource (Marcus et al 1993).
These resourceshave been successfully incorporated intostatistical parsers such as the Apple Pie parser(Sekine & Grishman, 1995).
Unfortunately, theystill require extensive labor to do the annotations.NU-LEX is different in that it is automaticallycompiled without relying on a hand-annotatedcorpus.
Instead, it combines crowd-sourced data,Wiktionary, with existing lexical resources.This research was possible because of theexisting lexical resources WordNet and VerbNet.WordNet is a virtual thesaurus that groups wordstogether by semantic similarity into synsetsrepresenting a lexical concept (Felbaum, 1998).VerbNet is an extension of Levin?s (1993) verbclass research.
It represents verb meaning in aclass hierarchy where each verb in a class hassimilar semantic meanings and identical syntacticusages (Kipper et al 2000).
Since its creation ithas been expanded to include classes not inLevin?s original research (Kipper et al 2006).These two resources have already been mapped,which facilitated applying subcategorizationframes to WordNet verbs.Furthermore, WordNet has existing links toOpenCyc.
OpenCyc is an open-source version ofthe ResearchCyc knowledge base that containshierarchical definitional information but ismissing much of the lower level instantiated factsand linguistic knowledge of ResearchCyc(Matuszek et al 2006).
Previous research byMcFate (2010) used these links and VerbNethierarchies to create verb semantic frames whichare used in EANLU, the parser NU-LEX wastested on.3 Creating NU-LEXThe NU-LEX describes words as CycLassertions.
Each form of a word has its ownentry.
For the purposes of integration into aparser that already uses Comlex, the formattingwas kept similar.
Because the lexificationprocess is automatic, formatting changes are easyto implement.3.1 NounsNoun lemmas were initially taken fromFellbaum?s (1998) WordNet index.
Each Lemmawas then queried in Wiktionary to retrieve itsplural form resulting in a triple of word, POS,and plural form:(boat Noun (("plural" "boats")))This was used to create a definition for eachform.
Each definition contains a list of WordNetsynsets from the original word, the orthographicword form which was assumed to be the same asthe word, countability taken from Wiktionarywhen available, the root which was the base formof the word, and the agreement which was eithersingular or plural.
(definitionInDictionary WordNet "Boat"(boat (noun(synset ("boat%1:06:01:?
?boat%1:06:00::"))(orth "boat")(countable +)(root boat) (agr 3s))))3.2 VerbsLike Nouns, verb base lemmas were taken fromthe WordNet index.
Similarly, each verb wasqueried in Wiktionary to retrieve its tense formsresulting in a list similar to that for nouns:(give Verb ((("third-person singular simple present""gives")("present participle" "giving")("simple past" "gave")("past participle" "given"))))These lists in turn were used to create the word,form, and agreement information for a verbentry.
The subcategorization frames were takendirectly from VerbNet.
Root and Orthographicalform were again kept the same.
(definitionInDictionary WordNet "Give"(give (verb(synset ("give%2:41:10::??
"give%2:34:00::"))(orth "give")(vform pres)(subcat (?
S np-v-np-np-pp.assetnp-v-np-pp.recipient-pp.assetnp-v-np-pp.assetnp-v-pp.recipientnp-v-npnp-v-np-dative-np 364np-v-np-pp.recipient))(root give)(agr (?
a 1s 2s 1p 2p 3p)))))3.3 Adjectives and AdverbsAdjectives and adverbs were simply taken fromWordNet.
No information from Wiktionary wasadded for this version of NU-LEX, so it does notinclude comparative or superlative forms.
Thiswill be added in future iterations by usingWiktionary.
The lack of comparatives andsuperlatives caused no errors.
Each definitioncontains the Word, POS, and Synset list:(definitionInDictionary WordNet "Funny"(funny (adjective(root funny)(orth "funny")(synset ("funny%4:02:01::""funny%4:02:00::")))))3.4 Manual AdditionsWordNet only contains open-class words:Nouns, Adjectives, Adverbs, and Verbs (Milleret al 1993).
Thus determiners, subordinatingconjunctions, coordinating conjunctions, andpronouns all had to be hand created.Likewise, Be-verbs had to be manually addedas the Wiktionary page proved too difficult toparse.
These were the only categories added.Notably, proper names and cardinal numbersare missing from NU-LEX.
Numbers arerepresented as nouns, but not as cardinals orordinals.
These categories were not explicit inWordNet (Miller et al 1993).4 Experiment SetupThe sample sentences consisted of 50 samplesfrom the Simple English Wikipedia2 articles onthe heart, lungs, and George Washington.
Theheart set consisted of the first 25 sentences of thearticle, not counting parentheticals.
The lungs setconsisted of the first 13 sentences of the article.The George Washington set consisted of the first12 sentences of that article.
These setscorresponded to the first section or first twosections of each article.
There were 239 uniquewords in the whole set out of 599 words total.Each set was parsed by the EANLU parser.EANLU is a bottom-up chart parser that usescompositional semantics to translate naturallanguage into Cyc predicate calculusrepresentations (Tomai & Forbus 2009).
It isbased on a Allen?s (1995) parser.
It runs on top2http://simple.wikipedia.org/wiki/Main_Pageof the FIRE reasoning engine which it uses toquery the Cyc KB (Forbus et al 2010).Each sentence was evaluated as correct basedon whether or not it returned the proper wordforms.
Since we are not evaluating EANLU?sgrammar, we did not formally evaluate theparser?s ability to generate a complete parse fromthe lex-items, but we note informally that parsecompleteness was generally the same.
Failureoccurred if any lex-item was not retrieved or ifthe parser was unable to parse the sentence dueto system memory constraints.5 ResultsCan NU-LEX perform comparably to existingsyntactic resources despite being automaticallycompiled from multiple resources?
Does itsincreased coverage significantly improveparsing?
How accurate is this lexicon?In particular we wanted to uncover words thatdisappeared or were represented incorrectly as aresult of the screen-scraping process.Overall, across all 50 samples NU-LEX andComlex performed similarly.
NULEX got 25 outof 50 (50%) correct and Comlex got 26 out of 50(52%) of the sentences correct.
The two systemsmade many of the same errors, and a primarysource of errors was the lack of proper nouns ineither resource.
Proper nouns caused sevensentences to fail in both parsers or 29% of totalerrors.Of the NU-LEX failures not caused by propernouns, five of them (20%) were caused bylacking cardinal numbers.
The rest were due tomissing lex-items across several categories.Comlex primarily failed due to missing medicalterminology in the lungs and heart test set.Out of the total 239 unique words, NULEXfailed on 11 unique words not counting propernouns or cardinal numbers.
One additionalfailure was due to the missing pronoun?themselves?
which was retroactively added tothe hand created pronoun section.
This a failurerate of 4.6%.
Comlex failed on 6 unique words,not counting proper nouns, giving it a failure rateof 2.5%.5.1 The HeartFor the heart set 25 sentences were run throughthe parser.
Using NU-LEX, the system correctlyidentified the lex-items for 17 out of 25sentences (68%).
Of the sentences it did not getcorrect, five were incorrect only because of the365lack of cardinal number representation.
Onefailed because of system memory constraints.Using Comlex, the parser correctly identifiedall lex-items for 16 out of 25 sentences (64%).The sentences it got wrong all failed because ofmissing medical terms.
In particular, atrium andvena cava caused lexical errors.5.2 The LungsFor the lung set 13 sentences were run throughthe parser.
Using NU-LEX the system correctlyidentified all lex-items for 6 out of 13 sentences(46%).
Two errors were caused by the lack ofcardinal number representation and one sentencefailed due to memory constraints.
One sentencefailed because of the medical specific term para-bronchi.Four additional errors were due to amalformed verb definitions and missing lexitemslost during screen scraping.Using Comlex the parser correctly identifiedall lex-items for 7 out of 13 sentences (53%).Five failures were caused by missing lex-items,namely medical terminology like alveoli andparabronchi.
One sentence failed due to systemmemory constraints.5.3 George WashingtonFor the George Washington set 12 sentenceswere run through the parser.
This was a set thatwe expected to cause problems for NU-LEX andComlex because of the lack of proper nounrepresentation.
NU-LEX got only 2 out of 12correct and seven of these errors were caused byproper nouns such as George Washington.Comlex did not perform much better, getting 3out of 12 (25%) correct.
All but one of theComlex errors was caused by missing propernouns.6 DiscussionNU-LEX is unique in that it is a syntactic lexiconautomatically compiled from several open-sourceresources and a crowd-sourced website.
Likethese resources it too is open-license.
We?vedemonstrated that its performance is on par withexisting state of the art resources like Comlex.By virtue of being automatic, NU-LEX can beeasily updated or reformatted.
Because it scrapesWiktionary for tense information, NU-LEX canconstantly evolve to include new forms orcorrections.
As its coverage (over 100,000words) is derived from Fellbaum?s (1998)WordNet, it is also significantly larger thanexisting similar syntactic resources.NU-LEX?s first trial demonstrated that it wassuitable for general purpose parsing.
However,much work remains to be done.
The majority oferrors in the experiments were caused by eithermissing numbers or missing proper nouns.Cardinal numbers could be easily added toimprove performance.
Furthermore, solutions tomissing numbers could be created on thegrammar side of the process.Missing proper nouns represent both a gap andan opportunity.
One approach in the future couldbe to manually add important people or places asneeded.
Because the lexicon is Cyc compliant,other options could include querying the Cyc KBfor people and then explicitly representing theexamples as definitions.
This method has alreadyproven successful for EANLU usingResearchCyc, and could transfer well toOpenCyc.
Screen-scraping Wiktionary could alsoyield proper nouns.With proper noun and number coverage, totalfailures would have been reduced by 48%.
Thus,simple automated additions in the future cangreatly enhance performance.Errors caused by missing or malformeddefinitions were not abundant, showing up inonly 12 of the 50 parses and under half of thetotal errors.
The total error rate for words wasonly 4.6%.
We believe that improvements to thescreen-scrapping program or changes inWiktionary could lead to improvements in thefuture.Because it is CycL compliant the entirelexicon can be formally represented in the Cycknowledge base (Matuszek et al 2006).
Thissupports efficient reasoning and allows systemsthat use NU-LEX to easily make use of the CycKB.
It is easily adaptable in LISP or Cyc basedapplications.
When partnered with the EANLUparser and McFate?s (2010) OpenCyc verbframes, the result is a semantic parser that usescompletely open-license resources.It is our hope that NU-LEX will provide apowerful tool for the natural languagecommunity both on its own and combined withexisting resources.
In turn, we hope that itbecomes better through use in future iterations.ReferencesAllen, James.
1995.
Natural LanguageUnderstanding: 2nd edition.
Benjamin/CummingsPublishing Company, Inc. Redwood City, CA.366Fellbaum, Christiane.
Ed.
1998.
WordNet: AnElectronic Database.
MIT Press, Cambridge, MA.Forbus, K., Hinrichs, T., de kleer, J., and Usher, J.2010.FIRE: Infrastructure for Experience-basedSystems with Common Sense.
AAAI Fall Symposiumon Commonsense Knowledge.
Menlo Park, CA.AAAI Press.Kipper, Karin, Hoa Trang Dang, and Martha Palmer.2000.
Class-Based Construction of a Verb Lexicon.In AAAI-2000 Seventeenth National Conference onArtificial Intelligence, Austin, TX.Kipper, Karin,  Anna Korhonen, Neville Ryant, andMartha Palmer.
2006.
Extending VerbNet with NovelVerb Classes.
In Fifth International Conference onLanguage Resources and Evaluation (LREC 2006).Genoa, Italy.Levin, Beth.
1993.
English Verb Classes andAlternation: A Preliminary Investigation.
TheUniversity of Chicago Press, Chicago.Macleod, Catherine, Ralph Grishman, and AdamMeyers.
1994 Creating a Common SyntacticDictionary of English.
Presented at SNLR:International Workshop on Sharable NaturalLanguage Resources,  Nara, Japan.Marcus, Mitchell, Beatrice Santorini, Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
ComputationalLinguistics.
19(2): 313-330.Matuszek, Cynthia, John Cabral, Michael Witbrock,and John DeOliveira.
2006.
An Introduction to theSyntax and Content of Cyc.
In Proceedings of the2006 AAAI Spring Symposium on Formalizing andCompiling Background Knowledge and ItsApplications to Knowledge Representation andQuestion Answering, Stanford, CA.McFate, Clifton.
2010.
Expanding Verb Coverage inCyc With VerbNet.
In proceedings of the ACL 2010Student Research Workshopl.
Uppsala, Sweden,Miller, George, Richard Beckwith, ChristianeFellbaum, Derek Gross, and Katherine Miller.
1993.Introduction to WordNet: An On-line LexicalDatabase.
In Fellbaum, Christiane.
Ed.
1998.WordNet: An Electronic Database.
MIT Press,Cambridge, MA.Sekine, Satoshi, and Ralph Grishman.
1995.
ACorpus-based Probabilistic Grammar with Only TwoNon-terminals.
In Fourth International Workshop onParsing Technologies.
Prague, Czech Republic.Tomai, Emmet, and Kenneth Forbus.
2009.
EA NLU:Practical Language Understanding for CognitiveModeling.
In Proceedings of the 22nd InternationalFlorida Artificial Intelligence Research SocietyConference, Sanibel Island, FL.367
