Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
257?266, Prague, June 2007. c?2007 Association for Computational LinguisticsRecovering Non-Local Dependencies for ChineseYuqing GuoNCLT, School of ComputingDublin City UniversityDublin 9, Irelandyguo@computing.dcu.ieHaifeng WangToshiba (China)Research and Development CenterBeijing, 100738, Chinawanghaifeng@rdc.toshiba.com.cnJosef van GenabithNCLT, School of ComputingDublin City UniversityIBM CAS, Dublin, Irelandjosef@computing.dcu.ieAbstractTo date, work on Non-Local Dependencies(NLDs) has focused almost exclusively onEnglish and it is an open research questionhow well these approaches migrate to otherlanguages.
This paper surveys non-local de-pendency constructions in Chinese as repre-sented in the Penn Chinese Treebank (CTB)and provides an approach for generatingproper predicate-argument-modifier struc-tures including NLDs from surface context-free phrase structure trees.
Our approach re-covers non-local dependencies at the levelof Lexical-Functional Grammar f-structures,using automatically acquired subcategorisa-tion frames and f-structure paths linking an-tecedents and traces in NLDs.
Currently ouralgorithm achieves 92.2% f-score for traceinsertion and 84.3% for antecedent recoveryevaluating on gold-standard CTB trees, and64.7% and 54.7%, respectively, on CTB-trained state-of-the-art parser output trees.1 IntroductionA substantial number of linguistic phenomena suchas topicalisation, relativisation, coordination andraising & control constructions, permit a constituentin one position to bear the grammatical role asso-ciated with another position.
These relationshipsare referred to Non-Local Dependencies (NLDs),where the surface location of the constituent iscalled /antecedent0, and the site where the an-tecedent should be interpreted semantically is called/trace0.
Capturing non-local dependencies is cru-cial to the accurate and complete determination ofsemantic interpretation in the form of predicate-argument-modifier structures or deep dependencies.However, with few exceptions (Model 3 ofCollins, 1999; Schmid, 2006), output trees pro-duced by state-of-the-art broad coverage statisticalparsers (Charniak, 2000; Bikel, 2004) are only sur-face context-free phrase structure trees (CFG-trees)without empty categories and coindexation to repre-sent displaced constituents.
Because of the impor-tance of non-local dependencies in the proper de-termination of predicate-argument structures, recentyears have witnessed a considerable amount of re-search on reconstructing such hidden relationshipsin CFG-trees.
Three strategies have been proposed:(i) post-processing parser output with pattern match-ers (Johnson, 2002), linguistic principles (Campbell,2004) or machine learning methods (Higgins, 2003;Levy and Manning, 2004; Gabbard et al, 2006) torecover empty nodes and identify their antecedents;1(ii) integrating non-local dependency recovery intothe parser by enriching a simple PCFG model withGPSG-style gap features (Collins, 1999; Schmid,2006); (iii) pre-processing the input sentence witha finite-state trace tagger which detects empty nodesbefore parsing, and identify the antecedents on theparser output with the gap information (Dienes andDubey, 2003a; Dienes and Dubey, 2003b).In addition to CFG-oriented approaches, a num-ber of richer treebank-based grammar acquisitionand parsing methods based on HPSG (Miyao etal., 2003), CCG (Clark and Hockenmaier, 2002),LFG (Riezler et al, 2002; Cahill et al, 2004) andDependency Grammar (Nivre and Nilsson, 2005)incorporate non-local dependencies into their deepsyntactic or semantic representations.A common characteristic of all these approaches1(Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe post-processing methods to recover NLDs, which are applied to syn-tactic dependency structures converted from CFG-trees.257is that, to date, the research has focused almostentirely on English,2 despite the disparity in typeand frequency of non-local dependencies for vari-ous languages.
In this paper, we address recover-ing non-local dependencies for Chinese, a languagedrastically different from English and whose spe-cial features such as lack of morphological inflectionmake NLD recovery more challenging.
Inspired by(Cahill et al, 2004)?s methodology which was origi-nally designed for English and Penn-II treebank, ourapproach to Chinese non-local dependency recoveryis based on Lexical-Functional Grammar (LFG), aformalism that involves both phrase structure treesand predicate-argument structures.
NLDs are re-covered in LFG f-structures using automatically ac-quired subcategorisation frames and finite approxi-mations of functional uncertainty equations describ-ing NLD paths at the level of f-structures.The paper is structured as follows: in Section 2 weoutline the distinguishing features of Chinese non-local dependencies compared to English.
In Section3 we review (Cahill et al, 2004)?s method for recov-ering English NLDs in treebank-based LFG approx-imations.
In Section 4, we describe how we mod-ify and substantially extend the previous methodto recover all types of NLDs for Chinese data.We present experiments and provide a dependency-based evaluation in Section 5.
Finally we concludeand summarise future work.2 Non-Local Dependencies in ChineseIn the Penn Chinese Treebank (CTB) (Xue et al,2002) non-local dependencies are represented interms of empty categories (ECs) and (for some ofthem) coindexation with antecedents, as exemplifiedin Figure 1.
Following previous work for Englishand the CTB annotation scheme, we use /non-local dependencies0as a cover term for all miss-ing or dislocated elements represented in the CTBas an empty category (with or without coindexa-tion/antecedent), and our use of the term remains ag-nostic about fine-grained distinctions between non-local dependencies drawn in the theoretical linguis-tics literature.In order to give an overview on the character-2 (Levy and Manning, 2004) is the only approach we areaware of that has been applied to both English and German.
(1) ?
?u?
?k d?
 # ?
[not want look-for train have potential DE new writer?
(People) don?t want to look for and train new writers whohave potential.
?IPNP-SBJ-NONE-*pro*VPADVPAD?notVPVV?wantIP-OBJNP-SBJ-NONE-*PRO*VPVPVVu?look forNP-OBJ-NONE-*RNR*-2PU!VPVV?trainNP-OBJ-2CPWHNP-1-NONE-*OP*CPIPNP-SBJ-NONE-*T*-1VPVEkhaveNPNNd?potentialDECDEADJPJJ#newNPNN?
[writerFigure 1: Example of non-local annotations in CTB,including dropped subject (*pro*), control subject(*PRO*), relative clause (*T*), and coordination(*RNR*).istics of Chinese non-local dependencies, we ex-tracted all empty categories together with coindexedantecedents from the Penn Chinese Treebank ver-sion 5.1 (CTB5.1).
Table 1 gives a breakdown of themost frequent types of empty categories and theirantecedents, which account for 43,791 of the total43,954 (99.6%) ECs in CTB5.1.3According to their different linguistics properties,we divide the empty nodes listed in Table 1 intothree major types: null relative pronouns, locallymediated dependencies, and long-distance depen-dencies.Null Relative Pronouns (lines 2, 7) themselvesare local dependencies, and thus are not coindexedwith an antecedent.
But they mediate non-local de-pendencies by functioning as antecedents for the dis-3An extensive description of the types of empty categoriesand the use of coindexation in CTB can be found in Section VIof the bracketing guidelines.258Antecedent POS Label Count Description1 WHNP NP *T* 11670 WH trace (e.g.
*OP*?I/Chinau/launch*T*/DE?
(/satellite)2 WHNP *OP* 11621 Empty relative pronouns (e.g.
*OP*?I/Chinau/launch/DE?
(/satellite)3 NP *PRO* 10946 Control constructions (e.g.
?p/here?/notN/allow*PRO*?
?/smoke)4 NP *pro* 7481 Pro-drop situations (e.g.
*pro*?/notQ/ever?/encounter/DE?K/problem)5 IP IP *T* 575 Topicalisation (e.g.
?
?/weU/canI/win?/he`/say*T*)6 WHPP PP *T* 337 WH trace (e.g.
*OP*<?/population*T*?8/dense/?/area)7 WHPP *OP* 337 Empty relative pronouns (e.g.
*OP*<?/population?8/dense/?/area)8 NP NP * 291 Raising & passive constructions (e.g.
??/we/BEI?
?/exclude*3	/outside)9 NP NP *RNR* 258 Coordinations (e.g.
y/encourage*RNR*?/and|?/support?
?/investment)10 CLP CLP *RNR* 182 Coordinations (e.g.
?/five*RNR*?/to?/ten?/hundred million/Yuan)11 NP NP *T* 93 Topicalisation (e.g.
Y/salary?/all^/use*T*5/for?W/pleasure)Table 1: The distribution of the most frequent types of empty categories and their antecedents in CTB5.1.The types with frequency less than 30 are ignored.located constituent inside a relative clause.4Locally Mediated Dependencies are non-local asthey are projected through a third lexical item (suchas a control or raising verb) which involves a de-pendency between two adjacent levels and they aretherefore bounded.
This type encompasses: (line8) raising constructions, and short-bei constructions(passivisation); (line 3) control constructions, whichincludes two different types: a generic *PRO* withan arbitrary reading (approximately equals to unex-pressed subjects of to-infinitive and gerund verbs inEnglish); and a *PRO* with definite reference (sub-ject or object control).5Long-Distance Dependencies (LDDs) differfrom locally mediated dependencies, in that thepath linking the antecedent and trace might beunbounded (also called unbounded, long-rangedependencies).
LDDs include the followingphenomena:Wh-traces in relative clauses, where an argument(line 1) or adjunct (line 6) /moves0and is coin-dexed with the/extraction0site.Topicalisation (lines 5, 11) is one of the typicalLDDs in English, whereas in Chinese not all topicsinvolve displacement, for instance (2).
(2) ?
?U ?
{Beijing autumn most beautiful?Autumn is the most beautiful in Beijing.
?4Null relative pronouns used in the CTB annotation are todistinguish relative clauses in which an argument or adjunct ofthe embedded verb is /missing0from complement (apposi-tive) clauses which do not involve non-local dependencies.5However in this case the CTB annotation doesn?t coindexthe locus (trace) with its controller (antecedent).Coordination is divided into two groups: rightnode raising of an NP phrase which is an argumentshared by the coordinate predicates (line 9); and thecoordination of quantifier phrases (line 10) and ver-bal phrases (3), in which the antecedent and traceare both predicates and possibly take their own ar-guments or adjuncts.
(3) ??
?O  ?i ?
*RNR*?I and he respectively go to company and *RNR* hospital?I went to the company and he went to the hospital re-spectively.
?Pro-drop situations (line 4) are prominent inChinese because subject and object are only seman-tically but not syntactically required.
Neverthelesswe also treat pro-drop as a long-distance depen-dency as in principle the dropped subjects can bedetermined from the general (often inter-sentential)context.Table 2 gives a quantitative comparison of NLDsbetween Chinese data in CTB5.1 and English inPenn-II.
The data reveals that: first, NLDs in Chi-nese are much more frequent than in English (bynearly 1.5 times); and moreover 69% are not explic-itly linked to an antecedent, compared to 43% forEnglish, due to the high prevalence of pro-drop inChinese.# of # of # of # non- % non-sent EC EC/sent coindex coindexChinese 18,804 43,954 2.34 30,429 69.23English 49,207 79,245 1.61 34,455 43.48Table 2: Comparison of NLDs between Chinese datain CTB5.1 and English in Penn-II .259(4) a ?
?^ 5?Wmoney we use to please?Money, we use for pleasure.?IPNP-TPC-1NNamoneyNP-SBJPN??weVPVPVV^useNP-OBJ-NONE-*T*-1IPNP-SBJ-NONE-*PRO*VPMSP5toVV?WpleaseIPNP-TPC[?TOPIC=?][?TOPIC=?COMP*OBJ]NN[?=?]amoneyNP-SBJ[?SUBJ=?]PN[?=?]??weVP[?=?]VV[?=?]^useVP[?XCOMP=?][?SUBJ=?XCOMP:SUBJ]MSP[?msp=?5?]5toVV[?=?
]?Wpleasef1 :?????????????????????????
?PRED ?^?SUBJ, OBJ, XCOMP?
?GLOSS ?use?TOPIC f2 :[PRED ?a?GLOSS ?money?
]1SUBJ f3 :[PRED ???
?GLOSS ?we?
]2OBJ 1XCOMP f4 :?????
?PRED ??W?SUBJ?
?GLOSS ?please?SUBJ 2MSP ?5?????????????????????????????????
(a) (b) (c)Figure 2: (a) the CTB tree; (b) LFG c-structure with functional equations; (c) corresponding f-structure.(?)
in the functional annotation refers to the f-structure associated with the mother node and (?)
to that ofthe local node.3 NLD Recovery in LFG Approximations3.1 Lexical Functional GrammarLexical Functional Grammar (Kaplan and Bres-nan, 1982) is a constraint-based grammar formal-ism which minimally involves two levels of syn-tactic representation: c(onstituent)-structure andf(unctional)-structure.
C-structure takes the form ofCFG-trees and captures surface grammatical config-urations.
F-structure encodes more abstract gram-matical functions (GFs) such as SUBJ(ect), OBJ(ect),COMP(lement), ADJ(unct) and TOPIC etc., in theform of Attribute Value Matrices which approxi-mate to basic predicate-argument-adjunct structuresor dependency relations.
C-structures are related tof-structures by functional annotations (cf.
Figure 2(b) & (c)).In LFG, non-local dependencies are captured atf-structure level in terms of reentrancies, indicated1 for the topicalisation and 2 for the control con-struction in Figure 2(c) obviating the need for tracesand coindexation in the c-structure (Figure 2(b)), un-like in CTB trees (Figure 2(a)).
LFG uses func-tional uncertainty (FU) equations (regular expres-sions) to specify paths in f-structures between thetrace and its antecedent.
To account for the reen-trancy 1 in the f-structure, a FU equation of theform ?TOPIC=?COMP*OBJ is required (as the lengthof the dependency might be unbounded).
The equa-tion states that the value of the TOPIC attribute istoken identical with the value of the final OBJ argu-ment along a path through the immediately enclos-ing f-structure along zero or more COMP attributes.In addition to FU equations, subcategorisation in-formation is also a significant ingredient in LFG?saccount of non-local dependencies.
Subcategorisa-tion frames (subcat frames) specify the governablegrammatical functions (i.e.
arguments) required bya particular predicate.
In Figure 2(c) each predicatein the f-structure is followed by its subcat frame.3.2 F-Structure Based NLD Recovery(Cahill et al, 2004) presented a NLD recovery al-gorithm operating at LFG f-structure for treebank-based LFG approximations.
The method automati-cally converts Penn-II treebank trees with traces andcoindexation into proper f-structures where tracesand coindexation in treebank trees (Figure 2(a))are represented as corresponding reentrances in f-structures (Figure 2(c)), and from the f-structuresautomatically extracts subcat frames by collectingall arguments of the local predicate at each level ofthe f-structures, and further acquires finite approxi-mations of FU equations by extracting paths linkingthe reentracies occurring in the f-structures.
(Cahill et al, 2004)?s approach for English re-solves three LDD types in parser output trees with-out traces and coindexation (Figure 2(b)), i.e.
topi-calisation (TOPIC), wh-movement in relative clauses(TOPIC REL) and interrogatives (FOCUS).
Given260a set of subcat frames s for lemma w with prob-abilities P (s|w), a set of paths p linking reen-trancies conditioned on the triggering antecedent a(TOPIC, TOPIC REL or FOCUS) with probabilitiesP (p|a), the core algorithm recursively traverses anf-structure f to:- find a TOPIC|TOPIC REL|FOCUS:g pair;- traverse f along path p to the sub-f-structure h;- retrieve the local PRED:w at h, and insert g to hiff* all GFs specified in the subcat frame s ex-cept g are present at h (completeness con-dition)* no other governable GFs present at h arespecified in s (coherence condition)- rank resolution candidates according to theproduct of subcat frame and NLD path prob-abilities (Eq.
1).P (s|w) ?
P (p|a) (1)4 NLD Recovery Algorithm for Chinese4.1 Automatic F-Structure GenerationOur NLD recovery is done at the level of LFG f-structures.
Inspired by (Cahill et al, 2004; Burke etal., 2004), we have implemented an f-structure anno-tation algorithm to automatically obtain f-structuresfrom CFG-trees in the CTB5.1.
The f-structure an-notation algorithm, described below, is applied bothto the original CTB trees providing functional tags,traces and coindexation to generate the training cor-pus, and to the parser output trees without tracesand coindexation to provide the f-structure input forNLD recovery.1.
The CFG-trees are head-lexicalised by head-finding rules similar to (Collins, 1999), adaptedto CTB.2.
Each local subtree of depth one is partitionedby the head into left and right context.
Left-right context rules exploiting configurational,categorial and CTB functional tag informationare used to assign each left and right constituentwith appropriate functional equations.3.
Empty nodes and coindexation in the CTB treesare automatically captured into correspondingreentrances at f-structure via functional equa-tions.4.
All the functional equations are collected andthen passed to a constraint solver to generatef-structures.4.2 Adaptation to Chinese(Cahill et al, 2004)?s algorithm (Section 3.2) onlyresolves certain NLDs with known types of an-tecedents (TOPIC, TOPIC REL and FOCUS) at f-structures.
However, as illustrated in Section 2, ex-cept for relative clauses, the antecedents in ChineseNLDs do not systematically correspond to types ofgrammatical function.
Furthermore nearly 70% ofall empty categories are not coindexed with an an-tecedent.
In order to resolve all Chinese NLDs rep-resented in the CTB, we modify and substantiallyextend the (Cahill et al, 2004) (henceforth C04 forshort) algorithm as follows:Given the set of subcat frames s for the word w,and a set of paths p for the trace t, the algorithmtraverses the f-structure f to:- predict a dislocated argument t at a sub-f-structure h by comparing the local PRED:w tow?s subcat frames s- t can be inserted at h if h together with t iscomplete and coherent relative to subcat frames- traverse f starting from t along the path p- link t to it?s antecedent a if p?s ending GF aexists in a sub-f-structure within f ; or leave twithout an antecedent if an empty path for t ex-istsIn the modified algorithm, we condition the proba-bility of NLD path p (including the empty path with-out an antecedent) on the GF associated of the tracet rather than the antecedent a as in C04.
The pathprobability P (p|t) is estimated as:P (p|t) = count(p, t)?ni=1 count(pi, t)(2)In contrast even to English, Chinese has very lit-tle morphological information.
As a result, everyword in Chinese has a unique form regardless of itssyntactic distribution.
For this reason we use moresyntactic features w feats in addition to word formto discriminate between appropriate subcat frames s.For a given word w, w feats include:261- w pos: the part-of-speech of w- w gf: the grammatical function of wP (s|w,w feats) replaces C04?s P (s|w) as lexicalsubcat frame probability and is estimated as:P (s|w, w feats) = count(s, w, w feats)?ni=1 count(si, w, w feats)(3)As more conditioning features may cause seversparse-data problems, in order to increase the cov-erage of the automatically acquired subcat frames,the subcat frame frequencies count(s,w,w feats)are smoothed by backing off to w?s part-of-speechw pos according to Eq.
(4).
P (s|w pos) is esti-mated according to Eq.
(5) and weighted by a param-eter ?.
The lexical subcat frame probabilities are es-timated from the smoothed frequencies as shown inEq.
(6).countbk(s, w, w feats) = count(s, w, w feats) (4)+?P (s|w pos)P (s|w pos) = count(s, w pos, w gf)?ni=1 count(si, w pos, w gf)(5)Pbk(s|w, w feats) =countbk(s, w, w feats)?ni=1 countbk(si, w, w feats)(6)Finally, NLD resolutions are ranked according to:Pbk(s|w,w feats) ?m?j=1P (p|tj) (7)As, apart from the maximum number of argumentsin a subcat frame, there is no a priori limit onthe number of dislocated arguments in a local f-structure, we rank resolutions with the product ofthe path probabilities of each (of m) missing argu-ment(s).4.3 A Hybrid Fine-Grained StrategyAs described in Section 2, there are three typesof NLDs in the CTB, and their different lin-guistic properties may require fine-grained recov-ery strategies.
Furthermore, as the NLD recov-ery method described in Section 4.2 is triggeredby /missing0subcategorisable grammatical func-tions, a few cases of NLDs in which the trace is notan argument in the f-structure, e.g.
an ADJUNCT orTOPIC in relative clauses or an null PRED in verbalcoordination, can not be recovered by the algorithm.Table 3 shows the types of NLD that can be recov-ered by C04 and by the algorithm presented in Sec-tion 4.2.
Table 3 shows that a hybrid methodologyis required to resolve all types of NLDs in the CTB.The hybrid method involves four strategies:?
Applying a few simple heuristic rules to insertthe empty PRED for coordinations and null rel-ative pronouns for relative constructions.
Theformer is done by comparing the part-of-speechof the local predicates and their arguments ineach coordinate; and the latter is triggered byGF ADJUNCT REL in our system.?
Inserting an empty node with GF SUBJ forshort-bei construction, control and raising con-structions, and relate it to the upper-levelSUBJ or OBJ accordingly.?
Exploiting the C04 algorithm to resolve the wh-trace in relativisation, including ungovernableGFs TOPIC and ADJUNCT.?
Using our modified algorithm (Section 4.2) toresolve the remaining types, viz.
long-distancedependencies in Chinese.Antecedent TraceTopic Rel Other Null Argument AdjunctC04?
?
?Ours?
?
?
?Table 3: Comparison of the ability of NLD recoveryfor Chinese between C04 and our algorithm5 Experiments and EvaluationFor all our experiments, we used the first 760articles (chtb 001.fid to chtb 931.fid, 10,384 sen-tences) of CTB5.1, from which 75 double-annotatedfiles (chtb 001.fid to chtb 043.fid and chtb 900.fidto chtb 931.fid, 1,046 sentences) were used as testdata,6 75 files (chtb 306.fid to chtb 325.fid andchtb 400.fid to chtb 454.fid, 1,082 sentences) wereheld out as development data, while the other 610files (8,256 sentences) were used as training data.Experiments were carried out on two different kindsof input: first on CTB gold standard trees stripped ofall empty nodes and coindexation information; and6The complete list of double-annotated files can be found inthe documentation of CTB5.1.262second, on the output trees of Bikel?s parser (Bikel,2004).The evaluation metric adopted by most previouswork used the label and string position of the traceand its antecedent (Johnson, 2002).
As pointedout by (Campbell, 2004), this metric is insensitiveto the correct attachment of the EC into the parsetree, and more importantly it is not clear whetherit adequately measures performance in predicate-argument structure recovery.
Therefore, we usea predicate-argument based evaluation method in-stead.
The NLD recovery is represented as a triple inthe form of REL(PRED : loc, GF : loc), where REL isthe relation between the dislocated GF and the PRED.In the evaluation for insertion of traces, the GF isrepresented by the empty category, and in the eval-uation for antecedent recovery, the GF is realised bythe predicate of the antecedent, e.g.
OBJ(^/use:3,a/money:1) in Figure 2(c).
The antecedent andPRED are both numbered with their string positionin the input sentence.
Precision, recall and f-scoreare calculated for the evaluation.5.1 CTB-Based F-Structure and NLDResources Acquisition5.1.1 Automatically Acquired F-StructuresAs described in Section 4.1, we automaticallygenerate LFG f-structures from the CTB trees to ob-tain the training data and generate f-structures fromthe parser output trees, on which the NLDs will berecovered.
To evaluate the performance of the auto-matic f-structure annotation algorithm, we randomlyselected 200 sentences from the test set and man-ually annotated the f-structures to generate a goldstandard.
The evaluation metric is the same as forNLD recovery in terms of predicate-argument rela-tions.
Table 4 reports the results against the 200-sentence gold standard given the original CTB treesand trees output by Bikel?s parser.Dependencies Precision Recall F-ScoreCTB Trees 95.60 95.82 95.71Parser Output 74.37 73.15 73.75Table 4: Evaluation of f-structure annotation5.1.2 Acquiring Subcat Frames and NLD PathsFrom the automatically generated f-structuretraining data, we extract 144,119 different lexicalsubcat frames and 178 paths linking traces and an-tecedents for NLD recovery.
Tables 5 & 6 showsome examples of the automatically extracted sub-cat frames and NLD paths respectively.Word:POS-GF(Subcat Frames) Prob.M?
:VV-adj rel([subj,obj]) 0.7655M?
:VV-adj rel([subj]) 0.1537M?
:VV-adj rel([subj,xcomp]) 0.0337......
...M?
:VV-coord([subj,obj]) 0.7915M?
:VV-coord([subj]) 0.0975......
...M?
:VV-top([subj,obj]) 0.5247M?
:VV-top([subj,comp]) 0.2077...... ...Table 5: Examples of subcat framesTrace (Path) Prob.adjunct(up-adjunct:down-topic rel) 0.9018adjunct(up-adjunct:up-coord:down-topic rel) 0.0192adjunct(NULL) 0.0128...... ...obj(up-obj:down-topic rel) 0.7915obj(up-obj:up-coord:down-coord:down-obj) 0.1108...... ...subj(NULL) 0.3903subj(up-subj:down-topic rel) 0.2092...... ...Table 6: Examples of NLD paths5.2 The Basic ModelThe basic algorithm described in Section 4.2 canbe used to indiscriminately resolve almost all NLDtypes for Chinese including locally mediated de-pendencies with few exceptions (traces with modi-fier GFs, which accounts for about 1.5% of all NLDsin CTB5.1).
Table 7 shows the results of the basic al-gorithm for trace insertion and antecedent recoveryon both stripped CTB trees and parser output trees.For comparison, we implemented the C04 algorithmon our data and evaluated the result.
Since the ba-sic algorithm focus on argument traces, results forarguments only are given separately.Table 7 shows that the C04 algorithm achieves ahigh precision but as expected a low recall due toits limitation to certain types of NLDs.
By con-trast, our basic algorithm scored higher recall butlower precision, which is understandable as the C04algorithm identifies the trace given a known an-tecedent, whereas our algorithm tries to identifyboth the trace and antecedent.
Compared to trace263Insertion RecoveryCTB Trees Parser Output CTB Trees Parser OutputPrec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
F(Cahill et al, 2004)overall 95.98 57.86 72.20 73.00 40.28 51.91 90.16 54.35 67.82 65.54 36.16 46.61args only 98.64 42.03 58.94 82.69 30.54 44.60 86.36 36.80 51.61 66.08 24.40 35.64Basic Modeloverall 92.44 91.28 91.85 63.87 62.15 63.00 63.12 62.33 62.72 42.69 41.54 42.10args only 89.42 92.95 91.15 60.89 63.45 62.15 47.92 49.81 48.84 31.41 32.73 32.06Basic Model with Subject Path Constraintoverall 92.16 91.36 91.76 63.72 62.20 62.95 75.96 75.30 75.63 50.82 49.61 50.21args only 89.04 93.08 91.02 60.69 63.52 62.07 66.15 69.15 67.62 42.77 44.76 44.76Table 7: Evaluation of trace insertion and antecedent recovery for C04 algorithm, our basic algorithm andbasic algorithm with the subject path constraint.Insertion RecoveryBasic Model Hybrid Model Basic Model Hybrid ModelPrec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
FOverall 92.16 91.36 91.76 92.86 91.45 92.15 75.96 75.30 75.63 84.92 83.64 84.28SUBJ 92.95 97.81 95.32 94.38 97.81 96.06 66.93 70.42 68.63 81.61 84.57 83.06OBJ 65.28 64.98 65.13 78.95 55.30 65.04 61.57 61.29 61.43 75.66 53.00 62.33ADJUNCT 0.0 0.0 0.0 38.24 25.49 30.59 0.0 0.0 0.0 38.24 25.49 30.59TOPIC 0.0 0.0 0.0 33.33 35.14 34.21 0.0 0.0 0.0 33.33 35.14 34.21TOPIC REL 99.85 99.39 99.62 99.85 99.39 99.62 99.85 99.39 99.62 99.85 99.39 99.62COORD 90.00 100.00 94.74 90.00 100.00 94.74 90.00 100.00 94.74 90.00 100.00 94.74Table 8: Breakdown of trace insertion and antecedent recovery results on stripped CTB trees for the hybridmodel by major grammatical functions.insertion, the general results for antecedent identifi-cation are rather poor.
Examining the developmentdata, we found that most recovery errors were dueto wrongly treating missing SUBJs as a PRO (usingempty NLD paths).
Since the subject in Chinese hasa very strong tendency to be omitted if it can be in-ferred from context, the empty NLD path (withoutany antecedent) has the greatest probability in allresolution paths conditioned on SUBJ, and preventsthe SUBJ from finding a proper antecedent in certaincases.
To test the effect of the empty path on SUBJ,we weighted non-empty paths for SUBJ so as to sup-press the empty path.
After testing on the develop-ment set, the optimal weight was found to be 1.9.The subject path constraint model shows a dramaticimprovement of 12.9% and 8.1% for the overall re-sult of antecedent recovery on CTB trees and parseroutput trees.5.3 The Hybrid Fine-Grained ModelAs proposed in Section 4.3, we implemented a morefine-grained strategy to capture specific linguisticproperties of different NLD types in the CTB.
Wealso combine our basic algorithm (Section 4.2) with(Cahill et al, 2004)?s algorithm in order to resolvethe modifier-function traces.
The two algorithmsmay conflict due to (i) inserting the same trace atthe same site but related to different antecedents or(ii) resolving the same antecedent to different traces.We keep the traces inserted by the C04 algorithmand abandon those inserted by our algorithm in caseof conflict, as the results in Section 5.2 suggest thatC04 has a higher precision than ours.
Table 8 re-ports the results of trace insertion and antecedent re-covery, respectively, on stripped CTB trees, brokendown by major GFs.The fine-grained hybrid model allows us to re-cover NLDs with traces with modifier functions and,more importantly it is sensitive to particular linguis-tic properties of different NLD types.
As the hybridmodel separates the locally mediated dependenciesfrom other long-distance dependencies, it increasesthe f-score by 8.7% for antecedent recovery com-pared with the basic model.
Table 9 reports theresults of the hybrid model on parser output trees,which shows an increase of 3.6% for antecedent re-264covery (compared with Table 7).Insertion RecoveryPrec.
Rec.
F Prec.
Rec.
Foverall 64.07 62.37 63.21 54.53 53.08 53.79Table 9: Evaluation of hybrid model for trace inser-tion and antecedent recovery on parser output trees.5.4 Better Training for Parser OutputOur experiments show that although our NLD recov-ery algorithm performs well on stripped CTB trees,it is sensitive to the noise in parser output trees, witha performance drop of about 30%.
This is in con-trast to English data, on which (Johnson, 2002) re-ports a drop of 7-9% moving from treebank trees toparser output trees.
No doubt this is partially due tothe poor performance of the parser on Chinese data.It is widely accepted that parsing Chinese is moredifficult than parsing other more configurational orricher morphological languages, such as English.7Our NLD recovery algorithm runs on automaticallygenerated LFG f-structures.
The f-structure annota-tion algorithm is highly tailored to the CTB brack-eting scheme (using configurational, categorial andfunctional tag information), and suffers consider-ably from errors produced by the parser.
Table 4shows that performance of the f-structure annotationdecreases sharply (about 22%) for the parser outputtrees and this contributes to the eventual trace inser-tion and antecedent recovery performance drop.Since the f-structures automatically generatedfrom parser output trees are substantially differentfrom those generated from the original CTB trees,our method to obtain the NLD resolution trainingdata suffers from a serious drawback: the trainingdata come from perfect CTB trees, whereas test dataare derived from imperfect parser output trees.
Thisconstitutes a serious drawback for machine learningbased approaches, such as ours: ideally, instancesseen during training should be similar to unseen testdata.
To make training examples more similar to testinstances, we reparse the training set to obtain bet-ter training data.
To avoid running the parser on thetraining data, we carried out 10-fold-cross training,dividing the training data into 10 parts and parsing7(Bikel, 2004) reports 89% f-score for English parsing ofPenn-II treebank data and 79% f-score for Chinese parsing onCTB version 3.each part in turn with the parser trained on the re-maining 9 parts.
The reparsed training data are moresimilar to the test data than the original perfect CTBtrees.
We then converted both the reparsed train-ing data and the original CTB trees into f-structures,and by comparing with the f-structures generatedfrom the original CTB trees, we recovered the emptynodes and coindexation on the f-structures gener-ated from the reparsed training data.
We used parseroutput based f-structures to train our NLD recoverymodel and recovered NLDs for parser output treesfrom the test data.
Table 10 presents the resultsfor trace insertion and antecedent recovery on parseroutput trees using the improved training method,which shows a clear increase in precision and almostthe same recall over the normal training (Table 9).Insertion RecoveryPrec.
Rec.
F Prec.
Rec.
Foverall 67.29 62.33 64.71 56.88 52.69 54.71Table 10: Evaluation of hybrid model for trace inser-tion and antecedent recovery on parser output treeswith better training.6 ConclusionWe have presented an algorithm for recovering non-local dependencies for Chinese.
Our method revisesand considerably extends the approach of (Cahill etal., 2004) originally designed for English, and, tothe best of our knowledge, is the first NLD recov-ery algorithm for Chinese.
The evaluation showsthat our algorithm considerably outperforms (Cahillet al, 2004)?s with respect to Chinese data.In future work, we will refine and extend the con-ditioning features in our models to discriminate sub-cat frames and explore the possibilities to use theChinese Propbank and Hownet to supplement ourautomatically acquired subcat frames.
We will in-vestigate ways of closing the gap between the per-formance of gold-standard and parer output trees,including improving parsing result for Chinese.
Wealso plan to adapt other NLD recovery methods (Ji-jkoun and Rijke, 2004; Schmid, 2006) to Chineseand compare them with the current results.AcknowledgementsThis research is funded by Science Foundation Ire-land grant 04/IN/I527.265ReferencesAoife Cahill, Michael Burke, Ruth O?Donovan, Josef vanGenabith and Andy Way.
2004.
Long-Distance De-pendency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations.
In Proceed-ings of the 42nd Annual Meeting of the Association for Com-putational Linguistics, pages 320-327.
Barcelona, Spain.Daniel M. Bikel.
2004.
On the Parameter Space of Gener-ative Lexicalized Statistical Parsing Models.
Ph.D. thesis,Department of Computer & Information Science, Universityof Pennsylvania.
Philadelphia, PA.Derrick Higgins.
2003.
A machine-learning approach to theidentification of WH gaps.
In Proceedings of the 10th Con-ference of the European Chapter of the Association for Com-putational Linguistics, 99-102.
Budapest, Hungary.Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.In Proceedings of the 1st Annual Meeting of the North Amer-ican Chapter of the Association for Computational Linguis-tics, pages 132-139.
Seattle, WA.Helmut Schmid.
2006.
Trace Prediction and Recovery WithUnlexicalized PCFGs and Slash Features.
In Proceedingsof the 21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Association forComputational Linguistics, pages 177-184.
Sydney, Aus-tralia.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-Projective De-pendency Parsing.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics, pages99-106.
Ann Arbor, Michigan.Mark Johnson.
2002.
A Simple Pattern-Matching Algorithmfor Recovering Empty Nodes and Their Antecedents.
InProceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 136-143.
Philadelphia,PA.Michael Burke, Olivia Lam, Rowena Chan, Aoife Cahill, RuthO?Donovan, Adams Bodomo, Josef van Genabith and AndyWay.
2004.
Treebank-Based Acquisition of a ChineseLexical-Functional Grammar.
In Proceedings of the 18th Pa-cific Asia Conference on Language, Information and Com-putation, pages 161-172, Tokyo, Japan.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Department ofComputer & Information Science, University of Pennsylva-nia.
Philadelphia, PA.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002.Building a Large-Scale Annotated Chinese Corpus.
In Pro-ceedings of the 19th International Conference on Computa-tional Linguistics, pages 1100-1106.
Taipei, Taiwan.Pe?ter Dienes and Amit Dubey.
2003a.
Deep syntactic process-ing by combining shallow methods.
In Proceedings of the41st Annual Meeting of the Association for ComputationalLinguistics, pages 431-438.
Sapporo, Japan.Pe?ter Dienes and Amit Dubey.
2003b.
Antecedent Recov-ery: Experiments with a Trace Tagger.
In Proceedings ofthe 2003 Conference on Empirical Methods in Natural Lan-guage Processing, pages 33-40.
Sapporo, Japan.Richard Campbell.
2004.
Using Linguistic Principles to Re-cover Empty Categories.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguistics,pages 645-652.
Barcelona, Spain.Roger Levy and Christopher D. Manning.
2004.
Deep Depen-dencies from Context-Free Statistical Parsers: Correcting theSurface Dependency Approximation.
In Proceedings of the42nd Annual Meeting of the Association for ComputationalLinguistics, pages 327-334.
Barcelona, Spain.Ronald M. Kaplan and Joan Bresnan.
1982.
Lexical FunctionalGrammar: a Formal System for Grammatical Representa-tion.
The Mental Representation of Grammatical Relations,pages 173-282.
MIT Press, Cambridge, MA.Ryan Gabbard, Seth Kulick, and Mitch Marcus.
2006.
FullyParsing the Penn Treebank In Proceedings of the HumanLanguage Technology Conference / North American Chapterof the Association of Computational Linguistics, pages 184-191.
New York, USA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. Maxwell III and Mark Johnson.
2002.
Pars-ing the Wall Street Journal using a Lexical-Functional Gram-mar and Discriminative Estimation Techniques.
In Proceed-ings of the 40th Annual Meeting of the Association for Com-putational Linguistics, pages 271-278.
Philadelphia, PA.Stephen Clark and Julia Hockenmaier.
2002.
Building DeepDependency Structures with a Wide-CoverageCCG Parser.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, pages 327-334.Philadelphia, PA.Valentin Jijkoun.
2003.
Finding Non-Local Dependencies: Be-yond Pattern Matching.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics,pages 37-43.
Sapporo, Japan.Valentin Jijkoun and Maarten de Rijke.
2004.
Enriching theOutput of a Parser Using Memory-Based Learning.
In Pro-ceedings of the 42nd Annual Meeting of the Associationfor Computational Linguistics, pages 311-318.
Barcelona,Spain.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2003.Probabilistic Modeling of Argument Structures IncludingNon-Local Dependencies.
In Proceedings of the 2003 Con-ference on Recent Advances in Natural Language Process-ing, pages 285-291.
Philadelphia, PA.266
