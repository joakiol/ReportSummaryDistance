Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306?315,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsExploring Syntactic Structural Features for Sub-Tree Alignmentusing Bilingual Tree KernelsJun Sun1,2                            Min Zhang1                    Chew Lim Tan21 Institute for Infocomm Research    2School of Computing, National University of Singaporesunjun@comp.nus.edu.sg     mzhang@i2r.a-star.edu.sg     tancl@comp.nus.edu.sgAbstractWe propose Bilingual Tree Kernels (BTKs) tocapture the structural similarities across a pair ofsyntactic translational equivalences and applyBTKs to sub-tree alignment along with someplain features.
Our study reveals that the struc-tural features embedded in a bilingual parse treepair are very effective for sub-tree alignmentand the bilingual tree kernels can well capturesuch features.
The experimental results showthat our approach achieves a significant im-provement on both gold standard tree bank andautomatically parsed tree pairs against a heuris-tic similarity based method.
We further applythe sub-tree alignment in machine translationwith two methods.
It is suggested that the sub-tree alignment benefits both phrase and syntaxbased systems by relaxing the constraint of theword alignment.1 IntroductionSyntax based Statistical Machine Translation(SMT) systems allow the translation process to bemore grammatically performed, which providesdecent reordering capability.
However, most of thesyntax based systems construct the syntactic trans-lation rules based on word alignment, which notonly suffers from the pipeline errors, but also failsto effectively utilize the syntactic structural fea-tures.
To address those deficiencies, Tinsley et al(2007) attempt to directly capture the syntactictranslational equivalences by automatically con-ducting sub-tree alignment, which can be definedas follows:A sub-tree alignment process pairs up sub-treepairs across bilingual parse trees whose contextsare semantically translational equivalent.
Accord-ing to Tinsley et al (2007), a sub-tree alignedparse tree pair follows the following criteria:(i) a node can only be linked once;(ii) descendants of a source linked node mayonly link to descendants of its targetlinked counterpart;(iii) ancestors of a source linked node may on-ly link to ancestors of its target linkedcounterpart.By sub-tree alignment, translational equivalentsub-tree pairs are coupled as aligned counterparts.Each pair consists of both the lexical constituentsand their maximum tree structures generated overthe lexical sequences in the original parse trees.Due to the 1-to-1 mapping between sub-trees andtree nodes, sub-tree alignment can also be consi-dered as node alignment by conducting multiplelinks across the internal nodes as shown in Fig.
1.Previous studies conduct sub-tree alignments byeither using a rule based method or conductingsome similarity measurement only based on lexi-cal features.
Groves et al (2004) conduct sub-treealignment by using some heuristic rules, lack ofextensibility and generality.
Tinsley et al (2007)Figure 1: Sub-tree alignment as referred toNode alignment306and Imamura (2001) propose some score functionsbased on the lexical similarity and co-occurrence.These works fail to utilize the structural features,rendering the syntactic rich task of sub-tree align-ment less convincing and attractive.
This may bedue to the fact that the syntactic structures in aparse tree pair are hard to describe using plain fea-tures.
In addition, explicitly utilizing syntactic treefragments results in exponentially high dimen-sional feature vectors, which is hard to compute.Alternatively, convolution parse tree kernels (Col-lins and Duffy, 2001), which implicitly explorethe tree structure information, have been success-fully applied in many NLP tasks, such as Semanticparsing (Moschitti, 2004) and Relation Extraction(Zhang et al 2006).
However, all those studies arecarried out in monolingual tasks.
In multilingualtasks such as machine translation, tree kernels areseldom applied.In this paper, we propose Bilingual Tree Ker-nels (BTKs) to model the bilingual translationalequivalences, in our case, to conduct sub-treealignment.
This is motivated by the decent effec-tiveness of tree kernels in expressing the similaritybetween tree structures.
We propose two kinds ofBTKs named dependent Bilingual Tree Kernel(dBTK), which takes the sub-tree pair as a wholeand independent Bilingual Tree Kernel (iBTK),which individually models the source and the tar-get sub-trees.
Both kernels can be utilized withindifferent feature spaces using various representa-tions of the sub-structures.Along with BTKs, various lexical and syntacticstructural features are proposed to capture the cor-respondence between bilingual sub-trees using apolynomial kernel.
We then attempt to combinethe polynomial kernel and BTKs to construct acomposite kernel.
The sub-tree alignment task isconsidered as a binary classification problem.
Weemploy a kernel based classifier with the compo-site kernel to classify each candidate of sub-treepair as aligned or unaligned.
Then a greedy searchalgorithm is performed according to the three cri-teria of sub-tree alignment within the space ofcandidates classified as aligned.We evaluate the sub-tree alignment on both thegold standard tree bank and an automaticallyparsed corpus.
Experimental results show that theproposed BTKs benefit sub-tree alignment on bothcorpora, along with the lexical features and theplain structural features.
Further experiments inmachine translation also suggest that the obtainedsub-tree alignment can improve the performanceof both phrase and syntax based SMT systems.2 Bilingual Tree KernelsIn this section, we propose the two BTKs andstudy their capability and complexity in modelingthe bilingual structural similarity.
Before elaborat-ing the concepts of BTKs, we first illustrate somenotations to facilitate further understanding.Each sub-tree pair ??
?
??
can be explicitly de-composed into multiple sub-structures which be-long to the given sub-structure spaces.
??
???
?, ?
, ?
?, ?
, ???
refers to the source tree sub-structure space; while ??
?
??
?, ?
, ?
?, ?
, ???
refersto the target sub-structure space.
A sub-structurepair ??
?, ???
refers to an element in the set of theCartesian product of the two sub-structure spaces:??
?, ???
?
??
?
?
?.2.1 Independent Bilingual Tree Kernel(iBTK)Given the sub-structure spaces ??
and ?
?, we con-struct two vectors using the integer counts of thesource and target sub-structures:????
?
?#???
?, ?
, #???
?, ?
, #??|??|??????
?
?#???
?, ?
, #???
?, ?
, #???????
?where #????
and #????
are the numbers of oc-currences of the sub-structures ??
and ??.
In orderto compute the dot product of the feature vectorsin the exponentially high dimensional featurespace, we introduce the tree kernel functions asfollows:???????
?
?, ??
?
???
?
??
?, ???
?
??
?, ??
?The iBTK is defined as a composite kernel con-sisting of a source tree kernel and a target treekernel which measures the source and the targetstructural similarity respectively.
Therefore, thecomposite kernel can be computed using the ordi-nary monolingual tree kernels (Collins and Duffy,2001).??
?, ???
??
???
?, ?????
??
?
??
???????????
?
?
??????
????????
?|??|????
?
?
???
?, ???
????????????
?where ??
and ???
refer to the node sets of thesource sub-tree ?
and ??
respectvely.
??????
is anindicator function which equals to 1 iff the sub-structure ??
is rooted at the node ??
and 0 other-wise.???
?, ???
?
?
?
???????
?
??????
??|??|???
is the num-ber of identical sub-structures rooted at ??
and ???
.Then we compute the ???
?, ???
?
function as follows:307(1) If the production rule at ??
and ???
are different,???
?, ???
?
?
0;(2)else if both ?
?and ???
are POS tags, ???
?, ???
?
?
?
;(3)else, ???
?, ???
?
?
??
?1 ?
?????
?, ?
?, ?????
, ?????????????
.where ??????
is the child number of ?
?,  ???
?, ?
?is the lth child of ?
?, ?
is the decay factor used tomake the kernel value less variable with respect tothe number of sub-structures.Similarly, we can decompose the target kernelas ??
?, ???
?
?
?
???
?, ????????????????
and run thealgorithm above as well.The disadvantage of the iBTK is that it fails tocapture the correspondence across the sub-structure pairs.
However, the composite style ofconstructing the iBTK helps keep the computa-tional complexity comparable to the monolingualtree kernel, which is ?
?| ?
?| ?
| ??
?|  ?
|?
?| ?
|???
|?.2.2 Dependent Bilingual Tree Kernel (dBTK)The iBTK explores the structural similarity of thesource and the target sub-trees respectively.
As analternative, we further define a kernel to capturethe relationship across the counterparts withoutincreasing the computational complexity.
As aresult, we propose the dependent Bilingual Treekernel (dBTK) to jointly evaluate the similarityacross sub-tree pairs by enlarging the featurespace to the Cartesian product of the two sub-structure sets.A dBTK takes the source and the target sub-structure pair as a whole and recursively calculateover the joint sub-structures of the given sub-treepair.
We define the dBTK as follows:Given the sub-structure space ??
?
??
, we con-struct a vector using the integer counts of the sub-structure pairs to represent a sub-tree pair:???
?
??
?
?#??
?, ??
?, ?
, #??
?, ?????
?, #??
?, ???,?
, #??|?
?|, ??
?, ?
, #??|?
?|, ??????
?where #??
?, ???
is the number of occurrences ofthe sub-structure pair ??
?, ???.???????
?
?, ??
?
?????
???
?
?
?, ????
?
???
??
?
??
?
????
?, ?????????????
??
?
??????
, ???
????????????????|?????|????
?
?
?
?
????
?, ???,????
, ?????????????????????????????
(1)?
?
?
?
?
????
?, ???
?
????
?, ?????????????????????????????
(2)?
?
?
???
?, ???
?????????????
?
?
???
?, ?????????????????
??
?, ???
?
??
?, ??
?It is infeasible to explicitly compute the kernelfunction by expressing the sub-trees as featurevectors.
In order to achieve convenient computa-tion, we deduce the kernel function as the above.The deduction from (1) to (2) is derived accord-ing to the fact that the number of identical sub-structure pairs rooted in the node pairs ??
?, ???
and????
, ????
equals to the product of the respectivecounts.
As a result, the dBTK can be evaluated asa product of two monolingual tree kernels.
Herewe verify the correctness of the kernel by directlyconstructing the feature space for the inner prod-uct.
Alternatively, Cristianini and Shawe-Taylor(2000) prove the positive semi-definite characte-ristic of the tensor product of two kernels.
Thedecomposition benefits the efficient computationto use the algorithm for the monolingual tree ker-nel in Section 2.1.The computational complexity of the dBTK isstill ?
?| ?
?| ?
| ??
?|  ?
|?
?| ?
|???
|?.3 Sub-structure Spaces for BTKsThe syntactic translational equivalences underBTKs are evaluated with respective to the sub-structures factorized from the candidate sub-treepairs.
In this section, we propose different sub-structures to facilitate the measurement of syntac-tic similarity for sub-tree alignment.
Since theproposed BTKs can be computed by individuallyevaluating the source and target monolingual treekernels, the definition of the sub-structure can besimplified to base only on monolingual sub-trees.3.1 Subset TreeMotivated from Collins and Duffy (2002) in mo-nolingual tree kernels, the Subset Tree (SST) canbe employed as sub-structures.
An SST is any sub-graph, which includes more than one non-terminalnode, with the constraint that the entire rule pro-ductions are included.
Fig.
2 shows an example ofthe SSTs decomposed from the source sub-treerooted at VP*.3.2 Root directed Subset TreeMonolingual Tree kernels achieve decent perfor-mance using the SSTs due to the rich explorationof syntactic information.
However, the sub-treealignment task requires strong capability of dis-criminating the sub-trees with their roots acrossadjacent generations, because those candidatesshare many identical SSTs.
As illustrated in Fig 2,the source sub-tree rooted at VP*, which shouldbe aligned to the target sub-tree rooted at NP*,may be likely aligned to the sub-tree rooted at PP*,308which shares quite a similar context with NP*.
Itis also easy to show that the latter shares all theSSTs that the former obtains.
In consequence, thevalues of the SST based kernel function are quitesimilar between the candidate sub-tree pair rootedat (VP*,NP*) and (VP*,PP*).In order to effectively differentiate the candi-dates like the above, we propose the Root directedSubset Tree (RdSST) by encapsulating each SSTwith the root of the given sub-tree.
As shown inFig 2, a sub-structure is considered identical to thegiven examples, when the SST is identical and theroot tag of the given sub-tree is NP.
As a result,the kernel function in Section 2.1 is re-defined as:??
?, ???
?
?
?
???
?, ???
????
?, ?????????????????
???
?, ????
?
?
???
?, ???
????????????
?where ??
and ???
are the root nodes of the sub-tree ?
and ??
respectively.
The indicator function???
?, ????
equals to 1 if ??
and ???
are identical, and 0otherwise.
Although defined for individual SST,the indicator function can be evaluated outside thesummation, without increasing the computationalcomplexity of the kernel function.3.3 Root generated Subset TreeSome grammatical tags (NP/VP) may have iden-tical tags as their parents or children which maymake RdSST less effective.
Consequently, we stepfurther to propose the sub-structure of Root gener-ated Subset Tree (RgSST).
An RgSST requires theroot node of the given sub-tree to be part of thesub-structure.
In other words, all sub-structuresshould be generated from the root of the givensub-tree as presented in Fig.
2.
Therefore the ker-nel function can be simplified to only capture thesub-structure rooted at the root of the sub-tree.??
?, ???
?
???
?, ???
?where ??
and ???
are the root nodes of the sub-tree ?
and ??
respectively.
The time complexity isreduced to  ?
?| ?
?| ?
| ??
?| ?
|?
?| ?
|???
|?.3.4 Root onlyMore aggressively, we can simplify the kernel toonly measure the common root node without con-sidering the complex tree structures.
Therefore thekernel function is simplified to be a binary func-tion with time complexity ??1?.??
?, ???
?
???
?, ???
?4 Plain featuresBesides BTKs, we introduce various plain lexicalfeatures and structural features which can be ex-pressed as feature functions.
The lexical featureswith directions are defined as conditional featurefunctions based on the conditional lexical transla-tion probabilities.
The plain syntactic structuralfeatures can deal with the structural divergence ofbilingual parse trees in a more general perspective.4.1 Lexical and Word Alignment FeaturesIn this section, we define seven lexical features tomeasure semantic similarity of a given sub-treepair.Internal Lexical Features: We define two lex-ical features with respective to the internal span ofthe sub-tree pair.????|??
?
??
?
???|????????????????
??|????
?|Figure 2: Illustration of SST, RdSST and RgSST309????|??
?
??
?
???|????????????????
??|????
?|where ???|??
refers to the lexical translationprobability from the source word ?
to the targetword ?
within the sub-tree spans, while ???|?
?refers to that from target to source; ?????
refers tothe word set for the internal span of the sourcesub-tree ?, while ?????
refers to that of the targetsub-tree ?.Internal-External Lexical Features: Thesefeatures are motivated by the fact that lexicaltranslation probabilities within the translationalequivalence tend to be high, and that of the non-equivalent counterparts tend to be low.????|??
?
??
?
???|?????????????????
??|?????|????|??
?
??
?
???|?????????????????
??|????
?|where ??????
refers to the word set for the ex-ternal span of the source sub-tree ?, while ?????
?refers to that of the target sub-tree ?.Internal Word Alignment Features: The wordalignment links account much for the co-occurrence of the aligned terms.
We define theinternal word alignment features as follows:???
?, ??
??
?
??
?, ??
?
????|??
?
???|????????????????????|????
?| ?
|?????|???where??
?, ??
?
?1        if ?
?, ??
is aligned0                       otherwiseThe binary function ??
?, ??
is employed to trig-ger the computation only when a word alignedlink exists for the two words ?
?, ??
within the sub-tree span.Internal-External Word Alignment Features:Similar to the lexical features, we also introducethe internal-external word alignment features asfollows:???
?, ??
??
?
??
?, ??
?
????|??
?
???|?????????????????????|?????
?| ?
|?????|??????
?, ??
??
?
??
?, ??
?
????|??
?
???|?????????????????????|????
?| ?
|??????|???where??
?, ??
?
?1        if ?
?, ??
is aligned0                        otherwise4.2 Online Structural FeaturesIn addition to the lexical correspondence, we alsocapture the structural divergence by introducingthe following tree structural features.Span difference: Translational equivalent sub-tree pairs tend to share similar length of spans.Thus the model will penalize the candidate sub-tree pairs with largely different length of spans.???
?, ??
?
?|?????||?????|?
|?????||?????|??
and ?
refer to the entire source and target parsetrees respectively.
Therefore, |????
?| and |????
?| arethe respective span length of the parse tree used fornormalization.Number of Descendants: Similarly, the num-ber of the root?s descendants of the aligned sub-trees should also correspond.???
?, ??
?
?|????||????|?
|????||???
?|?where ??.
?
refers to the descendant set of theroot to a sub-tree.Tree Depth difference: Intuitively, translation-al equivalent sub-tree pairs tend to have similardepth from the root of the parse tree.
We allow themodel to penalize the candidate sub-tree pairs withquite different distance of path from the root of theparse tree to the root of the sub-tree.???
?, ??
?
???????????????????
?????????????????
?5 Alignment ModelGiven feature spaces defined in the last two sec-tions, we propose a 2-phase sub-tree alignmentmodel as follows:In the 1st phase, a kernel based classifier, SVMin our study, is employed to classify each candi-date sub-tree pair as aligned or unaligned.
Thefeature vector of the classifier is computed using acomposite kernel:???
?
?, ??
?
???
????????
?
?, ??
?
???
?
?
????????
??
?
?, ??
?
???K????????,??
is the normalized form of the polynomi-al kennel ????,??
, which is a polynomial kernelwith the degree of 2, utilizing the plain features.??????
??,??
is the normalized form of the BTK?????
??,??
, exploring the corresponding sub-structure space.
The composite kernel can be con-structed using the polynomial kernel for plain fea-tures and various BTKs for tree structure by linearcombination with coefficient ?
?, where ?
??K???
?
1.In the 2nd phase, we adopt a greedy search withrespect to the alignment probabilities.
Since SVMis a large margin based discriminative classifierrather than a probabilistic model, we introduce asigmoid function to convert the distance againstthe hyperplane to a posterior alignment probabilityas follows:310???
?|?, ??
?11 ?
???????
?|?, ??
?11 ?
???
?where ??
is the distance for the instances classi-fied as aligned and ??
is that for the unaligned.We use ???
?|?, ??
as the confidence to conductthe sure links for those classified as aligned.
Onthis perspective, the alignment probability is suit-able as a searching metric.
The search space isreduced to that of the candidates classified asaligned after the 1st phase.6 Experiments on Sub-Tree AlignmentsIn order to evaluate the effectiveness of the align-ment model and its capability in the applicationsrequiring syntactic translational equivalences, weemploy two corpora to carry out the sub-treealignment evaluation.
The first is HIT gold stan-dard English Chinese parallel tree bank referred asHIT corpus1.
The other is the automatically parsedbilingual tree pairs selected from FBIS corpus (al-lowing minor parsing errors) with human anno-tated sub-tree alignment.6.1 Data preparationHIT corpus, which is collected from English learn-ing text books in China as well as example sen-tences in dictionaries, is used for the gold standardcorpus evaluation.
The word segmentation, toke-nization and parse-tree in the corpus are manuallyconstructed or checked.
The corpus is constructedwith manually annotated sub-tree alignment.
Theannotation strictly reserves the semantic equiva-lence of the aligned sub-tree pair.
Only sure linksare conducted in the internal node level, withoutconsidering possible links adopted in word align-ment.
A different annotation criterion of the Chi-nese parse tree, designed by the annotator, is em-ployed.
Compared with the widely used PennTreeBank annotation, the new criterion utilizessome different grammar tags and is able to effec-tively describe some rare language phenomena inChinese.
The annotator still uses Penn TreeBankannotation on the English side.
The statistics ofHIT corpus used in our experiment is shown inTable 1.
We use 5000 sentences for experimentand divide them into three parts, with 3k for train-ing, 1k for testing and 1k for tuning the parametersof kernels and thresholds of pruning the negativeinstances.1HIT corpus is designed and constructed by HIT-MITLAB.http://mitlab.hit.edu.cn/index.php/resources.html .Most linguistically motivated syntax basedSMT systems require an automatic parser to per-form the rule induction.
Thus, it is important toevaluate the sub-tree alignment on the automati-cally parsed corpus with parsing errors.
In addition,HIT corpus is not applicable for MT experimentdue to the problems of domain divergence, annota-tion discrepancy (Chinese parse tree employs adifferent grammar from Penn Treebank annota-tions) and degree of tolerance for parsing errors.Due to the above issues, we annotate a new dataset to apply the sub-tree alignment in machinetranslation.
We randomly select 300 bilingual sen-tence pairs from the Chinese-English FBIS corpuswith the length ?
30 in both the source and targetsides.
The selected plain sentence pairs are furtherparsed by Stanford parser (Klein and Manning,2003) on both the English and Chinese sides.
Wemanually annotate the sub-tree alignment for theautomatically parsed tree pairs according to thedefinition in Section 1.
To be fully consistent withthe definition, we strictly reserve the semanticequivalence for the aligned sub-trees to keep ahigh precision.
In other words, we do not conductany doubtful links.
The corpus is further dividedinto 200 aligned tree pairs for training and 100 fortesting as shown in Table 2.6.2 Baseline approachWe implement the work in Tinsley et al (2007) asour baseline methodology.Given a tree pair ?
?, ?
?
, the baseline ap-proach first takes all the links between the sub-treepairs as alignment hypotheses, i.e., the Cartesianproduct of the two sub-tree sets:?
?
?, ?
, ?
?, ?
, ???
?
?
?
?, ?
, ??
, ?
, ??
?By using the lexical translation probabilities,each hypothesis is assigned an alignment score.All hypotheses with zero score are pruned out.Chinese English# of Sentence pair 300Avg.
Sentence Length 16.94 20.81Avg.
# of sub-tree 28.97 34.39Avg.
# of alignment 17.07Table 2.
Statistics of FBIS selected CorpusChinese English# of Sentence pair 5000Avg.
Sentence Length 12.93 12.92Avg.
# of sub-tree 21.40 23.58Avg.
# of alignment 11.60Table 1.
Corpus Statistics for HIT corpus311Then the algorithm iteratively selects the link ofthe sub-tree pairs with the maximum score as asure link, and blocks all hypotheses that contradictwith this link and itself, until no non-blocked hy-potheses remain.The baseline system uses many heuristics insearching the optimal solutions with alternativescore functions.
Heuristic skip1 skips the tied hy-potheses with the same score, until it finds thehighest-scoring hypothesis with no competitors ofthe same score.
Heuristic skip2 deals with thesame problem.
Initially, it skips over the tied hy-potheses.
When a hypothesis sub-tree pair ?
?
?, ??
?without any competitor of the same score is found,where neither ??
nor ??
has been skipped over, thehypothesis is chosen as a sure link.
Heuristicspan1 postpones the selection of the hypotheseson the POS level.
Since the highest-scoring hypo-theses tend to appear on the leaf nodes, it may in-troduce ambiguity when conducting the alignmentfor a POS node whose child word appears twice ina sentence.The baseline method proposes two score func-tions based on the lexical translation probability.They also compute the score function by splittingthe tree into the internal and external components.Tinsley et al (2007) adopt the lexical transla-tion probabilities dumped by GIZA++ (Och andNey, 2003) to compute the span based scores foreach pair of sub-trees.
Although all of their heuris-tics combinations are re-implemented in our study,we only present the best result among them withthe highest Recall and F-value as our baseline,denoted as skip2_s1_span12.2  s1 denotes score function 1 in Tinsley et al (2007),skip2_s1_span1 denotes the utilization of heuristics skip2 andspan1 while using score function 16.3 Experimental settingsWe use SVM with binary classes as the classifier.In case of the implementation, we modify the TreeKernel tool (Moschitti, 2004) and SVMLight(Joachims, 1999).
The coefficient ??
for the com-posite kernel are tuned with respect to F-measure(F) on the development set of HIT corpus.
Weempirically set C=2.4 for SVM and use ?
?
0.23,the default parameter ?
?
0.4 for BTKs.Since the negative training instances largelyoverwhelm the positive instances, we prune thenegative instances using the thresholds accordingto the lexical feature functions (?
?, ?
?, ?
?, ??)
andonline structural feature functions (?
?, ?
?, ??
).Those thresholds are also tuned on the develop-ment set of HIT corpus with respect to F-measure.To learn the lexical and word alignment fea-tures for both the proposed model and the baselinemethod, we train GIZA++ on the entire FBIS bi-lingual corpus (240k).
The evaluation is conductedby means of Precision (P), Recall (R) and F-measure (F).6.4 Experimental resultsIn Tables 3 and 4, we incrementally enlarge thefeature spaces in certain order for both corporaand examine the feature contribution to the align-ment results.
In detail, the iBTKs and dBTKs arefirstly combined with the polynomial kernel forplain features individually, then the best iBTK anddBTK are chosen to construct a more complexcomposite kernel along with the polynomial kernelfor both corpora.
The experimental results showthat:?
All the settings with structural features of theproposed approach achieve better performancethan the baseline method.
This is because theFeature Space P R FLex 73.48 71.66 72.56Lex +Online Str 77.02 73.63 75.28Plain +dBTK-STT 81.44 74.42 77.77Plain +dBTK-RdSTT 81.40 69.29 74.86Plain +dBTK-RgSTT 81.90 67.32 73.90Plain +dBTK-Root 78.60 80.90 79.73Plain +iBTK-STT 82.94 79.44 81.15Plain +iBTK-RdSTT 83.14 80 81.54Plain +iBTK-RgSTT 83.09 79.72 81.37Plain +iBTK-Root 78.61 79.49 79.05Plain +dBTK-Root+iBTK-RdSTT82.70 82.70 82.70Baseline 70.48 78.70 74.36Table 4.
Structure feature contribution for FBIS test setFeature Space P R FLex 61.62 58.33 59.93Lex +Online Str 70.08 69.02 69.54Plain +dBTK-STT 80.36 78.08 79.20Plain +dBTK-RdSTT 87.52 74.13 80.27Plain +dBTK-RgSTT 88.54 70.18 78.30Plain +dBTK-Root 81.05 84.38 82.68Plain +iBTK-STT 81.57 73.51 77.33Plain +iBTK-RdSTT 82.27 77.85 80.00Plain +iBTK-RgSTT 82.92 78.77 80.80Plain +iBTK-Root 76.37 76.81 76.59Plain +dBTK-Root+iBTK-RgSTT85.53 85.12 85.32Baseline 64.14 66.99 65.53Table 3.
Structure feature contribution for HIT test set*Plain= Lex +Online Str312baseline only assesses semantic similarity usingthe lexical features.
The improvement suggeststhat the proposed framework with syntacticstructural features is more effective in modelingthe bilingual syntactic correspondence.?
By introducing BTKs to construct a compositekernel, the performance in both corpora is sig-nificantly improved against only using the poly-nomial kernel for plain features.
This suggeststhat the structural features captured by BTKs arequite useful for the sub-tree alignment task.
Wealso try to use BTKs alone without the poly-nomial kernel for plain features; however, theperformance is rather low.
This suggests that thestructure correspondence cannot be used tomeasure the semantically equivalent tree struc-tures alone, since the same syntactic structuretends to be reused in the same parse tree andlose the ability of disambiguation to some extent.In other words, to capture the semantic similari-ty, structure features requires lexical features tocooperate.?
After comparing iBTKs with the correspondingdBTKs, we find that for FBIS corpus, iBTKgreatly outperforms dBTK in any feature spaceexcept the Root space.
However, when it comesthe HIT corpus, the gaps between the corres-ponding iBTKs and dBTKs are much closer,while on the Root space, dBTK outperformsiBTK to a large amount.
This finding can be ex-plained by the relationship between the amountof training data and the high dimensional featurespace.
Since dBTKs are constructed in a jointmanner which obtains a much larger high di-mensional feature space than those of iBTKs,dBTKs require more training data to excel itscapability, otherwise it will suffer from the datasparseness problem.
The reason that dBTK out-performs iBTK in the feature space of Root inFBIS corpus is that although it is a joint featurespace, the Root node pairs can be constructedfrom a close set of grammar tags and to form arelatively low dimensional space.As a result, when applying to FBIS corpus,which only contains limited amount of trainingdata, dBTKs will suffer more from the datasparseness problem, and therefore, a relativelylow performance.
When enlarging the amount oftraining corpus to the HIT corpus, the ability ofdBTKs excels and the benefit from data increas-ing of dBTKs is more significant than iBTKs.?
We also find that the introduction of BTKs gainsmore improvement in HIT gold standard corpusthan in FBIS corpus.
Other than the factor ofthe amount of training data, this is also becausethe plain features in Table 3 are not as effectiveas those in Table 4, since they are trained onFBIS corpus which facilitates Table 4 more withrespect to the domains.
On the other hand, thegrammatical tags and syntactic tree structuresare more accurate in HIT corpus, which facili-tates the performance of BTKs in Table 3.?
On the comparison across the different featurespaces of BTKs, we find that STT, RdSTT andTgSTT are rather selective, since Recalls ofthose feature spaces are relatively low, exp.
forHIT corpus.
However, the Root sub-structureobtains a satisfactory Recall for both corpora.That?s why we attempt to construct a morecomplex composite kernel in adoption of thekernel of dBTK-Root as below.?
To gain an extra performance boosting, we fur-ther construct a composite kernel which includesthe best iBTK and the best dBTK for each cor-pus along with the polynomial kernel for plainfeatures.
In the HIT corpus, we use dBTK in theRoot space and iBTK in the RgSST space; whilefor FBIS corpus, we use dBTK in the Rootspace and iBTK in the RdSST space.
The expe-rimental results suggest that by combining iBTKand dBTK together, we can achieve more im-provement.7 Experiments on Machine TranslationIn addition to the intrinsic alignment evaluation,we further conduct the extrinsic MT evaluation.We explore the effectiveness of sub-tree alignmentfor both phrase based and linguistically motivatedsyntax based SMT systems.7.1 Experimental configurationIn the experiments, we train the translation modelon FBIS corpus (7.2M (Chinese) + 9.2M (English)words in 240,000 sentence pairs) and train a 4-gram language model on the Xinhua portion of theEnglish Gigaword corpus (181M words) using theSRILM Toolkits (Stolcke, 2002).
We use thesesentences with less than 50 characters from theNIST MT-2002 test set as the development set (tospeed up tuning for syntax based system) and theNIST MT-2005 test set as our test set.
We use theStanford parser (Klein and Manning, 2003) toparse bilingual sentences on the training set andChinese sentences on the development and test set.The evaluation metric is case-sensitive BLEU-4.313For the phrase based system, we use Moses(Koehn et al, 2007) with its default settings.
Forthe syntax based system, since sub-tree alignmentcan directly benefit Tree-2-Tree based systems,we apply the sub-tree alignment in a syntax sys-tem based on Synchronous Tree SubstitutionGrammar (STSG) (Zhang et al, 2007).
The STSGbased decoder uses a pair of elementary tree3 as abasic translation unit.
Recent research on treebased systems shows that relaxing the restrictionfrom tree structure to tree sequence structure(Synchronous Tree Sequence Substitution Gram-mar: STSSG) significantly improves the transla-tion performance (Zhang et al, 2008).
We imple-ment the STSG/STSSG based model in the Piscesdecoder with the identical features and settings inSun et al (2009).
In the Pisces decoder, theSTSSG based decoder translates each span itera-tively in a bottom up manner which guaranteesthat when translating a source span, any of its sub-spans is already translated.
The STSG based de-coding can be easily performed with the STSSGdecoder by restricting the translation rule set to beelementary tree pairs only.As for the alignment setting, we use the wordalignment trained on the entire FBIS (240k) cor-pus by GIZA++ with heuristic grow-diag-final forboth Moses and the syntax system.
For sub-tree-alignment, we use the above word alignment tolearn lexical/word alignment feature, and trainwith the FBIS training corpus (200) using thecomposite kernel of Plain+dBTK-Root+iBTK-RdSTT.7.2 Experimental resultsCompared with the adoption of word alignment,translational equivalences generated from struc-tural alignment tend to be more grammatically3 An elementary tree is a fragment whose leaf nodes can beeither non-terminal symbols or terminal symbols.aware and syntactically meaningful.
However,utilizing syntactic translational equivalences alonefor machine translation loses the capability ofmodeling non-syntactic phrases (Koehn et al,2003).
Consequently, instead of using phrasesconstraint by sub-tree alignment alone, we attemptto combine word alignment and sub-tree align-ment and deploy the capability of both with twomethods.?
Directly Concatenate (DirC) is operated by di-rectly concatenating the rule set genereted fromsub-tree alignment and the original rule set gen-erated from word alignment (Tinsley et al,2009).
As shown in Table 5, we gain minor im-provement in the Bleu score for all configura-tions.?
Alternatively, we proposed a new approach togenerate the rule set from the scratch.
We con-strain the bilingual phrases to be consistent withEither Word alignment or Sub-tree alignment(EWoS) instead of being originally consistentwith the word alignment only.
The method helpstailoring the rule set decently without redundantcounts for syntactic rules.
The performance isfurther improved compared to DirC in all sys-tems.The findings suggest that with the modeling ofnon-syntactic phrases maintained, more emphasison syntactic phrases can benefit both the phraseand syntax based SMT systems.8 ConclusionIn this paper, we explore syntactic structure fea-tures by means of Bilingual Tree Kernels and ap-ply them to bilingual sub-tree alignment alongwith various lexical and plain structural features.We use both gold standard tree bank and the au-tomatically parsed corpus for the sub-tree align-ment evaluation.
Experimental results show thatour model significantly outperforms the baselinemethod and the proposed Bilingual Tree Kernelsare very effective in capturing the cross-lingualstructural similarity.
Further experiment showsthat the obtained sub-tree alignment benefits bothphrase and syntax based MT systems by deliver-ing more weight on syntactic phrases.AcknowledgmentsWe thank MITLAB4 in Harbin Institute of Tech-nology for licensing us their sub-tree alignmentcorpus for our research.4 http://mitlab.hit.edu.cn/  .System Model BLEUMoses BP* 23.86DirC  23.98EWoS  24.48SyntaxSTSGSTSG 24.71DirC  25.16EWoS  25.38Syntax STSSG 25.92STSSG DirC  25.95EWoS  26.45Table 5.
MT evaluation on various systems*BP denotes bilingual phrases314ReferencesDavid Burkett and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In Pro-ceedings of EMNLP-08.
877-886.Nello Cristianini and John Shawe-Taylor.
2000.
Anintroduction to support vector machines and otherkernelbased learning methods.
Cambridge: Cam-bridge University Press.Michael Collins and Nigel Duffy.
2001.
ConvolutionKernels for Natural Language.
In Proceedings ofNIPS-01.Declan Groves, Mary Hearne and Andy Way.
2004.Robust sub-sentential alignment of phrase-structuretrees.
In Proceedings of COLING-04, pages 1072-1078.Kenji Imamura.
2001.
Hierarchical Phrase AlignmentHarmonized with Parsing.
In Proceedings ofNLPRS-01, Tokyo.
377-384.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Sch?olkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods -Support Vector Learning, MIT press.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of ACL-03.
423-430.Philipp Koehn, Franz Josef Och and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL-03.
48-54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, Ri-chard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of ACL-07.
177-180.Franz Josef Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19-51, March.Alessandro Moschitti.
2004.
A Study on ConvolutionKernels for Shallow Semantic Parsing.
In Proceed-ings of ACL-04.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP-02.901-904.Jun Sun, Min Zhang and Chew Lim Tan.
2009.
A non-contiguous Tree Sequence Alignment-based Modelfor Statistical Machine Translation.
In Proceedingsof ACL-IJCNLP-09.
914-922.John Tinsley, Ventsislav Zhechev, Mary Hearne andAndy Way.
2007.
Robust language pair-independentsub-tree alignment.
In Proceedings of MT SummitXI -07.John Tinsley, Mary Hearne and Andy Way.
2009.
Pa-rallel treebanks in phrase-based statistical machinetranslation.
In Proceedings of CICLING-09.Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.2006.
A Composite Kernel to Extract Relations be-tween Entities with both Flat and Structured Fea-tures.
In Proceedings of ACL-COLING-06.
825-832.Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, ShengLi and Chew Lim Tan.
2007.
A tree-to-tree align-ment-based model for statistical machine transla-tion.
In Proceedings of MT Summit XI -07.
535-542.Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li,Chew Lim Tan and Sheng Li.
2008.
A tree sequencealignment-based tree-to-tree translation model.
InProceedings of ACL-08.
559-567.315
