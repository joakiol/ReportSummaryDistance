PLAN REVISION IN PERSON-MACHINEDIALOGUECldo JULLIENJean-Charles MARTYGrenoble Research Center of CAP  SESA INNOVATION33, Chemin du Vieux Chine38240 Meylan Francee-mail: jullien@capsogeti.fr, marty@capsogeti.frAbstractDialogue is a cooperative process in whicheach speech act of the participants con-tributes to the overall purpose of the dia-logne.
Participating in a full dialogue im-plies understanding at each point of thedialogue session the role of each speechact with respect to the rest of the dia-logne.
We concentrate in this paper onspeech acts that diverge from the straight-forward unfolding of the dialogue.
Suchspeech acts represent dialogue deviations.We analyze the representation of differ-ent types and degrees of deviations andpresent a plan revision mechanism fordialogue management that permits theirtreatment in the context of advice givingdialogues between a novice user and an ex-pert problem solver.1 A Sample DialogueIn ESPRIT project ESTEAM-3161, weare designing a dialoguer that managesthe interaction between a novice user andZESTEAM-316: ~An Architecture for Interac-tive Problem Solving by Cooperating Data andKnowledge Bases m supported in part by the CEC.advice--giving expert systems.
The test-bed application is personal financial in-vestment \[5\].
As an introduction to ourpaper, we present a dialogue in which thesystem obtains from the user the informa-tion needed to activate the financial expertsystem.01,S: Hello.
Tell ms about your investmentproblem.02,U: I 'd like to buy a car in two years.08,S: Ok.
Normally an investment plancomprise8 four 8ubparte: an emergencyfund, for unforeseen ezpenses; a cashneed when you know you'll need a cer-tain sum at a certain date, aa is the casewith your car; a fized-term investmentto earn the mazimum in a specified timeperiod; and a long-term investment foran indefinite amount of time which canbe designed either for capital growth orfor a regular income.
How much do youwant to investf04,U: $,~0000.05,S: How much .do you want to put into anemergency fundf06,U: Let's talk about my car first.07,S: Ok, how much do you think your carwill coetf08,U: Oh, let'8 say $9000, I 9ue88.- 153-09,S: And you plan to buy it in two yearsfIO,U: Yes.11,S: Ok, if you put $7,500 in a money.market fund earning 9.9~ p.a., in twoyears you'll have $9,000.
Let's returnto the emergency fund, now.
How muchdo you want to put in itf12,U: $5000.18,S: Tell me also the delay.14,U: I don't understand tohat is the delay ofan emergency fund.15,S: The delay is how soon you can haveyour money back after asking for it.
5o,for your emergency fund, what is thisdelayf16,U: ~ days.17,S: Do you want a ftxed term investmentf18,U: No.19,S: ~o, it leaves $7,500 for your long-terminvestment.
Is it okf20,U: I wanted $10000.21,S: If you want art investment plan for$~0,000 with an emergency fund of$5,000 and a cash-need of $7,500, thenit only leaves $7,500 for a long-term in.vestment.
If you want $I0,000 for along-term investment, hen you shouldmodify something else in your plan.22,U: Could I put $1~500 into the emergencyfundf28,S: Ok.
So, to sum up, you want to invest$~0,000, with $1~,500 in an emergencyfund ~th  a delay of 1~ days, $7,500in a cash need to buy a $9000 car inI# years, no riffled-term investment, and$10,000 in a long-term investment.
Isthat dghtf24,U: That's fine.The above sample dialogue has a quitesimple structure: presentation of the mainsub-investment types and collection of in-formation from the user concerning eachsub-investment.
We notice, however, thatat some points the user does not react di-rectly as the system would expect.
Forinstance, in sentence 6, the user is askedabout saving money in an emergency fund,but she wants to talk about buying her earfirst.
Later, in sentence 14, the user is un-able to answer a question about the delayof the emergency fund because she doesnot understand what such a delay is.Such reactions illustrate dialogue devia-tions.
Dialogue deviations are sentencesthat diverge form the straightforward un-folding of the dialogue, while having a co-herent interpretation with respect o thewhole dialogue.
These unexpected reac-tions are inevitable in a dialogue where theparticipants are independent agents withtheir own goals and differing degrees ofknowledge about each other and the sub-ject under discussion.Before describing our dialogue managerand its mechanism for handling such devi-ations, we present in the next section theframework we adopted to model flexibledialogue management.2 Deviat ions in DialogueDialogue is considered to be a coopera-tive activity where the goals and actionsof each participant contribute to the over-all purpose of the dialogue \[3\].
In task-oriented ialogues, we distinguish betweentask level goals and plans (e.g., investing,traveling), and communicative l vel inten-tions and speech acts (e.g., explaining, re-questing information) \[2,6,11\].
We call154 -these aspects the intentional dimension.Each step in the dialogue concerns a par-ticular topic.
Intuitively the notion oftopic might be described by a subset of ob-jects of the problem under discussion.
Infact, the boundary of this asubset ~is notstrict: one can only say that some objectsare more salient than others.
The atten-tional state is hence better epresented bydifferent levels in the focus of attention,corresponding to embedded subsets of ob-jects \[9\].It is important o note that in the con-text of dialogue, the term deviation is notused with its strict boolean meaning: it isa complex and a relative notion.Deviation are complex because they in-volve both intentional and attentional di-mensions.
Deviations in this coopera-tive process arise from inconsistencies be-tween the observed speech act and the di-alogue considered as a coherent plan \[4\].They can be classified according to thetype of interactions among of interactionsamong intentions of the participants andand changes on the focus of attention.
Thesample session above illustrates severaltypes of such deviations: at the commu-nicative level, the novice user requests forexplanations before giving a requested in-formation; at the task level, the user givesan inconsistent value or does not want agiven action in the task plan.
Deviationsare generally combined with changes ofsub jets.Within each dimension there exist differ-ent degrees of deviations: speech acts mayhave indirect effects, changes in the fo-cus of attention may be be more or lessabrupt, deviations depend on the expecta-tions each participant has concerning thereactions of the other.Therefore, we adopted models of the dia-logue structure where the relationship be-tween the intentions and the evolution ofthe focus of attention are made explicit\[10\].The detection and analysis of deviation af-ter a user speech act relies on expectationsand predictions in both the intentional andattentional dimensions.
The system, hav-ing produced a speech act and waiting fora reaction of the user, expects in the firstplace a reaction corresponding exactly tothe effect it intended to produce.
If theuser reacts differently, the system will useknowledge about possible types of devia-tion and the state of dialogue to analyzethe nature of the deviation.Once a deviation has been identified, thesystem may need to modify more or lessdeeply the planned course of the dialogue:from local adaptation like embedding asmall clarification subdialogue (sentences14-15), to more global revision like re-ordering entire sub-topics (sentence 6).Hence to interpret he influence of an un-expected speech act at a certain point inthe dialogue the representation f the stateof dialogue should reflect the structure ofthe whole dialogue: keeping track of pastexchanges and anticipating the remainderof the dialogue.3 Overview of the Dia-logue ManagerThe general organization of the Esteam-316 Dialogue System \[1\] is depicted in fig-ure 1.A Natural Language Front-End (NLF)transforms natural language utterancesinto literal meaning and vice-versa.
Theliteral meaning corresponds to an iso-lated surface speech act.
The Recognizertakes a literal meaning from the Front-Endand determines whether the corresponding155 -\[ Planner I\[Recofnizer i SSI \[ Expr.
Sps.
I~ " ~  task-plan J 4 Representat ion  of/ State of D ia logue\[Front End\[Figure 1: Overview of the Dialogue Man-agersurface act of the user could be an exam-ple of, or a part of, one of the commu-nicative actions that the system expectsfrom the user in the context of the cur-rent dialogue.
The expectations are con-trolled by the Planner which conducts thedialogue and maintains a structure of thesystem's intentions (SSI), while reacting touser's intentions detected by the Recog-nizer.
The Planner interacts with the Ex-pression Specialists for constructing fromthe communicative acts it intends to per-form (what o communicate) an appropri-ate literal meaning (how to say it).An advice giving dialogue is a particularcase of task-oriented dialogue.
The task-level plan reflects the problem of the user.The advice giving system has only commu-nicative intentions for constructing and re-fining the task-level plan.
A complete ad-vice giving session contains three phases:problem formulation, resolution and pre-sentation of the solution.
In this paper, weconcentrate on first phase.
During prob-lem formulation, the system helps the userto refine, select and instantiate appropri-ate subplans according to the user's goals.The task plan is initialized by a stereo-typical pattern of actions which could berecommanded as part of a "good" solu-tion.
The result of the problem formu-lation phase is a coherent task-level planwhich can be passed to an expert problemtheThe communicative intentions of the sys-tem are stored in the Structure of SystemIntentions: the SSI reflects the state of theplan of dialogue from the system's point ofview.4.1 Communicat ive  Leve l  P lansThe Planner uses a hierarchical set ofplans for constructing the SSI.
Plansare associated with the various commu-nicative level intentions of the system.We have designed two types of plans:dialogue.plans and communicative-plans.Dialogue-P lans are the most abstractplans of the Planner.
They express thestrategy of the overall advice-giving ses-sion \[7\].
The purpose of these plans isto capture procedural knowledge for an"ideal" advice-giving session.
They areused to initiate the SSI, but also includemeans for adaptation at execution time\[8\].Basically the models of dialogue-plans ex-press dominance and sequencing relationsamong the sub-parts of the dialogue ses-sion (decomposition), and the part of thetask level plan that is in focus at a givenstep of the communicative level plan (pa-rameter).Communicat ive -P lans  modelscontain the effects an elementary commu-nicative intention of the system in termsof the immediate xpectations about the- 156-h e l p ~ b l e mcollect i~  on EF/-.,ask paramefer amounts$IPlanEm.
-FundAmountFocusFigure 2: Structure of System's Intentionsand Attentional Statecommunicative acts of the user.
For thePlanner a communicative plan is seen asa primitive action to be be passed to theExpression Specialists for execution.4.2  Expectat ion  S tack  andSt ructure  o f  Sys tem In ten-t ionsThe SSI is a tree enhanced by orderingsrelations, in which each node represents acommunicative l vel plan of the system,and links decomposition relations of plansinto subplans.
In addition each node inthe SSI is related to a given subpart of thetask-plan.
At a given point in the dialoguethe attentional state can be representedby a stack in which the bottom containsthe focus associated with the most gen-eral plan and the top contains the focusof the plan currently executed.
For exam-ple, when the system asks for the amountof the emergency fund, there will be threefocus levels in the stack: the investmentplan at the bottom, the emergency fundin the middle and the amount on top (seefigure 2).Using this attentional state the Recognizercould only analyze changes of focus.
Theproblem is still to provide the Recognizerwith expectations concerning the inten-tions of the user.
The method of rep-resenting expectations i to attach a setuser's communicative acts to each type ofsystem's intentions in the SSI and to or-give p arm ,(c\]elay, E FJ,,reques~ expt laelay, ~r lgive parm ,{~mount x EFt.,request expl tamoun~, ~elrequest ex~.l ~F Jremse ~r  /Acts concerning .C~h Needand othel, subplansgive parmplan (\]~1}.request expI pl~m~,|PIJremse plan ~rqOthersFigure 3: A sample State of the Expecta-tion Stackganize these expectations according to theattentional state.
We obtain the Ezpecta-tion Stack.Let us consider an example to understandbetter the significance of the different ob-jects included in the Expectation Stack.Figure 3 represents the state of the Ex-pectation Stack when the system asks forthe delay of the emergency fund (sentence13 or 15).The most expected answer is the delay ofthe emergency fund, but the system alsoforesees the possibility of a request for ex-planation from the user.
At the next level,the system expects the user to speak aboutanother parameter of the same plan (theamount).
At the level still further be-low, he/she can speak about the emer-gency fund in general, he/she can for ex-ample refuse the emergency fund, or askfor explanation on it.
And so on, until thesystem reaches the most unexpected reac-tions of the user, i.e., even things that arenot related to the investment problem.
(inthe level "others")5 Dialogue Management:Execution and RevisionIn the previous ections, we have presentedthe different structures needed to reflectthe state of dialogue.
The aim of this part- 157-is to show how these structures are usedfor handling revisions during the executionof the dialogue.The SSI is generated by a depth-first ex-pansion of the abstract communicativegoals of the system (use of dialogue-planmodels).
This process stops as soonas the Planner reaches an atomic action(communicative-plan)that produces a re-quest toward the user.
At this point,the Expectation Stack is derived from thestate of the SSI and reflects the precisetopic of the question on top.The input of the user is analyzed by theNLF and the Recognizer \[1\] is called toderive the user's intentions encoded inthe answer.
The Recognizer eturns thespeech act in the Expectation Stack it wasable to match.
In most cases, the returnedspeech act corresponds to the direct ex-pected answer and the dialogue continueswithout revision.A deviation occurs, however, when theRecognizer does not return the level cor-responding to the most expected answer,or when the user tries to put inconsis-tent value in the task plan (an example ofsuch an inconsistent value is given in sen-tence 20).
In this case, the Planner mustuse consistency constraints attached to thetask plan.
Thus, the pointer returned bythe Recognizer in the Expectation Stackindicates all the changes of subject or ofintention by the user while the constraintsattached to parameters of the task planreveal inconsistent values.There is a revision associated with eachtype of deviation.
A revision is a struc-tural transformation pattern for correct-ing the SSI after the user's answer in orderto continue a coherent dialogue.We illustrate the result of a revision onsentences 5 and 6 (see figure 4.We can see that the transformation takesinto account a subsequent return tothe emergency fund and an explicit re-introduction of this subject.The same types of transformation are usedto treat the deviations caused by the re-quests for explanation (sentence 14).
Inthe case of inconsistent values (sentence20), a subplan is inserted for explainingwhy the value is inconsistent and askingthe user to change something in order tocorrect he violation (sentences 21).It is interesting to note here that the strat-egy of the dialoguer can easily be modifiedby changing either the models of the plansor the transformations a sociated with thecases of deviation.
We presented above astrategy in the dialogue that ~follows theuser" (i.e., the dialoguer is very cooper-ative and changes the subject each timethe user wants to).
For instance, whenthe user decides to speak about his/hercar, the system is cooperative and allowshim/her to do so.
It would also have beenpossible to adopt another strategy and tosay "ok, we will talk about your car later,but now we need to discuss the emergencyfund because people tend to forget it oth-erwise".6 Conclus ionThe adaptive planning method presentedabove takes into account both analyses ofintentions and changes of topic.
Recogni-tion of intentions is organized around theattentional state in order to delimit thescope of the revision.
The revision mech-anism is currently implemented in an ex-perimental prototype of the ESTEAM-316Dialogner (written in PROLOG and run-ning on SUN Workstations).This approach seems appropriate for thetype of dialogues where the Uexpert" ad-vice giving system has a quite directive- 158-give parm (amount, EF)request expl (amount, EF)give parm (delay, EF)request exp} (delay, EF)request expl (EF)refuse (EF)Acts concerning Cash Needand other subplansgive parm plan (P1)request expl plan (P1)refuse plan (P1)Othersrequest expl (CN)refuse (CN)Acts concerning .Emergency Fundand other subplansgive parm plan (Pl)request expl plan (PI)refuse plan (PI)OthersTrans/ormation o/ the Ezpectation Stackhelp user formulate problemask parameter amounthelp user formulate problemreturn ~ to 1~/ .
.~ask parameter amountTransformation of the SSIFigure 4: A Example of revision: Changing Subject- 159-control of the dialogue.
Revisions givesome degrees of flexibility to the "novice ~user who is unfamiliar with the domainand the progression i  the consultation.Future work will extend the set of revisionstrategies to take into account deviationsthat might arise the final phase of an ad-vice giving session, presentation and nego-tiation of the solution.References\[1\] ESTEAM 316.
An Architecture forInteractive Problem Solving by Coop-crating Data and Knowledge Bases.Technical Report Deliverable 3, ES-PRIT Program, 1987.\[2\] J. F. Allen and C. R. Perrault.Analyzing intentions in utterences.Artificial Intelligence, 3(15):143-178,1980.\[3\] James F. Allen.
Plans, goals and nat-ural language.
Research Review ofComputer Science Department, Uni-versity of Rochester, 4-12, 1986.\[4\] Carol A. Broverman and W. BruceCroft.
Reasoning about exceptionsduring plan execution monitoring.Proc.
of AAAI, 190-195, 1987.\[5\] A. Bruffaerts, E. Henin, and V. Mar-lair.
An Expert System Prototypefor Financial Counseling.
TechnicalReport Research Report 507, PhilipsResearch Laboratory, 1986.\[6\] Philip R. Cohen and C. RaymondPerrault.
Elements of a plan-basedtheory of speech acts.
Cognitive Sci-ence, (3):177-212, 1979.\[7\] P. Decitre, T. Grossi, C. Jullien, andJP.
Solvay.
Planning for problemformulation i  advice-giving dialogue.Proe.
of A CL, 1987.\[8\] M. P. Georgeff and A. L. Lansky.Procedural Knowledge.
Technical Re-port Technical Note 411, SRI Inter-national, January 1987.\[9\] Barbara J. Grosz.
The Represen-tation and Use of Focus in Dia-logue Understanding.
Technical Re-port TR 151, Artificial IntelligenceCenter, SRI International, 1977.\[10\] Barbara J. Grosz and Candace L.Sidner.
Attention, intentions andthe structure of discourse.
Com-putational Linguistics, 12(3):175-205,1986.\[11\] Diane J. Litman.
Discourse andProblem Solving.
Technical Re-port TR 130, Computer ScienceDpt., University of Rochester, Octo-ber 1983.- 160-
