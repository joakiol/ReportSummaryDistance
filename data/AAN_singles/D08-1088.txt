Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 839?847,Honolulu, October 2008. c?2008 Association for Computational LinguisticsComplexity of finding the BLEU-optimal hypothesis in a confusion networkGregor Leusch and Evgeny Matusov and Hermann NeyRWTH Aachen University, Germany{leusch,matusov,ney}@cs.rwth-aachen.deAbstractConfusion networks are a simple representa-tion of multiple speech recognition or transla-tion hypotheses in a machine translation sys-tem.
A typical operation on a confusion net-work is to find the path which minimizes ormaximizes a certain evaluation metric.
In thisarticle, we show that this problem is gener-ally NP-hard for the popular BLEU metric,as well as for smaller variants of BLEU.
Thisalso holds for more complex representationslike generic word graphs.
In addition, we givean efficient polynomial-time algorithm to cal-culate unigram BLEU on confusion networks,but show that even small generalizations ofthis data structure render the problem to beNP-hard again.Since finding the optimal solution is thus notalways feasible, we introduce an approximat-ing algorithm based on a multi-stack decoder,which finds a (not necessarily optimal) solu-tion for n-gram BLEU in polynomial time.1 IntroductionIn machine translation (MT), confusion networks(CNs) are commonly used to represent alternativeversions of sentences.
Typical applications includetranslation of different speech recognition hypothe-ses (Bertoldi et al, 2007) or system combination(Fiscus, 1997; Matusov et al, 2006).A typical operation on a given CN is to find thepath which minimizes or maximizes a certain eval-uation metric.
This operation can be used in ap-plications like Minimum Error Rate Training (Och,2003), or optimizing system combination as de-scribed by Hillard et al (2007).
Whereas this iseasily achievable for simple metrics like the WordError Rate (WER) as described by Mohri and Riley(2002), current research in MT uses more sophisti-cated measures, like the BLEU score (Papineni etal., 2001).
Zens and Ney (2005) first described thistask on general word graphs, and sketched a com-plete algorithm for calculating the maximum BLEUscore in a word graph.
While they do not give anestimate on the complexity of their algorithm, theynote that already a simpler algorithm for calculatingthe Position independent Error Rate (PER) has anexponential worst-case complexity.
The same canbe expected for their BLEU algorithm.
Dreyer etal (2007) examined a special class of word graphs,namely those that denote constrained reorderings ofsingle sentences.
These word graphs have someproperties which simplify the calculation; for exam-ple, no edge is labeled with the empty word, andall paths have the same length and end in the samenode.
Even then, their decoder does not optimizethe true BLEU score, but an approximate versionwhich uses a language-model-like unmodified pre-cision.
We give a very short introduction to CNs andthe BLEU score in Section 2.In Section 3 we show that finding the best BLEUscore is an NP-hard problem, even for a simplifiedvariant of BLEU which only scores unigrams andbigrams.
The main reason for this problem to be-come NP-hard is that by looking at bigrams, we al-low for one decision to also influence the followingdecision, which itself can influence the decisions af-ter that.
We also show that this also holds for uni-gram BLEU and the position independent error rate(PER) on a slightly augmented variant of CNs whichallows for edges to carry multiple symbols.
The con-catenation of symbols corresponds to the interde-pendency of decisions in the case of bigram matchesabove.NP-hard problems are quite common in machine839translation; for example, Knight (1999) has shownthat even for a simple form of statistical MT mod-els, the decoding problem is NP-complete.
Morerecently, DeNero and Klein (2008) have proven theNP-completeness of the phrase alignment problem.But even a simple, common procedure as BLEUscoring, which can be performed in linear time onsingle sentences, becomes a potentially intractableproblem as soon as it has to be performed on aslightly more powerful representation, such as con-fusion networks.
This rather surprising result is themotivation of this paper.The problem of finding the best unigram BLEUscore in an unaugmented variant of CNs is not NP-complete, as we show in Section 4.
We present analgorithm that finds such a unigram BLEU-best pathin polynomial time.An important corollary of this work is that calcu-lating the BLEU-best path on general word graphsis also NP-complete, as CNs are a true subclassof word graphs.
It is still desirable to calculate a?good?
path in terms of the BLEU score in a CN,even if calculating the best path is infeasible.
In Sec-tion 5, we present an algorithm which can calculate?good?
solutions for CNs in polynomial time.
Thisalgorithm can easily be extended to handle arbitraryword graphs.
We assess the algorithm experimen-tally on real-world MT data in Section 6, and drawsome conclusions from the results in this article inSection 7.2 Confusion networksA confusion network (CN) is a word graph whereeach edge is labeled with exactly zero or one sym-bol, and each path from the start node to the endnode visits each node of the graph in canonical or-der.
Usually, we represent unlabeled edges by label-ing them with the empty word ?.Within this paper, we represent a CN by a list oflists of words {wi,j}, where each wi,j correspondsto a symbol on an edge between nodes i and i + 1.A path in this CN can be written as a string of inte-gers, an1 = a1, .
.
.
, an, such that the path is labeledw1,a1w2,a2 .
.
.
wn,an .
Note that there can be a differ-ent number of possible words, j, for different posi-tions i.2.1 BLEU and variantsThe BLEU score, as defined by Papineni et al(2001), is the modified n-gram precision of a hy-pothesis, with 1 ?
n ?
N , given a set of referencetranslations R. ?Modified precision?
here meansthat for each n-gram, its maximum number of oc-currences within the reference sentences is counted,and only up to that many occurrences in the hypothe-sis are considered to be correct.
The geometric meanover the precisions for all n is calculated, and mul-tiplied by a brevity penalty bp.
This brevity penaltyis 1.0 if the hypothesis sentence is at least as long asthe reference sentence (special cases occur if multi-ple reference sentences with different length exists),and less than 1.0 otherwise.
The exact formulationcan be found in the cited paper; for the proofs inour paper it is enough to note that the BLEU scoreis 1.0 exactly if all n-grams in the hypothesis oc-cur at least that many times in a reference sentence,and if there is a reference sentence which is as longas or shorter than the hypothesis.
Assuming thatwe can always provide a dummy reference sentenceshorter than this length, we do not need to regardthe brevity penalty in these proofs.
Within the fol-lowing proofs of NP-hardness, we will only requireconfusion networks (and word graphs) which do notcontain empty words, and where all paths from thestart node to the end node have the same length.Usually, in the definition of the BLEU score, N isset to 4; within this article we denote this metric as4BLEU.
We can also restrict the calculations to un-igrams only, which would be 1BLEU, or to bigramsand unigrams, which we denote as 2BLEU.Similar to the 1BLEU metric is the Position in-dependent Error Rate PER (Tillmann et al, 1997),which counts the number of substitutions, insertions,and deletions that have to be performed on the uni-gram counts to have the hypothesis counts match thereference counts.
Unlike 1BLEU, for PER to be op-timal (here, 0.0), the reference counts must matchthe candidate counts exactly.Given a CN {wi,j} and a set of reference sen-tences R, we define the optimization problemDefinition 1 (CN-2BLEU-OPTIMIZE) Amongall paths aI1 through the CN, what is the path withthe highest 2BLEU score?Related to this is the decision problemDefinition 2 (CN-2BLEU-DECIDE) Among allpaths aI1 through the CN, is there a path with a2BLEU score of 1.0?Similarly we define CN-4BLEU-DECIDE, CN-PER-DECIDE, etc.8403 CN-2BLEU-DECIDE is NP-completeWe now show that CN-2BLEU-DECIDE is NP-complete.
It is obvious that the problem is in NP:Given a path aI1, which is polynomial in size to theproblem, we can decide in polynomial time whetheraI1 is a solution to the problem ?
namely by calcu-lating the BLEU score.
We now show that there isa problem known to be NP-complete which can bepolynomially reduced to CN-2BLEU-DECIDE.
Forour proof, we choose 3SAT.3.1 3SATConsider the following problem:Definition 3 (3SAT) Let X = {x1, .
.
.
, xn}be a set of Boolean variables, let F =?ki=1 (Li,1?Li,2?Li,3) be a Boolean formula,where each literal Li,j is either a variable x or itsnegate x.
Is there a assignment ?
: X ?
{0, 1}such that ?
|= F?
In other words, if we replaceeach x in F by ?
(x), and each x by 1?
?
(x), doesF become true?It has been shown by Karp (1972) that 3SAT isNP-complete.
Consequently, if for another problemin NP there is polynomial-size and -time reductionof an arbitrary instance of 3SAT to an instance of thisnew problem, this new problem is also NP-complete.3.2 Reduction of 3SAT toCN-2BLEU-DECIDELet F be a Boolean formula in 3CNF, and let k beits size, as in Definition 3.
We will now reduce it to acorresponding CN-2BLEU-DECIDE problem.
Thismeans that we create an alphabet ?, a confusion net-work C, and a set of reference sentencesR, such thatthere is a path through C with a BLEU score of 1.0exactly if F is solvable:Create an alphabet ?
based on F as ?
:={x1, .
.
.
, xn} ?
{x1, .
.
.
, xn} ?
{}.
Here, the xiand xi symbols will correspond to the variable withthe same name or their negate, respectively, whereas will serve as an ?isolator symbol?, to avoid un-wanted bigram matches or mismatches between sep-arate parts of the constructed CN or sentences.Consider the CN C from Figure 1.Consider the following set of reference sentences:R := {  (x1)k(x2)k .
.
.
(xn)k (x1)k(x2)k .
.
.
(xn)k,(x1)k  (x1)k  .
.
.
(xn)k  (xn)k  ()k+n }where (x)k denotes k subsequent occurrences of x.Clearly, both C and R are of polynomial size in nand k, and can be constructed in polynomial time.Then,There is an assignment ?
such that ?
|= F?There is a path aI1 through C such thatBLEU(aI1, R) = 1.0.Proof: ??
?Let ?
be an assignment under which F becomestrue.
Create a path aI1 as follows: Within A, foreach set of edges Li,1, Li,2, Li,3, choose the paththrough an x where ?
(x) = 1, or through an xwhere ?
(x) = 0.
Note that there must be such anx, because otherwise the clause Li,1 ?
Li,2 ?
Li,3would not be true under ?.
Within B, select the pathalways through xi if ?
(xi) = 0, and through xi if?
(xi) = 1.Then, aI1 consists of, for each i,?
At most k occurrences of both xi and xi?
At most k occurrences of each of the bigramsxi, xi, xi, xi, xixi, and xixi?
No other bigram than those listed above.For all of these unigram and bigram counts, there isa reference sentence in R which contains at least asmany of those unigrams/bigrams as the path.
Thus,the unigram and bigram precision of aI1 is 1.0.
In ad-dition, there is always a reference sentence whoselength is shorter than that of aI1, such that the brevitypenalty is also 1.0.
As a result, BLEU(aI1, R) =1.0.??
?Let aI1 be a path through C such thatBLEU(aI1, R) = 1.0.
Because there is no bi-gram xixi or xixi in R, we can assume that for eachxi, either only xi edges, or only xi edges appearin the B part of aI1, each at most k times.
As nounigram xi and xi appears more than k times in R,we can assume that, if the xi edges are passed inB, then only the xi edges are passed in A, and viceversa.
Now, create an assignment ?
as follows:?
:={0 ifxi edges are passed inB1 otherwiseThen, ?
|= F .
Proof: Assume that F?
= 0.
Thenthere must be a clause i such that Li,1?Li,2?Li,3 =841A :=             L1,1L1,2L1,3                        L2,1L2,2L2,3             ?
?
?                        Lk,1Lk,2Lk,3                        B :=             x1x1            x1x1?
?
?            x1x1?
??
?k times                        x2x2?
?
??
??
?k             ?
?
?            xnxn?
?
??
??
?k            C := A             BFigure 1: CN constructed from a 3SAT formula F .
C is the concatenation of the left part A, and the right path B,separated by an isolating .0.
At least one of the edges Li,j associated with theliterals of this clause must have been passed by aK1in A.
This literal, though, can not have been passedin B.
As a consequence, ?
(Li,j) = 1.
But thismeans that Li,1 ?
Li,2 ?
Li,3 = 1 ; contra-diction.Because CN-2BLEU-DECIDE is in NP, and wecan reduce an NP-complete problem (3SAT) in poly-nomial time to a CN-2BLEU-DECIDE problem, thismeans that CN-2BLEU-DECIDE is NP-complete.3.3 CN-4BLEU-DECIDEIt is straightforward to modify the constructionabove to create an equivalent CN-4BLEU-DECIDEproblem instead: Replace each occurrence of theisolating symbol  in A,B, C, R by three consecu-tive isolating symbols .
Then, everything saidabout unigrams still holds, and bi-, tri- and four-grams are handled equivalently: Previous unigrammatches on  correspond to uni-, bi-, and trigrammatches on , , .
Bigram matches on x corre-spond to bi-, tri-, and fourgram matches on x, x,x, and similar holds for bigram matches x, x,x.
Unigram matches x, x, and bigram matchesxx etc.
stay the same.
Consequently, CN-4BLEU-DECIDE is also an NP-complete problem.3.4 CN*-1BLEU-DECIDEIs it possible to get rid of the necessity for bi-gram counts in this proof?
One possibility might beto look at slightly more powerful graph structures,CN*.
In these graphs, each edge can be labeledby arbitrarily many symbols (instead of just zero orone).
Then, consider a CN* graph C?
:= A            B?,B?
:=             (x1)k(x1)k                        x1x1??
??
?k times?
?
?            (xn)k(xn)k             ?
?
?Figure 2: Right part of a CN* constructed from a 3SATformula F .with B?
as in Figure 2.WithR?
:= {(x1)k(x1)k .
.
.
(xn)k(xn)k()k}we can again assume that either xi or xi ap-pears k times in the B?-part of a path aK1 with1BLEU(aK1 , R?)
= 1.0, and that for every solution?
to F there is a corresponding path aK1 through C?and vice versa.
In this construction, we also haveexact matches of the counts, so we can also use PERin the decision problem.While CN* are generally not word graphs bythemselves due to the multiple symbols on edges,it is straightforward to create an equivalent wordgraph from a given CN*, as demonstrated in Fig-ure 3.
Consequently, deciding unigram BLEU andunigram PER are NP-complete problems for generalword graphs as well.4 Solving CN-1BLEU-DECIDE inpolynomial timeIt is not a coincidence that we had to resort tobigrams or to edges with multiple symbols forNP-completeness: It turns out that CN-1BLEU-DECIDE, where the order of the words does not842CN*: ?
?
?
            (x1)k(x1)k             ?
?
?
;WG: ?
?
?
         x1x1              x1x1?
?
?
              x1x1?
??
?k times             ?
?
?Figure 3: Construction of a word graph from a CN* as inB?.matter at all, can be decided in polynomial timeusing the following algorithm, which disregards abrevity penalty for the sake of simplicity:Given a vocabulary X , a CN {wi,j}, and a set ofreference sentences R together with their unigramBLEU counts c(x) : X ?
N and C :=?x?X c(x),1.
Remove all parts fromw where there is an edgelabeled with the empty word ?.
This step willalways increase unigram precision, and can nothurt any higher n-gram precision here, becausen = 1.
In the example in Figure 4, the edgeslabeled very and ?
respectively are affected inthis step.2.
Create nodes A0 := {1, .
.
.
, n}, one for eachnode with edges in the CN.
In the example inFigure 5, the three leftmost column heads cor-respond to these nodes.3.
Create nodes B := {x.j |x ?
X, 1?j?c(x)}.In other words, create a unique node for each?running?
word in R ?
e.g.
if the first andsecond reference sentence contain x once each,and the third reference contains x twice, createexactly x.1 and x.2.
In Figure 5, those are therow heads to the right.4.
Fill A with empty nodes to match the totallength: A := A0 ?
{?.j | 1 ?
j ?
C ?
n}.If n > C, the BLEU precision can not be 1.0.The five rightmost columns in Figure 5 corre-spond to those.5.
Create edgesE := {(i, wi,j .k) | 1?
i?n, all j, 1?c(wi,j)}?
{(i, ?.j) | 1 ?
i ?
n, all j}.
These edges aredenoted as ?
or ?
in Figure 5.C :=             onat            thethat            very?            daytimeR := { on the same day,at the time and the day }Figure 4: Example for CN-1BLEU-DECIDE.1 2 3 ?.1 ?.2 ?.3 ?.4 ?.5?
?
?
?
?
?
on?
?
?
?
?
?
the.1?
?
?
?
?
?
the.2?
?
?
?
?
same?
?
?
?
?
?
day?
?
?
?
?
?
at?
?
?
?
?
?
time?
?
?
?
?
andFigure 5: Bipartite graph constructed to find the optimal1BLEU path in Figure 4.
One possible maximum bipar-tite matching is marked with ?.6.
Find the maximum bipartite matching M be-tween A and B given E. Figure 5 shows sucha matching with ?.7.
If all nodes in A and B are covered by M ,then 1BLEU({wi,j}, R) = 1.0.
The words thatare matched to A0 then form the solution paththrough {wi,j}.Figure 4 gives an example of a CN and a set of ref-erences R, for which the best 1BLEU path can beconstructed by the algorithm above.
The bipartitegraph constructed in Step 1 to Step 4 for this exam-ple, given in matrix form, can be found in Figure 5.Such a solution to Step 6, if found, correspondsexactly to a path through the confusion network with1BLEU=1.0, and vice versa: for each position 1 ?i ?
n, the matched word corresponds to the wordthat is selected for the position of the path; ?surplus?counts are matched with ?s.Step 6 can be performed in polynomial time(Hopcroft and Karp, 1973) O((C +n)5/2); all othersteps in linear time O(C + n).
Consequently, CN-1BLEU can be decided in polynomial time O((C +n)5/2).
Similarly, an actual optimum 1BLEU score843can be calculated in O((C + n)5/2).It should be noted that the only alterations in thehypothesis length, and as a result the only alterationsin the brevity penalty, will come from Step 1.
Con-sequently, the brevity penalty can be taken into ac-count as follows: Consider that there are M nodeswith an empty edge in {wi,j}.
Instead of remov-ing them in Step 1, keep them in, but for each1 ?
m ?
M , run through steps 2 to 6, but addm nodes ?.1, .
.
.
, ?.m to B in Step 3, and add corre-sponding edges to these nodes to E in Step 5.
Aftereach iteration (which leads to a constant hypothesislength), calculate precision and brevity penalty.
Se-lect the best product of precision and brevity penaltyin the end.
The overall time complexity now is inM ?O((C + n)5/2).A PER score can be calculated in a similar fash-ion.5 Finding approximating solutions forCN-4BLEU in polynomial timeKnowing that the problem of finding the BLEU-bestpath is an NP-complete problem is an unsatisfactoryanswer in practice ?
in many cases, having a good,but not necessarily optimum path is preferable tohaving no good path at all.A simple approach would be to walk the CN fromthe start node to the end node, keeping track of n-grams visited so far, and choosing the word nextwhich maximizes the n-gram precision up to thisword.
Track is kept by keeping n-gram count vec-tors for the hypothesis path and the reference sen-tences, and update those in each step.The main problem with this approach is that of-ten the local optimum is suboptimal on the globalscale, for example if a word occurs on a later posi-tion again.Zens and Ney (2005) on the other hand proposeto keep all n-gram count vectors instead, and onlyrecombine path hypotheses with identical count vec-tors.
As they suspect, the search space can becomeexponentially large.In this paper, we suggest a compromise betweenthese two extremes, namely keeping active a suffi-ciently large number of ?path hypotheses?
in termsof n-gram precision, instead of only the first best,or of all.
But even then, edges with empty wordspose a problem, as stepping along an empty edgewill never decrease the precision of the local path.In certain cases, steps along empty edges may affectthe n-gram precision for higher n-grams.
But thiswill only take effect after the next non-empty step, itdoes not influence the local decision in a node.
Step-ping along a non-empty edge will often decrease thelocal precision, though.
As a consequence, a simplealgorithm will prefer paths with shorter hypotheses,which leads to a suboptimal total BLEU score, be-cause of the brevity penalty.
One can counter thisproblem for example by using a brevity penalty al-ready during the search.
But this is problematic aswell, because it is difficult to define a proper partialreference length in this case.The approach we propose is to compare only par-tial path hypotheses with the same number of emptyedges, and ending in the same position in the confu-sion network.
This idea is illustrated in Figure 6: Wecompare only the partial precision of path hypothe-ses ending in the same node.
Due to the simple na-ture of this search graph, it can easily be traversed ina left-to-right, top-to-bottom manner.
With regard toa node currently being expanded, only the next nodein the same row, and the corresponding columns inthe next row need to be kept active.
When imple-menting this algorithm, Hypotheses should be com-pared on the modified BLEUS precision by Lin andOch (2004) because the original BLEU precisionequals zero as long as there are no higher n-grammatches in the partial hypotheses, which rendersmeaningful comparison hard or impossible.In the rightmost column, all path hypotheseswithin a node have the same hypothesis length.
Con-sequently, we can select the hypothesis with the best(brevity-penalized) BLEU score by multiplying theappropriate brevity penalty to the precision of thebest path ending in each of these nodes.
If we al-ways expand all possible path hypotheses within thenodes, and basically run a full search, we will al-ways find the BLEU-best path this way.
From theproof above, it follows that the number of path hy-pothesis we would have to keep can become expo-nentially large.
Fortunately, if a ?good?
solution isgood enough, we do not have to keep all possiblepath hypotheses, but only the S best ones for a givenconstant S, or those with a precision not worse thanc times the precision of the best hypothesis withinthe node.
Assuming that adding and removing anelement to/from a size-limited stack of size S takestimeO(logS), that we allow at mostE empty edgesin a solution, and that there are j edges in each of then positions, this algorithm has a time complexity of844Figure 6: Principle of the multi-stack decoder used to finda path with a good BLEU score.
The first row showsthe original confusion network, the following rows showthe search graph.
Duplicate edges were removed, but noword was considered ?unknown?.O(E ?
n ?
j ?
S logS).To reduce redundant duplicated path hypotheses,and by this to speed up the algorithm and reduce therisk that good path hypotheses are pruned, the confu-sion network should be simplified before the search,as shown in Figure 6:1.
Remove all words in the CN which do not ap-pear in any reference sentence, if there at leastone ?known?
non-empty word at the same po-sition.
If there is no such ?known?
word, re-place them all by a single token denoting the?unknown word?.2.
Remove all duplicate edges in a position, thatis, if there are two or more edges carrying thesame label in one position, remove all but onefor them.These two steps will keep at least one of the BLEU-best paths intact.
But they can remove the averagebranching factor (j) of the CN significantly, whichleads to a significantly lower number of duplicatepath hypotheses during the search.Table 1: Statistics of the (Chinese?
)English MT corporaused for the experimentsNIST NIST2003 2006number of systems 4 4number of ref.
4 4 per sent.sentences 919 249system length 28.4 33.2 words?ref.
length 27.5 34.2 words?best path 24.4 33.9 words?CN length 40.7 39.5 nodes?best single system 29.3 52.5 BLEU30.5 51.6 BLEUS?
?average per sentenceOur algorithm can easily be extended to handle ar-bitrary word graphs instead of confusion networks.In this case, each ?row?
in Figure 6 will reflect thestructure of the word graph instead of the ?linear?structure of the CN.While this algorithm searches for the best pathfor a single sentence only, a common task is tofind the best BLEU score over a whole test set ?which can mean suboptimal BLEU scores for in-dividual sentences.
This adds an additional com-binatorial problem over the sentences to the actualdecoding process.
Both Zens and Ney (2005) andDreyer et al(2007) use a greedy approach here; thelatter estimated the impact of this to be insignifi-cant in random sampling experiments.
In our exper-iments, we used the per-sentence BLEUS score as(greedy) decision criterion, as this is also the prun-ing criterion.
One possibility to adapt this approachto Zens?s/Dreyer?s greedy approach for system-levelBLEU scores might be to initialize n-gram countsand hypothesis length not to zero at the beginningof each sentence, but to those of the corpus so far.But as this diverts from our goal to optimize thesentence-level scores, we have not implemented itso far.6 Experimental assessment of thealgorithmThe question arises how many path hypotheses weneed to retain in each step to obtain optimal paths.To examine this, we created confusion networks outof the translations of the four best MT systems of845ll ll l l l l l l l l l l l5 10 15 200.320.340.360.380.40# path hypsBLEU, BLEUSlsys?BLEUavg.
seg?BLEUSavg.
seg?BLEUFigure 7: Average of the sentence-wise BLEU andBLEUS score and the system-wide BLEU score versusthe number of path hypotheses kept per node during thesearch.
NIST MT03 corpus.the NIST 2003 and 2006 Chinese?English evalu-ation campaigns, as available from the LinguisticData Consortium (LDC).
The hypotheses of the bestsingle system served as skeleton, those of the threeremaining systems were reordered and aligned to theskeleton hypothesis.
This corpus is described in Ta-ble 1.
Figures 7 and 8 show the measured BLEUscores in three different definitions, versus the max-imum number of path hypotheses that are kept ineach node of the search graph.
Shown are the av-erage sentence-wise BLEUS score, which is whatthe algorithm actually optimizes, for comparison theaverage sentence-wise BLEU score, and the totaldocument-wise BLEU score.All scores increase with increasing number of re-tained hypotheses, but stabilize around a total of 15hypotheses per node.
The difference over a greedyapproach, which corresponds to a maximum of onehypothesis per node if we leave out the separation bypath length, is quite significant.
No further improve-ments can be expected for a higher number of hy-potheses, as experiments up to 100 hypotheses show.7 ConclusionsIn this paper, we showed that deciding whether agiven CN contains a path with a BLEU score of 1.0is an NP-complete problem for n-gram lengths ?
2.ll l ll l l l l l l l l l l l5 10 15 20 250.650.670.690.71# path hypsBLEU, BLEUSlsys?BLEUavg.
seg?BLEUSavg.
seg?BLEUFigure 8: Average of the sentence-wise BLEU andBLEUS score and the system-wide BLEU score versusthe number of path hypotheses kept per node during thesearch.
NIST MT06 corpus.The problem is also NP-complete if we only lookat unigram BLEU, but allow for CNs where edgesmay contain multiple symbols, or for arbitrary wordgraphs.
As a corollary, any proposed algorithm tofind the path with an optimal BLEU score in a CN,even more in an arbitrary word graph, which runsin worst case polynomial time can only deliver anapproximation1.We gave an efficient polynomial time algorithmfor the simplest variant, namely deciding on a uni-gram BLEU score for a CN.
This algorithm can eas-ily be modified to decide on the PER score as well,or to calculate an actual unigram BLEU score for thehypothesis CN.Comparing these results, we conclude that theability to take bi- or higher n-grams into account,be it in the scoring (as in 2BLEU), or in the graphstructure (as in CN*), is the key to render the prob-lem NP-hard.
Doing so creates long-range depen-dencies, which oppose local decisions.We also gave an efficient approximating algo-rithm for higher-order BLEU scores.
This algorithmis based on a multi-stack decoder, taking into ac-count the empty arcs within a path.
Experimentalresults on real-world data show that our method isindeed able to find paths with a significantly better1provided that P 6= NP , of course.846BLEU score than that of a greedy search.
The re-sulting BLEUS score stabilizes already on a quiterestricted search space, showing that despite theproven NP-hardness of the exact problem, our al-gorithm can give useful approximations in reason-able time.
It is yet an open problem in how farthe problems of finding the best paths regarding asentence-level BLEU score, and regarding a system-level BLEU score correlate.
Our experiments heresuggest a good correspondence.8 AcknowledgmentsThis paper is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023.The proofs and algorithms in this paper emergedwhile the first author was visiting researcher atthe Interactive Language Technologies Group ofthe National Research Council (NRC) of Canada,Gatineau.
The author wishes to thank NRC andAachen University for the opportunity to jointlywork on this project.ReferencesNicola Bertoldi, Richard Zens, and Marcello Federico.2007.
Speech translation by confusion network de-coding.
In IEEE International Conference on Acous-tics, Speech, and Signal Processing, pages 1297?1300,Honululu, HI, USA, April.John DeNero and Dan Klein.
2008.
The complexityof phrase alignment problems.
In Human LanguageTechnologies 2008: The Conference of the Associationfor Computational Linguistics, Short Papers, pages25?28, Columbus, Ohio, June.
Association for Com-putational Linguistics.Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.2007.
Comparing Reordering Constraints for SMTUsing Efficient BLEU Oracle Computation.
In AMTAWorkshop on Syntax and Structure in Statistical Trans-lation (SSST) at the Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (NAACL-HLT), pages 103?110,Rochester, NY, USA, April.Jonathan G. Fiscus.
1997.
A post-processing system toyield reduced word error rates: Recogniser output vot-ing error reduction (ROVER).
In Proceedings 1997IEEE Workshop on Automatic Speech Recognition andUnderstanding, pages 347?352, Santa Barbara, CA.Dustin Hillard, Bjo?rn Hoffmeister, Mari Ostendorf, RalfSchlu?ter, and Hermann Ney.
2007. iROVER: Improv-ing system combination with classification.
In HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Companion Volume, Short Pa-pers, pages 65?68, Rochester, New York, April.John E. Hopcroft and Richard M. Karp.
1973.
An n5/2algorithm for maximum matchings in bipartite graphs.SIAM Journal on Computing, 2(4):225?231.Richard M. Karp.
1972.
Reducibility among combina-torial problems.
In R. E. Miller and J. W. Thatcher,editors, Complexity of Computer Computations, pages85?103.
Plenum Press.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25(4):607?615, December.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluation automatic evaluation metrics formachine translation.
In Proc.
COLING 2004, pages501?507, Geneva, Switzerland, August.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multiplemachine translation systems using enhanced hypothe-ses alignment.
In Conference of the European Chap-ter of the Association for Computational Linguistics,pages 33?40, Trento, Italy, April.Mehryar Mohri and Michael Riley.
2002.
An efficientalgorithm for the n-best-strings problem.
In Proc.
ofthe 7th Int.
Conf.
on Spoken Language Processing (IC-SLP?02), pages 1313?1316, Denver, CO, September.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
of the 41thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 160?167, Sapporo, Japan,July.Kishore A. Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
Bleu: a method for auto-matic evaluation of machine translation.
Technical Re-port RC22176 (W0109-022), IBM Research Division,Thomas J. Watson Research Center, September.Christoph Tillmann, Stephan Vogel, Hermann Ney, AlexZubiaga, and Hassan Sawaf.
1997.
AcceleratedDP based search for statistical translation.
In Euro-pean Conf.
on Speech Communication and Technol-ogy, pages 2667?2670, Rhodes, Greece, September.Richard Zens and Hermann Ney.
2005.
Word graphsfor statistical machine translation.
In 43rd AnnualMeeting of the Assoc.
for Computational Linguistics:Proc.
Workshop on Building and Using Parallel Texts:Data-Driven Machine Translation and Beyond, pages191?198, Ann Arbor, MI, June.847
