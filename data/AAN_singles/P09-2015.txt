Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 57?60,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPA Framework for Entailed Relation RecognitionDan Roth Mark Sammons V.G.Vinod VydiswaranUniversity of Illinois at Urbana-Champaign{danr|mssammon|vgvinodv}@illinois.eduAbstractWe define the problem of recognizing entailed re-lations ?
given an open set of relations, find all oc-currences of the relations of interest in a given doc-ument set ?
and pose it as a challenge to scalableinformation extraction and retrieval.
Existing ap-proaches to relation recognition do not address wellproblems with an open set of relations and a needfor high recall: supervised methods are not eas-ily scaled, while unsupervised and semi-supervisedmethods address a limited aspect of the problem, asthey are restricted to frequent, explicit, highly lo-calized patterns.
We argue that textual entailment(TE) is necessary to solve such problems, proposea scalable TE architecture, and provide preliminaryresults on an Entailed Relation Recognition task.1 IntroductionIn many information foraging tasks, there is a needto find all text snippets relevant to a target concept.Patent search services spend significant resourceslooking for prior art relevant to a specified patentclaim.
Before subpoenaed documents are used ina court case or intelligence data is declassified, allsensitive sections need to be redacted.
While theremay be a specific domain for a given application,the set of target concepts is broad and may changeover time.
For these knowledge-intensive tasks,we contend that feasible automated solutions re-quire techniques which approximate an appropri-ate level of natural language understanding.Such problems can be formulated as a relationrecognition task, where the information need is ex-pressed as tuples of arguments and relations.
Thisstructure provides additional information whichcan be exploited to precisely fulfill the informa-tion need.
Our work introduces the Entailed Rela-tion Recognition paradigm, which leverages a tex-tual entailment system to try to extract all relevantpassages for a given structured query without re-quiring relation-specific training data.
This con-trasts with Open Information Extraction (Bankoand Etzioni, 2008) and On-Demand InformationExtraction (Sekine, 2006), which aim to extractlarge databases of open-ended facts, and with su-pervised relation extraction, which requires addi-tional supervised data to learn new relations.Specifically, the contributions of this paper are:1.
Introduction of the entailed relation recognitionframework; 2.
Description of an architecture and asystem which uses structured queries and an exist-ing entailment engine to perform relation extrac-tion; 3.
Empirical assessment of the system on acorpus of entailed relations.2 Entailed Relation Recognition (ERR)In the task of Entailed Relation Recognition, a cor-pus and an information need are specified.
Thecorpus comprises all text spans (e.g.
paragraphs)contained in a set of documents.
The informationneed is expressed as a set of tuples encoding rela-tions and entities of interest, where entities can beof arbitrary type.
The objective is to retrieve allrelevant text spans that a human would recognizeas containing a relation of interest.
For example:Information Need: An organization acquires weapons.Text 1: ...the recent theft of 500 assault rifles by FARC...Text 2: ...the report on FARC activities made three main ob-servations.
First, their allies supplied them with the 3?
mor-tars used in recent operations.
Second, ...Text 3: Amnesty International objected to the use of artilleryto drive FARC militants from heavily populated areas.An automated system should identify Texts 1 and2 as containing the relation of interest, and Text 3as irrelevant.
The system must therefore detectrelation instances that cross sentence boundaries(?them?
maps to ?FARC?, Text 2), and that re-quire inference (?theft?
implies ?acquire?, Text 1).It must also discern when sentence structure pre-cludes a match (?Amnesty International... use...artillery?
does not imply ?Amnesty International57acquires artillery?, Text 3).The problems posed by instances like Text 2are beyond the scope of traditional unsuper-vised and semi-supervised relation-extraction ap-proaches such as those used by Open IE and On-Demand IE, which are constrained by their de-pendency on limited, sentence-level structure andhigh-frequency, highly local patterns, in whichrelations are explicitly expressed as verbs andnouns.
Supervised methods such as (Culotta andSorensen, 2004) and (Roth and Yih, 2004) pro-vide only a partial solution, as there are many pos-sible relations and entities of interest for a givendomain, and such approaches require new anno-tated data each time a new relation or entity type isneeded.
Information Retrieval approaches are op-timized for document-level performance, and en-hancements like pseudo-feedback (Rocchio, 1971)are less applicable to the localized text spansneeded in the tasks of interest; as such, it is un-likely that they will reliably retrieve all correct in-stances, and not return superficially similar but in-correct instances (such as Text 3) with high rank.Attempts have been made to apply Textual En-tailment in larger scale applications.
For the taskof Question Answering, (Harabagiu and Hickl,2006) applied a TE component to rerank candidateanswers returned by a retrieval step.
However, QAsystems rely on redundancy in the same way OpenIE does: a large document set has so many in-stances of a given relation that at least some willbe sufficiently explicit and simple that standard IRapproaches will retrieve them.
A single correct in-stance suffices to complete the QA task, but doesnot meet the needs of the task outlined here.Recognizing relation instances requiring infer-ence steps, in the absence of labeled training data,requires a level of text understanding.
A suit-able proxy for this would be a successful TextualEntailment Recognition (TE) system.
(Dagan etal., 2006) define the task of Recognizing TextualEntailment (RTE) as: ...a directional relation be-tween two text fragments, termed T ?
the entailingtext, and H ?
the entailed text.
T entails H if, typ-ically, a human reading T would infer that H ismost likely true.
For relation recognition, the rela-tion triple (e.g.
?Organization acquires weapon?
)is the hypothesis, and a candidate text span thatmight contain the relation is the text.
The def-inition of RTE clearly accommodates the rangeof phenomena described for the examples above.However, the more successful TE systems (e.g.
(Hickl and Bensley, 2007)) are typically resourceintensive, and cannot scale to large retrieval tasksif a brute force approach is used.We define the task of Entailed Relation Recog-nition thus: Given a text collection D, and an in-formation need specified in a set of [argument, re-lation, argument] triples S: for each triple s ?
S,identify all texts d ?
D such that d entails s.The information need triples, or queries, encoderelations between arbitrary entities (specifically,these are not constrained to be Named Entities).This problem is distinct from recent work inTextual Entailment as we constrain the structureof the Hypothesis to be very simple, and we re-quire that the task be of a significantly larger scalethan the RTE tasks to date (which are typically ofthe order of 800 Text-Hypothesis pairs).3 Scalable ERR AlgorithmOur scalable ERR approach, SERR, consists oftwo stages: expanded lexical retrieval, and entail-ment recognition.
The SERR algorithm is pre-sented in Fig.
1.
The goal is to scale TextualEntailment up to a task involving large corpora,where hypotheses (queries) may be entailed bymultiple texts.
The task is kept tractable by de-composing TE capabilities into two steps.The first step, Expanded Lexical Retrieval(ELR), uses shallow semantic resources and simi-larity measures, thereby incorporating some of thesemantic processing used in typical TE systems.This is required to retrieve, with high recall, se-mantically similar content that may not be lexi-cally similar to query terms, to ensure return ofa set of texts that are highly likely to contain theconcept of interest.The second step applies a textual entailmentsystem to this text set and the query in order tolabel the texts as ?relevant?
or ?irrelevant?, and re-quires deeper semantic resources in order to dis-cern texts containing the concept of interest fromthose that do not.
This step emphasizes higher pre-cision, as it filters irrelevant texts.3.1 Implementation of SERRIn the ELR stage, we use a structured query thatallows more precise search and differential queryexpansion for each query element.
Semantic unitsin the texts (e.g.
Named Entities, phrasal verbs)are indexed separately from words; each index is58SERR AlgorithmSETUP:Input: Text set DOutput: Indices {I} over Dfor all texts d ?
DAnnotate d with local semantic contentBuild Search Indices {I} over DAPPLICATION:Input: Information need SEXPANDED LEXICAL RETRIEVAL (ELR)(s):R?
?Expand s with semantically similar wordsBuild search query qsfrom sR?
k top-ranked texts for qsusing indices {I}return RSERR:Answer set A?
?for all queries s ?
SR?
ELR(s)Answer set As?
?for all results r ?
RAnnotate s, r with NLP resourcesif r entails sAs?
As?
rA?
A ?
{As}return AFigure 1.
SERR algorithma hierarchical similarity structure based on a type-specific metric (e.g.
WordNet-based for phrasalverbs).
Query structure is also used to selectivelyexpand query terms using similarity measures re-lated to types of semantic units, including distribu-tional similarity (Lin and Pantel, 2001), and mea-sures based on WordNet (Fellbaum, 1998).We assess three different Textual Entailmentcomponents: LexPlus, a lexical-level systemthat achieves relatively good performance on theRTE challenges, and two variants of Predicate-based Textual Entailment, PTE-strict and PTE-relaxed, which use a predicate-argument repre-sentation.
The former is constrained to select asingle predicate-argument structure from each re-sult, which is compared to the query component-by-component using similarity measures similar tothe LexPlus system.
PTE-relaxed drops the single-predicate constraint, and can be thought of as a?bag-of-constituents?
model.
In both, features areextracted based on the predicate-argument compo-nents?
match scores and their connecting structure,and the rank assigned by ELR.
These features areused by a classifier that labels each result as ?rel-evant?
or ?irrelevant?.
Training examples are se-lected from the top 7 results returned by ELR forqueries corresponding to entailment pair hypothe-ses from the RTE development corpora; test exam-ples are similarly selected from results for queriesfrom the RTE test corpora (see section 3.2).3.2 Entailed Relation Recognition CorpusTo assess performance on the ERR task, we de-rive a corpus from the publicly available RTEdata.
The corpus consists of a set S of informa-tion needs in the form of [argument, relation, argu-ment] triples, and a set D of text spans (short para-graphs), half of which entail one or more s ?
Swhile the other half are unrelated to S. D com-prises all 1, 950 Texts from the IE and IR sub-tasks of the RTE Challenge 1?3 datasets.
Theshorter hypotheses in these examples allow us toautomatically induce their structured query formfrom their shallow semantic structure.
S was au-tomatically generated from the positive entailmentpairs in D, by annotating their hypotheses with apublicly available SRL tagger (Punyakanok et al,2008) and inferring the relation and two main ar-guments to form the equivalent queries.Since some Hypotheses and Texts appear mul-tiple times in the RTE corpora, we automaticallyextract mappings from positive Hypotheses to oneor more Texts by comparing hypotheses and textsfrom different examples.
This provides the label-ing needed for evaluation.
In the resulting corpus,a wide range of relations are sparsely represented;they exemplify many linguistic and semantic char-acteristics required to infer the presence of non-explicit relations.4 Results and DiscussionTop # Basic ELR Rel.Impr.
Err.Redu.1 48.1% 55.2% +14.8% 13.7%2 68.1% 72.8% +6.9% 14.7%3 75.2% 78.5% +4.4% 17.7%Table 1.
Change in relevant results retrieved in top 3positions for basic and expanded lexical retrievalSystem Acc.
Prec.
Rec.
F1Baseline 18.1 18.1 100.0 30.7LexPlus 81.6 44.9 62.5 55.5PTE-relax.
71.9 37.7 72.0 49.0(0.1) (5.5) (6.2) (4.1)PTE-strict 83.6 55.4 61.5 57.9(1.3) (3.4) (7.9) (2.1)Table 2.
Comparison of performance of SERR withdifferent TE algorithms.
Numbers in parentheses arestandard deviations.Table 1 compares the results of SERR with and59# System RTE 1 RTE 2 RTE 3 Avg.
Acc.LexPlus 49.0 65.2 [3] 76.5 [2] 66.3PTE-relaxed 54.5 (1.0) 68.7 (1.5) [3] 82.3 (2.0) [1] 71.2 (1.2)PTE-strict 64.8 (2.3) [1] 71.2 (2.6) [3] 76.0 (3.2) [2] 71.8 (2.6)Table 3.
Performance (accuracy) of SERR system variants on RTE challengeexamples; numbers in parentheses are standard deviations, while numbers inbrackets indicate where systems would have ranked in the RTE evaluations.ComparisonsStandard TE 3,802,500SERR 13,650Table 4.
Entailment compar-isons needed for standard TEvs.
SERRwithout the ELR?s semantic enhancements.
Foreach rank k, the entries represent the proportion ofqueries for which the correct answer was returnedin the top k positions.
The semantic enhancementsimprove the number of matched results at each ofthe top 3 positions.Table 2 compares variants of the SERR imple-mentation.
The baseline labels every result re-turned by ELR as ?relevant?, giving high recallbut low precision.
PTE-relaxed performs betterthan baseline, but poorly compared to PTE-strictand LexPlus.
Our analysis shows that LexPlushas a relatively high threshold, and correctly labelsas negative some examples mislabeled by PTE-relaxed, which may match two of the three con-stituents in a hypothesis and label that result aspositive.
PTE-strict will correctly identify somesuch examples as it will force some match edges tobe ignored, and will correctly identify some neg-ative examples due to structural constraints evenwhen LexPlus finds matches for all query terms.PTE-strict strikes the best balance between preci-sion and recall on positive examples.Table 3 shows the accuracy of SERR?s clas-sification of the examples from each RTE chal-lenge; results not returned in the top 7 ranks byELR are labeled ?irrelevant?.
PTE-strict and PTE-relaxed perform comparably overall, though PTE-strict has more uniform results over the differentchallenges.
Both outperform the LexPlus systemoverall, and perform well compared to the best re-sults published for the RTE challenges.The significant computational gain of SERR isshown in Table 4, exhibiting the much greaternumber of comparisons required by a brute forceTE approach compared to SERR: SERR performswell compared to published results for RTE chal-lenges 1-3, but makes only 0.36% of the TE com-parisons needed by standard approaches on ourERR task.5 ConclusionWe have proposed an approach to solving the En-tailed Relation Recognition task, based on Tex-tual Entailment, and implemented a solution thatshows that a Textual Entailment Recognition sys-tem can be scaled to a much larger IE problemthan that represented by the RTE challenges.
Ourpreliminary results demonstrate the utility of theproposed architecture, which allows strong perfor-mance in the RTE task and efficient application toa large corpus (table 4).AcknowledgmentsWe thank Quang Do, Yuancheng Tu, and KevinSmall.
This work is funded by a grant from Boeingand by MIAS, a DHS-IDS Center for MultimodalInformation Access and Synthesis at UIUC.References[Banko and Etzioni2008] M. Banko and O. Etzioni.
2008.The Tradeoffs Between Open and Traditional Relation Ex-traction.
In ACL-HLT, pages 28?36.
[Culotta and Sorensen2004] A. Culotta and J. Sorensen.2004.
Dependency Tree Kernels for Relation Extraction.In ACL, pages 423?429.
[Dagan et al2006] I. Dagan, O. Glickman, and B. Magnini,editors.
2006.
The PASCAL Recognising Textual Entail-ment Challenge., volume 3944.
Springer-Verlag, Berlin.
[Fellbaum1998] C. Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.
[Harabagiu and Hickl2006] S. Harabagiu and A. Hickl.
2006.Methods for Using Textual Entailment in Open-DomainQuestion Answering.
In ACL, pages 905?912.
[Hickl and Bensley2007] A. Hickl and J. Bensley.
2007.
ADiscourse Commitment-Based Framework for Recogniz-ing Textual Entailment.
In ACL, pages 171?176.
[Lin and Pantel2001] D. Lin and P. Pantel.
2001.
Induction ofsemantic classes from natural language text.
In SIGKDD,pages 317?322.
[Punyakanok et al2008] V. Punyakanok, D. Roth, andW.
Yih.
2008.
The Importance of Syntactic Parsing andInference in Semantic Role Labeling.
CL, 34(2).
[Rocchio1971] J. Rocchio, 1971.
Relevance feedback in In-formation Retrieval, pages 313?323.
Prentice Hall.
[Roth and Yih2004] D. Roth and W. Yih.
2004.
A linear pro-gramming formulation for global inference in natural lan-guage tasks.
In CoNLL, pages 1?8.
[Sekine2006] S. Sekine.
2006.
On-Demand Information Ex-traction.
In COLING/ACL, pages 731?738.60
