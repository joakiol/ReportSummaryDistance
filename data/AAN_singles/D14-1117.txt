Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104?1114,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsKnowledge Graph and Corpus Driven Segmentation andAnswer Inference for Telegraphic Entity-seeking QueriesMandar Joshi?IBM Researchmandarj90@in.ibm.comUma SawantIIT Bombay, Yahoo Labsuma@cse.iitb.ac.inSoumen ChakrabartiIIT Bombaysoumen@cse.iitb.ac.inAbstractMuch recent work focuses on formal in-terpretation of natural question utterances,with the goal of executing the resultingstructured queries on knowledge graphs(KGs) such as Freebase.
Here we addresstwo limitations of this approach when ap-plied to open-domain, entity-oriented Webqueries.
First, Web queries are rarely well-formed questions.
They are ?telegraphic?,with missing verbs, prepositions, clauses,case and phrase clues.
Second, the KG isalways incomplete, unable to directly an-swer many queries.
We propose a noveltechnique to segment a telegraphic queryand assign a coarse-grained purpose toeach segment: a base entity e1, a rela-tion type r, a target entity type t2, andcontextual words s. The query seeks en-tity e2?
t2where r(e1, e2) holds, fur-ther evidenced by schema-agnostic wordss.
Query segmentation is integrated withthe KG and an unstructured corpus wherementions of entities have been linked tothe KG.
We do not trust the best or anyspecific query segmentation.
Instead, evi-dence in favor of candidate e2s are aggre-gated across several segmentations.
Ex-tensive experiments on the ClueWeb cor-pus and parts of Freebase as our KG, us-ing over a thousand telegraphic queriesadapted from TREC, INEX, and Web-Questions, show the efficacy of our ap-proach.
For one benchmark, MAP im-proves from 0.2?0.29 (competitive base-lines) to 0.42 (our system).
NDCG@10improves from 0.29?0.36 to 0.54.?Work done as Masters student at IIT Bombay1 IntroductionA majority of Web queries mention an entity ortype (Lin et al., 2012), as users increasingly ex-plore the Web of objects using Web search.
Tobetter support entity-oriented queries, commercialWeb search engines are rapidly building up largecatalogs of types, entities and relations, popu-larly called a ?knowledge graph?
(KG) (Gallagher,2012).
Despite these advances, robust, Web-scale,open-domain, entity-oriented search faces manychallenges.
Here, we focus on two.1.1 ?Telegraphic?
queriesFirst, the surface utterances of entity-oriented Webqueries are dramatically different from TREC-or Watson-style factoid question answering (QA),where questions are grammatically well-formed.Web queries are usually ?telegraphic?
: they areshort, rarely use function words, punctuationsor clausal structure, and use relatively flexibleword orders.
E.g., the natural utterance ?on thebank of which river is the Hermitage Museum lo-cated?
may be translated to the telegraphic Webquery hermitage museum river bank.
Evenon well-formed question utterances, 50% of in-terpretation failures are contributed by parsing orstructural matching failures (Kwiatkowski et al.,2013).
Telegraphic utterances will generally beeven more challenging.Consequently, whereas TREC-QA/NLP-styleresearch has focused on parsing and precise in-terpretation of a well-formed query sentence toa strongly structured (typically graph-oriented)query language (Kasneci et al., 2008; Pound etal., 2012; Yahya et al., 2012; Berant et al., 2013;Kwiatkowski et al., 2013), the Web search and in-formation retrieval (IR) community has focusedon telegraphic queries (Guo et al., 2009; Sarkaset al., 2010; Li et al., 2011; Pantel et al., 2012; Linet al., 2012; Sawant and Chakrabarti, 2013).
Interms of target schema richness, these efforts may1104appear more modest.
The act of query ?interpre-tation?
is mainly a segmentation of query tokensby purpose.
In the example above, one may re-port segments ?Hermitage Museum?
(a located ar-tifact or named entity), and ?river bank?
(the targettype).
This is reminiscent of record segmentationin information extraction (IE).
Over well-formedutterances, IE baselines are quite competitive (Yaoand Van Durme, 2014).
But here, we are interestedexclusively in telegraphic queries.1.2 Incomplete knowledge graphThe second problem is that the KG is alwayswork in progress (Pereira, 2013), and connec-tions found within nodes of the KG, between theKG and the query, or the KG and unstructuredtext, are often incomplete or erroneous.
E.g.,Wikipedia is considered tiny, and Freebase rathersmall, compared to what is needed to answer allbut the ?head?
queries.
Google?s Freebase an-notations (Gabrilovich et al., 2013) on ClueWeb(ClueWeb09, 2009) number fewer than 15 perpage to ensure precision.
Fewer than 2% are toentities in Freebase but not in Wikipedia.It may also be difficult to harness the KG foranswering certain queries.
E.g., answering thequery fastest odi century batsman, the intent ofwhich is to find the batsman holding the record forthe fastest century in One Day International (ODI)cricket, may be too difficult for most KG-only sys-tems, but may be answered quite effectively by asystem that also utilizes evidence from unstruc-tured text.There is a clear need for a ?pay-as-you-go?
ar-chitecture that involves both the corpus and KG.
Aquery easily served by a curated KG should giveaccurate results, but it is desirable to have a grace-ful interpolation supported by the corpus: e.g., ifthe relation r(e1, e2) is not directly evidenced inthe KG, but strongly hinted in the corpus, we stillwant to use this for ranking.1.3 Our contributionsHere, we make progress beyond the above frontierof prior work in the following significant ways.We present a new architecture for structural in-terpretation of a telegraphic query into these seg-ments (some may be empty):?
Mention/s e?1of an entity e1,?
Mention r?
of a relation type r,?
Mention?t2of a target type t2, and?
Other contextual matching words s (some-times called selectors),with the simultaneous intent of finding and rank-ing entities e2?
t2, such that r(e1, e2) is likelyto hold, evidenced near the matching words in un-structured text.Given the short, telegraphic query utterances,we limit our scope to at most one relation mention,unlike the complex mapping of clauses in well-formed questions to twig and join style queries(e.g., ?find an actor whose spouse was an Italianbookwriter?).
On the other hand, we need to dealwith the unhelpful input, as well as consolidatethe KG with the corpus for ranking candidate e2s.Despite the modest specification, our query tem-plate is quite expressive, covering a wide range ofentity-oriented queries (Yih et al., 2014).We present a novel discriminative graphicalmodel to capture the entity ranking inference task,with query segmentation as a by-product.
Ex-tensive experiments with over a thousand entity-seeking telegraphic queries using the ClueWeb09corpus and a subset of Freebase show that we canaccurately predict the segmentation and intent oftelegraphic relational queries, and simultaneouslyrank candidate responses with high accuracy.
Wealso present evidence that the KG and corpus havesynergistic salutary effects on accuracy.
?2 explores related work in more detail.
?3gives some examples fitting our query template,explains why interpreting some of them is nontriv-ial, and sets up notation.
?4 presents our core tech-nical contributions.
?5 presents experiments.
Datacan be accessed at http://bit.ly/Spva49and http://bit.ly/WSpxvr.2 Related workThe NLP/QA community has traditionally as-sumed that question utterances are grammaticallywell-formed, from which precise clause structure,ground constants, variables, and connective rela-tions can be inferred via semantic parsing (Kas-neci et al., 2008; Pound et al., 2012; Yahya etal., 2012; Berant et al., 2013; Kwiatkowski etal., 2013) and translated to lambda expressions(Liang, 2013) or SPARQL style queries (Kasneciet al., 2008), with elaborate schema knowledge.Such approaches are often correlated with the as-sumption that all usable knowledge has been cu-rated into a KG.
The query is first translated to astructured form and then ?executed?
on the KG.
A1105Telegraphic query e?1r?
t?2sfirst african american nobel prize winnernobel prize winner african american firstnobel prize - winner first african american- - winner first african american nobel prizedave navarro first banddave navarro band - firstdave navarro band band firstmerril lynch headquartersmerril lynch headquarters - -merril lynch - headquarters -spanish poet died in civil warspanish died in poet civil warcivil war died - spanish poetspanish in poet died civil warfirst american in space- - - first american in space- - american first, in spaceFigure 1: Example queries and some potential segmentations.large corpus may be used to build relation expres-sion models (Yao and Van Durme, 2014), but notas supporting evidence for target entities.In contrast, the Web and IR community gener-ally assumes a free-form query that is often tele-graphic (Guo et al., 2009; Sarkas et al., 2010; Liet al., 2011).
Queries being far more noisy, thegoal of structure discovery is more modest, and of-ten takes the form of a segmentation of the queryregarded as a token sequence, assigning a broadpurpose (Pantel et al., 2012; Lin et al., 2012) toeach segment, mapping them probabilistically toa relatively loose schema, and ranking responsesin conjunction with segmentations (Sawant andChakrabarti, 2013).
To maintain quality in the faceof noisy input, these approaches often additionallyexploit clicks (Li et al., 2011) or a corpus that hasbeen annotated with entity mentions (Cheng andChang, 2010; Li et al., 2010).
The corpus providescontextual snippets for queries where the KG fails,preventing the systems from falling off the ?struc-ture cliff?
(Pereira, 2013).Our work advances the capabilities of the lat-ter class of approaches, bringing them closer tothe depth of the former, while handling telegraphicqueries and retaining the advantage of corpus evi-dence over and above the KG.
Very recently, (Yaoet al., 2014) have concluded that for current bench-marks, deep parsing and shallow information ex-traction give comparable interpretation accuracy.The very recent work of (Yih et al., 2014) is simi-lar in spirit to ours, but they do not unify segmen-tation and answer inference, along with corpus ev-idence, like we do.3 Notation and examplesWe use e1, r, t2, e2to represent abstract nodes andedges (MIDs in case of Freebase) from the KG,and e?1, r?,?t2to represent their textual mentions orhints, if any, in the query.
s is a set of uninterpretedtextual tokens in the query that are used to matchand collect corpus contexts that lend evidence tocandidate entities.Figure 1 shows some telegraphic querieswith possible segmentation into the aboveparts.
Consider another example: davenavarro first band.
?Band?
is a hint fortype /music/musical group, so it com-prises?t2.
Dave Navarro is an entity, with men-tion words ?dave navarro?
comprising e?1.
r?
ismade up of ?band?, and represents the relation/music/group member/membership.
Fi-nally, the word first cannot be mapped to any sim-ple KG artifact, so are relegated to s (which makesthe corpus a critical part of answer inference).
Weuse s and s?
interchangeably.Generally, there will be enough noise and uncer-tainty that the search system should try out severalof the most promising segmentations as shown inFigure 1.
The accuracy of any specific segmenta-tion is expected to be low in such adversarial set-tings.
Therefore, support for an answer entity isaggregated over several segmentations.
The ex-pectation is that by considering multiple interpre-tations, the system will choose the entity with bestsupporting evidence from corpus and knowledgebase.4 Our ApproachTelegraphic queries are usually short, so we enu-merate query token spans (with some restrictions,similar to beam search) to propose segmentations(?4.1).
Candidate response entities are lined upfor each interpretation, and then scored in a globalmodel along with query segmentations (?4.2).
?4.3 describes how model parameters are trained.11061: input: query token sequence q2: initialize segmentations I = ?3: E1= (entity, mention) pairs from linker4: for all (e1, e?1) ?
E1do5: assign label E1to mention tokens e?16: for all contiguous span v ?
q \ e?1do7: label each word w ?
v as T2R8: label other words w ?
q \ e?1\ v as S9: add segments (E1, T2R,S) to I10: end for11: end for12: return candidate segmentations IFigure 2: Generating candidate query segmenta-tions.4.1 Generating candidate querysegmentationsEach query token can have four labels,E1, T2, R, S, corresponding to the mentionsof the base entity, target type, connecting relation,and context words.
We found that segmentshinting at T2and R frequently overlapped(e.g., ?author?
in the query zhivago author).In our implementation, we simplified to threelabels, E1, T2R,S, where tokens labeled T2Rare involved with both t2and r, the proposedstructured target type and connecting relation.Another reasonable assumption was that the baseentity mention and type/relation mentions arecontiguous token spans, whereas context wordscan be scattered in multiple segments.Figure 2 shows how candidate segmentationsare generated.
For step 3, we use TagMe (Ferrag-ina and Scaiella, 2010), an entity linker backed byan entity gazette derived from our KG.4.2 Graphical modelBased on the previous discussion, we assume thatan entity-seeking query q is a sequence of tokensq1, q2, .
.
., and this can be partitioned into differentkinds of subsequences, corresponding to e1, r, t2and s, and denoted by a structured (vector) label-ing z = z1, z2, .
.
..
Given sequences q and z, wecan separate out (possibly empty) token segmentse?1(q, z),?t2(q, z), r?
(q, z), and s?
(q, z).A query segmentation z becomes plausible inconjunction with proposals for e1, r, t2and e2from the KG.
The probability Pr(z, e1, r, t2, e2|q)is modeled as proportional to the product of sev-eral potentials (Koller and Friedman, 2009) in agraphical model.
In subsequent subsections, wewill present the design of specific potentials.?
?R(q, z, r) denotes the compatibility be-tween the relation hint segment r?
(q, z) anda proposed relation type r in the KG (?4.2.1).?
?T2(q, z, t2) denotes the compatibility be-tween the type hint segment?t2(q, z) and aproposed target entity type t2in the KG(?4.2.2).?
?E1,R,E2,S(q, z, e1, r, e2) is a novel corpus-based evidence potential that measures howstrongly e1and e2appear in corpus snippetsin the proximity of words in s?
(q, z), and ap-parently related by relation type r (?4.2.3).?
?E1(q, z, e1) denotes the compatibility be-tween the query segment e?1(q, z) and entitye1that it purportedly mentions (?4.2.4).?
?S(q, z) denotes selector compatibility.
Se-lectors are a fallback label, so this is pinnedarbitrarily to 1; other potentials are balancedagainst this base value.?
?E1,R,E2(e1, r, e2) is A if the relationr(e1, e2) exists in the KG, and is B > 0 oth-erwise, for tuned/learnt constants A > B >0.
Note that this is a soft constraint (B > 0);if the KG is incomplete, the corpus may beable to supplement the required information.?
?E2,T2(e2, t2) is 1 if e2belongs to t2andzero otherwise.
In other words, candidate e2smust be proposed to be instances of the pro-posed t2?
this is a hard constraint, but canbe softened if desired, like ?E1,R,E2.Figure 3 shows the relevant variable states ascircled nodes, and the potentials as square factornodes.
To rank candidate entities e2, we pin thenode E2to each entity in turn.
With E2pinned,we perform a MAP inference over all other hiddenvariables and note the score of e2as the product ofthe above potentials maximized over choices of allother variables: score(e2) =maxz,t2,r,e1?T2(q, z, t2)?R(q, z, r)?E1(q, z, e1)?S(q, z)?E2,T2(e2, t2)?E1,R,E2(e1, r, e2)?E1,R,E2,S(q, z, e1, r, e2).
(1)We rank candidate e2s by decreasing score, whichis estimated by max-product message-passing(Koller and Friedman, 2009).As noted earlier, any of the relation/type, orquery entity partitions may be empty.
To handle1107TargettypeConnectingrelationQueryentity SelectorsTypelanguagemodelRelationlanguagemodelEntitylanguagemodelSegmentationCandidateentityCorpus-assistedentity-relationevidence potentialFigure 3: Graphical model for query segmentation and entity scoring.
Factors/potentials are shown assquares.
A candidate e2is observed and scored using equation (1).
Query q is also observed but notshown to reduce clutter; most potentials depend on it.this case, we allow each of the entity, relation ortarget type nodes in the graphical to take the value?
or ?null?.
To support this, the value of the fac-tor between the query segmentation node Z and?E1(q, z, e1), ?T2(q, z, t2), and ?R(q, z, r)) areset to suitable low values.Next, we will describe the detailed design ofsome of the key potentials introduced above.4.2.1 Relation language model for ?RPotential ?R(q, z, r) captures the compatibilitybetween r?
(q, z) and the proposed relation r.E.g., if the query is steve jobs death rea-son, and r?
is (correctly chosen as) death rea-son, then the correct candidate r is /people/deceased_person/cause_of_death.
Anincorrect r is /people/deceased_person/place_of_death.
An incorrect z may lead tor?
(q, z) being jobs death.Using corpus: Considerable variation may existin how r is represented textually in a query.
Therelation language model needs to build a bridgebetween the formal r and the textual r?, so that(un)likely r?s have (small) large potential.
Manyapproaches (Berant et al., 2013; Berant and Liang,2014; Kwiatkowski et al., 2013; Yih et al., 2014)to this problem have been intensely studied re-cently.
Given our need to process billions of Webpages efficiently, we chose a pattern-based ap-proach (Nakashole et al., 2012): with each r, dis-cover the most strongly associated phrase patternsfrom a reference corpus, then mark these patternsinto much larger payload corpus.We started with the 2000 (out of approximately14000) most frequent relation types in Freebase,and the ClueWeb09 corpus annotated with Free-base entities (Gabrilovich et al., 2013).
For eachtriple instance of each relation type, we located allcorpus sentences that mentioned both participat-ing entities.
We made the crude assumption thatif r(e1, e2) holds and e1, e2co-occur in a sen-tence then this sentence is evidence of the rela-tionship.
Each such sentence is parsed to obtaina dependency graph using the Malt Parser (Hallet al., 2014).
Words in the path connecting theentities are joined together and added to a candi-date phrase dictionary, provided the path is at mostthree hops.
(Inspection suggested that longer de-pendency paths mostly arise out of noisy sentencesor botched parses.)
30% of the sentences werethus retained.
Finally, we defined?R(q, z, r) =n(r, r?
(q, z))?p?n(r, p?
), (2)where p?ranges over all phrases that are known tohint at r, and n(r, p) denotes the number of sen-tences where the phrase p occurred in the depen-dency path between the entities participating in re-lation r.Assuming entity co-occurrence implies evi-dence is admittedly simplistic.
However, the pri-mary function of the relation model is to retrievetop-k relations that are compatible with the type/s1108of e1and the given relation hint.
Moreover, theremaining noise is further mitigated by the collec-tive scoring in the graphical model.
While we maymiss relations if they are expressed in the querythrough obscure hints, allowing the relation to be?
acts as a safety net.Using Freebase relation names: As mentionedearlier, queries may express relations differently ascompared to the corpus.
A relation model basedsolely on corpus annotations may not be able tobridge that gap effectively, particularly so, becauseof sparsity of corpus annotations or the rarity ofFreebase triples in ClueWeb.
E.g., for the Freebaserelation /people/person/profession, wefound very few annotated sentences.
One wayto address this problem is to utilize relation typenames in Freebase to map hints to relation types.Thus, in addition to the corpus-derived relationmodel, we also built a language model that usedFreebase relation type names as lemmas.
E.g., theword ?profession?
would contribute to the relationtype /people/person/profession.Our relation models are admittedly simple.
Thisis mainly because telegraphic queries may ex-press relations very differently from natural lan-guage text.
As it is difficult to ensure precision ofquery interpretation stage, our models are gearedtowards recall.
The system generates a large num-ber of interpretations and relies on signals fromthe corpus and KG to bring forth correct interpre-tations.4.2.2 Type language model for ?T2Similar to the relation language model, we needa type language model to measure compatibil-ity between t2and?t2(q, z).
Estimating the tar-get entity type, without over-generalizing or over-specifying it, has always been important for QA.E.g., when?t2is ?city?, a good type language modelshould prefer t2as /location/citytownover /location/location while avoiding/location/es_autonomous_city.A catalog like Freebase suggests a straight-forward method to collect a type language model.Each type is described by one or more phrasesthrough the link /common/topic/alias.
Wecan collect these into a micro-?document?
anduse a standard Dirichlet-smoothed language modelfrom IR (Zhai, 2008).
In Freebase, an entitynode (e.g., Einstein, /m/0jcx) may be linkedto a type node (e.g.
/base/scientist/physicist) using an edge with label /type/object/type.But relation types provide additional clues totypes of the endpoint entities.
Freebase relationtypes have the form /x/y/z, where x is thedomain of the relation, and y and z are stringrepresentations of the type of the entities partic-ipating in the relation.
E.g., the (directed) re-lation type /location/country/capitalconnects from from /location/country to/location/citytown.
Therefore, ?capital?can be added to the set of descriptive phrases ofentity type /location/citytown.It is important to note that while we use Free-base link nomenclature for relation and type lan-guage models, our models are not incompati-ble with other catalogs.
Indeed, most catalogshave established ways of deriving language mod-els that describe their various structures.
For ex-ample, most YAGO types are derived from Word-Net synsets with associated phrasal descriptions(lemmas).
YAGO relations also have readablenames such as actedIn, isMarriedTo, etc.
whichcan be used to estimate language models.
DB-Pedia relations are mostly derived from (mean-ingfully) named attributes taken from infoboxes,hence they can be used directly.
Furthermore, oth-ers (Wu and Weld, 2007) have shown how to asso-ciate language models with such relations.4.2.3 Snippet scoringThe factor ?E1,R,E2,S(q, z, e1, r, e2) should belarge if many snippets contain a mention of e1ande2, relation r, and many high-signal words from s.Recall that we begin with a corpus annotated withentity mentions.
Our corpus is not directly anno-tated with relation mentions.
Therefore, we getfrom relations to documents via high-confidencephrases.
Snippets are retrieved using a combinedentity + word index, and scored for a given e1, r,e2, and selectors s?
(q, z).Given that relation phrases may be noisy andthat their occurrence in the snippet may not nec-essarily mean that the given relation is being ex-pressed, we need a scoring function that is cog-nizant of the roles of relation phrases and enti-ties occurring in the snippets.
In a basic ver-sion, e1, p, e2, s?
are used to probe a combined en-tity+word index to collect high scoring snippets,with the score being adapted from BM25.
The sec-ond, refined scoring function used a RankSVM-1109style (Joachims, 2002) optimization.min?,???
?2+ C?e+,e??e+,e?
s.t.
?e+, e?
: ?
?
f(q,De+ , e+) + ?e+,e?
(3)?
?
?
f(q,De?
, e?)
+ 1.where e+and e?are positive and negative enti-ties for the query q and f(q,De, e) represents thefeature map for the set of snippets Debelongingto entity e. The assumption here is that all snip-pets containing e+are ?positive?
snippets for thequery.
f consolidates various signals like the num-ber of snippets where e occurs near query entitye1and a relation phrase, or the number of snippetswith high proportion of query IDF, hinting that eis a positive entity for the given query.
A partiallist of features used for snippet scoring is given inFigure 4.Number of snippets with distance(e2, e?1) < k1(k1= 5, 10)Number of snippets with distance(e2, relation phrase) < k2(k2= 3, 6)Number of snippets with relation r = ?Number of snippets with relation phrases as prepositionsNumber of snippets covering fraction of query IDF > k3(k3= 0.2, 0.4, 0.6, 0.8)Figure 4: Sample features used for learningweights ?
to score snippets.4.2.4 Query entity modelPotential ?E1(q, z, e1) captures the compatibil-ity between e?1(q, z) (i.e., the words that mentione1) and the claimed entity e1mentioned in thequery.
We used the TagMe entity linker (Fer-ragina and Scaiella, 2010) for annotating enti-ties in queries.
TagMe annotates the query withWikipedia entities, which we map to Freebase, anduse the annotation confidence scores as the poten-tial ?E1(q, z, e1).4.3 Discriminative parameter training withlatent variablesWe first set the potentials in (1) as explained in?4.2 (henceforth called ?Unoptimized?
), and gotencouraging accuracy.
Then we rewrote each po-tential as??(?
?
? )
= exp(w??
??(?
?
?
))(4)or log????(?
?
? )
=??w??
??(?
?
?
),with w?being a weight vector for a specific poten-tial ?, and ?
?being a corresponding feature vector.During inference, we seek to maximizemaxq,z,e1,t2,rw ?
?
(q, z, e1, t2, r, e2), (5)for a fixed w, to find the score of each candidateentity e2.
Here all w?and ?
?have been collectedinto unified weight and feature vectors w, ?.
Dur-ing training of w, we are given pairs of correct andincorrect answer entities e+2, e?2, and we wish tosatisfy constraints of the formmaxq,z,e1,t2,rw ?
?
(q, z, e1, t2, r, e+2) + ?
(6)?
1 + maxq,z,e1,t2,rw ?
?
(q, z, e1, t2, r, e?2),because collecting e+2, e?2pairs is less work thansupervising with values of z, e1, t2, r, e2for eachquery.
Similar distant supervision problems wereposed via bundle method by (Bergeron et al.,2008), and (Yu and Joachims, 2009), who usedCCCP (Yuille and Rangarajan, 2006).
These areequivalent in our setting.
We use the CCCP style,and augment the objective with an additional en-tropy term as in (Sawant and Chakrabarti, 2013).We call this LVDT (latent variable discriminativetraining) in ?5.5 Experiments5.1 TestbedCorpus and knowledge graph: We used theClueWeb09B (ClueWeb09, 2009) corpus contain-ing 50 million Web documents.
This corpuswas annotated by Google with Freebase enti-ties (Gabrilovich et al., 2013).
The average pagecontains 15 entity annotations from Freebase.
Weused the Freebase KG and its links to Wikipedia.Queries: We report on two sets of entity-seekingqueries.
A sample of about 800 well-formedqueries from WebQuestions (Berant et al., 2013)were converted to telegraphic utterances (such aswould be typed into commercial search engines)by volunteers familiar with Web search.
We callthis WQT (WebQuestions, telegraphic).
Queriesare accompanied by ground truth entities.
Thesecond data set, TREC-INEX, from (Sawant andChakrabarti, 2013) has about 700 queries sam-pled from TREC and INEX, available at http://bit.ly/WSpxvr.
These come with well-formed and telegraphic utterances, as well asground truth entities.1110There are some notable differences betweenthese query sets.
For WQT, queries were gener-ated by using Google?s query suggestions inter-face.
Volunteers were asked to find answers usingsingle Freebase pages.
Therefore, by construction,queries retained can be answered using the Free-base KG alone, with a simple r(e1, ?)
form.
Incontrast, TREC-INEX queries provide a balancedmix of t2and r hints in the queries, and direct an-swers from triples is relatively less available.5.2 Implementation detailsOn an average, the pseudocode in Figure 2generated 13 segmentations per query, withlonger queries generating more segmentationsthan shorter ones.We used an MG4J (Boldi and Vigna, 2005)based query processor, written in Java, over en-tity and word indices on ClueWeb09B.
The in-dex supplies snippets with a specified maximumwidth, containing a mention of some entity andsatisfying a WAND (Broder et al., 2003) predi-cate over words in s?.
In case of phrases in thequery, the WAND threshold was computed byadding the IDF of constituent words.
The indexreturned about 330,000 snippets on average forWAND threshold of 0.6.We retained the top 200 candidate entities fromthe corpus; increasing this horizon did not givebenefits.
We also considered as candidates for e2those entities that are adjacent to e1in the KGvia top-scoring r candidates.
In order to gener-ate supporting snippets for an interpretation con-taining entity annotation e, we need to match ewith Google?s corpus annotations.
However, re-lying solely on corpus annotations fails to retrievemany potential evidence snippets, because entityannotations are sparse.
Therefore we probed thetoken index with the textual mention of e1in thequery; this improved recall.We also investigated the feasibility of our pro-posals for interactive search.
There are three majorprocesses involved in answering a query - gener-ating potential interpretations, collecting/scoringsnippets, and inference (MAP for Unoptimizedand w?(?)
for LVDT).
For the WQT dataset, av-erage time per query for each stage was approx-imately - 0.2, 16.6 and 1.3 seconds respectively.Our (Java) code did not optimize the bottleneckat all; only 10 hosts and no clever load balancingwere used.
We believe commercial search enginescan cut this down to less than a second.5.3 Research questionsIn the rest of this section we will address thesequestions:?
For telegraphic queries, is our entity-relation-type-selector segmentation better than thetype-selector segmentation of (Sawant andChakrabarti, 2013)??
When semantic parsers (Berant et al., 2013;Kwiatkowski et al., 2013) are subjected totelegraphic queries, how do they performcompared to our proposal??
Are the KG and corpus really complementaryas regards their support of accurate ranking ofcandidate entities??
Is the prediction of r and t2from our ap-proach better than a greedy assignment basedon local language models?We also discuss anecdotes of successes and fail-ures of various systems.5.4 Benefits of relation in addition to typeFigure 5 shows entity-ranking MAP, MRR, andNDCG@10 (n@10) for two data sets and vari-ous systems.
?No interpretation?
is an IR baselinewithout any KG.
Type+selector is our implemen-tation of (Sawant and Chakrabarti, 2013).
Unopti-mized and LVDT both beat ?no interpretation?
and?type+selector?
by wide margins.
(Boldface im-plies best performing formulation.)
There are twonotable differences between S&C and our work.First, S&C do not use the knowledge graph (KG)and rely on a noisy corpus.
This means S&C failsto answer queries whose answers are found onlyin KG.
This can be seen from WQT results; theyperform only slightly better than the baseline.
Sec-ond, even for queries that can be answered throughthe corpus alone, S&C miss out on two importantsignals that the query may provide - namely thequery entity and the relation.
Our framework notonly provides a way to use a curated and high pre-cision knowledge graph but also attempts to pro-vide more reachability to corpus by the use of re-lational phrases.In case of TREC-INEX, LVDT improves uponthe unoptimized graphical model, where for WQT,it does not.
Preliminary inspection suggests this isbecause WQT has noisy and incomplete groundtruth, and LVDT trains to the noise; a non-convex1111Dataset Formulation map mrr n@10No interpretation .205 .215 .292TREC Type+selector .292 .306 .356-INEX Unoptimized .409 .419 .502LVDT .419 .436 .541No interpretation .080 .095 .131WQT Type+selector .116 .152 .201Unoptimized .377 .401 .474LVDT .295 .323 .406Figure 5: ?Entity-relation-type-selector?
segmen-tation yields better accuracy than ?type-selector?segmentation.objective makes matters worse.
The bias in ourunoptimized model circumvents training noise.5.5 Comparison with semantic parsersFor TREC-INEX, both unoptimized and LVDTbeat SEMPRE (Berant et al., 2013) convinc-ingly, whether it is trained with Free917 or Web-Questions (Figure 6).SEMPRE?s relatively poor performance, in thiscase, is explained by its complete reliance on theknowledge graph.
As discussed previously, theTREC-INEX dataset contains a sizable proportionof queries that may be difficult to answer usinga KG alone.
When SEMPRE is compared withour systems with a telegraphic sample of Web-Questions (WQT), results are mixed.
Our Unop-timized model still compares favorably to SEM-PRE, but with slimmer gains.
As before, LVDTfalls behind.Dataset Formulation map mrr n@10SEMPRE(Free917) .154 .159 .186TREC SEMPRE(WQ) .197 .208 .247-INEX Unoptimized .409 .419 .502LVDT .419 .436 .541SEMPRE(Free917) .229 .255 .285WQT SEMPRE(WQ) .374 .406 .449Unoptimized .377 .401 .474Jacana .239 .256 .329LVDT .295 .323 .406Figure 6: Comparison with semantic parsers.Our smaller gains over SEMPRE in case ofWebQuestions is explained by how WebQuestionswas assembled (Berant et al., 2013).
AlthoughGoogle?s query suggestions gave an eclectic pool,only those queries survived that could be answeredusing a single Freebase page, which effectively re-duced the role of a corpus.
In fact, a large frac-tion of WQT queries cannot be answered well us-ing the corpus alone, because FACC1 annotationsare too sparse and rarely cover common nouns andphrases such as ?democracy?
or ?drug overdose?which are needed for some WQT queries.For WQT, our system also compares favorablywith Jacana (Yao and Van Durme, 2014).
Giventhat they subject their input to natural langaugeparsing, their relatively poor performance is notunsurprsing.5.6 Complementary benefits of KG & corpusFigure 7 shows the synergy between the corpusand the KG.
In all cases and for all metrics, usingthe corpus and KG together gives superior perfor-mance to using any of them alone.
However, itis instructive that in case of TREC-INEX, corpus-only is better than KG-only, whereas this is re-versed for WQT, which also supports the aboveargument.Data Formulation map mrr n@10TREC-INEXUnoptimized (KG) .201 .209 .241Unoptimized (Corpus) .381 .388 .471Unoptimized (Both) .409 .419 .502LVDT (KG only) .255 .264 .293LVDT (Corpus) .267 .272 .315LVDT (Both) .419 .436 .541WQTUnoptimized (KG) .329 .343 .394Unoptimized (Corpus) .188 .228 .291Unoptimized (Both) .377 .401 .474LVDT (KG only) .257 .281 .345LVDT (Corpus only) .170 .210 .280LVDT (Both) .295 .323 .406Figure 7: Synergy between KB and corpus.5.7 Collective vs. greedy segmentationTo judge the quality of interpretations, we askedpaid volunteers to annotate queries with an appro-priate relation and type, and compared them withthe interpretations associated with top-ranked en-tities.
Results in Figure 8 indicate that in spiteof noisy relation and type language models, ourformulations produce high quality interpretationsthrough collective inference.Figure 9 demonstrates the benefit of collectiveinference over greedy segmentation followed by1112Formulation Type Relation Type/RelUnoptimized (top 1) 23 49 60Unoptimized (top 5) 29 57 68LVDT (top 1) 25 52 61LVDT (top 5) 33 61 69Figure 8: Fraction of queries (%) with correct in-terpretations of t2, r, and t2or r, on TREC-INEX.evaluation.
Collective inference boosts absoluteMAP by as much as 0.2.Dataset Formulation map mrr n@10Unoptimized (greedy) .343 .347 .432TREC Unoptimized .409 .419 .502-INEX LVDT (greedy) 205 .214 .259LVDT .419 .436 .541Unoptimized (greedy) .246 .271 .335Unoptimized .377 .401 .474WQT LVDT (greedy) .212 .246 .317LVDT .295 .323 .406Figure 9: Collective vs. greedy segmentation5.8 DiscussionCloser scrutiny revealed that collective infer-ence often overcame errors in earlier stagesto produce a correct ranking over answer en-tities.
E.g., for the query automobile com-pany makes spider the entity disambiguationstage fails to identify the car Alfa Romeo Spi-der (/m/08ys39).
However, the interpretationstage recovers from the error and segments thequery with Automobile (/m/0k4j) as the queryentity e1, /organization/organizationand /business/industry/companies astarget type t2and relation r respectively (from therelation/type hint ?company?
), and spider as se-Figure 10: Comparison of various approaches forNDCG at rank 1 to 10, TREC-INEX datasetFigure 11: Comparison of various approaches forNDCG at rank 1 to 10, WQT datasetlector to arrive at the correct answer Alfa Romeo(/m/09c50).
The corpus features also play a cru-cial role for queries which may not be accuratelyrepresented with an appropriate logical formula.For the query meg ryan bookstore movie, thetextual patterns for the relation ActedIn in con-junction with the selector word ?bookstore?
cor-rectly identifies the answer entity You?ve Got Mail(/m/014zwb).We also analyzed samples of queries whereour system did not perform particularly well.We observed that one of the recurring themesof these queries was that their answer enti-ties had very little corpus support, and thetype/relation hint mapped to too many or nocandidate type/relations.
For example, in thequery south africa political system, the rel-evant type/relation hint ?political system?
couldnot be mapped to /government/form_of_government and /location/country/form_of_government respectively.6 Conclusion and future workWe presented a technique to partition telegraphicentity-seeking queries into functional segmentsand to rank answer entities accordingly.
Whileour results are favorable compared to strong priorart, further improvements may result from relax-ing our model to recognize multiple e1s and rs.
Itmay also help to deploy more sophisticated para-phrasing models (Berant and Liang, 2014) or wordembeddings (Yih et al., 2014) for relation hints.It would also be interesting to supplement entity-linked corpora and curated KGs with extractedtriples (Fader et al., 2014).
Another possibility isto apply the ideas presented here to well-formedquestions.1113ReferencesJonathan Berant and Percy Liang.
2014.
Semanticparsing via paraphrasing.
In ACL Conference.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Empirical Methods inNatural Language Processing (EMNLP).Charles Bergeron, Jed Zaretzki, Curt Breneman, andKristin P. Bennett.
2008.
Multiple instance ranking.In ICML, pages 48?55.
ACM.Paolo Boldi and Sebastiano Vigna.
2005.
MG4J atTREC 2005.
In Ellen M. Voorhees and Lori P. Buck-land, editors, TREC, number SP 500-266 in SpecialPublications.
NIST.Andrei Z. Broder, David Carmel, Michael Herscovici,Aya Soffer, and Jason Zien.
2003.
Efficient queryevaluation using a two-level retrieval process.
InCIKM, pages 426?434.
ACM.Tao Cheng and Kevin Chen-Chuan Chang.
2010.
Be-yond pages: supporting efficient, scalable entitysearch with dual-inversion index.
In EDBT.
ACM.ClueWeb09.
2009. http://www.lemurproject.org/clueweb09.php/.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated andextracted knowledge bases.
In SIGKDD Confer-ence.Paolo Ferragina and Ugo Scaiella.
2010.
TAGME:on-the-fly annotation of short text fragments (bywikipedia entities).
CoRR/arXiv, abs/1006.3498.http://arxiv.org/abs/1006.3498.Evgeniy Gabrilovich, Michael Ringgaard, and Amar-nag Subramanya.
2013.
FACC1: Free-base annotation of ClueWeb corpora.
http://lemurproject.org/clueweb12/, June.
Ver-sion 1 (Release date 2013-06-26, Format version 1,Correction level 0).Sean Gallagher.
2012.
How Google and Microsofttaught search to ?understand?
the Web.
ArsTechnicaarticle.
http://goo.gl/NWs0zT.Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li.
2009.Named entity recognition in query.
In SIGIR Con-ference, pages 267?274.
ACM.Johan Hall, Jens Nilsson, and Joakim Nivre.
2014.Maltparser.
http://www.maltparser.org/.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In SIGKDD Conference,pages 133?142.
ACM.Gjergji Kasneci, Fabian M. Suchanek, Georgiana Ifrim,Maya Ramanath, and Gerhard Weikum.
2008.NAGA: Searching and ranking knowledge.
InICDE.
IEEE.Daphne Koller and Nir Friedman.
2009.
ProbabilisticGraphical Models: Principles and Techniques.
MITPress.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, andLuke S. Zettlemoyer.
2013.
Scaling seman-tic parsers with on-the-fly ontology matching.
InEMNLP Conference, pages 1545?1556.Xiaonan Li, Chengkai Li, and Cong Yu.
2010.
Enti-tyEngine: Answering entity-relationship queries us-ing shallow semantics.
In CIKM, October.
(demo).Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, andKuansan Wang.
2011.
Unsupervised query segmen-tation using clickthrough for information retrieval.In SIGIR Conference, pages 285?294.
ACM.Percy Liang.
2013.
Lambda dependency-based compositional semantics.
Technical ReportarXiv:1309.4408, Stanford University.
http://arxiv.org/abs/1309.4408.Thomas Lin, Patrick Pantel, Michael Gamon, AnithaKannan, and Ariel Fuxman.
2012.
Active objects:Actions for entity-centric search.
In WWW Confer-ence, pages 589?598.
ACM.Ndapandula Nakashole, Gerhard Weikum, and FabianSuchanek.
2012.
PATTY: A taxonomy of relationalpatterns with semantic types.
In EMNLP Confer-ence, EMNLP-CoNLL ?12, pages 1135?1145.
ACL.Patrick Pantel, Thomas Lin, and Michael Gamon.2012.
Mining entity types from query logs via userintent modeling.
In ACL Conference, pages 563?571, Jeju Island, Korea, July.Fernando Pereira.
2013.
Meaning in thewild.
Invited talk at EMNLP Conference.http://hum.csse.unimelb.edu.au/emnlp2013/invited-talks.html.Jeffrey Pound, Alexander K. Hudek, Ihab F. Ilyas, andGrant Weddell.
2012.
Interpreting keyword queriesover Web knowledge bases.
In CIKM.Nikos Sarkas, Stelios Paparizos, and PanayiotisTsaparas.
2010.
Structured annotations of Webqueries.
In SIGMOD Conference.Uma Sawant and Soumen Chakrabarti.
2013.
Learn-ing joint query interpretation and response ranking.In WWW Conference, Brazil.Fei Wu and Daniel S Weld.
2007.
Automatically se-mantifying Wikipedia.
In CIKM, pages 41?50.Mohamed Yahya, Klaus Berberich, Shady Elbas-suoni, Maya Ramanath, Volker Tresp, and GerhardWeikum.
2012.
Natural language questions for theWeb of data.
In EMNLP Conference, pages 379?390, Jeju Island, Korea, July.Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with Freebase.
In ACL Conference.
ACL.Xuchen Yao, Jonathan Berant, and Benjamin VanDurme.
2014.
Freebase QA: Information extrac-tion or semantic parsing?
In ACL 2014 Workshopon Semantic Parsing (SP14).Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation questionanswering.
In ACL Conference.
ACL.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural SVMs with latent variables.
InICML, pages 1169?1176.
ACM.A.
L. Yuille and Anand Rangarajan.
2006.
Theconcave-convex procedure.
Neural Computation,15(4):915?936.ChengXiang Zhai.
2008.
Statistical language modelsfor information retrieval: A critical review.
Founda-tions and Trends in Information Retrieval, 2(3):137?213, March.1114
