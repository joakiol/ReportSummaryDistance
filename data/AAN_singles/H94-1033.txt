Pattern Matching in a Linguistically-Motivated Text Understanding SystemDamaris M. Ayuso and the PLUM Research GroupBBN Systems and Technologies70 Fawcett St.Cambridge, MA 02138dayuso@bbn.comABSTRACTAn ongoing debate in text understanding efforts centers on the useof pattern-matching techniques, which some have characterized as"designed to ignore as much text as possible," versus approacheswhich primarily employ rules that are domain-independent a dlinguisticaUy-motivated.
For instance, in the message-processingcommunity, there has been a noticeable pulling back from large-coverage grammars to the point where, in some systems, traditionalmodels of syntax and semantics have been completely replaced bydomain-specific nite-state approximations.In this paper we report on a hybrid approach which uses suchdomain-specific patterns as a supplement to domain-independentgrammar rules, domain-independent semantic rules, and automati-cally hypothesized domain-specific semantic rules.
The surprisingresult, as measured on TIPSTER test data, is that domain-specificpattern matching improved performance, but only slightly, overmore general linguistically-motivated techniques.1.
IntroductionVirtually all systems which participated in the Fifth Message Un-derstanding Conference, MUC-5 \[1\], used finite-state (FS) pattemmatching to some extent.
Two useful tasks that this approach iswell suited for are:1. treating application-specific simple constructions that may notbelong in a general grammar of the language, and2.
detecting constructions which, though grammatical, may befound more reliably using domain-specific patterns.For example, special-purpose FS subgrammars were used widelyto efficiently and reliably recognize quipment ames and companynames.
This illustrates one (1) above.
An illustration of (2) appearsin the complex sentence below:Daio Paper Corp. said it will set up a cardboard factoryin Ho Chi Minh City, Vietnam, jointly with state-runCogido Paper Manufacturing Company.It is easy for any parser to err in not attaching the modifier "jointly"to "set up," and thereby miss the fact that a joint venture is beingreported.
One might argue that the sentence includes a discontigu-ous constituent ("set up ... jointly").
Nevertheless, it is easy towrite a general pattern to deal with the discontiguous constituentcorrectly for this domain.Finite-state parsers perform simple operations, and they are fast.In data-extraction applications, where much of the input can besafely ignored, they provide an easy means to skip text withoutdeep analysis.
Some of the best-performing systems in MUC-5relied heavily on the use of finite-state pattern-matching  crucialsystem components.However, there are several advantages in maintaining broadlinguistically-based coverage of a language, even in a data-extraction task.
First, it allows for well-defined linguistic struc-tures to be recognized and represented in a domain independentway.
This provides a level of linguistic representation that canbe used by other general linguistic omponents such as a domain-independent discourse processor.
In fact, this is a representationallevel which will probably be evaluated in the next Message Un-derstanding Conference, MUC-6.Secondly, general inguistic coverage provides application inde-pendence.
Different applications, uch as data detection (infor-mation retrieval) can use the linguistic representations for variouspurposes.
Achieving a synergistic operation of data-extraction a ddata-detection systems i  one of the key goals of ARPA's TIPSTERPhase II project.Another intuitive advantage is portability.
When porting a systemto a new application, a base level of understanding is achievedvery quickly before having to add domain-specific patterns.
Thisis possible because the bulk of the processing work is done by thedomain-independent rules.BBN's data-extraction system, PLUM \[2\], showed consistentlyhigh-ranking performance in the MUC-3 \[3\], MUC-4 \[4\], andMUC-5 evaluations.
We added two new finite-state pattern-matching modules to PLUM between MUC-4 and MUC-5, expect-ing a substantial payoff in performance.
The surprising result, asmeasured on TIPSTER test data, was that although domain-specificpattern matching improved performance, in the English domains itwas only a slight improvement over more general, linguistically-motivated techniques.In the next section we further discuss the movement towards FSapproximations in the community.
We then describe the role offinite-state pattern-matching  BBN's PLUM system in more de-tail.
Finally we present experiments u ed to measure the resultingeffect in PLUM.2.
A Shift in the CommunityText processing systems participating in the MUC evaluations(most recently, MUC-5) perform linguistic processing to variouslevels.
Some systems may attempt to do a deep level of under-standing whenever possible \[5\], whereas others use more shallow182"skimming" techniques \[6\], focusing only on information of inter-est and ignoring all other text.
Similarly, systems span the spectrumin their use of finite-state pattern-matching stead of the more tra-ditional, general, syntactic and semantic processing.There are several reasons for the recent shift to increased use ofFS approximations.
Work was published on deriving finite-stateapproximations from more general grammars \[7\].
Then in MUC-3it became vident hat, in certain data-extraction tasks, a systemwhich ignored much of the input text but focussed attention onthe items of interest could perform as well as other systems whichemphasized deeper understanding of all the text.
Once the problemof data-extraedon was perceived to only require the understandingof small fractions of the input text, some systems evolved to domore shallow processing and the use of finite-state approximationsincreased.It should be noted that incorporating finite-state elements can resultin advantages that are important for achieving operational data-extraction systems.
The simplicity of the finite-state formalismmakes FS rules more easily understandable (and thus, modifiable)by non-experts.
Since parsing finite-state grammars can be donevery efficiently, another advantage is fast processing, which is de-sirable in many real applications.In so/he systems, notably SRI's and GE's, there was a dramaticshift between MUC-3 and MUC-5 towards the use of finite-statepattern-matching  all the critical inguistic omponents, relyingheavily on domain-specific patterns.
Development of GE's MUC-5 system, SHOGUN \[8\], resulted in the complete replacement oftheir general syntactic parser by a complex FS grammar.
This newgrammar encodes domain-specific information which was formerlydistributed in other components.
SRI's new FASTUS \[9\] relies ona cascade of finite-state ransducers; the first stages find simple lin-guistic structures, and the final and most important stage consists ofmultiple levels of domain-specific nite-state patterns.
Informationnot matched is ignored.MessageiMessage Reader IMorphological Analyzer I1 ' I Lexical Pattern Matcher I IFast Partial Parser IlSemantic Interpreter I1' + I Sentence-Level Pattern Matcher\[ IDiscourse ITemplate Generator \]OutputFigure 1: PLUM'Architecture2.
as a backup strategy, identify patterns that are likely to havebeen fragmented during regular processing.Although both of these systems (along with BBN's PLUM), weretop performers in MUC-5, they now lack a large-coverage domain-independent syntactic and semantic model.
Rather, they rely onintensive analysis of domain corpora in order to encode patterns ineach new domain.3.
Role in PLUM's ArchitectureBBN's PLUM has a traditional and general-purpose processingcore, where morphological nalysis, syntactic parsing, semanticinterpretation, and discourse analysis take place.
Purely syntacticparse structures and general semantic interpretations are createdduring processing.
When porting to a new domain, we can useour automatic procedures for learning lexical-semantic case-frameinformation from annotated ata \[10\] to quickly obtain domain-specific understanding without using finite-state approximations.This then becomes the initial system on which more detailed e-velopment is based.During the development for TIPSTER, we added to the core PLUMsystem two new optional processing modules which do use finite-state patterns for the following specific purposes:1. detect domain-specific simple constructions that can be iden-tiffed on/he basis of shallow lexical information, andFigure 1 shows PLUM's architecture.
Parallel possible pathsare indicated where the optional pattern-matching modules ap-pear.
The two new modules, the Lexical Pattern Matcher andthe Sentence-Level Pattern Matcher, use the same core finite-statepattern-matching processor, SPURT, which is described in the nextsection.The Lexical Pattern Matcher operates before parsing but after tag-ging by part-of-speech to recognize constructions which can be de-tected based on component words, their parts-of-speech, and sim-ple properties of their lexical semantics.
This is used primarilyfor structures that could be part of the grammar, but can be moreefficiently recognized by a finite state machine.
Examples are cor-poration/organization names, person names, and measurements.The Sentence-Level Pattern Matcher eplaced our former fragmentcombining component which sought o attach contiguous fragmentsbased on syntactic and semantic properties.
The new pattern-matching component applies FS patterns to the fragments of theparsed and semantically interpreted input; the matched patterns'associated actions may modify or add new semantic information atthe level of the sentence.
That is, semantic relationships may beadded between objects in potentially widely separated fragmentsof the sentence, thereby handling the example of the discontiguousconstituent presented earlier.1834.
SPURT: A Finite-State Pattern-ActionLanguageWe defined our first version of the FS patteru-matcher and FSgrammar syntax for a gisting application \[11\].
The problem therewas to extract key information (e.g., plane-id, command) from theoutput of a speech recognizer whose input was (real) air-trafficcontroller and pilot interactions.
This initial version of the pattern-matcher 'was also utilized, for the purpose of detecting companynames, in the PLUM configuration used for the initial TIPSTERevaluations.Before M\[UC-5 we made the FS grammar syntax more powerful(though still finite-state) to give the rule-writer more flexibility.We also introduced optimizations to the parser and added an actioncomponent to the rules.
The resulting utility is named SPURT.
Wefirst used SPURT for applying sentence-level patterns, and laterreplaced the simple company name recognizer by SPURT to per-form general lexically-based pattern matching of various types ofconstructions.SPURT rules are finite-state patterns which can be used to searchfor complex patterns of information in a sentence and build se-mantic structures from that information.
A SPURT rule has a:PATTERN component which is the expansion (the "right-handside") of a finite-state grammar rule.
It optionally has an :UNDER-STANDING component which states actions to take if the patternis matched.
Examples of SPURT rules are included in subsequentsections.Rules are either top-level rules or sub-level (supporting-level) rules.Top-level rules indicate multiple entry points into the grammardefined by the patterns, and may invoke sub-level rules, as in acontext-free grammar where the fight hand side of a non-terminalmay be in terms of other non-terminals.
Top-level patterns areiterated over for each sentence, and the actions corresponding tomatched rules are executed.Rules are assigned aphase.
Rules belonging to phase n+l operateon the input after it is mutated by phase n. So far we have onlyseen the need for up to 2 phases in our rules.When the SPURT rules are read in at system load time, they arecompiled into a network of nodes and arcs.
Arcs coming out ofa node indicate multiple possible next states.
Nodes contain tests,so that if the test at the end-node of an arc is successful whenapplied to the input at the pointer, that arc is traversed.
The parsersimply matches an input against he network, performing a depth-first search, and selecting a path that matches the maximal amountof input.
At each decision point, arcs are tried in an order whichfavors paths that consume a maximal amount of input in a mean-ingful way (e.g., the parser only follows "don't-care" arcs whenother possibilities are exhausted).
Once a successful parse of thewhole input is found the search is terminated.
1 The resulting pathis then traversed to execute the corresponding actions.4.1.
Lexically-based SPURTThe Lexical Pattern Matcher applies SPURT patterns after mor-phological analysis but prior to parsing.
The input consists of1 In all-paths mode, the parser can be used to find arc probabilities basedon training data.
This was used in the gisting application, but has not yetbeen used in PLUM.word tokens with part-of-speech information.
A pattern can teston a token's word component, its part-of-speech, itssemantic type,or a top-level predicate in its lexical semantics.
When a pattern ismatched, the action component identifies ubstrings of the matchedsequence to add to the temporary lexicon.
These temporary defi-nitions are active for the duration of the message.For example, a pattern for recognizing company names could matcha sequence such as ("Bfidgestone" NP) ("Sports" NPS) ("Co." NP),where NP is the tag for proper nouns, and NPS for plural propernouns; the pattern's action results in this sequence being replacedby the singular token ("Bridgestone Sports Co." NP), which is,as a side effect, defined as a lexical entry having semantic typeCORPORATION.Figure 2 shows the roles used to match the example above.
Thefirst sub-rule, NP-PLUS, finds sequences of tokens that have beentagged as proper nouns.
The XXX-CO rule finds sequences ofthe type {'the'} \[proper-noun\]+ {\[proper-noun-plural\]} \[corp-designator\].
The :TERM-PRED operator appearing in this ruleallows for other simple tests on the tokens.
In this case, the corp-designator?
test tries to match one of a list of possible companydesignators, e.g., "Corp.".
The CO-INSTANCE rule determinesthe existence of a company name if one of many company patternsmatches.
If there is a match, the pattern assigns the tag tag-stringto the sequence, and the action component creates a lexical entryfor it.
The lexical entry is assigned type CORPORATION and as-signed the predicate NAME-OF relating the entry to a string createdout of the words in the matched sequence.
Finally, the top-levelrule CO finds multiple instances of companies in the input.
(def-sub-patt NP-PLUS(:pattern (:PLUS tagger::np)))(def-sub-patt XXX-CO(:pattern(:SEQ (:OPT "the") (:RULE NP-PLUS)(:OPT tagger::nps)(:TERM-PRED corp-designator?
))))(def-sub-patt (CO-INSTANCE (:args tag-string))(:pattern(:tag tag-string(:OR ... (:RULE XXX-CO) ...)))(:understanding((:type CORPORATION tag-string name)(:string STR tag-string)(:pred NAME-OF tag-string STR))))(def-top-patt CO(:pattern(:seq(:plus(:seq(:star :anyword)(:rule CO-INSTANCE corp-string)))(:star :anyword))))Figure 2: Lexical Pattern Example1844.2.
Sentence-Level SPURTThe input to Sentence-Level SPURT is a sentence object which hasalready been processed through the fragment semantic interpreter.Its fragments' parse nodes have already been annotated with a se-mantic interpretation.
SPURT's parser actually operates on the leafelements (the nodes corresponding to the terminals, or words) ofthe fragment parses.
The "pointer" can move along the input eitherat the word level, or at the level of higher structures, achieved bymatching nodes that are ancestors of the leaf nodes.
Thus patternscan test on words or phrases.
When a word is matched, the parsepointer moves to the next word's leaf node; if a phrase is matched,it is moved to the next possible word not spanned by the tree.A pattern can test both syntactic and semantic information associ-ated with the parse nodes.
When a pattern is matched, the actioncomponent specifies new semantic information to be added to par-ticular parse nodes (and thus to the fragment in which each node iscontained).
The new information is allowed to include predicatesconnecting semantic structures across different fragments-thissomething the fragment semantic interpreter is unable to do, as itis a compositional operation on the individual, independent, parsefragments.Below is an example of a sentence-level rule which will matchthe example given in the introduction.
This pattern matches e-quences of the type \[anyword\]* \[joint-word\] \[anyword\]* \[activity-corporation-or-venture-np\] \[anyword\]*, where \[joint-word\] (or*JOINT-WORDS* as specified below) is one of a list of wordssuch as "jointly" and "together".
The operator :AND-ENV intro-duces tests on phrases in a parse tree: :CAT indicates the phrasecategory; because some phrasetypes are recursive, :LOW (or othervalues) is used to indicate which level of the recursive structure isthe one to be looked at; and :CONCEPT indicates the semantic typethat is desired of that phrase.
The simple action component of thisrule adds the semantic type JOINT-VENTURE to the parse-nodewhere the joint-word occurred.
In effect this is indicating there isa joint-venture in the sentence.
Note that this pattern makes nodecisions regarding the role, if any, that \[activity-corporation-or-venture-np\] lays in the joint venture.
(def-top-patt JOINT-WORD-TRIGGER(:pattern(:seq(:star :anyword)(=tag joint-word-tag(:star :anyword)(:or(:and-env(:and-env(:and-env(:term *JOINT-WORDS*))(:cat np :low)(=concept JV-ACTIVITY))(:cat np :low)(:concept CORPORATION))(:cat np :low)(:concept POSS-JV)))(:star :anyword)))(:understanding((=type JOINT-VENTURE joint-word-tag))))Figure 3: Sentence-Level Pattern Example5.
ExperimentsIn order to measure the impact of the new FS components, we ranour MUC-5 English configurations (for English joint ventures andEnglish microelectronics) on two test sets.
The first is test dataused for the TIPSTER 18-month evaluation, the second is data thatwas released for training, but was kept blind.
For each pair ofdomain and test set, we ran experiments in each of 4 modes:?
Baseline: our default configuration, using both FS compo-nents;?
No Lexical FS: turn off lexical FS processing, except for theold company-name r cognizer;?
No Sentence FS: turn off sentence-level FS processing; and?
No FS: turn off both.The default configurations in the two domains share the sameprocessing modules, the same general domain-independent gram-mar and semantic rules, and the same company-name r cognizer.Each configuration contains its own set of domain-specific lexical-semantic definitions.
A lexical-semantic definition contains theword's semantic type and (optionally) case-frames identifying se-mantic tests on possible arguments to the word.
The semanticinterpreter uses these rules in compositionally assigning semanticsto parse-trees.
For EJV, the initial version of the lexical semanticsWas automatically generated from training data \[I0\]; it was thenmodified manually as needed.Although we tested both domains, we consider the test on EME tobe more representative of the effects of the new modules.
Most ofthe EJV development preceded the existence of the modules.
Infact, for EJV we added no new rules to the lexical FS component.EME development, however, was able to take advantage of the newutilities almost from the start.
It made heavier use of the front-endrules for some of the tricky technical constructions in that domain;it should be noted that even then, the impact of the lexical FS wasminimal in that domain.Table 1 shows the difference in ERR for the various modes.
ERRwas the primary error measure used in MUC-5; to show improvedperformance, the goal is to minimize this measure.
The new FScomponents, as evidenced by the Base results, improved ERR byat most 3 percentage points.Base No Lex No Sent No FSEJV- 1 66 66 68 68EJV-2 68 68 70 70EME- 1 59 60 61 62EME-2 62 63 63 63Table 1: Impact of New FS Components: Numbers are ERR mea-suresIt should be noted that the Japanese domains, JJV and JME, madeheavy use of the sentence-level patterns.
FS patterns for JJV gaveus a quick gain in performance, but the price paid was having littlecarryover to the JME domain once that development began.
We didnot test those domains without the FS components.
Based on ourexperience, if multiple Japanese domains are expected, we would185undoubtedly build a robust domain-independent core of semanticrules, which in the long-run maximizes re-usability and minimizeseffort :for each new domain.
We utilized FS pattems because ourJapanese xpert wanted to explore the capabilities, and limits, ofpattern-matching.6.
ConclusionFinite-state pattern-matching has already shown to be useful andvaluable in data-extraction applications.
Its full possible impact isstill being investigated.
For example, several groups are trying tofind automatic ways to derive FS patterns in order to surmount theporting: problem they pose in systems that heavily depend on them.However, maintaining a wide-coverage linguistic core can resultin excellent data-extraction capability as has been evidenced byPLUM's performance in the government-sponsored MUC evalua-tions.Perhaps the most interesting result was that domain-specific pat-terns, though in principle very powerful, added relatively little tothe performance of the linguistically motivated components.
Errorrate was improved by at most 3 percentage points.
Nevertheless,PLUM data extraction system's performance was among the high-est of all systems participating in MUC-5.While this one case study does not prove the relative efficacy ofdomain-specific patterns versus domain-independent, li guisticallymotivated processing, it does suggest that more research and de-velopment in linguistically motivated syntactic and semantic pro-cessing is promising even in the short term, not just in long rangeresearch.7.
AcknowledgementsThe work reported here was supported in part by the AdvancedResearch Projects Agency and was monitored by the Rome AirDevelopment Center under Contract No.
F30602-91-C-0051.
Theviews and conclusions contained in this document are those of theauthors and should not be interpreted as necessarily representingthe official policies, either expressed or implied, of the AdvancedResearch Projects Agency or the United States Government.The members of the PLUM Research Group are: Ralph Weischedel(Principal Investigator), Damaris M. Ayuso, Sean Boisen, HeidiFox, and Constantine Papageorgiou (BBN), and Dawn MacLaugh-lin (Boston University).References1.
Proceedings ofthe Fifth Message Understanding Conference(MUC-5), August 1993, to appear.2.
The PLUM System Group.
BBN PLUM: MUC-5 System De-scription.
To appear in Proceedings ofthe Fifth Message Un-derstanding Conference (MUC-5), August 1993.3.
Proceedings ofthe Third Message Understanding Conference(MUC-3), Morgan Kaufmann Publishers Inc., May 1991.4.
Proceedings of the Fourth Message Understanding Confer-ence (MUC-4), Morgan Kaufmann Publishers Inc., June 1992.5.
Grishman, R. and Sterling J.
New York University: Descrip-tion of the Proteus System as Used for MUC-5.
To appear inProceedings ofthe Fifth Message Understanding Conference(MUC-5), August 1993.6.
Lehnert, W., McCarthy, J., Soderland, S., Riloff, E., Cardie,C., Peterson, J., and Feng, F. UMASS/HUGHES: Descrip-tion of the CIRCUS System Used for MUC-5.
To appear inProceedings ofthe Fifth Message Understanding Conference(MUC-5), August 1993.7.
Pereira, F. Finite-State Approximations of Grammars.
Pro-ceedings of the Speech and Natural Language Workshop,pages 20-25.
Morgan Kaufmann Publishers Inc., June 1990.8.
Jacobs, P. (Contact).
GE-CMU: Description of the SHOGUNSystem Used for MUC-5.
To appear in Proceedings oftheFifth Message Understanding Conference (MUC-5), August1993.9.
Appelt, D., Hobbs, J., Bear, J., Israel, D., Kameyama, M., and'l~json, M. The SRI MUC-5 JV-FASTUS Information Extrac-tion System.
To appear in Proceedings ofthe Fifth MessageUnderstanding Conference (MUC-5), August 1993.10.
Weischedel, R., Ayuso, D., Bobrow, R., Boisen, S., Ingda,R., and Palmucci, J. Pa~ial Parsing: A Report on Work inProgress.
Proceedings ofthe Speech and Natural LanguageWorkshop, ages 204-209.
Morgan Kaufmann Publishers Inc.,Feb 1991.11.
Rohlicek, R., Ayuso, D., Bates, M., Bobrow, R., Boulanger,A., Gish, H., Jeanrenaud, P., Meteer, M., Siu, M., GistingConversational Speech" in Proceedings ofInternational Con-ference of Acoustics, Speech, and Signal Processing (ICASSP),Mar.
23-26, 1992, Vol.2, pp.
113-116.186
