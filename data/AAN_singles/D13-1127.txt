Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1281?1291,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsSummarizing Complex Events: a Cross-modal Solution of StorylinesExtraction and ReconstructionShize Xuxsz@pku.edu.cnShanshan Wangcheers echo mch@163.comYan Zhang?zhy@cis.pku.edu.cnDepartment of Machine Intelligence, Peking University, Beijing, ChinaKey Laboratory on Machine Perception, Ministry of Education, Beijing, ChinaAbstractThe rapid development of Web2.0 leads tosignificant information redundancy.
Espe-cially for a complex news event, it is diffi-cult to understand its general idea within asingle coherent picture.
A complex event of-ten contains branches, intertwining narrativesand side news which are all called storylines.In this paper, we propose a novel solution totackle the challenging problem of storylinesextraction and reconstruction.
Specifically, wefirst investigate two requisite properties of anideal storyline.
Then a unified algorithm isdevised to extract all effective storylines byoptimizing these properties at the same time.Finally, we reconstruct all extracted lines andgenerate the high-quality story map.
Exper-iments on real-world datasets show that ourmethod is quite efficient and highly compet-itive, which can bring about quicker, clearerand deeper comprehension to readers.1 IntroductionNews reports usually consist of various modalitiesof tremendous information, especially all kinds oftextual information and visual information, whichmake web users dazzled and lost.
The situation getsworse on complex news events.
To help readersquickly grasp the general information of the news,a more concise and convenient system over multi-modality information should be provided.
For ex-ample, given a large collection of texts and imagesrelated to a specified news event (e.g., East Japan?Corresponding authorEarthquake), such a system should present a terseand brief summarization about the event by showingdifferent clues of its development, and thus helpingreaders to effectively find out ?when, where, what,how and why?
at a glance.The researches (Goldstein et al 2000) on auto-matic multi-document summarization (MDS) havehelped a lot when we generate a description for aspecific event.
However, it traditionally exhibits in avery simple style like a ?0-dimensional?
point.
Theappearance of Timeline (Allan et al 2001) bringsabout a visual progress for massive documents anal-yses.
Readers can not only get the most importantideas, but also browse the story evolution in chrono-logical order.
Previous news summarization systemswith structured output (Yan et al 2011) have fo-cused on timeline generation.
Timeline becomes a?1-dimensional?
line.
This style of summarizationonly works for simple stories, which are linear in na-ture.
However, the structure of complex stories usu-ally turns out to be non-linear.
These stories branchinto storylines, dead ends, intertwining narrativesand side news.
To explore these lines, we need amap to reorganize all the information.
Therefore,a ?2-dimensional?
story map is in bad need.
Fig-ure 1 shows a part of the story map generated by oursystem for representing East Japan Earthquake.
Wenotice that the whole event evolves into 4 branches.Each of them focuses on a specific sub-topic and isdistinct from other lines.
Figure 2 takes a close lookat the 4 nodes from different lines, and they differ alot from each other as expected.Text information is more precise and exquisitewhen compared with images.
Nevertheless, as the1281Japan's nuclear safety agency says the cooling system of a third nuclear reactor at Fukushima has failed.
A government spokesman says the blast destroyed a building which housed a nuclear reactor, but the reactor escaped unscathed.
Around 170,000 people have been evacuated from a 12-mile radius around the Fukushima number one nuclear plant.
?
?..Mar 12 Mar 13 Mar 14 Mar 17, 2011 Mar 15 Mar 16 Mar 11A 9.0 magnitude quake triggers a devastating tsunami off northeast Japan, leaving some 19,000 people dead or missing.
A massive earthquake, 8.9 on the Richter scale, unleashes a huge tsunami which crashes through Japan's eastern coastline, sweeping buildings, boats, cars and people miles inland.
Japan's most powerful earthquake since records began has struck the north-east coast, triggering a massive tsunami.
?
?..A Japanese rescue team member walks through the completely leveled village of Saito, in northeastern Japan.
Half a million people have been made homeless by the devastating quake.
Fire crews from Greater Manchester and Lancashire have flown out to Japan as part of the UK's International Search and Rescue team.
?
?..Japan's cabinet on Friday approved a $49 billion budget to help in the reconstruction of areas decimated by last month's earthquake and tsunami.
Japan faces a reconstruction bill of at least $180 billion,  or 3 percent of its annual economic output.
In an televised statement after the blast, prime minister Kan urges those within 19 miles of the area to stay indoors.
????????
???
?Nuclear Crisis                     Post-disaster                    Rescue                     Tsunamis Blue GreenRedBlack?
Storyline?
Story node?
CandidateFigure 1: Four storylines are obtained in the story map of ?East Japan Earthquake?.
They focus on Tsunamis, NuclearCrisis, Rescue and Post-disaster respectively.saying goes, ?a picture paints a thousand words?,an image could provide far more information thanwords do.
In fact, a summarization including bothtexts and images will absolutely yield a more pow-erful and intuitive description about the news event.Under this motivation, we study on extracting andreconstructing tracks with different sub-topics for acomplex event.
To the best of our knowledge, the ex-ploration and analysis of 2-dimensional cross-modalsummarization is academically novel.We are faced with two main problems.
The firstis how to select the most important sentences andimages to make up the final story map.
The previ-ous work by Shahaf et alpresents a 2-D story mapcalled ?metro map?, which summarizes the com-plex topics (Shahaf et al 2012).
They study on thedocument-level, and use the entire news documentas one story node.
But on real web, this may con-front some difficulties.
On one hand, news articlesmay report the event from different perspectives, es-pecially those reviews or retrospective reports.
Thiskind of documents contains many useful saliency in-formation of different sub-topics, but they cannotbe further subdivided to help understand each bet-ter.
On the other hand, some documents, such asLine of ?Tsunamis?
on Mar 11 Line of ?Nuclear Explosion?
on Mar 12Line of ?Rescue?
on Mar 13  Line ?Post-disaster?
on Mar 17Figure 2: Word distributions vary a lot among nodes indifferent storylinescover news and interviewing reports, contain one ortwo famous remarks.
Since these documents also in-clude too much useless information, it?s inappropri-ate to use the whole document as a story node.
Soour work, the sentence-level story map extraction,is just aimed at this brand new problem.
The sec-ond problem is the way to utilize the cross-modal1282information suitably.
Our goal is not only to fusesentences and images together, but also to provide aunified framework to improve them mutually.In this paper, we introduce a novel solution forthe story map summarization problem.
All the sen-tences and images are the candidates for making upthe final map.
The analysis of complicated infor-mation usually requires a semantic-level knowledgestudy.
We address this in the pre-processing of datain Section 3.1.
The key task of our research is theextraction of storylines.
We reveal two fundamentalproperties of an ideal storyline, and propose an op-timization algorithm in Section 3.2.
A highly com-patible MDS sub-algorithm is also fused in and of-fers help to the sentences/images selection.
In Sec-tion 3.3, the extracted storylines are reconstructedas a final story map.
The experimental results con-ducted on four real datasets show that our approachcan perform effectively.The rest of this paper is organized as follows.Some related researches are demonstrated in Sec-tion 2.
We introduce our methodology in Section 3.The experimental results in Section 4 prove the ef-fectiveness of our approach.
Finally, we concludethis paper and present our future work in Section 5.2 Related WorkGenerally speaking, multi-document summarizationcan be either extractive or abstractive.
Researchersmainly focus on the former which extracts the in-formation deemed most important to the summary.Various techniques have been used for this type ofMDS (Haghighi and Vanderwende, 2009; Contrac-tor et al 2012).
Graph-based text summarizationtechniques have been widely used for years.
Thealgorithms, used in TextRank (Mihalcea and Tarau,2005) and LexPageRank (Radev et al 2004), whichare meant to compute sentence importance, are sim-ilar to those in PageRank and HITS.Recently, timeline becomes a popular style topresent a schedule of events and attracts many re-searchers consequently.
For example, Yan et almake use of timestamps to generate an evolutionarytimeline (Yan et al 2011).
Shahaf et alpresent a2-D story map called ?metro map?
to summarize thecomplex topic (Shahaf et al 2012).
But previouswork only studies on the document level, which in-evitably brings about much information redundancy.Previous studies show that the use of visual ma-terials not only leads to the conservation of infor-mation but also promotes comprehension (Panjwaniet al 2009).
Thus the cross-modal fusion is nec-essary.
Wu et alpropose a framework of multi-modal information fusion for multimedia data anal-ysis by learning the optimal combination of multi-modal information with the superkernel fusion (Wuet al 2004).
Borrowing the idea of recommendationin heterogeneous network into the cross-modal newssummarization is also a convincing research.
Xu etal.
tackle this task and bring out an 1-D cross-mediatimeline generation framework (Xu et al 2013).Summarization of multimedia involves researcheson information retrieval of multimedia.
Since tex-tual and visual information are quite different fromeach other, how to make a good transformation tomine latent knowledge from unannotated images isof great concern.
Feng and Lapata (2010) use visualwords to describe visual features and then proposea probabilistic model based on the assumption thatimages and their co-occurring textual data are gen-erated by mixtures of latent topics.However, to the best of our knowledge, no exist-ing research manages to generate a 2-dimensionalstory map automatically and integrate images andtexts into a unified framework at the same time.3 MethodologyThe original data of one event is a collection of newsdocuments on different days, or in a finer granu-larity, a set of sentences and images with differenttimestamps.
Each item in the data collection is acandidate for selection to form the final map.
Wedenote the data collection as C, and C = Cs ?
Cv,where Cs is the subset containing all sentences andCv contains all images.
In the following elaborationof our method, dateset C is the important knowl-edge base.
As the ultimate goal for a specific event,we would like to generate the ?2-D?
story mapM,whose main component is a set of ?1-D?
storylinesL = {L1, L2, .
.
.}.
Each storyline L ?
L is madeup of a set of ?0-D?
story nodes, L = {I1, I2, .
.
.
},each of which is composed of a set of candidatessharing the same timestamp I = {c1, c2, .
.
.}.
Formore concise, our method can be scheduled with the1283following three steps:1.
Prepare semantic knowledge for each candi-date, and then purify data collection C by eliminat-ing the noisy candidates;2.
Conduct OPT-LSH algorithm to extract L thatcontains all qualified storylines;3.
Reconstruct the storylines as the final mapM.3.1 Pre-processingBefore we extract the storylines, we have to solvetwo problems first.
Since our work is cross-modal,the semantic knowledge under the literal and visualsurface is basically required.
Recall from Section 2,there exist many effective ways to dig into the se-mantic level of image and text.
In this paper, we em-ploy the approach proposed by Jiang and Tan (2006).They present a convincing approach which employsa multilingual retrieval model to apply knowledgemining on semantic level.
The first is the sentencefeature vector generation.
With the preprocessingsuch as stemming and stop words removal, they ex-tract the textual TF-IDF feature V ecs of each sen-tence.
The second, also the challenging part, isthe image feature vector generation.
In this step,for each region they extract visual features that areconsisting of 6 color features and 60 gabor featureswhich have been proven to be useful in many ap-plications.
Color features are the means and vari-ances of the RGB color spaces.
Gabor features areextracted by calculating the means and variations ofthe filtered image regions on 6 orientations.
Afterthe visual feature vectors of the image regions areextracted, all image regions are clustered using thek-means algorithm.
The generated clusters, called?visterms?
or ?visual words?, are treated as a vocab-ulary for the images.
Besides visual features, theyalso utilize the context textual feature of each im-age as the semantic supplement to generate to finalfeature vector V ecv.Based on these feature vector of each canti-date, they further calculating the intra-modal sim-ilarity with classical IR methods, they obtain theinter-modal similarity through the vague transfor-mation (Mandl T, 1998).
We also note that trans-lation tools such as VIPS (Cai et al 2003) and We-bKit1 can help us to segment web documents and1http://www.webkit.orgpick those text blocks whose coordinates are neigh-boring to each specified image.
In this way we cansuccessfully obtain images, text contexts and con-tent sentences.
Three kinds of semantic similar-ity are now ready.
They are uniformly denoted assim(ci, cj), representing the similarity between twocandidates.
ci and cj can be any type of modalities.Another problem is the noise from irregular data.We would like to utilize the sentences and images ofhigh quality.
The intuitive assumption is that a goodcandidate should have substance in speech, and becoherent with other good candidates.
Fortunately,many useful measures are now available.
They canbe used to choose better candidates.
Inspired by theanalysis analogy to information retrieval, we extendthe idea of the classical PageRank algorithm to esti-mate the authority for each candidate.
The similaritybetween two candidates is regarded as the weighted?link?
between them.Inspired by the idea of classic ?update summa-rization?
task, we try to avoid those chronologicallyordered documents sets focusing on a constant topic.Therefore, given the particularity of our task, wealso have to develop the weighting function ?
withthe temporal factor before starting the ranking algo-rithm.
Our fundamental assumption is that the inter-date and inter-modal ?links?
have different influencecomparing with the intra ones.
Therefore the coreformula calculating the authority of ci is adapted asfollows to make it become weight-compatible:Auth(ci) =1?
q|C|+ q ?
[ ??cj?C??
(ci, cj) ?Auth(cj)O(cj)+ (1?
?)?ck?C???
(ci, ck) ?Auth(ck)O(ck)]C ?
is the subset of C which only includes the can-didates of the same modality with ci, and C ??
is thecross-modal candidates subset.
O(cj) denotes theout-degree of cj , and smoothing parameter q is setas the common value 0.85.
Parameter ?
is used tobalance the biases of intra- and inter-modal impact.If ?
is set to 1, it means the cross-modal informationis abandoned, and vice versa.
?
(ci, cj) contains twoterms as follows:?
(ci, cj) = sim(ci, cj) ?
e?
?ci.t?cj .t?2?21284The second term of ?
?s formula is Gaussian Ker-nel (Aliguliyev R, 2009), which is used to measurethe temporal gap between two candidates (?.t de-notes the date-based timestamp).
Note that the simi-larity metric is content-based and time-independent,since the time decay function is only used to adjustthe ranking impact strength.
In this way, we cangive higher authority to the more informative andcoherent candidates.
The optimal value of ?, whichcontrols the spread of kernel curves, is sensitive todatasets and will be discussed later.We eliminate the candidates whose authority goesunder threshold.
The data collection C is then sig-nificantly downsized and purified for later work.Authority is also used to determine the presentationsequence inside each story node of final map.3.2 Extraction: LSH-OPT AlgorithmOther traditional relative methods need to pre-decidehow many sub-topics are going to be obtained, likeclustering or other supervised models.
They failin the unsupervised automatic storylines extractionproblem.
In this Section, we propose a concrete al-gorithm to extract storyline set L from C. Our pro-posed LSH-based algorithm can automatically op-timize the number of storylines according to theirself-evaluation.3.2.1 Task FormalizationLet?s investigate the storyline first.
We may thinkof some basic attributes as well as many exten-sion properties.
The number of nodes in the sto-ryline L (denoted as |L|) is intuitively one of itsbasic attributes.
We would like to define anotherbasic attribute called the SUPPORT of L. In de-tail, Support(L) = minIk?L |Ik|, which denotesthe smallest size among all the story nodes it has.The most challenging part is to properly modelextension properties.
We observe that an effectivestoryline should meet three key requirements: (1)Coherence.
Within one storyline, news changesgradually as time goes and the evolution indicatesconsistency among component story nodes.
We relyon the notion of coherence developed in Connect-the-Dots (Shahaf and Guestrin, 2010) and transformit to what we exactly need in this research; (2) Di-versity.
According to MMR principle (Goldstein etal., 1999), though the work is about summary, westill can draw an analogy and derive that a good sto-ryline should be concise and contain redundant in-formation as few as possible, i.e., two sentences pro-viding information of similar content should not bepresented in different storylines; (3) Coverage.
Theextracted storyline set L should keep alignment withthe source collection C, which is intuitive and evenproved to be significant as proposed in (Li et al2009).
However, Coverage in some ways is tech-nically redundant in front of Diversity.
We decideto use the first two criteria in extraction process anduse the last one to verify the effectiveness.Since a storyline is composed of several nodes,we can select or abandon nodes mainly accordingto these two requirements.
In fact, both of them in-volve a measurement of similarity between two storynodes, denoted by two word distributions (see Fig-ure 2).
Specifically, for story node Ii, its distribu-tion probability of word w is estimated as p(w|Ii) =?c?IiTF (w)?w?c?IiTF (w) where the denominator is used fornormalization.
Then Kullback-Leibler divergence isemployed to denote the distance between two nodesIi and Ij :DKL(Ii, Ij) =?wp(w|Ii) logp(w|Ii)p(w|Ij)In addition, we introduce the decreasing andincreasing variants based on logistic functions,D?KL = 1/(1 + eDKL) and D?KL = eDKL/(1 +eDKL), to map the distance into [0, 1].
Given themeasurement, we can formulate the two properties.For Coherence, a storyline Li consists of a seriesof individual but correlated nodes, which do not nec-essarily have the serial timestamps.
We would liketo choose such a set of nodes {I1, I2, .
.
.
}, and at thesame time guarantee this criterion:Cor(Li) =1|Li|?1?k<|Li|D?KL(Ik, Ik+1)For Diversity, each storyline Li ?
L shoulddemonstrate quite different subtopics with other sto-rylines.
This is the most essential motivation for usto step into 2-dimensional field.
This criterion canbe used to maximize the minimum diversity valueamong all storylines:Div(L) = minLi,Lj?L{?Ik?Li?Ik?
?LjD?KL(Ik, Ik?
)|Li| ?
|Lj |}1285Then the problem can be transformed into the fol-lowing optimization problem.
Parameters ?1, ?2 and?3 denote the minimum number of nodes in eachline, the smallest size of candidates in each node andthe coherence lower bound respectively.
The task isto extract an optimal L out of C, such that:?L ?
L, |L| ?
?1 & Support(L) ?
?2;?L ?
L, Cor(L) ?
?3;Div(L) is maximized.3.2.2 Optimization AlgorithmIt can be proved that finding the optimal set L isan NP-Complete problem (not presented due to thelimited space).
Thus the brute-force exhaustive ap-proach is crashed.
We develop a near-optimal al-gorithm based on locality sensitive hashing (OPT-LSH).
The original LSH solution is a popular tech-nique used to solve the nearest neighbor searchproblems in high dimensions.
Its basic idea is tohash similar input items into the same bucket (i.e.,uniquely definable hash signature) with high proba-bility.
All potential storylines can be targeted fast ifwe make good use of this idea.LSH performs probabilistic dimension reductionof high dimensional data by projecting a higher d-dimensional vector V ecc (recall from Section 3.1)to a lower d?-dimensional vector (d?<<d), such thatthe candidates which are in close proximity in thehigher dimension get mapped into the same item inthe lower dimensional space with high probability.It guarantees a lower bound on the probability thattwo similar input items fall into the same bucket inthe projected space and also the upper bound on theprobability that two dissimilar vectors fall into thesame bucket (Indyk and Motwani, 1998).One of the key requirements for good perfor-mance of LSH is the careful selection of the fam-ily of hashing functions.
In OPT-LSH, we use thehashing scheme proposed by Charikar (Charikar M,2002).
In detail, d?
random unit d-dimensional vec-tors r?1, r?2, .
.
.
, r?d?
are generated first.
Each of thed entries of r?i is drawn from a standardized normaldistribution N(0,1).
Then the d?
hashing functionsare defined as:hi(c) ={1, if r?i ?
V ec(c) ?
00, if r?i ?
V ec(c) < 01 ?
i ?
d?We represent the d?-dimensional bucket feature hfor c, h(c) := [h1(c), .
.
.
, hd?(c)].
There are 2d?
dif-ferent buckets at most.
Each denotes a potential sto-ryline, so we have to verify the probability of similarcandidates falling into the same bucket, whose lowerbound is given by Charikar (Charikar M, 2002).Simply filtering and searching among all poten-tial lines in single pass may lead to empty resultset if in post-processing no bucket satisfies all con-straints.
This could probably happen because theinput parameter d?
is set so large that the optimalset of candidates is separated into different buck-ets.
However, we will get a suboptimal result inturn when d?
is too small.
We are then motivatedto tune LSH by iterative relaxation that varies d?
ineach iteration.
Changing the value of d?
balancesthe leverage between expected number of potentialstorylines and their properties.
We perform a binarysearch between 1 and d?
to identify the ideal numberof hash functions to employ.
Algorithm 1 shows thepseudo code of our OPT-LSH algorithm.
The LSHtime is bounded byO(d?|C| log |C|) since the binarysearch relaxation iteration runs for log |C| times inthe worst case and the hashing time is O(d?|C|).3.3 Storylines ReconstructionAt last, we manage to reconstruct all the storylinesin Lopt.
A real-world storyline may sometimes in-tertwine with another, educe other branches, andend its own evolvement.
The way to reconstruct amore effective layout of the story map requires fur-ther study and provides a good research direction inthe future.
However, in this paper we order the sen-tences/images in each story node according to theirauthority scores.
Next, all storylines are arranged toproceed along the timestamps, thus a storyline neverturns back in the map.
Then we adjust the structureto make story nodes sharing the same timestamp stayclose, though they belong to different lines.
Figure 1shows the sample output of our system.4 Experiments4.1 DatasetThere is no existing standard evaluation data set for2-dimensional cross-modal summarization methods.We randomly choose 4 news topics from 4 selectednews websites: New York Times, BBC, CNN and1286Algorithm 1 OPT-LSH AlgorithmInput: Candidate set C, similarity function sim,bucket dimensions d?Output: A near-optimal storylines set Lopt// Main Algorithm1: Initialize left = 1, right = d?, max = ?12: repeat3: d?
= (left + right)/24: L ?
LSH(C, d?
)5: Revise all word distributions p(w|I) in L6: if ?L?L, |L|?
?1 and Support(L)?
?2 andCor(L)?
?3 then7: if Div(L) > max then8: Lopt ?
L, max?
Div(L)9: end if10: left = d?
+ 111: else12: right = d?
?
113: end if14: until left > right15: return Lopt// LSH(C, d?)
: Buckets16: Generate d?
unit vectors randomly17: for j = 1 to |C| do18: for i = 1 to d?
do19: if r?i ?
V ec(cj) ?
0 then20: hi(cj)?
121: else22: hi(cj)?
023: end if24: end for25: h(cj) = [h1(cj), .
.
.
, hd?
(cj)]26: end for27: return Buckets?
{h(cj)|cj ?
C}Reuters.
We query each event confined to these sitesand crawl webpages?
html docs.
Referring to Sec-tion 3.1, timestamps, text contents, images and theirtext contexts are extracted.
Table 1 shows the de-tails.
These 4 datasets all contain massive informa-tion and complex evolutions.4.2 Analysis of Our SystemSince there is no standard for us to verify the ef-fectiveness of our solution, we have to utilize con-vincing criteria based on manual evaluation of ex-Table 1: Statistics of Datasets.Event (Query) Document Time Span ?
?EJE 504 Mar 11-Apr 8, 2011 3 0.9OWS 638 Sept 17-Dec 10, 2011 12 0.7NBA 489 July 1-Dec 28, 2011 17 0.6ME 437 June 21-Aug 31, 2011 9 0.7* Abbreviations EJE, OWS, NBA, ME denote East JapanEarthquake, Occupy Wall Street, NBA Lockout and Murdock?sEavesdropping respectively.perts, and then we can compare them with other ap-proaches.
In order to setup the system, based on theoptimization problem shown in Section 3.2, we as-sign the value of 1 to both ?1 and ?2, and empiri-cally set ?3 as 0.6 which can balance the number ofpotential lines and the quality of story map as well.Then the problem can be re-interpreted in naturallanguage as follows.
Given the data collection, wemanage to find out a set of storylines such that everyline in it contains at least one non-null story nodeand keeps self-coherence not less than 0.6.
What?smore, the diversity of the whole set is maximized.Before comparing OPT-LSH with other systems, wedo some further analysis of inherent properties first.4.2.1 CompactnessThe essential idea of summarization is to reducethe data size, so that a more concise representationwill be generated and help users to fast grasp themain points.
Therefore the Compactness of a storymap needs to be guaranteed.
In the pre-processingmodule, we have already excluded significant num-ber of inferior candidates with the extended PageR-ank.
Nevertheless what we really care is the com-pactness that OPT-LSH brings about.
In the candi-dates and story nodes selection processes, only themost saliency and coherent candidates can appearin the final representation.
We count the number ofsentences and images in Lopt, denoted as ||Lopt||,and then we compare it with the collection size |C|(both before and after pre-processing) to test thecompactness.
Table 2 shows that OPT-LSH furtherreduces the representation scale significantly.4.2.2 CoverageObviously, only the verification of compactnessis far from enough.
As mentioned in Section 3.2,the storyline set L we extract should keep alignmentwith the source collection, and contain informativeas well as comprehensive information inC.
Thus we1287Table 2: The compactness of OPT-LSHDataset EJE OWS NBA ME|C|-before 12049 22890 20403 10237|C|-after 1454 2357 1187 1042||L|| 87 168 113 92Downsizing 94.0% 92.9% 90.5% 91.2%need to verify another property of our summariza-tion, the Coverage.
Inspired by (Shannon C, 2001),we employ the Information Entropy to represent theinformation quantity based on solid mathematicaltheory.
The less information quantity decreases af-ter summarizing, the more story comprehensivenessis maintained.
In this way we verify the property ofcoverage.
Particularly, Shannon denotes the entropyH as follows of a discrete random variableX , whichin fact is a word distribution.
The base knowledgein our work is the global probability mass functionP (WC) based on the entire vocabulary of C, withpossible values {p(w1), .
.
.
, p(w|WC |)}.H(X) = E{?
log (P (WC))?X}= ?
?wi?Xp(wi) log (p(wi))Although different word distributions may havethe sameH , we do not focus on the similarity of twocorpora, but the difference of information quantitiesthey are carrying.
So H is an ideal criterion.Besides the comparison of the entropies of C andL, it gives us a chance to study different modules?contributions in our solution.
There are two placesthat we may simplify the solution.
One is the fea-ture of temporal gap.
If we set the parameter ?
toinfinity, then we can remove the second term ofthe calculation of ?
(i.e.
???)
and then bring outa time-insensitive system.
The other is the cross-modal feature.
We set parameter ?
as 1 to makethe system work in one single modality, and ignoreall images (only the textual sentences are available)to make up story map.
The work then becomes thestudy with text-bias.
We also implement the sim-plest system that blocks images as well as tempo-ral feature.
Figure 3 highlights the good perfor-mance of our system.
We are maintaining infor-mation by a larger proportion of the original datacollection.
Considering the property of high com-pactness, our solution tackles the information re-H(X)010203040EJE OWS NBA MEOriginal COur systemTime-insensitiveText-bias modalSimplestFigure 3: The y-axis denotes the entropy.
And the largerH is, the richer information it brings.dundancy quite well, and promisingly delivers entireknowledge with compact structure.During our experiments on Coverage, we havesome interesting findings.
Datasets perform differ-ently when we take different values of ?
and ?,which controls the temporal decaying rate and cross-modal learning respectively.
The events with shortlife-cycles prefer a smaller value of ?
to dominatethe influence from neighbors, as well as the intra-modal bias.
On the contrary, long-living events pre-fer lager ?
and more inter-modal bias to get informa-tion replenishment from different dates and modal-ity.
Due to the limited space we don?t present thetuning details, but the optimal values are shown inTable 1.
In fact, using the cross-modal mutual influ-ence can, more or less, help to improve the effective-ness of information extraction and summarization.4.3 User StudyBefore we introduce other existing methods that canalso tackle the cross-modal 2-dimensional summa-rization problem, we have to setup the appropriatestandards to quantify users?
evaluation.4.3.1 MetricsIn the user study, we evaluate the effectiveness ofour story maps in aiding users to integrate differentaspects of multi-faceted information.
(Shahaf et al2012) also focuses on story map generation and putsforward two convincing metrics to answer the fol-lowing questions:?
Micro-Knowledge: Can the maps help users re-trieve information faster than other methods??
Macro-Knowledge: Can the maps help users un-derstand the big picture better than other methods?1288For micro-knowledge, we wish to see how mapshelp users answer specific questions.
We comparethe level of knowledge attained by users using ourmethod with two other systems: Google News andTDT.
Google News is a computer-generated site thataggregates headlines from news sources worldwide.News-viewing tools are dominated by portal andsearch approaches, and Google News is a typicalrepresentative of those tools.
TDT (Nallapati et al2004) is a successful system which captures the richstructure and dependencies of news events.We have noticed that making comparisons be-tween different systems is not convincing, since theoutput of Google News and TDT is different both incontent and in presentation (and in particular, can-not be double-blind).
In order to isolate the effectsof sentence selection vs. map organization, we in-troduce a hybrid system into the study: the systemwith structureless story map displays the same sen-tences and images as our system but with none of thestructure.
Its output is basically the same with ourfull system but with a single storyline and mergesnode content for each date.
And each story nodesare sorted chronologically and displayed similarly toGoogle News.
We implement TDT based on (Nalla-pati et al 2004) (cos + TD + SimpleThresholding),and pick a representative article from each cluster.The purpose of the study is to test a single query.We also obtain the results from Google News usingthe same queries.We recruit 64 volunteers to browse all the fourevents, and one of the four systems is assigned toeach person randomly.
After browsing, users areasked to answer a short questionnaire (8 questions),composed by domain experts.
Users answer as manyquestions as possible in limited time (8 minutes).The statistics of their answers are promised to eval-uate the micro-knowledge on different systems.
Inorder to aid in comprehension, we give some exam-ples about those asked questions.
For the event ofEJE, we ask that1.
How many magnitude was initially reported bythe USGS, and what about the finally report?2.
List at least six countries that had dispatchedtheir rescue teams.3.
.
.
.And for OWS, we ask thatTable 3: Macro-knowledge performance on four datasetsDataset Our System Google News TDTEJE 56.3% 23.2% 20.5%OWS 62.2% 22.1% 15.7%NBA 58.3% 18.9% 22.8%ME 47.2% 26.3% 26.5%1.
What was the attitude of President Obama aboutthe protesters on October?2.
When did the protesters begin dressing ?corpo-rate zombies?
in New York?3.
.
.
.These questions can effectively help us to investi-gate users?
micro-knowledge about the events.As for macro-knowledge, unlike the retrievalstudy that evaluates users?
ability to answer ques-tions, we are interested in the use of story maps ashigh-level overviews, allowing users to understandthe big picture.
We believe that the ability to ex-plain a certain issue is the only proof of understand-ing.
Therefore, the 64 volunteers are then asked towrite four paragraphs to summarize the four eventsrespectively.
This time, all three systems?
(the struc-tureless system presents the same content as our sys-tem) results are provided and we let users choose thesentences with complete freedom.
Then we countthe number of sentences they employed from eachsystem and derive the average proportions.
Accord-ing to the results we can research on the macro-knowledge that different systems deliver.4.3.2 ResultsWe take the time cost and the average numbers ofcorrect answers of different systems to evaluate onthe micro-knowledge.
Figure 4 shows the results.We can find out that our system outperforms theothers significantly when users taking less time tolearn the knowledge.
The failure of structurelesssystem proves that our work of storyline reconstruc-tion makes lots of advantages to help reading.On the other hand, Table 3 analyzes the statisticsof macro-knowledge.
It?s also obvious that userswould like to refer to the sentences that our sys-tem provides.
A reasonable explanation is that thestory maps we generate can clarify users?
thoughtsand views on the complicated events.1289minutesCorrect NumberEast Japan EarthquakeminutesCorrect NumberOccupy Wall StreetminutesCorrect NumberNBA LockoutminutesCorrect NumberMurdock's EavesdroppingFigure 4: Micro-knowledge performance on four datasetsTable 4: Average runtime of different datasetsDataset EJE OWS NBA ME|C| 1454 2357 1187 1042Iterations 3.5 4.7 5.4 3.7Runtime (ms) 1582 3214 3089 13144.4 Runtime AnalysisAt last, we analyze the time performance of ourOPT-LSH algorithm on a PC server (16G RAM,2.67GHz 4-processors CPU).
The average iterationsfor different initial value of d?
and the runtime areshown in Table 4.
The results are acceptable.5 ConclusionsIn this paper, we study the feasibility of automati-cally generating cross-modal story maps and presenta novel solution to this challenging problem.
Ourworks mainly tackle the problems of storylines ex-traction and reconstruction.
Specifically, we inves-tigate two requisite properties of an ideal storyline,Coherence and Diversity.
Then the convincing cri-teria are devised to model both.
We formalize thetask as an optimization problem and design an al-gorithm to solve it.
Classical IR and text analyzingtechniques like PageRank are fused into the unifiedframework, and a near-optimal solution is employedto deal with the NP-complete problem.
Experimentson web datasets show that our method is quite effi-cient and competitive.
We also verify that it bringsquicker, clearer and deeper comprehension to users.As a future work, we plan to adapt parame-ters automatically on the basis of different types ofdatasets.
Improving the layout quality of story mapby concerning the interactivity of different media(e.g.
images order) is also significant.
Furthermore,our framework is universal, so that the media otherthan text and image can be adopted as well.AcknowledgmentsWe sincerely thank all the anonymous reviewers fortheir valuable comments, which have helped to im-prove this paper greatly.This work is supported by NSFC with Grant No.61073081 and 61370054, and 973 Program withGrant No.
2014CB340405.1290ReferencesAgrawal R, Gollapudi S, Kannan A, et al2011 En-riching textbooks with images.
Proceedings of the 20thACM International Conference on Information andKnowledge Management, ACM, pages 1847-1856.Aliguliyev R M. 2009 A new sentence similarity mea-sure and sentence based extractive technique for auto-matic text summarization.
Expert Systems with Appli-cations, 2009, 36(4): 7764-7772.Allan J, Gupta R, Khandelwal V. 2001 Temporal sum-maries of new topics.
Proceedings of the 24th AnnualInternational ACM Conference on SIGIR, pages 10-18.Cai D, Yu S, Wen J R, et al2003 VIPS: a visionbasedpage segmentation algorithm.
Microsoft Technical Re-port, MSR-TR-2003-79.Charikar M S. 2002 Similarity estimation techniquesfrom rounding algorithms.
Proceedings of the 34thAnnual ACM Symposium on Theory of Computing,ACM, pages 380-388.Chen Y, Jin O, Xue G R, et al2010 Visual contex-tual advertising: Bringing textual advertisements toimages.
Proceedings of the 24th AAAI Conference,AAAI, pages 1314-1320.Contractor D, Guo Y, Korhonen A.
2012.
Using Argu-mentative Zones for Extractive Summarization of Sci-entific Articles.
COLING, pages 663-678.Evans D K, McKeown K, Klavans J L. 2005 Similarity-based multilingual multi-document summarization.IEEE Transactions on Information Theory, pages1858C1860.Feng Y, Lapata M. 2010 Topic models for image annota-tion and text illustration.
Human Language Technolo-gies: The 2010 Annual Conference of NAACL, pages831-839.Goldstein J, Mittal V, Carbonell J, Kantrowitz M.2000 Multi-document summarization by sentenceextraction.
Proceedings of the 2000 NAACL-ANLPWorkshop on Automatic summarization-Volume 4.
Association for Computational Linguistics,pages 40-48.Goldstein J, Kantrowitz M, Mittal V, et al1999 Summa-rizing text documents: sentence selection and evalua-tion metrics.
Proceedings of the 22nd Annual Interna-tional ACM Conference on SIGIR, ACM, pages 121-128.Haghighi A, Vanderwende L. 2009.
Exploring contentmodels for multi-document summarization.
Proceed-ings of Human Language Technologies: The 2009 An-nual Conference of NAACL, Association for Compu-tational Linguistics, pages 362-370.Indyk P, Motwani R. 1998 Approximate nearest neigh-bors: towards removing the curse of dimensionality.Proceedings of the 30th Annual ACM Symposium onTheory of Computing, ACM, pages 604-613.Jiang T, Tan A H. 2006 Discovering image-text associa-tions for cross-media web information fusion.
Knowl-edge Discovery in Databases: PKDD 2006, pages 561-568.Li L, Zhou K, Xue G R, et al2009 Enhancing diver-sity, coverage and balance for summarization throughstructure learning.
Proceedings of the 18th Interna-tional Conference on World Wide Web, ACM, pages71-80.Mandl T. 1998 Vague transformations in informationretrieval.
ISI 1998, pages 312-325.Mihalcea R, Tarau P. 2005 A language independent al-gorithm for single and multiple document summariza-tion.
Proceedings of IJCNLP 2005.Nallapati R, Feng A, Peng F, et al2004 Event threadingwithin news topics.
Proceedings of the 13rd ACM In-ternational Conference on Information and KnowledgeManagement, ACM, pages 446-453.Panjwani S, Micallef L, Fenech K, et al2009 Effects ofintegrating digital visual materials with textbook scansin the classroom.
International Journal of Educationand Development using ICT, 2009, 5(3).Radev D R, Jing H, et al2004 Centroid-based summa-rization of multiple documents.
Information Process-ing & Management, 2004, 40(6): 919-938.Radev D, Winkel A, Topper M. 2002 Multi documentcentroid-based text summarization.
ACL Demo Ses-sion, 2002.Shahaf D, Guestrin C. 2010 Connecting the dots be-tween news articles.
Proceedings of the 16th ACMConference on SIGKDD, ACM, pages 623-632.Shahaf D, Guestrin C, Horvitz E. 2012 Trains ofthought: Generating information maps.
Proceedingsof the 21st International Conference on World WideWeb, ACM, pages 899-908.Shannon C E. 2001 A mathematical theory of commu-nication.
ACM SIGMOBILE Mobile Computing andCommunications Review, 2001, 5(1): 3-55.Wu Y, Chang E Y, Chang K C C, et al2004 Optimalmultimodal fusion for multimedia data analysis.
Pro-ceedings of the 12th annual ACM International Con-ference on Multimedia, ACM, 2004: 572-579.Xu S, Kong L, Zhang Y.
2013 A cross-media evolution-ary timeline generation framework based on iterativerecommendation.
Proceedings of the 3rd ACM confer-ence on International Conference on Multimedia Re-trieval, ACM, pages 73-80.Yan R, Wan X, Otterbacher J, et al2011 Evolution-ary timeline summarization: a balanced optimizationframework via iterative substitution.
Proceedings ofthe 34th International ACM Conference on SIGIR,ACM, pages 745-754.1291
