Acquisition of Lexical Paraphrases from TextsKazuhide YamamotoATR Spoken Language Translation Research Laboratories2-2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288 Japanyamamoto@fw.ipsj.or.jpAbstractAutomatic acquisition of paraphrase knowledgefor content words is proposed.
Using only anon-parallel text corpus, we compute the para-phrasability metrics between two words fromtheir similarity in context.
We then filter wordssuch as proper nouns from external knowledge.Finally, we use a heuristic in further filtering toimprove the accuracy of the automatic acqui-sition.
In this paper, we report the results ofacquisition experiments.1 IntroductionParaphrasing research has attracted increasedattention, and the work in this field has becomemore active recently.
Paraphrasing involves var-ious types of transformations of expressions intothe same language, and thus there is generallyno all-purpose design and information resource.Among the many types of paraphrasing, a hand-written construction may be best for syntacticparaphrasing knowledge or knowledge of func-tional words because the number of resultingphenomena can be counted.
On the other hand,we need to acquire lexical paraphrasing knowl-edge automatically or efficiently, since there isan enormous number of phenomena observed foran enormous number of content words.Some works, such as Barzilay and McKeown(2001), have acquired paraphrasing knowledgeautomatically.
All of those works found dif-ferences from a paraphrase corpus, where eachexpression is aligned to another expression (ormore) with the same meaning and in the samelanguage.
Unfortunately, there is no paraphrasecorpus widely available except for a few collec-tions such as those prepared by Shirai et al(2001) and Zhang et al (2001).
Most of thoseworks collected paraphrase corpora by employ-ing special situations, such as multiple news re-sources from the same events or multiple trans-lations of the same (and well-known) story inother languages.
However, since these situa-tions seem to be really special, we believe thatthe collection of many paraphrase corpora in thenear future is quite hopeless.
Consequently, itis necessary to conduct a feasibility study oncollecting various kinds of paraphrase knowl-edge from non-paraphrase corpora, particularlyfrom raw text corpora.
Although we have al-ready reported extracting paraphrasing knowl-edge of Japanese noun modifiers from a raw cor-pus (Kataoka et al, 1999), we need to exploreother types of expressions.With this motivation, we have attemptedto acquire paraphrasing knowledge on contentwords, mainly nouns and verbs.
As a knowl-edge source, we use newspaper articles collectedover one year in text format, which is regardedas the most generally used corpus.
In this trial,we propose the following two principles of ac-quisition:?
Conditions for applying each type of para-phrasing knowledge should be obtained.?
Paraphrasing knowledge should have direc-tionality.In other words, all of the paraphrasing pat-terns obtained by the conventional methodsseem to have been applied unconditionally, thatis, conventional approaches tend to target onlyunconditional patterns.
However, most para-phrasing phenomena depend on their context;paraphrasing can be possible only if a para-phrased expression fits the context.Moreover, directionality of the rules is animportant issue for paraphrasing, although noother works have discussed this.
Despite theexistence of synonymy, even if expression E1can be replaced by expression E2, it is unsurewhether E2can be paraphrased by using E1.We discuss this feature in the experiments.2 Contextual Similarity vs.SynonymyParaphrasability is the degree of replacabilityfor two expressions E1and E2, which are re-garded as different from each other in somesense.
This definition implies the notion that E1should not be judged as the same (or similar) asE2in the sense of meaning.
Of course, similarityof meaning and paraphrasability are very closelycorrelated with each other, and Kurohashi andSakai (1999) utilize this feature to paraphraseJapanese expressions (in order to comprehendthem more easily).
They use a Japanese dic-tionary written for humans (or more precisely,children) to replace a part of the target expres-sion with a different one by judging its localcontext computed by a thesaurus.We propose that replacability (obtained bythe corpus, for example) is a more importantfactor in judging the paraphrasability of expres-sions than their meaning as defined in a dictio-nary.
For example, words used only in some spe-cial situations, such as for children or in ancientdocuments, should not be used in a paraphraseeven though it has synonymy.On the contrary, even if synonymy is not sat-isfied, we still focus on expressions that are re-placeable.
Hypernymy is one example of this.A hypernym is not a paraphrase in a strictsense due to the loss of information.
However,this kind of paraphrasing is still useful fromthe engineering point of view.
For instance,these loose (and therefore many) paraphrasesare more effective in the case of reluctant pro-cessing, where we must necessarily change anexpression for various reasons such as our re-quirement in paraphrase-based machine trans-lation (Yamamoto, 2002).
Moreover, this kindof paraphrase loses nothing when it is used asanaphora or when it is trivial and out of majorinterest in the context used.
Not all hypernymsare always paraphrasable, so we cannot list thistype of paraphrase from only a thesaurus.3 Approach and ImplementationThis section describes our approach to acquiringparaphrase knowledge from a text corpus.
Weuse Perl programming language to implementall of the following processes and experiments.3.1 Collection of context from corpusWe first define the term context in this paper.The context of a certain content word is de-fined as direct dependency relations between theword and the words that surround it in texts.That is, the context of a content word c is, in oursense, defined as the collection of words uponwhich c directly depends as well as the collec-tion of words that directly depend on c.Under this definition, we first collected all ofthe dependency relations observed in the cor-pus.
Each article is segmented and part-of-speech tagged by the morphological analyzerJUMAN1 and then parsed by the KNP2 parser.We then obtained a relation triplet (c1, r, c2)from each article, where a word c1depends ona word c2with the relation r. A complete listof r types and their examples is shown below:?
(Noun, r1, Noun)e.g.
???
(this time)?
(of)??
(law)????
(terrorism)??
(bill)??
(Adjective, r2, Noun)e.g.
????
(new)??
(law)??
(Noun, r3, Verb)e.g.
????
(Lower House)?
(SUB)????
(approve)??
(Verb, r4, Noun)e.g.
?????
(bombing)??
(U.S. army)?In this list, rican be a particle, such as acase particle (r3) or an associative particle ???(r1).
Another type of possible riin the list isa syntactic relation expressed without any par-ticle or other functional marker.
For instance,a verb or an adjective directly modifies a nounwithout using any functional words in Japanese.In this case, we introduce the notion of a con-stituent boundary proposed by Furuse and Iida1http://www-nagao.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html2http://www-nagao.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html(1994), which is a virtual functional marker in-serted between two consecutive content words,in order to more easily analyze a sentence.
Forinstance, if there are two consecutive nouns, weassume that ?nn?
is inserted between the twonouns, and consequently the relation riis ?nn?.3.2 Bigraph constructionWe then transform the collection of triplets intoa bigraph (2-partite graph).
In the first step,each triplet in the collection is converted intotwo couplets consisting of a content word andan operator by the following definition: an op-erator consists of a content word c and a rela-tion with directionality r. It is defined as eitherr ?
c (something depends on c by r) or r ?
c(c depends on something by r).
For instance,suppose that a triplet is (c1, r, c2), then botha couplet for the first content word c1, i.e., (c1,r ?
c2) and a couplet for the second contentword c2, i.e., (c2, r ?
c1) are extracted in thisoperation.We perform this conversion for all of thetriplets, and a list of couplets is obtained.
Fromthe viewpoint of graph theory, this couplet listis a bigraph, such as figure 1, which consistsof two sets (content word set and operator set)and a list of edges, where each edge connects anelement in one set to an element on the otherside.
This bigraph is a weighted graph, and eachweight expresses the frequency of appearing inthe corpus.3.3 Paraphrasability computationIn the next step, we compute paraphrasability.In this work, the paraphrasability P for any twocontent words ciand cjis defined in the follow-ing formula:P (ci, cj) =?m?M(ci)?M(cj)p(m, ci)?m?M(ci)p(m, ci)(1)M(ci) = {m|f(m, ci) > 1} (2)p(m, ci) =f(m, ci)?cf(m, c)(3)In this formula, let f(m0, c0) be frequency ofcontent word c0with operator m0.
In othercontentwords operatorsm5c2c4m3Figure 1: Example of a bigraph (2-partitegraph)words, f(m0, c0) is a weight of edge (c0, m0)in the bigraph.This formulation can be explained as follows.Paraphrasability between two content words ciand cjincreases if these words behave similarlyin terms of their dependency relations.
That is,this metrics compares the similarity of the con-textual situations of the two input words.
Thedefinition states that paraphrasability computesthe number of operators that cjlinks among theoperators that cilinks.However, we believe that the importance ofeach operator m is not equivalent to that of theothers.
For example, in figure 1, the operatorm3is linked by only two words, c2and c4, whilethe operator m5is linked by almost all of thewords.
In this situation, it is not reasonableto handle the two operators equally, since m3may confirm that the two words are similar orparaphrasable, whereas m5may be a generaloperator widely used in various situations.
Inother words, when we compute paraphrasabilityfrom c2to c4, the edge (c4, m5) is judged as lessimportant than the edge (c4, m3) or (c2, m3).Consequently, each operator is weighted by thedefinition of formula (3).
Moreover, instancesof low frequency are regarded as accidental andinsignificant, so we filter out links where an in-stance appears only once.It is obvious in the definition of (1) that0 ?
P (ci, cj) ?
1, and a higher score expressesa higher possibility of paraphrasing.
More im-portantly, the definition indicates the relationP (ci, cj) = P (cj, ci), i.e., there is a directional-ity that gives larger differences than any similar-ity metrics.
Even if an expression E1has a largeparaphrasability for an expression E2, it is com-pletely uncertain whether the paraphrasabilityof E2into E1is high or low.3.4 Paraphrase knowledge filteringBy only taking the discussion of the last sub-section into account, we can compute para-phrasability between any two content words.However, this measure is not the final judgmentof paraphrasability: some pairs score very higheven though they are not paraphrasable.
For ex-ample, the pair three and four may have a veryhigh score but are of course not paraphrasable.In our observation, the following kinds are foundto be misjudged as paraphrasable by our defini-tions.1.
number, e.g., ???
(three) ????(four)2.
proper noun,e.g., ????
(Beijing) ?????(Taipei)3.
antonym, e.g., ???
(right) ????
(left)Obviously, these errors occurred due to oneof the limitations of our approach; since the for-mula only has an interest in the context of thewords found in the corpus, not in the sense ofthe words found in a dictionary.However, we can filter out these kinds of wordpairs by introducing language resources exter-nal to the corpus.
First, we can judge whetherthe word is a number by applying some sim-ple rules.
Second, we can now easily obtain ex-tensive lists of both major proper nouns andantonyms.
We obtain the proper noun list fromGoiTaikei3, one of the largest Japanese elec-tronic thesauri, in which 169,682 proper nounentries are extracted.
We obtain the antonymlist from both Gakken Kokugo Daijiten (aJapanese word dictionary) and Kadokawa RuigoShinjiten (a Japanese thesaurus), which have11,981 antonym pairs in total.3http://www.kecl.ntt.co.jp/icl/mtg/resources/GoiTaikei/c1c2c3(a)(b)(c):judged as paraphrasableFigure 2: Heuristic by number of linksIn fact, further filtering is necessary in orderto reduce errors.
For example, in English, gui-tar, piano, and flute have very similar contexts,such as ?to play the ,?
?an electric ,?
?aviolin and a ,?
and so on, although they arenaturally not paraphrasable.
We predict that inorder to use lexical paraphrase collection for fil-tering, future research will need to concentrateon how to collect word pairs that are not para-phrasable but have the same context.3.5 Further filtering by heuristicmethodIn the final process, we filter the pairs furtherby using our proposed heuristic to improve theacquisition accuracy.From our observations of the results obtainedby the above operations, we found a clear ten-dency in words that have a very high frequencyor very broad sense: these words tend to bejudged as having a high paraphrasability frommany words or to many words, even if they arenot actually paraphrasable.
For example, in fig-ure 2 (a), a content word such as c1tends to bemisjudged as paraphrasable if c1links to manywords and/or if c1is linked by many words.In other words, case (b) of the figure, where aword c2connects to only one word, would morelikely have its paraphrasing judged as proper.We also build a hypothesis that case (c), whereTable 1: Evaluation for Content WordsCase 1 Case 2 TotalExtracted 668 1149 1684Paraphrasable 422 780 1117Accuracy 63.2% 67.9% 66.3%Case 1: a word paraphrases to one wordCase 2: a word is paraphrased from one wordtwo words are exchangeable, has more accuracythan the other two cases, which are evaluatedin the next section.We assume these errors occurred becausesuch words can have dependency relations withmany words, i.e., such words are general andfrequently appearing.
Consequently, such casesare unexpectedly judged as being highly para-phrasable from or to many words.
As thesewords are used many times in many contexts,the possibility of inserted noises also increases.Therefore, distinguishing noises from real para-phrases becomes difficult.These spurious paraphrases should not re-main in the final results, so we conduct anotherfiltering according to the above analysis.
Theactual process is conducted as follows.
For eachci, we count the number of cjthat satisfies therelation P (ci, cj) > Pconst.
If there is only oneword cjthat satisfies this relation, we finally de-termine that cican paraphrase to cj.
Similarly,for each cj, we count the number of cithat sat-isfies the relation P (ci, cj) > Pconst.
If there isonly one word cithat satisfies the relation, wefinally determine that cican paraphrase to cj.In the experiment below, we set Pconst= 0.1.In this heuristic filtering, some word pairsthat are actually paraphrasable may, unfortu-nately, also be lost.
The problem of saving themremains for our future work.4 Knowledge AcquisitionExperiment4.1 Experiment on content wordparaphrasingWe have conducted an experiment of paraphras-ing knowledge acquisition in the following con-4These two words have the same string but differentpart-of-speech, so our tagger judges these two as differ-ent.Table 2: Highest Paraphrasable PairsParaphrase pair P P (?)??
(anecdote) ??
(story) 1 .0015???????????
1 .2539????
(only) ???
(only) 1 .1877???
(scheme) ???
(form) .9982 .2671?????
(panic) ???
(confusion) .9978 .0496??
(win) ???
(win)4 .9802 .5176?????
(hockey) ???
(baseball) .9752 .0286??
(formation) ???
(formation) .9672 .0449???
(incongruity) ???
(pain) .9667 .0352??
(drastic change) ???
(change) .9582 .0177ditions.
The corpus we used was all articles ofThe Mainichi Shimbun, which is one of the na-tional daily newspapers of Japan, published inthe year 1995.
The size of the corpus is 87.3MB, consisting of 1.33 million sentences.Table 1 illustrates evaluation results of knowl-edge acquisition.
The results show that our pro-posed process can choose approximately 1,700paraphrase pairs that have 66% accuracy.
Al-though this accuracy is not satisfactory for anautomatic process, it is already helpful fromthe engineering point of view; accordingly, wecan obtain a large amount of high-quality para-phrase pairs with a minimum human check insignificantly less time than one day.We also show the acquired paraphrase pairswith the highest paraphrasabilities in Table 2.Note that P (?)
in the table denotes the para-phrasability of the inverted paraphrases, fromright to left direction, and the symbol ?
indi-cates that this direction is also judged as para-phrasable, i.e., these two words are determinedto be paraphrasable with each other.
We foundthat most of the entries in the list are correctlyjudged to be paraphrasable, even though someof them cannot be paraphrasable, such as ??????
(underarm throwing)?
into ??????
(overarm throwing)?5.We can also confirm that the directionality ofthe proposed measure works quite well.
For ex-ample, we can paraphrase the term ???
(anec-dote)?
with the more general term ??
(story),?but it is impossible to replace the latter withthe former except in some restricted context.The outputs seen in this table illustrate such anintuition.5Both are names of techniques in sumo wrestling.Table 3: Paraphrasability of OperatorsParaphrase pair P??
(request) ?
???
(order) ?
1??
(director) ?
???
(professor) ?
.9940??
(branch) ????
(district court) ?
.9334??
(high court) ????
?
.8734?nn???
(short term) ??nn???
(college) .8723?nn??
(Swallows) ??nn??
(Giants) .8553??
(every week) ?nn???nn??
(night) .8123??
(city councillor) ?nn????
?nn?
.8063??
(several) ?nn???
(several) ?nn?
.7961??
(pref.
assemblyman) ?nn????
?nn?
.7859If the process judges that the two words canparaphrase each other, these words are consid-ered to be a paraphrase in a narrow sense.
Inthis experiment, we can extract 114 pairs thatsatisfy this relation, and 75 of these pairs areevaluated as being correct, for an accuracy of65.8%.4.2 Experiment for acquisition ofoperator paraphraseSo far in this paper, we have been using an op-erator set to compute any of two words in thecontent word set in the bigraph.
We found thatwe can also do this in the reverse way: comput-ing any of two operators by using the contentword set.
This is possible because even if weturn a bigraph upside-down, it is still a bigraph.In this subsection, we report an experiment oncomputing the paraphrasability of operators bythe same procedure as above.After multiple filtering, 432 pairs were judgedas paraphrasable.
From these we found thatthe number of correct pairs was 312 (72.2%accuracy).
Table 3 illustrates the final para-phrasable pairs with the highest paraphrasabil-ity.Unfortunately, these pairs include errors,so their performance in an automatic processshould be improved.
However, this performanceis still promising for a human-assisted tool.We investigated the pairs and found thatthere were various kinds of paraphrasing knowl-edge obtained in this process.
Not only para-phrases of content words but also paraphraseknowledge of the following types were obtainedin this experiment.?
insertion and deletion of the particle ??
?in noun-noun sequences?
paraphrasing for case particles; inJapanese, it may be possible to change aparticle under a certain context.?
voice conversion?
different description of the same word, e.g.,from a Chinese-origin word to a nativeJapanese word5 Related WorksLexical paraphrasing is very useful in infor-mation retrieval, since it is necessary to ex-pand terms for improving retrieval coverage.Jacquemin et al (1997) have proposed acquir-ing syntactic and morpho-syntactic variationsof the multi-word terms using a corpus-basedapproach.
They have searched for variation,i.e., similar expressions using (a part of) the in-put words, such as technique for measurementagainst measurement technique, while our tar-get is the paraphrase of a single content word.The goal of our work is to obtain lexicalknowledge for paraphrasing.
For this purposewe use contextual similarity, which is also usedin the sense similarity computation task in thefields of natural language processing, artificialintelligence, and cognitive science.
Moreover,the idea of corpus-based context extraction isbasically the same and also used in the task ofautomatic construction of thesauri or sense de-termination of unknown words.Although this is the first work to use con-text for paraphrase knowledge extraction, manypreviously reported works have used contextfor similarity calculation.
Paraphrasability andword sense similarity may seem like similar met-rics, but there are critical differences betweenthe two tasks.
First, similarity satisfies the sym-metrical property while paraphrasability doesnot (explained in 3.3).
Second, similarity isa relative measure while paraphrasability is anabsolute measure; in many cases, we can answer?Can E1paraphrase to E2?
?, but it is hard toanswer ?Is E1similar to E2??.
In other words,it is important to collect paraphrases while itmay be pointless to collect similar words, sincethe border for the former is clearer than that ofthe latter.The kind of information used for defining con-text is important.
For this question, Nagamatsuand Tanaka (1996) used a deep case (seen in asemantically tagged corpus), and Kanzaki et al(2000) only extracted relations of nominal modi-fication.
The most closely related work in termsof similarity source is the work of Grefenstette(1994), where they obtained subject-verb, verb-object, adjective-noun, and noun-noun relationsfrom a corpus.
In contrast, as discussed in sub-section 3.1, we propose extracting all of the de-pendency relations around content words, i.e.,nouns, verbs, and adjectives.
This is the firstattempt to introduce these features into a con-text definition, and it is obvious that coverageof extracted pairs becomes wider by using var-ious features.
However, we have not conductedenough experiments to prove that these factorsare effective.
This remains for our future work.6 ConclusionsWe propose a process to acquire paraphras-ing pairs of content words from a non-parallelraw corpus.
We utilize contextual similarity,obtained from the corpus, to compute para-phrasability between any two content words.Some of the word pairs that unexpectedly havehigh paraphrasability are filtered out by us-ing external linguistic knowledge such as propernouns and antonyms.
Moreover, our proposedheuristic, obtained through observation, can in-crease acquisition accuracy.
These processes incombination are able to obtain more than 1,700paraphrase pairs with approximately 66% accu-racy.Our interest in this research is not to pursuehigher accuracy in automatic processing but toobtain any kind of paraphrasing knowledge asfast as possible.
From this point of view, thecoverage of the acquisition process is a more se-rious problem for us than accuracy.
Our prelim-inary experiment showed that a drastic drop inaccuracy is observed even if we increase cover-age gradually.
We need to find another filteringcriterion to avoid this problem.AcknowledgmentThe research reported here was supported in part bya contract with the Telecommunications Advance-ment Organization of Japan entitled, ?A study ofspeech dialogue translation technology based on alarge corpus.
?ReferencesRegina Barzilay and Kathleen R. McKeown.2001.
Extracting paraphrases from a parallelcorpus.
In Proc.
of ACL-2001, pages 50?57.Osamu Furuse and Hitoshi Iida.
1994.
Con-stituent boundary parsing for example-basedmachine translation.
In Proc.
of Coling?94,pages 105?111.Gregory Grefenstette.
1994.
Corpus-derivedfirst, second, third-order word affinities.
InProc.
of EURALEX?94.Christian Jacquemin, Judith L. Klavans, andEvelyne Tzoukermann.
1997.
Expansion ofmulti-word terms for indexing and retrievalusing morphology and syntax.
In Proc.
ofACL-EACL?97, pages 24?31.Kyoto Kanzaki, Qing Ma, and Hitoshi Isahara.2000.
Similarities and differences among se-mantic behaviors of Japanese adnominal con-stituents.
In Proc.
of ANLP/NAACL 2000Workshop on Syntactic and Semantic Com-plexity in Natural Language Processing Sys-tem, pages 59?68.Akira Kataoka, Shigeru Masuyama, andKazuhide Yamamoto.
1999.
Summarizationby shortening a Japanese noun modifier intoexpression ?A no B?.
In Proc.
of NLPRS?99,pages 409?414.Sadao Kurohashi and Yasuyuki Sakai.
1999.Semantic analysis of Japanese noun phrases:A new approach to dictionary-based under-standing.
In Proc.
of ACL?99, pages 481?488.Kenji Nagamatsu and Hidehiko Tanaka.
1996.Estimating point-of-view-based similarity us-ing POV reinforcement and similarity prop-agation.
In Proc.
of Pacific Asia Conferenceon Language, Information, and Computation(PACLIC), pages 373?382.Satoshi Shirai, Kazuhide Yamamoto, and Fran-cis Bond.
2001.
Japanese-English paraphrasecorpus.
In Proc.
of NLPRS2001 Workshop onLanguage Resources in Asia, pages 23?30.Kazuhide Yamamoto.
2002.
Machine transla-tion by interaction between paraphraser andtransfer.
In Proc.
of COLING2002.Yujie Zhang, Kazuhide Yamamoto, andMasashi Sakamoto.
2001.
Paraphrasingutterances by reordering words using semi-automatically acquired patterns.
In Proc.
ofNLPRS2001, pages 195?202.
