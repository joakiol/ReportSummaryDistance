Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 604?611, Vancouver, October 2005. c?2005 Association for Computational LinguisticsThe Use of Metadata, Web-derived Answer Patterns and PassageContext to Improve Reading Comprehension PerformanceYongping DuMedia Computing and WebIntelligence LaboratoryFudan UniversityShanghai, Chinaypdu@fudan.edu.cnHelen MengHuman-ComputerCommunication LaboratoryThe Chinese University ofHong KongHongKong.
SAR.
Chinahmmeng@se.cuhk.edu.hkXuanjing HuangMedia Computing and WebIntelligence LaboratoryFudan UniversityShanghai, Chinaxjhuang@fudan.edu.cnLide WuMedia Computing and WebIntelligence LaboratoryFudan UniversityShanghai, Chinaldwu@fudan.edu.cnAbstractA reading comprehension (RC) systemattempts to understand a document and returnsan answer sentence when posed with aquestion.
RC resembles the ad hoc questionanswering (QA) task that aims to extract ananswer from a collection of documents whenposed with a question.
However, since RCfocuses only on a single document, the systemneeds to draw upon external knowledgesources to achieve deep analysis of passagesentences for answer sentence extraction.This paper proposes an approach towards RCthat attempts to utilize external knowledge toimprove performance beyond the baseline setby the bag-of-words (BOW) approach.
Ourapproach emphasizes matching of metadata(i.e.
verbs, named entities and base nounphrases) in passage context utilization andanswer sentence extraction.
We have alsodevised an automatic acquisition process forWeb-derived answer patterns (AP) whichutilizes question-answer pairs from TREC QA,the Google search engine and the Web.
Thisapproach gave improved RC performances forboth the Remedia and ChungHwa corpora,attaining HumSent accuracies of 42% and69% respectively.
In particular, performanceanalysis based on Remedia shows that relativeperformances of 20.7% is due to metadatamatching and a further 10.9% is due to theapplication of Web-derived answer patterns.1.
IntroductionA reading comprehension (RC) system attempts tounderstand a document and returns an answersentence when posed with a question.
The RCtask was first proposed by the MITRECorporation which developed the Deep Readreading comprehension system (Hirschman et al,1999).
Deep Read was evaluated on the RemediaCorpus that contains a set of stories, each with anaverage of 20 sentences and five questions (oftypes who, where, when, what and why).
TheMITRE group also defined the HumSent scoringmetric, i.e.
the percentage of test questions forwhich the system has chosen a correct sentence asthe answer.
HumSent answers were compiled by ahuman annotator, who examined the stories andchose the sentence(s) that best answered thequestions.
It was judged that for 11% of theRemedia test questions, there is no single sentencein the story that is judged to be an appropriateanswer sentence.
Hence the upper bound for RCon Remedia should by 89% HumSent accuracy.
(Hirschman et al 1999) reported a HumSentaccuracy of 36.6% on the Remedia test set.Subsequently, (Ng et al, 2000) used a machinelearning approach of decision tree and achievedthe accuracy of 39.3%.
Then (Riloff and Thelen,2000) and (Charniak et al, 2000) reportedimprovements to 39.7% and 41%, respectively.They made use of handcrafted heuristics such asthe WHEN rule:if contain(S, TIME), then Score(S)+=4i.e.
WHEN questions reward candidate answersentences with four extra points if they contain aname entity TIME.RC resembles the ad hoc question answering(QA) task in TREC.1  The QA task finds answersto a set of questions from a collection ofdocuments, while RC focuses on a single1 http://www.nist.gov.604document.
(Light et al 1998) conducted adetailed compared between the two tasks.
Theyfound that the answers of most questions in theTREC QA task appear more than once within thedocument collection.
However, over 80% of thequestions in the Remedia corpus correspond toanswer sentences that have a single occurrenceonly.
Therefore an RC system often has only oneshot at finding the answer.
The system is in direneed of extensive knowledge sources to help withdeep text analysis in order to find the correctanswer sentence.Recently, many QA systems have exploitedthe Web as a gigantic data repository in order tohelp question answering (Clarke et al, 2001;Kwok et al, 2001; Radev et al, 2002).
Ourcurrent work attempts to incorporate a similar ideain exploiting Web-derived knowledge to aid RC.In particular, we have devised an automaticacquisition process for Web-derived answerpatterns.
Additionally we propose to emphasizethe importance of metadata matching in ourapproach to RC.
By metadata, we are referring toautomatically labeled verbs, named entities as wellas base noun phrases in the passage.
It isimportant to achieve a metadata match betweenthe question and a candidate answer sentencebefore the candidate is selected as the final answer.The candidate answer sentence may be one with ahigh degree of word overlap with the posedquestion, or it may come from other sentences inthe neighboring context.
We apply these differenttechniques step by step and obtain better resultsthan have ever previously been reported.Especially, we give experiment analysis forunderstanding the results.In the rest of this paper, we will first describethree main aspects of our approach towards RC ?
(i) metadata matching, (ii)automatic acquisition ofWeb-derived answer patterns and (iii) the use ofpassage context.
This will be followed by adescription of our experiments, analysis of resultsand conclusions.2.
Metadata MatchingA popular approach in reading comprehension isto represent the information content of eachquestion or passage sentence as a bag of words(BOW).
This approach incorporates stopwordremoval and stemming.
Thereafter, two words areconsidered a match if they share the samemorphological root.
Given a question, the BOWapproach selects the passage sentence with themaximum number of matching words as theanswer.
However, the BOW approach does notcapture the fact that the informativeness of a wordabout a passage sentence varies from one word toanother.
For example, it has been pointed out by(Charniak et al 2000) that the verb seems to beespecially important for recognizing that a passagesentence is related to a specific question.
In viewof this, we propose a representation for questionsand answer sentences that emphasizes three typesof metadata:(i) Main Verbs (MVerb), identified by the linkparser (Sleator and Temperley 1993);(ii) Named Entities (NE), including names oflocations (LCN), persons (PRN) and organizations(ORG), identified by a home-grown named entityidentification tool; and(iii) Base Noun Phrases (BNP), identified by ahome-grown base noun phrase parser respectively.We attempt to quantify the relative importanceof such metadata through corpus statisticsobtained only from the training set of the Remediacorpus, which has 55 stories.
The Remedia test set,which contains 60 stories, is set aside forevaluation.
On average, each training story has 20sentences and five questions.
There are 274questions in all in the entire training set.
Eachquestion corresponds to a marked answer sentencewithin the story text.
We analyzed all thequestions and divided them into three questionsets (Q_SETS) based on the occurrences ofMVerb, NE and BNP identified with the toolsmentioned above.
The following are illustrativeexamples of the Q_SETS as well as their sizes:Q_SETMverb(Count:169)Who helped the Pilgrims?Q_SETNE(Count:62)When was the first merry-go-round built in the United States?Q_SETBNP(Count:232)Where are the northern lights?Table 1.
Examples and sizes of question sets (Q_SETS)with different metadata ?
main  verb (MVerb), namedentity (NE) and base noun phrase (BNP).It may also occur that a question belongs tomultiple Q_SETS.
For example:605Q_SETMVerbWhen was the first merry-go-round builtin the United States?Q_SETNEWhen was the first merry-go-round builtin the United States?Q_SETBNPWhen was the first merry-go-round builtin the United States?Table 2.
An example sentence that belongs to multipleQ_SETS.As mentioned earlier, each questioncorresponds to an answer sentence, which isannotated in the story text by MITRE.
Hence wecan follow the Q_SETS to divide the answersentences into three answer sets (A_SETS).Examples of A_SETS that correspond to Table 1include:A_SETMVerbAn Indian named Squanto cameto help them.A_SETNEThe first merry-go-round in theUnited States was built in 1799.A_SETBNPThen these specks reach the airhigh above the earth.Table 3.
Examples of the answer sets (A_SETS)corresponding to the different metadata categories,namely, main verb (MVerb), named entity (NE) andbase noun phrase) (BNP).In order to quantify the relative importance ofmatching the three kinds of metadata betweenQ_SET and A_SET for reading comprehension,we compute the following relative weights basedon corpus statistics:|_|||MetadataMetadataMetadata SETASWeight =  ?..Eqn (1)where SMetadata is the set of answer sentences in|A_SETMetadata| that contain the metadata of itscorresponding question.
For example, referring toTables 2 and 3, the question in Q_SETNE ?Whenwas the first merry-go-round built in the UnitedSates??
contains the named entity (underlined)which is also found in the associated answersentence from A_SETNE, ?The first merry-go-round in the United States was built in 1799.?Hence this answer sentence belongs to the set SNE.Contrarily, the question in Q_SETBNP ?Where arethe northern lights??
contains the base nounphrase (underlined) but it is not found in theassociated answer sentence from A_SETBNP,?Then these specks reach the air high above theearth.?
Hence this answer sentence does notbelong to the set SBNP.
Based on the three sets, weobtain the metadata weights:WeightMVerb=0.64, WeightNE=0.38, WeightBNP=0.21To illustrate how these metadata weights areutilized in the RC task, consider again thequestion, ?Who helped the Pilgrims??
togetherwith three candidate answers that are ?equallygood?
with a single word match when the BOWapproach is applied.
We further search formatching metadata among these candidateanswers and use the metadata weights for scoring.Question Who helped the Pilgrims?MVerb identified: ?help?BNP identified: ?the Pilgrams?CandidateSentence 1An Indian named Squanto came to help.Matched MVerb (underlined)Score= WeightMVerb=0.64CandidateSentence 2By fall, the Pilgrims had enough food forthe winter.Matched BNP (underlined)Score= WeightBNP=0.21CandidateSentence 3Then the Pilgrims and the Indians ate andplayed games.Matched BNP (underlined)Score= WeightBNP=0.21Table 4.
The use of metadata matching to extend thebag-of-words approach in reading comprehension.3.
Web-derived Answer PatternsIn addition to using metadata for RC, the proposedapproach also leverages knowledge sources thatare external to the core RC resources ?
primarilythe Web and other available corpora.
This sectiondescribes our approach that attempts toautomatically derive answer patterns from theWeb as well as score useful answer patterns to aidRC.
We utilize the open domain question-answerpairs (2393 in all) from the Question Answeringtrack of TREC (TREC8-TREC12) as a basis forautomatic answer pattern acquisition.3.1 Deriving Question PatternsWe define a set of question tags (Q_TAGS) thatextend the metadata above in order to representquestion patterns.
The tags include one for mainverbs (Q_MVerb), three for named entities(Q_LCN, Q_PRN and Q_ORG) and one for basenoun phrases (Q_BNP).
We are also careful toensure that noun phrases tagged as named entitiesare not further tagged as base noun phrases.606A question pattern is expressed in terms ofQ_TAGS.
A question pattern can be used torepresent multiple questions in the TREC QAresource.
An example is shown in Table 5.Tagging the TREC QA resource provides us witha set of question patterns {QPi} and for eachpattern, up to mi example questions.Question Pattern (QPi):When do Q_PRN Q_MVerb Q_BNP?Represented questions:Q1: When did Alexander Graham Bell invent thetelephone?Q2: When did Maytag make Magic Chefrefrigerators?Q3: When did Amumdsen reach the South Pole?
(mi example questions in all)Table 5.
A question pattern and some examplequestions that it represents.3.2  Deriving Answer PatternsFor each question pattern, we aim to deriveanswer patterns for it automatically from the Web.The set of answer patterns capture possible waysof embedding a specific answer in an answersentence.
We will describe the algorithm forderiving answer patterns as following andillustrate with the following question answer pairfrom TREC QA:Q: When did Alexander Graham Bell invent thetelephone?A: 18761.
Formulate the Web QueryThe question is tagged and the Web query isformulated as ?Q_TAG?+ ?ANSWER?, i.e.Question: ?When did Alexander Graham Bellinvent the telephone?
?QP:            When do Q_PRN Q_MVerb Q_BNP ?where Q_PRN= ?Alexander Graham Bell?,Q_MVerb= ?invent?, and  Q_BNP=  ?thetelephone?hence Web query:  ?Alexander Graham Bell?+?invent?
+ ?the telephone?
+ ?1876?2.
Web Search and Snippet SelectionThe Web query is submitted to the searchengine Google using the GoogleAPI and the top100 snippets are downloaded.
From eachsnippet, we select up to ten contiguous words tothe left as well as to the right of the ?ANSWER?for answer pattern extraction.
The selectedwords must be continuous and do not cross thesnippet boundary that Google denotes with ???.3.
Answer Pattern SelectionWe label the terms in each selected snippet withthe Q_TAGs from the question as well as theanswer tag <A>.
The shortest string containingall these tags (underlined below) is extracted asthe answer pattern (AP).
For example:Snippet 1: 1876, Alexander Graham Bellinvented the telephone in the United States?AP 1:   <A>, Q_PRN Q_MVerb Q_BNP.(N.B.
The answer tag <A> denotes ?1876?
in thisexample).Snippet 2: ?which has been invented byAlexander Graham Bell in 1876?AP 2:    Q_MVerb by Q_PRN in <A>.As may be seen in above, the acquisitionalgorithm for Web-derived answer questions callsfor specific answers, such as a factoid in a word orphrase.
Hence the question-answer pairs fromTREC QA are suitable for use.
On the other hand,Remedia is less suitable here because it containslabelled answer sentences instead of factoids.Inclusion of whole answer sentences in Webquery formulation generally does not return theanswer pattern that we seek in this work.3.3 Scoring the Acquired Answer PatternsThe answer pattern acquisition algorithm returnsmultiple answer patterns for every question-answer pair submitted to the Web.
In thissubsection we present an algorithm for derivingscores for these answer patterns.
Themethodology is motivated by the concept ofconfidence level, similar to that used in datamining.
The algorithm is as follows:1.
Formulate the Web QueryFor each question pattern QPi (see Table 5)obtained previously, randomly select an examplequestion among the mi options that belongs to thispattern.
The question is tagged and the Webquery is formulated in terms of the Q_TAGs only.
(Please note that the corresponding answer isexcluded from Web query formulation here,which differs from the answer pattern acquisitionalgorithm).
E.g.,Question: ?When did Alexander Graham Bellinvent the telephone?Q_TAGs: Q_PRN Q_MVerb Q_BNPWeb query:  ?Alexander Graham Bell?+?invent?
+ ?the telephone?2.
Web Search and Snippet SelectionThe Web query is submitted to the search engine607Google and the top 100 snippets are downloaded.3.
Scoring each Answer Pattern APij relating toQPiBased on the question, its pattern QPi, the answerand the retrieved snippets, totally the followingcounts for each answer pattern APij relating toQPi .cij ?
# snippets matching APij and for which thetag <A> matches the correct answer.nij ?
#  snippets matching APij and for which thetag <A> matches any termCompute the ratio rij= cij / nij..........Eqn(2)Repeat steps 1-3 above for another examplequestion randomly selected from the pool of miexample under QPi.
We arbitrarily set themaximum number of iterations to be ki = ?????
?im32in order to achieve decent coverage of theavailable examples.
The confidence for APij.iscomputed askrAPConfidencekiijij?== 1)( ?
?Eqn(3)Equation (3) tries to assign high confidencevalues to answer patterns APij that choose thecorrect answers, while other answer patterns areassigned low confidence values.
E.g.
:<A>, Q_PRN Q_MVerb Q_BNP     (Confidence=0.8)Q_MVerb by Q_PRN in <A>.
(Confidence=0.76)3.4 Answer Pattern Matching in RCThe Web-derived answer patterns are used in theRC task.
Based on the question and its QP, weselect the related AP to match among the answersentence candidates.
The candidate that matchesthe highest-scoring AP will be selected.
We findthat this technique is very effective for RC as itcan discriminate among candidate answersentences that are rated ?equally good?
by theBOW or metadata matching approaches, e.g.
:Q:   When is the Chinese New Year?QP: When is the Q_BNP?where Q_BNP=Chinese New YearRelated AP:  Q_BNP is <A> (Confidence=0.82)Candidate answer sentences 1: you must wait a few moreweeks for the Chinese New Year.Candidate answer sentences 2: Chinese New Year is mostoften between January 20 and February 20.Both candidate answer sentences have the samenumber of matching terms ?
?Chinese?, ?New?and ?Year?
and the same metadata, i.e.Q_BNP=Chinese New Year.
The term ?is?
isexcluded by stopword removal.
However theWeb-derived answer pattern is able to select thesecond candidate as the correct answer sentence.Hence our system gives high priority to theWeb-derived AP ?
if a candidate answer sentencecan match an answer pattern with confidence >0.6, the candidate is taken as the final answer.
Nofurther knowledge constraints will be enforced.4.
Context AssistanceDuring RC, the initial application of the BOWapproach focuses the system?s attention on a smallset of answer sentence candidates.
However, itmay occur the true answer sentence is notcontained in this set.
As was observed by (Riloffand Thelen, 2000) and (Charniak et al, 2000), thecorrect answer sentence often precedes/follows thesentence with the highest number of matchingwords.
Hence both the preceding and followingcontext sentences are searched in their work tofind the answer sentence especially for whyquestions.Our proposed approach references this idea inleveraging contextual knowledge for RC.Incorporation of contextual knowledge is veryeffective when used in conjunction with namedentity (NE) identification.
For instance, whoquestions should be answered with words taggedwith Q_PRN (for persons).
If the candidatesentence with the highest number of matchingwords does not contain the appropriate NE, it willnot be selected as the answer sentence.
Instead,our system searches among the two preceding andtwo following context sentences for theappropriate NE.
Table 6 offers an illustration.Data analysis Remedia training set shows that thecontext window size selected is appropriate forwhen, who and where questions.Football Catches On Fast(LATROBE, PA., September 4, 1895) - The newgame of football is catching on fast, and each month newteams are being formed.Last night was the first time that a football player waspaid.
The man's name is John Brallier, and he was paid$10 to take the place of someone who was hurt.
?Question: Who was the first football player to be paid?Sentence with maximum # matching words: Last nightwas the first time that a football player was paid.Correct answer sentence: The man's name is JohnBrallier, and he was paid $10 to take the place ofsomeone who was hurt.Table 6.
An example illustrating the use of contextualknowledge in RC.608As for why questions, a candidate answersentence is selected from the context window if itsfirst word is one of ?this?, ?that?, ?these?,?those?, ?so?
or ?because?.
We did not utilizecontextual constraints for what questions.5.
ExperimentsRC experiments are run on the Remedia corpus aswell as the ChungHwa corpus.
The Remediatraining set has 55 stories, each with about fivequestions.
The Remedia test set has 60 stories and5 questions per story.
The ChungHwa corpus isderived from the book, ?English ReadingComprehension in 100 days,?
published byChung Hwa Book Co., (H.K.)
Ltd.  TheChungHwa training set includes 100 Englishstories and each has four questions on average.The ChungHwa testing set includes 50 stories andtheir questions.
We use HumSent as the primeevaluation metric for reading comprehension.The three kinds of knowledge sources are usedincrementally in our experimental setup andresults are labeled as follows:Result TechniqueResult_1 BOWResult_2 BOW+MDResult_3 BOW+MD+APResult_4 BOW+MD+AP+ContextTable 7.
Experimental setup in RC evaluations.Abbrievations are: bag-of-words (BOW), metadata(MD), Web-derived answer patterns (AP), contextualknowledge (Context).5.1 Results on RemediaTable 8 shows the RC results for various questiontypes in the Remedia test set.When Who What Where WhyResult_1 32.0% 30.0% 31.8% 29.6% 18.6%Result_2 40.0% 28.0% 39.0% 38.0% 20.0%Result_3 52.6% 42.8% 40.6% 38.4% 21.0%Result_4 55.0% 48.0% 40.6% 36.4% 27.6%Table 8.
HumSent accuracies for the Remedia test set.We observe that the HumSent accuracies varysubstantially across different interrogatives.
Thesystem performs best for when questions andworst for why questions.
The use of Web-derivedanswer patterns brought improvements to all thedifferent interrogatives.
The other knowledgesources, namely, meta data and context, bringimprovements for some question types butdegraded others.Figure 1 shows the overall RC results of oursystem.
The relative incremental gains due to theuse of metadata, Web-derived answer patterns andcontext are 20.7%, 10.9% and 8.2% respectively.We also ran pairwise t-tests to test the statisticalsignificance of these improvements and results areshown in Table 9.
The improvements due tometadata matching and Web-derived answerpatterns are statistically significant (p<0.05) butthe improvement due to context is not.29%35%38.80% 42%0%5%10%15%20%25%30%35%40%45%Result_1 Result_2 Result_3 Result_4HumSentPrecisionFigure 1.
HumSent accuracies for Remedia.PairwiseComparisonResult_1 &Result_2Result_2 &Result_3Result_3 &Result_4t-test Results t(4)=2.207,p=0.046t(4)=2.168,p=0.048t(4)=1.5,p=0.104Table 9.
Tests of statistical significance in theincremental improvements over BOW among the useof metadata, Web-derived answer patterns and context.We also compared our results across variousinterrogatives with those previously reported in(Riloff and Thelen, 2000).
Their system is basedon handcrafted rules with deterministic algorithms.The comparison (see Table 10) shows that ourapproach which is based on data-driven patternsand statistics can achieve comparable performance.Question Type Riloff &Thelen 2000 Result_4When 55% 55.0%Who 41% 48.0%What 28% 40.6%Where 47% 36.4%Why 28% 27.6%Overall 40% 42.0%Table 10.
Comparison of HumSent results with aheuristic based RC system (Riloff & Thelen 00).5.2 Results on ChungHwaExperimental results for the ChungHwa corpus arepresented in Figure 2.
The HumSent accuraciesobtained are generally higher than those with609Remedia.
We observe similar trends as before, i.e.our approach in the use of metadata, Web-derivedanswer patterns and context bring incrementalgains to RC performance.
However, the actualgain levels are much reduced.65%66%68%69%63%64%65%66%67%68%69%70%Result_1 Result_2 Result_3 Result_4HumSentPrecisionFigure 2.
HumSent accuracies for ChungHwa.5.3.
Analyses of ResultsIn order to understand the underlying reason forreduced performance gains as we migrated fromRemedia to Chunghwa, we analyzed the questionlengths as well as the degree of word matchbetween questions and answers among the twocorpora.
Figure 3 shows that the average lengthof questions in Chunghwa are longer thanRemedia.
Longer questions contain moreinformation which is beneficial to the BOWapproach in finding the correct answer.32.67.532.56013.354.1010203040506070?4 5,6,7 ?8Question LengthPercent of Questions(%)Remedia ChungHwaFigure 3.
Distribution of question lengths among theRemedia and ChungHwa corpora.The degree of word match between questionsand answers among the two corpora is depicted inFigure 4.
We observe that ChungHwa has a largerproportion of questions that have a match- size (i.e.number of matching words between a questionand its answer) larger than 2.
This presents anadvantage for the BOW approach in RC.
It is alsoobserved that approximately 10% of the Remediaquestions have no correct answers (i.e.
match-size=-1) and about 25% have no matching wordswith the correct answer sentence.
This explainsthe overall discrepancies in HumSent accuraciesbetween Remedia and ChungHwa.05101520253035-1 0 1 2 3 4 5 ?6Match SizePercent of Questions(%)Remedia ChungHwaFigure 4.
Distribution of match-sizes (i.e.
the numberof matching words between questions and theiranswers) in the two corpora.While our approach has leveraged a variety ofknowledge sources in RC, we still observe thatour system is unable to correctly answer 58% ofthe questions in Remedia.
An example of suchelusive questions is:Question: When do the French celebrate theirfreedom?Answer Sentence: To the French, July 14 has thesame meaning as July 4th does to the UnitedStates.6.
ConclusionsA reading comprehension (RC) system aims tounderstand a single document (i.e.
story or passage)in order to be able to automatically answer questionsabout it.
The task presents an information retrievalparadigm that differs significantly from that found inWeb search engines.
RC resembles the questionanswering (QA) task in TREC which returns ananswer for a given question from a collection ofdocuments.
However, while a QA system canutilize the knowledge and information in a collectionof documents, RC systems focuses only on a singledocument only.
Consequently there is a dire need todraw upon a variety of knowledge sources to aiddeep analysis of the document for answer generation.This paper presents our initial effort in designing anapproach for RC that leverages a variety ofknowledge sources beyond the context of thepassage, in an attempt to improve RC performancebeyond the baseline set by the bag-of-words (BOW)approach.
The knowledge sources include the use ofmetadata (i.e.
verbs, named entities and base nounphrases).
Metadata matching is applied in ourapproach in answer sentence extraction as well asuse of contextual sentences.
We also devised an610automatic acquisition algorithm for Web-derivedanswer patterns.
The acquisition process utilizesquestion-answer pairs from TREC QA, the Googlesearch engine and the Web.
These answer patternscapture important structures for answer sentenceextraction in RC.
The use of metadata matching andWeb-derived answer patterns improved readingcomprehension performances for the both Remediaand ChungHwa corpora.
We obtain improvementsover previously reported results for Remedia, withan overall HumSet accuracy of 42%.
In particular, arelative gain of 20.7% is due to metadata matchingand a further 10.9% is due to application of Web-derived answer patterns.AcknowledgementThis work is partially supported by the DirectGrant from The Chinese University of Hong Kong(CUHK) and conducted while the first author wasvisiting CUHK.
This work is supported by NaturalScience Foundation of China under GrantNo.60435020.ReferencesCharles L.A. Clarke, Gordon V. Cormack, Thomas R.Lynam.
2001.
Exploiting Redundancy in QuestionAnswering.
In Proceedings of the 24th ACMConference on Research and Development inInformation Retrieval (SIGIR-2001, New Orleans,LA).
ACM Press.
New York, 358?365.Cody C. T. Kwok, Oren Etzioni, Daniel S. Weld.
2001.Scaling Question Answering to the Web.
InProceedings of the 10th World Wide WebConference (WWW?2001).
150-161.Daniel Sleator and Davy Temperley.
1993.
ParsingEnglish with a Link Grammar.
Third InternationalWorkshop on Parsing Technologies.Deepak Ravichandran and Eduard Hovy.
2002.Learning Surface Text Patterns for a QuestionAnswering System.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics (ACL-2002).
41-47.Dell Zhang, Wee Sun Lee.
2002.
Web Based PatternMining and Matching Approach to QuestionAnswering.
In Proceedings of the TREC-11Conference.
2002.
NIST, Gaithersburg, MD, 505-512.Dragomir Radev, Weiguo Fan, Hong Qi, Harris Wu,Amardeep Grewal.
2002.
Probabilistic QuestionAnswering on the Web.
In Proceedings of the 11thWorld Wide Web Conference (WWW?2002).Ellen Riloff and Michael Thelen.
2000.
A Rule-basedQuestion Answering System for ReadingComprehension Test.
ANLP/NAACL-2000Workshop on Reading Comprehension Tests asEvaluation for Computer-Based LanguageUnderstanding Systems.Eugene Charniak, Yasemin Altun, Rodrigo de SalvoBraz, Benjamin Garrett, Margaret Kosmala, TomerMoscovich, Lixin Pang, Changhee Pyo, Ye Sun,Wei Wy, Zhongfa Yang, Shawn Zeller, and LisaZorn.
2000.
Reading Comprehension Programs in aStatistical-Language-Processing Class.
ANLP-NAACL 2000 Workshop: Reading ComprehensionTests as Evaluation for Computer-Based LanguageUnderstanding Systems.Hwee Tou Ng, Leong Hwee Teo, Jennifer Lai PhengKwan.
2000.
A Machine Learning Approach toAnswering Questions for Reading ComprehensionTests.
Proceedings of the 2000 Joint SIGDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora 2000.Lynette Hirschman, Marc Light, Eric Breck, and JohnBurger.
1999.
Deep Read: A ReadingComprehension System.
Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics.Marc Light, Gideon S. Mann, Ellen Riloff and EricBreak.
1998.
Analyses for Elucidating CurrentQuestion Answering Technology.
NaturalLanguage Engineering.
Vol.
7, No.
4.Martin M. Soubbotin, Sergei M. Soubbotin.
2002.
Useof Patterns for Detection of Likely Answer Strings:A Systematic Approach.
In Proceedings of theTREC-11 Conference.
2002.
NIST, Gaithersburg,MD, 134-143.611
