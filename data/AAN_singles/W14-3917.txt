Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 139?143,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsThe Tel Aviv University Systemfor the Code-Switching Workshop Shared TaskKfir BarSchool of Computer ScienceTel Aviv UniversityRamat Aviv, Israelkfirbar@post.tau.ac.iNachum DershowitzSchool of Computer ScienceTel Aviv UniversityRamat Aviv, Israelnachumd@tau.ac.ilAbstractWe describe our entry in the EMNLP 2014code-switching shared task.
Our systemis based on a sequential classifier, trainedon the shared training set using variouscharacter- and word-level features, somecalculated using a large monolingual cor-pora.
We participated in the Twitter-genreSpanish-English track, obtaining an accu-racy of 0.868 when measured on the tweetlevel and 0.858 on the word level.1 IntroductionCode switching is the act of changing languagewhile speaking or writing, as often done by bilin-guals (Winford, 2003).
Identifying the transitionpoints is a necessary first step before applyingother linguistic algorithms, which usually target asingle language.
A switching point may occur be-tween sentences, phrases, words, or even betweencertain morphological components.
Code switch-ing happens frequently in informal ways of com-munication, such as verbal conversations, blogsand microblogs; however, there are many exam-ples in which languages are switched in formalsettings.
For example, alternating between Collo-quial Egyptian Arabic and Modern Standard Ara-bic in modern Egyptian prose is prevalent (Rosen-baum, 2000).This shared task (Solorio et al., 2014),1the firstof its kind, challenges participants with identify-ing those switching points in blogs as well as inmicroblog posts.
Given posts with a mix of aspecific pair of languages, each participating sys-tem is required to identify the language of ev-ery word.
Four language-pair tracks were offeredby the task organizers: Spanish-English, Nepali-English, Modern Standard Arabic and Colloquial1http://emnlp2014.org/workshops/CodeSwitch/call.htmlArabic, and Mandarin-English.
For each languagepair, a training set of Twitter2statuses was pro-vided, which was manually annotated with a labelfor every word, indicating its language.
In addi-tion to the two language labels, a few additionallabels were used.
Altogether there were six labels:(1) lang1?the first language; (2) lang2?the sec-ond language; (3) ne?named entity; (4) ambigu-ous?for ambiguous words belonging to both lan-guages; (5) mixed?for words composed of mor-phemes in each language; and (6) other?for caseswhere it is impossible to determine the language.For most of the language pairs, the organizers sup-plied three different evaluation sets.
The first setwas composed of a set of unseen Twitter statuses,provided with no manual annotation.
The othertwo sets contained data from a ?surprise genre?,mainly composed of blog posts.We took part only in the Spanish-English track.Both English and Spanish are written in Latinscript.
The Spanish alphabet contains some addi-tional letters, such as those indicating stress (vow-els with acute accents: ?a, ?e, ?
?, ?o, ?u), a u adornedwith a diaeresis (?u), the additional letter ?n (e?ne),and inverted question and exclamation punctua-tion marks ?
and ?
(used at the beginning of ques-tions and exclamatory phrases, respectively).
Al-though social-media users are not generally con-sistent in their use of accents, their appearancein a word may disclose its language.
By andlarge, algorithms for code switching have usedthe character-based k-mer feature, introduced by(Cavnar and Trenkle, 1994).3Our system is an implementation of a multi-class classifier that works on the word level, con-sidering features that we calculate using largeSpanish as well as English monolingual corpora.Working with a sequential classifier, the predicted2http://www.twitter.com3We propose the term ?k-mer?
for character k-grams, incontradistinction to word n-grams.139labels of the previous words are used as features inpredicting the current word.In Section 2, we describe our system and thefeatures we use for classification.
Section 3 con-tains the evaluation results, as published by the or-ganizers of this shared task.
We conclude with abrief discussion.2 System DescriptionWe use a supervised framework to train a classifierthat predicts the label of every word in the orderwritten.
The words were originally tokenized bythe organizers, preserving punctuation, emoticons,user mentions (e.g., @emnlp2014), and hashtags(e.g., #emnlp2014) as individual tokens.
The in-formal language, as used in social media, intro-duces an additional challenge in predicting the lan-guage of every word.
Spelling mistakes as wellas grammatical errors are very common.
Hence,we believe that predicting the language of a givenword merely using dictionaries for the two lan-guages is likely to be insufficient.Our classifier is trained on a learning set, as pro-vided by the organizers, enriched with some addi-tional features.
Every word in the order written istreated as a single instance for the classifier, eachincluding features from a limited window of pre-ceding and successive words, enriched with thepredicted label of some of the preceding words.We ran a few experiments with different windowsizes, based on 10-fold cross validation, and foundthat the best token-level accuracy is obtained us-ing a window of size 2 for all features, that is, twowords before the focus word and two words after.The features that we use may be grouped inthree main categories, as described next.2.1 FeaturesWe use three main groups of features:Word level: The specific word in focus, as wellas the two previous words and the two followingones are considered as features.
To reduce thesparsity, we convert words into lowercase.
In ad-dition, we use a monolingual lexicon for Englishwords that are typically used in Twitter.
For thispurpose, we employ a sample of the Twitter Gen-eral English lexicon, released by Illocution, Inc.,4containing the top 10K words and bigrams froma relatively large corpus of public English tweets4http://www.illocutioninc.comthey collected over a period of time, along withfrequency information.
We bin the frequency ratesinto 5 integer values (with an additional value forwords that do not exist in the lexicon), which areused as the feature value for every word in focus,and for the other four words in its window.
Thisfeature seems to be quite noisy, as some commonSpanish words appear in the lexicon (e.g., de, no,a, me); on the other hand, it may capture typi-cal English misspellings and acronyms (e.g., oomf,noww, lmao).
We could not find a similar resourcefor Spanish, unfortunately.To help identify named entities, we created a listof English as well Spanish names of various en-tity types (e.g., locations, family and given names)and used it to generate an additional boolean fea-ture, indicating whether the word in focus is an en-tity name.
The list was compiled out of all wordsbeginning with a capital letter in relatively largemonolingual corpora, one for English and anotherfor Spanish.
To avoid words that were capitalizedbecause they occur at the beginning of a sentence,regardless of whether they are proper names, wefirst processed the text with a true-casing tool, pro-vided as part of Moses (Koehn et al., 2007)?the open source implementation for phrase-basedstatistical machine translation.
Our list containsabout 146K entries.Intra-word level: Spanish, as opposed to En-glish, is a morphologically rich language, demon-strating a complicated suffix-based derivationalmorphology.
Therefore, in order to capture re-peating suffixes and prefixes that may character-ize the languages, we consider as features sub-strings of 1?3 prefix and suffix characters of theword in focus and the other four words in its win-dow.
Although it is presumed that capitalizationis not used consistently in social media, we con-sider a boolean feature indicating whether the firstletter of each word in the window was capitalizedin the original text or not.
At this level, we usetwo additional features that capture the level of un-certainty of seeing the sequence of characters thatform the specific word in each language.
This isdone by employing a 3-mer character-based lan-guage model, trained over a large corpus in eachlanguage.
Then, the two language models, one foreach language, are applied on the word in focusto calculate two log-probability values.
These arebinned into ten discrete values that are used as thefeatures?
values.
We add a boolean feature, indi-140cating which of the two models returned a lowerlog probability.Inter-word level: We capture the level of un-certainty of seeing specific sequences of words ineach language.
We used 3-gram word-level lan-guage models, trained over large corpora in eachof the languages.
We apply the models to the fo-cus word, considering it to be the last in a sequenceof three words (with the two previous words) andcalculate log probabilities.
Like before, we bin thevalues into ten discrete values, which are then usedas the features?
values.
An additional boolean fea-ture is used, indicating which of the two modelsreturned a lower log probability.2.2 Supervised FrameworkWe designed a sequential classifier running on topof the Weka platform (Frank et al., 2010) that iscapable of processing instances sequentially, sim-ilar to YamCha (Kudo and Matsumoto, 2003).We use LibSVM (Chang and Lin, 2011), an im-plementation of Support Vector Machines (SVM)(Cortes and Vapnik, 1995), as the underlying tech-nology, with a degree 2 polynomial kernel.
Sincewe work on a multi-class classification problem,we take the one-versus-one approach.
As men-tioned above, we use features from a window of?2 words before and after the word of interest.
Inaddition, for every word, we consider as featuresthe predicted labels of the two prior words.3 Evaluation ResultsWe report on the results obtained on the unseentask evaluation sets, which were provided by theworkshop organizers.5There are three evaluationsets.
The first is composed of a set of unseen Twit-ter statuses and the other two contain data from a?surprise genre?.
The results are available onlineat the time of writing only for the first and secondsets.
The results of the third set will be publishedduring the upcoming workshop meeting.The training set contains 11,400 statuses, com-prising 140,706 words.
Table 1 shows the distri-bution of labels.The first evaluation set contains 3,060 tweets.However, we were asked to download the statusesdirectly from Twitter, and some of the statuseswere missing.
Therefore, we ended up with only1,661 available statuses, corresponding to 17,7235http://emnlp2014.org/workshops/CodeSwitch/results.phpLabel Numberlang1 77,101lang2 33,099ne 2,918ambiguous 344mixed 51other 27,194Table 1: Label distribution in the training set.Accuracy 0.868Recall 0.720Precision 0.803F1-Score 0.759Table 2: Results for the first evaluation set, mea-sured on tweet level.words.
According to the organizers, the evaluationwas performed only on the 1,626 tweets that wereavailable for all the participating groups.
Out ofthe 1,626, there are 1,155 monolingual tweets and471 code-switched tweets.
Table 2 shows the eval-uation results for the Tel Aviv University (TAU)system on the first set, reported on the tweet level.In addition, the organizers provide evaluationresults, calculated on the word level.
Table 3shows the label distribution among the words inthe first evaluation set, and Table 4 shows the ac-tual results.
The overall accuracy on the word levelis 0.858.The second evaluation set contains 1,103 wordsof a ?surprise?
(unseen) genre, mainly blog posts.Out of the 49 posts, 27 are monolingual and 22 arecode-switched posts.
Table 5 shows the results forthe surprise set, calculated on the post level.As for the first set, Table 6 shows the distribu-tion of the labels among the words in the surpriseset, and in Table 7 we present the results as mea-sured on the word level.
The overall accuracy onthe surprise set is 0.941.4 DiscussionWe believe that we have demonstrated the po-tential of using sequential classification for code-switching, enriched with three types of features,some calculated using large monolingual corpora.Compared to the other participating systems aspublished by the workshop organizers, our systemobtained encouraging results.
In particular, we ob-serve relatively good results in relating words to141Label Countlang1 (English) 7,040lang2 (Spanish) 5,549ne 464mixed 12ambiguous 43other 4,311Table 3: Label distribution in the first evaluationset.Label Recall Precision F1-Scorelang1 (English) 0.900 0.830 0.864lang2 (Spanish) 0.869 0.914 0.891ne 0.313 0.541 0.396mixed 0.000 1.000 0.000ambiguous 0.023 0.200 0.042other 0.845 0.860 0.853Table 4: Results for the first evaluation set, mea-sured on word level.their language; however, identifying named enti-ties did not work as well.
We plan to further in-vestigate this issue.
The results on the surprisegenre are similar to that for the genre the systemwas trained on.
However, since the surprise setis relatively small in size, we refrain from draw-ing conclusions about this.
Trying the same code-switching techniques on other pairs of languagesis part of our planned future research.ReferencesWilliam B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings ofthe 3rd Annual Symposium on Document Analysisand Information Retrieval (SDAIR-94), pages 161?175.Chih C. Chang and Chih J. Lin.
2011.
LIBSVM:A Library for Support Vector Machines.
ACMTransactions on Intelligent Systems and Technology,2(27):1?27, May.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20:273?297.Eibe Frank, Mark Hall, Geoffrey Holmes, RichardKirkby, Bernhard Pfahringer, Ian H. Witten, and LenTrigg.
2010.
Weka?A machine learning work-bench for data mining.
In Oded Maimon and LiorRokach, editors, Data Mining and Knowledge Dis-covery Handbook, chapter 66, pages 1269?1277.Springer US, Boston, MA.Accuracy 0.864Recall 0.708Precision 0.803F1-Score 0.753Table 5: Results for the second, ?surprise?
evalua-tion set, measured on the post level.Label Countlang1 (English) 636lang2 (Spanish) 306ne 38mixed 1ambiguous 1other 120Table 6: Label distribution in the ?surprise?
eval-uation set.Label Recall Precision F1-Scorelang1 (English) 0.883 0.824 0.853lang2 (Spanish) 0.864 0.887 0.876ne 0.293 0.537 0.379mixed 0.000 1.000 0.000ambiguous 0.022 0.200 0.039other 0.824 0.843 0.833Table 7: Results for the ?surprise?
evaluation set,measured on the word level.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the Interactive Poster and Demon-stration Sessions of the 45th Annual Meeting of theACL (ACL ?07), pages 177?180, Stroudsburg, PA,USA.
Association for Computational Linguistics.Taku Kudo and Yuji Matsumoto.
2003.
Fast methodsfor kernel-based text analysis.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 24?31, Sapporo,Japan.Gabriel M. Rosenbaum.
2000.
Fushammiyya: Alter-nating style in Egyptian prose.
Journal of ArabicLinguistics (ZAL), 38:68?87.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, SteveBethard, Mona Diab, Mahmoud Gonheim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirshberg, AlisonChang, and Pascale Fung.
2014.
Overview for thefirst shared task on language identification in code-switched data.
In Proceedings of the First Workshop142on Computational Approaches to Code-Switching.EMNLP 2014, Conference on Empirical Methods inNatural Language Processing, Doha, Qatar.Donald Winford, 2003.
Code Switching: LinguisticAspects, chapter 5, pages 126?167.
Blackwell Pub-lishing, Malden, MA.143
