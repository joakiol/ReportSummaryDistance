Redef in ing the "Level" of theMelissa MacphersonEDS Research5951 Jefferson NEAlbuquerque, NM 87109-3432melissa @edsr.
eds.
corn"Word"AbstractUsing dictionaries as a model for lexicon development perpetuates the notion thatthe level of "the word", as structurally defined, is the right starting place for semanticrepresentation.
Difficulties stemming from that assumption are sufficiently seriousthat they may require a re-evaluation of common notions about lexical representa-tion.1 Int roduct ionIn a recent paper Boguraev and Levin (1990) point out inadequacies in common concep-tions of what a Lexical Knowledge Base (LKB) should be, inadequacies which stem fromthe assumption that a machine-readable dictionary (MRD) is not only the right source foracquisition of a lexicon but also the right model for its form.
Their points about the kindsof generalizations that the standardly conceived LKB does not incorporate, especially if itis built from a dictionary source, are well-taken.
They cite a need for the representation ofvarious kinds of dynamic potential--specifically, the capacity to participate in predictablesyntactic alternations and regular meaning extensions--which words of particular seman-tic classes possess in common, and which constitute the basis for lexical productivity.Representation of this kind of dynamic potential is missing from standard dictionaries,they suggest, because it is seen as being more predictable to a native speaker than arespecific -nym relations.
The same representation is missing from most LKBs both becauseit was not in the source dictionary and because it cannot be encoded in the usual LKBformat.
Boguraev and Levin conclude that tile conception of the LKB must be extendedto include both representation f tile semantic lasses whose members typically participatein these productive alternations and extensions, and sets of inference rules which capturethe productive potential of members of those classes.
They see this as a partial answerto the open-endedness problem; when a system of the sort they envision is given a newword, we need only specify its semantic lass, and we will then have many aspects of bothits syntactic and its meaning behavior in hand.However perhaps a more serious difficulty arising from the adoption of the dictionary asmodel for the lexicon, and one which Boguraev and Levin do not discuss, is the implicitassumption that the level of the word, as structurally defined, is a semantically well-defined level also.
1 In the sections that follow, I will examine this assumption fromseveral angles.
Section 2 reviews the semantic behavior of what are structurally words,1Boguraev and Levin do mention the possibility that generalizations of the sort they require mayhold only over partial nodes in a lexicM network; this would seem to indicate that semantic lasses maybe defined in terms of bundles of features, only some of which might be relevant in determining theapplicability of a given extension or alternation.127illustrating cases where it seems to take more than one "word" to make a single meaning,and where a single "word" is semantically decomposable into several meaning components.Section 3 discusses the problem of extricating a functor's argument requirements fromother aspects of its meaning.
In Section 4 the discussion turns to difficulties inherent indistinguishing individual word senses.
Sections 5 explores ome possibilities for escapingfrom the dictionary model and solving some of these problems.2 Morphemes and "Semantemes"We are led as readers of English, and as dictionary users, to see white space betweenwords as indicating a tangible boundary between units of some sort.
We base much ofour linguistic analysis on the assumption that there is such a thing as the "word level".If you go "below" this level of structure you are dealing with morphology or with listsof idiosyncratic information; "above" it is the level of syntax.
Under this assumptionsemantics is also of two sorts: the largely classificatory operations of lexical semantics,and the compositional functions of syntactic structure-building.
As a working assumption,this division of linguistic knowledge has some merit, and has allowed much useful work toproceed.But in fact, even on the most superficial examination of real language, it is commonto find gross disruptions of this convenient pattern.
Unfortunately, lexicalization is anarbitrary process even within a single language, and it is probably only the merest accidentwhen it happens that an atomic chunk of meaning is linked bi-uniquely to an invariantchunk of phonology.
The more common run of structure-meaning correspondences aremuch more complicated.For one thing, combinations of two or more surface words often function synchronicallyas more or less absolute semantic units.
2 These collocations may be able to be takenapart for syntactic purposes, but semantically they are not decomposable; they link tomeanings which are more than, less than, or different from, the sum of the meanings oftheir individual parts.
The commonly cited examples are phrasal verbs ('hold up', 'kidoneself' brim over with') and idiomatic expressions of various sorts ('house of cards', 'lielow', 'eat \[someone\] out of house and home').
The difficulties here, including locating theboundaries of true idiomaticity, are much discussed, and I will not dwell on them here.Second, even what are structurally single words can exhibit the kind of composition-ality ordinarily expected in syntactic onstructions.
The most interesting category here isthat of lexicalized everbal compounds, uch as clambake, carwash, ill-advised, and time-saving.
Roeper and Siegel (1978), and Selkirk (1982), among others, have explored thecompositional regularities found in this highly productive component of the lexicon, andfound that the construction and interpretation of novel verbal compounds i constrainedaccording to criteria whose canonical field of application is syntax.Both these cross-over phenomena carl be accommodated structurally by means of an-alytical tricks of one sort or another.
We have ways of representing verb-particle combi-nations, for example, so that they are looked up in the same fashion as structurally singlewords, even while their parts can be separated by theoretically indefinite stretches of thesentence.
It is also possible to install vestiges of verbal structure into the representation f2This is true even if we leave aside such trouhlesome orthographic conventions as the fact that fullylexicalized compounds are often written as separate words.128derivationally deverbal items, so that the regularities which they exhibit in compoundingare predicted.A more severe problem than either of these for locating the "level" of the "word" isexemplified by these examples of perfectly legitimate paraphrase (from Talmy, 1985):be of interest o somebody = interest somebodycarry out an investigation i to something = investigate somethingCertainly we would want a natural anguage understanding system to arrive at the sameconclusions from either variant in each pair.
It is not at all clear, however, whether weshould cause the single-word versions to be represented as decomposable into the phrasalexpressions, or analyze the phrasal expressions as stylistically or pragmatically inspiredexpansions of the more succinct variants, or take some other other tack, such as justfinding a way to list both variants as meaning the same thing.
3Even when they are not so naturally expanded to phrasal versions, many--maybe most,depending on the granularity of the analysis-- of what we consider to be monomorphemicwords have flagrantly compositional meanings; the most timeworn examples are kill (='cause-to-die') and other verbs in which a change-of-state (one meaning component) isbrought about by a cause (the other meaning component).
Accurate analysis of eventdescriptions containing such words would seem to require such sub-word semantic analysis,whether it is done as such or as an external listing of presuppositions and entailments ofthedescription as a whole.
The trouble, however, is sorting out some set of possible meaningcomponents which is adequate to the compositional task and yet smaller (preferably bothin cardinality and in size required for storage) than the set of English words.Compare, for example, the pairs kill / murder and send_away / banish.
Both pairs in-corporate an additional meaning element in the second word of each pair which is at leastsomewhat similar in both cases; it would mean something like 'with malice aforethoughtand with the intention of instituting a permanent solution to a perceived problem andprobably done with heightened actional intensity'.
It is possible that we could find otherpairs of lexical expressions whose only difference in meaning is the presence of this addi-tional element in one of them; this would be a good indication that a productive meaningcomponent was at work.
But if this component i self can only be expressed in English bymeans of a very long phrase, we have a considerable difficulty of representation.It is not that the notions of "word" and "morpheme" make no sense, where the latteris defined as "the minimal meaningful unit of language" (Pei and Gaynor, 1980:140);rather, both are invaluable from the vantagepoint of surface structure.
But when we usesemantics instead as the starting point, and we define something called a "semanteme",4say, as "the minimal unit of meaning that has a sound", then it is not at all clear that thetwo constructs would meet in the middle and we would arrive at the same set of wordsagain.Nor is this to say that the expanse of meaning encompassed by a structural wordcannot be a unit.
It must be true, as Jackendoff (1983, 1990) claims, that linguisticcategories correspond fairly directly to conceptual categories.
However his formulation ofthe mapping leaves the status of the structural word itself somewhat indeterminate:3One of the pervasive fallacies inherent  in the not ion of the "level" of the "word" is that  below thiseveryth ing  must  be l isted, while above this level everyth ing must  be representable as sets of rules.4 My apologies to anyone who has used this made-up  term previously for a different purpose.129Every content-bearing major phrasal constituent of a sentence (S, NP, AP,PP, etc.)
corresponds to a conceptual constituent of some major conceptualcategory.
(1990:44) \[emphasis added\]We are still left with the question of the proper representation f lexical semantics in alexicon when the borders of what is "lexical" are so fuzzy.3 Argument  Structure and ValenceThe problem of the imperfect mapping between structural words and atomic or evenbounded meanings extends into what might have been expected to be one of the simpleraspects of representing lexical semantics, that of the exposition of argument structure.Attempts to assign thematic role structure to a wide range of verbs run immediately intotwo major problems.
One is that, as Jackendoff (1990) points out, it may be only partof the meaning of a verb which is responsible for assigning a particular thematic role toa given argument.
For instance, the verb pass (in the construction pass the house) isanalyzed as incorporating a Path functor, similar in its function to a surface prepositionby, which is part of the meaning of the verb.
What appears in surface structure thereforeas the object of pass is better analyzed semantically as the object of the underlying by;this is why there is no very satisfactory thematic role label for the object of pass.
It isalso possible for a verb to assign a role to only part of the referent of an argument.
Forexample, the verb climb, in climb the mountain, has as its Goal not the mountain but onlythe top of the mountain.
In general, Jackendoff's arguments hat the standard conceptionof thematic roles requires an accompanying commitment to lexicai decomposition aredifficult to refute.At the same time, thematic role labels, or something like them, are necessary for thedescription of events whether we are willing to commit o lexical decomposition or not.The fact that the set of labels does not seem to be able to be cleanly inventoried atthe level of the structural word does not change the fact that some such set of labels isnecessary to let us individuate vent ypes, that is, to distinguish events in which 'X verbsY' from ones in which 'Y verbs X' (Carlson 1984).
While we may not be able to arrive ata satisfactorily comprehensive s t of labels without complete semantic decomposition, wemust have a sufficient set to be able to extract he basic 'who did what to whom' structurefrom a sentence.However, finding a boundary between the semantic ontribution of the verb itself andthat of its expected arguments i often practically impossible.
Boguraev and Levin'scontention that semantic lass determines syntactic behavior, while arguable on othergrounds, surely stems from the accurate observation that verbs which describe similarsituations naturally tend to govern similar numbers of arguments in similar ways.
5 Par-SUnfortunately, the arbitrariness of lexicalization rears its ugly head here also, and the notion of"semantic class" necessary to make this linkage between syntactic behavior and meaning may be somewhatcircular; that is, in doubtful cases will we decide whether an item is a change-of-state verb on the basisof whether it undergoes the causative/inchoative alternation?
Carlson and Tanenhaus (1988) would nothave been able to construct their experiments comparing the effects of thematic structure ambiguity withthose of semantic ambiguity, if there did not exist verbs of the same semantic lass which do not sharethe same possibilities for thematic structure alternation.
An example pair from their study is load / filhload / fill the truck (with bricks)load / *fill the bricks130ticularly where it is true that a whole semantic lass of verbs exhibits similar syntacticbehavior, it becomes extremely difficult to separate argument structure as an independentphenomenon from the core meaning,Sf there is such a thing, of the assigning verb.
Whatwould it mean to 'eat' without 'eating something' ?- - i t  seems pointless even to considersuch a question.Even the definition of individual thematic roles is susceptible to influence from themeanings of expected fillers for those roles; this fact, along with the the difficulty ofdiscovering how these roles are assigned to arguments by word-level functors, may help toexplain the persistent lack of consensus on the proper set of thematic roles, as exemplifiedin Wilkins (1988) and elsewhere.
Jackendoff faces one aspect of this problem by removingAgent from the set of primary thematic roles; rather, Agent and Patient are derived roletypes, designated on a separate Action Tier and superimposed on basic thematic structurethrough an interaction of the semantics of the verb and the nature of the entities fillingpotential Agent and Patient role slots.
That is, a sentence with kill always implies a killer,but it has an Agent just if the killer is an entity capable of Agent-like activity, for instancea person.Syntax too has an effect on the definition of thematic roles, and that in turn has apervasive ffect on verbal semantics.
Talmy (1985) discusses the fact that subjecthood,because of its frequent association with Agenthood, "may tend to confer upon any seman-tic category expressed in it some initiatory or instigative characteristics."
Consequently aStimulus expressed as Subject may be seen to be acting purposefully or at least activelyupon the Experiencer, while where tile Experiencer is Subject, "the mental event may befelt to arise autonomously and to direct itself outward toward a selected object."
Verbsof experience whose canonical syntactic pattern includes either Stimulus or Experienceras Subject (for example, please and admire, respectively) are thus under the influence ofthis effect in one direction or the other; this may be one source of the ambiguity in thesentence 'He pleases me' and hence in the verb please.4 Senses  o f  wordsThis blurring of what we would rather see as a clear division between the semantic on-tributions of functor and argument also complicates the proper treatment of word senses.Compare the following uses of the verb introduce:1.
We then introduced a catalyst into the solution.2.
She wants to introduce a little culture into his life.3.
I introduced Pete to green chili / green chili to Pete.
64.
I introduced Joe to Pete / Joe and Pete.Only load undergoes the locative alternation.Talmy (1985) also gives numerous examples of verbs in which shared semantic lass does not guaranteeidentity of valence.
An example is the triple emanate, emit, and radiate.
Emanate must have Figure assubject, emit must have Ground as subject, and radiate can have either.
Likewise steal, rob, and rip off,which all have Agent prominently in focus as subject, differ in their t reatment  of Figure and Ground.Steal makes Figure the object, rob makes Ground the object, and rip off can put either one in thatposition.6I do not  know why the second version of this sentence is so much less acceptable than the first; Iassume it has to do with a violation of focusing constraints.1315.
Ronco has introduced several fascinating new products.6.
Joe introduced the coyote into Santa Fe stores.As the definitions and examples in the ~llowing (partial) dictionary entries uggest,it is common to think of each of these sentences as utilizing a different sense of the verbintroduce.
(~om OALDgE)4883820, ..<ent h=introduce><def>bring in</def><def>bring forward</def><ip>introduce into / to</ip><ex>introduce a Bill before Parliament</ex><def>bring (sth) into use or into operation for thefirst time</def><def>cause (sb) to be acquainted with (sth)</def><ex>introduce nee ideas into a business</ex><ex>Tobacco was introduced into Europe from America</ex><ex>The teacher introduced his young pupils to theintricacies of geometry</ex><ip>introduce sb (to sb)</ip><def>make (persons) known by name (to one another),esp in the usual formal way</def><ex>introduce two friends</ex><ex>He introduced me to his parents</ex><ex>The chairman introduced the lecturer to theaudience</ex><ip>introduce (into)</ip><def>insert</def><ex>introduce a tube into a wound</ex><ex>introduce a subject into a conversation</ex>?
?
.
?
?
?
(from CED Fact Base)c_DEF( \[ "introduce", 1,1, I \],c DEF(\[ "introduce",1,1,2 \],c_SAMP(\[ "introduce", 1,1,2 \] ,c_DEF(\[ "introduce",l,l,3 \],c_SAMP(\[ "introduce",1,1,3 \],c_DEF(\[ "introduce",1,1,4 \],c_SAMP(\[ "introduce",l,l,4 \ ] ,c_DEF(\[ "introduce",1,1,5 \]," (o f ten  fo111 by to) to present(someone) by name (to another person)or (two or more people to each other)" ,2 ).
"(follJ by to) to cause to experiencefor the first time" ,1 ).
"to introduce a visitor to beer" ).
"to present for consideration orapproval, espl before a legislativebody" ,I ).
"to introduce a draft bill in Congress" ).
"to bring in; establish" ,1 ).
"to introduce decimal currency" ).
"to present (a radio or television132programme, etcl)verbally" ,1 ).c_DEF(\[ "introduce",1,1,6 \], "(folll by with) to start" ,I ).c_SAMP(\[ "introduce",l,l,6 \], "he introduced his talk with some music" ).c_DEF(\[ "introduce",l,l,7 \], "(often fo111 by into) to insert orinject" .I ).c_SAMP(\[ "introduce",1,1,7 \], "he introduced the needle into his arm" ).o o  ?
o o oHowever the example sentences (1)-(6) seem to illustrate not separate senses but anatural continuum of meanings, in which the contribution of introduce itself does notperceptibly change.
In sentence (1), introduce comes closest to what we would probablycall its core meaning, something like 'bring / put something into a new place'.
Sentence(2) relocates this action in a different kind of space; both the Theme 'something' and theGoal 'new place' are abstract in this case.
7 In Sentence (3), the Goal location is some sortof recognition space, associated either with the recognizer or the subject matter;  the effectis that Pete now knows about green chili.
Obviously this sense extension for introduceonly works if at least one of its arguments i an entity which is capable of recognizing, i.e.a person.
Moreover, where the introduce-ee is also a person as in (4), (or where the singleObject of introduce refers to more than one person) the introduction becomes reciprocal;the "actional content" of the verb, in Talmy's words, has been doubled.In (5), the meaning extension present in (2), (3), and (4) is further specialized sothat the implicit Goal location is not only a "recognition space", but is understood to bethe recognition space belonging collectively to the individuals making up a market.
Wemake this additional extension, however, not because we are now using a different senseof the verb introduce, but simply because of what we know about products.
That  is, ourunderstanding of the type of space in which the introduction is effected is governed by boththe core meaning of the verb--we know introduce means putting something into somespace just as we know that eating means eating something--and the semantic ontent ofits first argument.In (6) the inference from 'product'  to 'market '  is reversed.
In the most plausiblereading of the sentence, the coyote refers generically to a decorative motif  or to some kindof typically salable item, not to an actual animal.
This is true because we know thatSanta Fe stores represents a particular kind of market, and so we prefer an interpretationin which the thing introduced is something which can typically be marketed.If  we try to decide then what the senses of introduce are, as the dictionary model wouldsuggest hat we should do, we can take several tacks.
We could just take a dictionary'sword for the number and definitions of separate senses (leaving aside for the moment heproblems of mapping between the different sense divisions of different dictionaries), butthen we have clearly missed a generalization about the extensions of meaning possible forthe verb introduce and others like it.
We could distinguish separate senses on the basis ofthe use in some of the examples of into and in others of to, but that distinction cross-cutswhat are clearly more important semantic distinctions.On the basis of the thematic structure that the verb introduce instantiates, which ismore or less the same for all six examples, we might say that it has only one sense, s7This is a legitimate and extremely common meaning extension, from physical space to abstract space;in fact it is exactly the sort of regularity that a LKB should include among its lexical rules.SThe one problematical construction here would be the one with a plural Theme and no explicit Goal,as in 'I introduced Joe and Pete.'
In this case, as noted above, the introduction is reciprocal nd thereforethe  themat ic  s t ruc ture  would apparently have to be copied over into another, simultaneous action.133Likewise it would decompose in every case to something likeCAUSE (GO (X (FROM (outside-some-space))(TO inside-some-space))))Under this analysis it is the characteristics of 'X', the locatum, and of the respectiveentities which serve as Goal that differentiate senses; sentence (1) makes reference to a realthing and a physical space, (2), (3), and (4) refer to recognizable ntities and individualcognitive space, and (5) and (6) refer to something which can be sold and a particularkind of collective cognitive space.
In (5), naming a product allows us to infer a market asthe implicit Goal, while in (6) knowing that the Goal is a market lets us know that thecoyote is to be construed as a product.
What seems to be happening is what Cruse (1986)calls "modulation" of a word sense by context, except that in this case the "context" isan integral part of the argument structure of the verb.In any case, it is clear that splitting the verb introduce into a set of separate senses atthe word level will be arbitrary in one way or another.5 Words  as Wor ldsThe ideal, but computationally outlandish, solution to this problem would be to representevery word in English (not just each functor, but every word) as the union of all thesituations in which it could potentially participate, so that all the combinatorial potentialof each word would be an inherent part of its inventory of meaning(s).
Under this system,a structural word would just be the entry point into a rich representation which might ormight not observe that bit of structure further, being built instead around the way theword works in combination with others.
All lexical categories, not just the prototypicalfunctors, would have combinatorial preferences, as McCawley (1986) suggests.
Semanticcomposition of functor and arguments in a sentence then would consist of unifying theserepresentations in the most cognitively perspicuous way, in accordance with the boundsimposed by syntax, to create a single coherent scene.
In the process of unification, gapswould be filled, construals or views chosen in accordance with constraints instantiated byspecific lexical interactions and general cognitive conventions, and aspects of the unifyingrepresentations which are contradictory or not currently in focus would fall away.
A newsemantic ombination constructed in this way would be acceptable in inverse proportionto the strain which constructing it placed on these general conventions, and selectionalrestrictions would simply be statistically based thresholds of strain.Various efforts have been made toward constructing a realistic version of this ambitiousmodel.
Raskin (1986) discusses a system he calls Script-based Semantics, which capturesmany of the ideas described above.
In his system, the appearance of a word in a sentencebeing analyzed evokes both a set of syntactic senses and one or more scripts, where a scriptis defined as "a large chunk of semantic information surrounding the word or evoked byit .
.
.
a cognitive structure internalized by the native speaker, \[representing\] knowledge ofa small part of the world.
~ These scripts are represented as graphs with lexical nodes andsemantic relation arcs, thus bearing a strong resemblance to tile Conceptual Structures ofSowa (1984).
Under both schemes a set of combinatorial rules then unifies graphs in sucha way that ambiguity between word senses disappears and a complete representation fora sentence is constructed.What is not clear in either system is where the graphs for individual word senses comefrom.
If we were to use the dictionary definitions given earlier as a source of the script(s)134or graph(s) for the verb introduce, in combination with the following entries for product,we would still have absolutely no basis for combining them to mean what we want themto mean in examples (5) and (6).
,(from OALD3E)7296955, .
.<ent h=product><def>sth produced (by nature or by man)</def><ex>farm produces</ex><ex>the ch ie f  products  of  Scot land</ex><def>(maths) quant i ty  obta ined by mult ip l i ca t ion</def><def>(chem) substance obta ined by chemical  react ion</def>?
?
?
,  ?
?
(from CED Fact Base)c_DEF(\[ "product", 1, I ,  1 \]c_DEF(\[ "product",l,l,2 \]c_DEF( \[ "product", 1,1 ,3  \]c_DEF( \[ "product", 1,1,4 \]c_DEF( \[ "product", 1,1,4, 1c_DEF ( \[ "product", 1,1,4,2. , .
, ,  ., "something produced by effort, orsome mechanical or industrial process" , 1 )., "the result of some natural process" ,I )., "a result or consequence" ,I )., "a substance formed in a chemicalreaction" ,1 ).\], "the result of the mult ipl ication of?.o or more numbers, quantities, etc" ,I ).\], "another name for intersection ,1,3" ,I ).Hand-building such scripts on a sense-by-sense basis for large-scale fforts, on theother hand, would require not only massive redundancy but also an enormous amount ofsophisticated-yet-tedious lexical analysis.
Not only that, but by pegging these scripts orconceptual graphs to individual word senses, we return to the problem of knowing how toarbitrarily distinguish those senses.The most promising answer to many of these problems can be found in the approachof collocational semantics, in which the "meaning" of a word consists of sets of observeduses of that word.
Practitioners of this approach ave sometimes apologized for it (see forinstance Pazienza and Velardi, 1989) as a practical but unprincipled substitute for "real"semantics.
But in fact the collection, classification, and normalization of collocational pat-terns may constitute the most realistic way to sidestep various serious difficulties involvedin other methods of building a large inventory of lexical knowledge; it might be the way todiscover what it "means" for a text to talk about the introducing aproduct.
Collocationalsemantics provides an objective methodology for detecting functional equivalence betweenexpressions, which is really what we want when we attempt o encode synonymy, and forpinpointing functional distinctions, which is what we are trying to do when we attemptto distinguish word sense meanings.
Above all, this approach offers promise preciselybecause it does not depend crucially on the level of the structural word for the definitionof semantic units.As for the operation of a language-understanding system including such a lexical store,obviously allowing the appearance of each word in a sentence to immediately evoke ev-erything we know about its uses would be a computational nightmare.
Instead, what isrequired is a mechanism for incrementally introducing lexical information into sentence135analysis in such a way that just enough, and no more, is present at any given stage, afterthe fashion of ltirst (1987) with his "Polaroid" words.
Under such a methodology severalstages of lexical lookup would be necessary.
Syntactic patterns, morphologically-derivedcategorizations, and minimal thematic structure, as discussed above, might be availablein the usual way at the first level.
The matching of partially disambiguated, intermediatesentence graphs constructed at the first level against an inventory of highly schematic"entry" graphs, and then against successive layers of more completely specified modelgraphs, would funnel the analysis through the next levels of lookup.
9 A system of seman-tic classes, along with a set of redundancy rules encoding systematic relations betweenand alternations possible for members of those classes, would constitute yet another levelof lexical knowledge; this level would be important in the organization of the lexicon, butmight be accessed irectly during lookup only ill the case of the appearance ofpreviouslyunknown words.How would we construct this kind of multi-level lexical knowledge base?
The first leveldescribed above can and should be constructed on the basis of information from machine-readable dictionaries.
The next set of lookup levels described, in which the structural wordis no longer the primary entity but just a handle for indexing raphs, can be built on thebasis of knowledge assembled from large volumes of text; the methodology employed bySmadja and McKeown (1990) would be one of many techniques possible for obtainingand organizing the various levels of model graphs.
Our idea is that the graded levels ofspecificity would be constructed by means of successive generalization perations over themost specific set of graphs, extracted more or less directly from text.
The final level ofknowledge--that is, what Boguraev and Levin have found missing from standard LKBs--must still be installed by linguists.
Dictionaries do contain some clues, and certainly theability to systematically and automatically pull together usage instances can help; butthe real work must be done by people.6 Conc lus ionThe notion that the level of "the word", as structurally defined, is the appropriate startingplace for semantic representation, has been implicit in the design of most lexical knowledgebases.
The use of machine-readable dictionaries as a source of lexical knowledge reinforcesthis notion, at the cost of considerable descriptive loss.
This discussion has also revealed afurther detrimental ssumption fostered by the use of the dictionary as the model for thelexicon: the idea that a single store of lexical knowledge, with a single lookup function anda unified structure, is the necessary mechanism for bringing word meanings into sentenceanalysis.
On the contrary, the multi-faceted semantic behavior of the structural units wecall "words" requires a base of knowledge consisting of multiple knowledge stores, eachorganized in a way that is appropriate to the knowledge it contains and the stage ofprocessing to which it must contribute.
The structural word can be used as the indexingkey which relates the separate, differently structured stores, but it need not be the basiccurrency of all of "lexical" semantics.9 One or more of these levels of distillation would presumably be equivalent to the single set of canonicalgraphs envisioned by Sowa (1984).136References\[1\] Boguraev, Bran, and Beth Levin (1990).
"Models for lexical knowledge bases", inElectronic Text Research: Proceedings of the Sixth Annual Conference of the UWCentre for the New OED, pp.
65-78.\[2\] Carlson, G. (1984).
"Thematic roles and their role in semantic interpretation", Lin-guistics 22, pp.
259-279.\[3\] Carlson, G. and Tanenhaus, M. (1988).
"Thematic roles and language comprehen-sion', in Wilkins, W.
(ed.
), Syntax and Semantics 21: Thematic Relations.
San Diego,CA: Academic Press, pp.
263-291.\[4\] CED Proiog Fact Base, extracted from the Collins English Dictionary (edited byPatrick Hanks, 1979) by E. A.
Fox and R. France, 1987.\[5\] Cruse, D. A.
(1986).
Lexical Semantics.
Cambridge, England: Cambridge UniversityPress.\[6\] Hirst, G. (1987).
Semantic Interpretation and the Resolution of Ambiguity.
Cam-bridge, England: Cambridge University Press.\[7\] Jackendoff, Ray (1983).
Semantics and Cognition.
Cambridge, MA: MIT Press.\[8\] Jackendoff, Ray (1990).
Semantic Structure.
Cambridge, MA: MIT Press.\[9\] McCawley, James D. (1986).
"What linguists might contribute to dictionary makingif they could get their act together", in Bjarkman, Peter, and Victor Raskin (eds.
),The Real-World Linguist.
Norwood, NJ: Ablex Publishing Co., pp.
3-18.\[10\] Pet, Mario A., and F. Gaynor (1980).
A Dictionary of Linguistics.
Totowa, NJ:Littlefield, Adams & Co.\[11\] Raskin, Victor (1986).
"On possible applications of Script-based Semantics", in Bjark-man, Peter, and Victor Raskin (eds.
), The Real-World Linguist.
Norwood, N J: AblexPublishing Co., pp.
19-45.\[12\] Roeper, T., and M. E. A. Siegel (1978).
"A lexical transformation for verbal com-pounds", Linguistic Inquiry 9, pp.
199-260.\[13\] Selkirk, E. O.
(1982).
The Syntax of Words.
Cambridge, MA: MIT Press.\[14\] OALD3e.
Electronic version of the Oxford Advanced Learner's Dictionary of CurrentEnglish, edited by A. S. Hornby (1974), prepared in electronic form by F. W. Tompaand Oxford University Press.\[15\] Sowa, J. F., 1984.
Conceptual Structures: h~formation Processing in Mind and Ma-chine.
Reading, MA: Addison-Wesley Publishing Co.\[16\] Smadja, F. A., and K. McKeown (1990).
"Automatically Extracting and RepresentingCollocations for Language Generation", Proceedings of the 28th Annual Meeting ofthe ACL, pp.
252-259.137\[17\] Talmy, Leonard (1985).
"Lexicalization patterns: Semantic structure in lexicalforms", in Shopen, Timothy (ed.
), Language Typology and Syntactic Description:Grammatical Categories and the Lexicon.
Cambridge, England: Cambridge Univer-sity Press, pp.
57-149.\[18\] Velardi, P. and M. T. Pazienza (1989).
"Computer aided interpretation of lexicalcooccurrences", in Proceedings of the 27th Annual Meeting off the Association forComputational Linguistics, pp.
185-192.\[19\] Wilkins, Wendy (1988).
Syntax and Semantics 21: Thematic Relations.
San Diego,CA: Academic Press.138
