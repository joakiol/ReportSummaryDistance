SYNTACTIC AND SEMANTIC PARSABILITYGeoffrey K. PullumSyntax Research Center, Cowell College, UCSC, Santa Cruz, CA 95064andCenter for the Study of Language and Information, Stanford, CA 94305ABSTRACTThis paper surveys some issues that arise in thestudy of the syntax and semantics of naturallanguages (NL's) and have potential relevance tothe automatic recognition, parsing, and translationof NL's.
An attempt is made to take into accountthe fact that parsing is scarcely ever thoughtabout with reference to syntax alone; semanticulterior motives always underly the assignment of asyntactic structure to a sentence.
First I con-sider the state of the art with respect to argu-ments about the language-theoretic complexity ofNL's: whether NL's are regular sets, deterministicCFL's, CFL's, or whatever.
While English stillappears to be a CFL as far as I can tell, new argu-ments (some not yet published) appear to show forthe first time that some languages are not CFL's.Next I consider the question of how semanticfiltering affects the power of grammars.
Then Iturn to a brief consideration of some syntacticproposals that employ more or less modest exten-sions of the power of context-free gr-mm-rs.I.
INTRODUCTIONParsing as standardly defined is a purely syn-tactic matter.
Dictionaries describe parsing asanalysing a sentence into its elements, or exhibit-ing the parts of speech composing the sentence andtheir relation to each other in terms of governmentand agreement.
But in practice, as soon as parsinga natural language (NL) is under discussion, peopleask for much more than that.
Let us distinguishthree kinds of algorithm operating on strings ofwords:recognit ionoutput: a decision concerning whether thestring is a member of the language or notparsingoutput: a syntactic analysis of the string(or an error message if the string is notin the language)translationoutput: a translation (or set of transla-tions) of the string into some language ofsemantic representation (or an error mes-sage if the string is not in the language)Much potential confusion will be avoided if we arecareful to use these terms as defined.
However,further refinement is needed.
What constitutes a"syntactic analysis of the string" in the defin-ition of parsing?
In applications development workand when modeling the whole of the native speaker'sknowledge of the relevant part of the language, wewant ambiguous sentences to be repesented as such,and we want Time flies like an arrow to be mappedonto a whole list of different structures.
Forrapid access to a database or other back-end systemin an actual application, or for modeling aspeaker's performance in a conversational context,we will prefer a program that yields one syntacticdescription in response to a given string presenta-tion.
Thus we need to refer to two kinds of algo-rithm:all-paths parseroutput: a list of all structural descriptionsof the string that the grammar defines (oran error message if the string is not inthe language)one-path parseroutput: one structural description that thegrammar defines for the string (or an errormessage if the string is not in thelanguage)By ana logy,  we w i l l  occas iona l ly  want to ta lk  ofa l l -paths  or one-path  recogn izers  and t rans la torsas we l l .There i s  a c ruc ia l  connect ion  between the theoryof parsing and the theory of languages.
There isno parsing without a definition of the language tobe parsed .
This  should  be c lear  enough from thel i te ra ture  on the de f in i t ion  and pars ing  of  p ro -gramming languages, but for some reason it is occa-sionally denied in the context of the much largerand richer multi-purpose languages spoken byhumans.
I frankly cannot discern a sensibleinterpretation of the claims made by some artifi-cial intelligence researchers about parsing a NLwithout having a defined syntax for it.
Assumethat some program P produces finite, meaningfulresponses to sentences from some NL ~ over someterminal vocabulary T, producing error messages ofsome sort in response to nonsentences.
It seems tome that automatically we have a generative grammarfor 2.
Moreover, since ~ is clearly recursive, wecan even enumerate the sentences of L in canonicalorder.
One algorithm to do this simply enumeratesthe strings over the terminal vocabulary in orderof increasing length and in alphabetical orderwithin a given string-length, and for each one,tests it for grammaticality using P, and adds it tothe output if no error message is returned.Given that parsability is thus connected todefinability, it has become standard not only forparser-designers to pay attention to the grammarfor the language they are trying to parse, but also112for linguists to give some thought to the parsabil-ity claims entailed by their linguistic theory.This is all to the good, since it would hardly besensible for the study of NL's to proceed for everin isolation from the study of ways in which theycan be used by finite organisms.Since 1978, following suggestions by StanleyPeters, Aravind Joshi, and others, developed mostnotably  in the work of Gerald Gazdar, there  hasbeen a strong resurgence of the idea that context-free phrase structure grammars could be used forthe description of NL's.
A significant motivationfor the original suggestions was the existence ofalready known high-efficiency algorithms (recogni-tion in deterministic time proportional to the cubeof the string length) for recognizing and parsingcontext-free languages (CFL's).This was not, however, the motivation for theinterest that signficant numbers of linguists beganto show in context-free phrase structure grammars(CF-PSG's) from early 1979.
Their motivation wasin nearly all cases an interest sparked by thee legant  so lu t ions  to pure ly  l ingu is t i c  problemsthat Gazdar and others began to put forward invarious articles, initially unpublished workingpapers.
We have now seen nearly half a decade ofwork using CF-PSG to successfully tackle problemsin linguistic description (the Coordinate StructureConstraint (Gazdar 1981e), the English auxiliarysystem (Gazdar et al 1982), etc.)
that had provedsomewhat recalcitrant even for the grossly morepowerful t rans format iona l  theor ies  of grn~---r thathad formerly dominated linguistics.
The influenceof the parsing argument on linguists has probablybeen overestimated.
It seems to me that when Gaz-dar (1981b, 267) saysour grammars can be shown to be formal lyequ iva lent  o what are known as the context - f reephrase s t ruc ture  grammars \[which\] has the e f fec tof making potentially relevant to naturallanguage grammars a whole literature ofmathematical results on the parsability andlearnability of context-free phrase structuregrammarshe is making a point exactly analogous to the onemade by Robert Nozick in his book Anarchy, Stateand Utonia,  when he says of a proposed soc ia lorganization (1974, 302):We seem to have e realization of the economists"model of a competitive market.
This is mostwelcome, for it gives us immediate access to apowerful, e laborate ,  and soph is t i ca ted  body oftheory and ana lys i s .We are surely not to conclude from this remark ofNozick's that his libertarian utopia of interestgroups competing for members is motivated solely bya desire to have a society that functions like acompetitive market.
The point is one of serendi-pity: if a useful theory turns out to be equivalentto one that enjoys a rich technical literature,that is very fortunate, because we may be able tomake use of some of the results therein.The idea of returning to CF-PSC as e theory ofNL's looks re t rogress ive  unt i l  one rea l i zes  thatthe arguments that  had led l ingu is ts  to consignCF-PSG's to the scrap-heap of h i s to ry  can be shownto be fa l lac ious  (c f .
espec ia l l y  Pullom and Gazdar(1982)).
In view of that  development,  I th ink  i twould be reasonable  for  someone to ask whether wecould not re turn  a l l  the way to f in i te -s ta te  gram-mars, whichwould give us even more efficient pars-ing (guaranteed deterministic linear time).
It maytherefore be useful if I briefly reconsider thisquestion, first dealt with by Chomsky nearly thirtyyears  ago.2.
COULD NL'S BE REGULAR SETS?Chomsky's negative answer to this question was thecorrect one.
Although his original argument inSyntactic Structures (1957) for the non-regularcharacter of English was not given in anything likea valid form (cf.
Daly 1974 for a critique), otherscan be given.
Consider the following, patterneda f te r  a suggest ion  by Brandt Corst ius  (see Levelt1974, 25-26).
The set  (1):(I) {a white male (whom a white male) n (hired) nhired another white male I n ~ 0}i s  the in tersect ion  of Engl ish  wi th  the regu lar  set& whi;e male (.whom a white male)* h i red*  anotherwhite male.
But (1) i s  not regu lar ,  yet  the regu-la r  se ts  are c losed under in tersect ion ;  henceEngl ish i s  not regu lar .
Q.E.D.I t  is  per fec t ly  poss ib le  that  some NL's happennot to p resent  the inherent ly  se l f -embedding conf i -gurat ions  that  make a language non- regu lar .Languages in which paratax is  i s  used much more thanhypotaxis  ( i .e .
languages in which separate  c lausesare strung out l inear ly  ra ther  than embedded) arenot at a l l  uncommon.
However, i t  should not bethought that  non- regu lar  conf igurat ions  w i l l  befound to be rare  in languages of the world.
Thereare l i ke ly  to be many languages that  fu rn i sh  bet terarguments for  non- regu lar  character  than Engl ishdoes; for  example, according to Bag~ge (1976),center-embedding seems to be commoner and moreacceptab le  in severa l  Central  Sudanic languagesthan i t  i s  in Eng l i sh .
In Moru, we f ind  examplessuch as th i s  ( s l ight ly  s imp l i f ied  from Ha~ege(1976, 200); X i  i s  the possess ion  marker fo r  nonhu-man nouns, and ro  i s  the equ iva lent  fo r  humannouns):(2) kokyE \[toko \[odrupi  \[ma ro\] ro\] r i \ ]  d ra te1 2 3 3 2 1dog wife b ro ther  me of of of i s -dead"My brother's chief wife's black dog is dead.
"The center-embedding word order here is the onlyone allowed; the alternative right-branching order("dog chief-wife-of brother-of me-of"), which aregu lar  grammar could handle ,  i s  ungrammatical .Presumably, the in tersect ion  of odrupi*  ma to*drate  with Moru isn n{odrupi ma ro drate  \[ n > 0}(an in f in i te  set  of sentences  wi th  meanings l ike'~y brother ' s  b ro ther ' s  b ro ther  i s  dead" where n -3).
This c lear ly  non- regu lar ,  hence so is  Moru.113The fac t  that  NL's are  not regu lar  does notnecessar i l y  mean that  techn iques  for  pars ing  regu-la r  languages are  i r re levant  to NL pars ing .Langendoen (1975) and Church (1980) have both ,  inra ther  d i f fe rent  ways, proposed that  hearers  pro -cess  sentences  as i f  they were f in i te  automata (oras i f  they were pushdown automata w i th  a f in i tes tack  depth l im i t ,  which i s  weakly equ iva lent )ra ther  than showing the behav ior  that  would becharacter i s t i c  of a more powerful  dev ice .
To theextent  that  p rogress  a long these  l ines  cas ts  l ighton the human pars ing  ab i l i ty ,  the theory  of regu la rgrammars and finite automata will continue to beimportant in the study of natural languages eventhough they are not regular sets.The fact that NL's are not regular sets is bothsurprising and disappointing from the standpoint ofparsahility.
It is surprising because there is nosimpler way to obtain infinite languages than toadmit union, concatenation, and Kleene closure onfinite vocabularies, and there is no apparentpriori reason why humans could not have been wellserved by regular languages.
Expressibility con-siderations, for example, do not appear to berelevant: there is no reason why a regular languagecould not express any proposition expressible by asentence of any finite-string-length language.Indeed, many languages provide ways of expressingsentences with self-ambedding structure in non-self-embedding ways as well.
In an SOV languagelike Korean, for example, sentences with the tree-structure (3a) are also expressible with left-branching tree-structure as shown in (3b).(3)a.
S b. S/I\ /J\I I \  I I \I I  \ I I \NP S V S NP V/1\  /1 \/ I \  / I \/ I \ / I \NP S V S NP V zlxClear ly  such s t ruc tura l  rear rangement  w i l l  nota l te r  the capac i ty  of a language to express  propo-s i t ions ,  any more than an optimizing compiler makescertain programs inexpressible when it irons outtrue recursion into tail recursion wherever possi-ble.If NL's were regular sets, we know we couldrecognize them in deterministic linear time usingthe fastest and simplest abstract computing devicesof all, finite state machines.
However, there aremuch larger classes of languages that have lineartime recognition.
One such class is the deter-ministic context-free languages (DCFL's).
It mightbe reasonable, therefore~ to raise the questiondealt with in the following section.3.
COULD NL'S BE DCFL'S?To the best  of my knowledge, th i s  quest ion  hasnever  p rev ious ly  been ra i sed ,  much less  answered,in the literature of linguistics or computer sci-ence.
Rich (1983) is not atypical in dismissingthe entire literature on DCFL's without a glance onthe basis of an invalid argument which is supposedto show that English is not even a CFL, hencefortiori not a DCFL.I should make it clear that the DCFL's are notjust those CFL's for which someone has written aparser that is in some way deterministic.
They arethe CFL's that are accepted by some deterministicpushdown stack automaton.
The term "deterministicparsing" is used in many different ways (cf.
Marcus(1980) for an attempt to motivate a definition ofdeterminism specifically for the parsing of NL's).For example, a translator system with a post-processor to rank quantifier-scope ambiguities forplausibility and output only the highest-rankedtranslation might be described as deterministic,but there is no reason why the language it recog-nizes should he a DCFL; it might be any recursivelanguage.
The parser currently being implementedby the natural language team at HP Labs (in partic-ular, by Derek Proudian and Dan Flickinger) intro-duces an interesting compromise between determinismand nondeterminism in that it ranks paths throughthe rule system so as to make some structural pos-sibilities highly unlikely ones, and there is atoggle that can be set to force the output to con-tain only likely parses.
When this option isselected, the parser runs faster, but can stillshow ambiguities when both readings are defined aslikely.
This is an intriguing development, butagain is irrelevant to the language-theoretic ques-tion about DCFL status that I am raising.It would be an easy slip to assume that NL'scannot be DCFL's on the grounds that English iswell known to be ambiguous.
We need to distinguishcarefully between ambiguity and inherent ambiguity.An inherently ambiguous language is one such thatall of the gra~mrs  that weakly generate it areambiguous.
LR gr-----rs are never ambiguous; butthe LR grammars characterize exactly the set ofDCFL's, hence no inherently ambiguous language is aDCFL.
But it has never been argued, as far as Iknow, that English as a stringset is inherentlyambiguous.
Rather, it has been argued that adescriptively adequate grammar for it should, toaccount for semantic intuitions, be ambiguous.
Butobviously, a DCFL can have an ambiguous grammar.In fact, all languages have ambiguous grazmnars.
(The proof is trivial.
Let w be a string in alanguage ~ generated by a grannnar G with initialsymbol S and production set P. Let B be a nonter-minal not used by G. Construct a new grammar G"with production set P" = E U {S --> B, B --> w}.G" is an ambiguous grannuar that assigns two struc-tural descriptions to w.)The relevance of this becomes clear when weobserve that in natural language processing appli-cations it is often taken to be desirable that aparser or translator should yield just a singleanalysis of an input sentence.
One can imagine animpemented natural language processing system inwhich the language accepted is described by anambiguous CF-PSG but is nonetheless (weakly) aDCFL.
When access to all possible analyses of aninput is desired (say, in development work, or whenone  wants to take no risks in using a databasefront end), an all-paths parser~translator is used,but when quick-and-dirty responses are required, atthe risk of missing certain potential parses of114ambiguous strings, this is replaced by a deter-ministic one-path parser.
Despite the differencein results, the language analyzed and the grammarused could be the same.The idea of a deterministic parser with an ambi-guous grammar, which arises directly out of whathas been done for programming languages in, forexample, the Yacc system (Johnson 1978), isexplored for natural languages in work by FernandoPereira and Stuart Shieber.
Shieber (1983)describes an implementation of a parser which usesan ambiguous grammar but parses deterministically.The parser uses shift-reduce scheduling in themanner proposed by Pereira (1984).
Shieber (1983,i16) gives two rules for resolving conflictsbetween parsing actions:(I) Resolve shift-reduce conflicts by shifting.
(If) Resolve reduce-reduce conflicts by performingthe longer reduction.The first of these is exactly the same as the onegiven for Yacc by Johnson (1978, 13).
The secondis more principled than the corresponding Yaccrule, which simply says that a rule listed earlierin the grammar should take precedence over a rulelisted later to resolve a reduce-reduce conflict:But it is particularly interesting that the two arein practice equivalent in all sensible cases, forreasons I will briefly explain.A reduce-reduce conflict arises when a string ofcategories on the stack appears on the right handside of two different rules in the grammar.
If oneof the reducible sequences is longer than theother, it must properly include the other.
But inthat case the prior application of the properlyincluding rule is mandated by an extension intoparsing theory of the familiar rule interactionprinciple of Proper Inclusion Precedence, due ori-ginally to the ancient Indian grammarian Panini(see Pullum 1979, 81-86 for discussion and refer-ences).
Thus, if a rule N_~P --> NP PP were orderedbefore a rule VP --> V NP PP in the list accessedby the parser, it would be impossible for thesequence "NP PP" ever to appear in a VP, since itwould always be reduced to NP by the earlier rule;the VP rule is useless, and could have been leftout of the grammar.
But if the rule with the prop-erly including expansion "V NP PP" is orderedfirst, the NP rule is not useless.
A string "V NPPP PP", for example, could in principle be reducedto "V NP PP" by the NP rule and then to "VP" by theVP rule.
Under a principle of rule interactionmade explicit in the practice of linguists, there-fore, the proposal made by Pereira and Shieber canbe seen to be largely equivalent to the cruder Yaccresolution procedure for deterministic parsing withambiguous grammars.Techniques straight out of programming languageand compiler design may, therefore, be of consider-able interest in the context of natural languageprocessing applications.
Indeed, Shieber goes sofar as to suggest psycholinguistic implications.He considers the,class of "garden-path sentences"such as those in (4).
(4) The diners hurried through their meal wereannoyed.That shaggy-looking sheep should be sheared isimportant.On these, his parser fails.
Strictly speaking,therefore, they indicate that the language parsedis not the same under the one-path and the all-paths parsers.
But interestingly, human beings areprone to fail just as badly as Shieber's parser onsentences such as these.
The trouble with thesecases is that they lack the prefix property---thatis, they have an initial proper substring which isa sentence.
(From this we know that English doesnot have an LR(0) grammar, incidentally.)
Englishspeakers tend to mis-parse the prefix as a sen-tence, and baulk at the remaining portion of thestring.
We might think of characterizing thenotion "garden-path sentence" in a rigorous andnon-psychological way in terms of an all-pathsparser and a deterministic one-path parser for thegiven language: the garden path sentences are justthose that parse under the former but fail underthe latter.To say that there might be an appropriate deter-ministic parser for English that fails on certainsentences, thus defining them as garden-path sen-tences, is not to deny the existence of a deter-ministic pushdown automaton that accepts the wholeof English, garden-path sentences included, it isan open question, as far as I can see, whetherEnglish as a whole is weakly a DCFL.
The likeli-hood that the answer is positive is increased bythe results of Bermudez (1984) concerning theremarkable power and richness of many classes ofdeterministic parsers for subsets of the CFL's.If the answer were indeed positive, we wouldhave some interesting corollaries.
To take justone example, the intersection between two dialectsof English that were both DCFL's would itself be aDCFL (since the DCFL's are closed under intersec-tion).
This seems right: if your dialect and mineshare enough for us to communicate without hin-drance, and both our dialects are DCFL's, it wouldbe peculiar indeed if our shared set of mutuallyagreed-upon sentences was not a DCFL.
Yet with theCFL's in general we do not have such a result.Claiming merely that English dialects are CFL'swould not rule out the strange situation of havinga pa i r  of  d ia lec ts ,  both CFL's, such that  theintersection is not a CFL.4.
ARE ALL NL'S CFL'S?More than a quarter-century of mistaken effortshave attempted to show that not all NL's are CFL's.This history is carefully reviewed by Pullum andGazdar (1982).
But there is no reason why futureattempts should continue this record Of failure.It is perfectly clear what sorts of data from a NLwould show it to be outside the class of CFL's.
Forexample, an infinite intersection with a regularset having the form of a triple-counting languageor a string matching language (Pullum 1983) wouldsuffice.
However, the new arguments for non-115context-freeness of English that have appearedbetween 1982 and the present all seem to be quitewide of the  mark.Manaster-Ramer (1983) points to the contemptuousreduplication pattern of Yiddish-influencedEnglish, and suggests that it instantiates aninfinite string matching language.
But does ourability to construct phrases like Manaster-RamerSchmanaster-Ramer (and analogously for any otherword or phrase) really indicate that the syntax ofEnglish constrains the process?
I do not think so.Manaster-Ramer is missihg the distinction betweenthe structure of a language and the culture of ver-bal play associated with it.
I can speak in rhym-ing couplets, or with adjacent word-pairs deli-berately Spoonerized, or solely in sentences havingan even number of words, if I wish.
The structureof my language allows for such games, but does notlegislate regarding them.Higginbotham (1984) presents a complex pumping-lemma argument on the basis of the alleged factthat sentences containing the construction a N suchthat S always contain an anaphoric pronoun withinthe clause S that is in syntactic agreement withthe noun N. But his claim is false.
Consider aphrase like any society such that more people getdivorced than get married in an average 7ear.
Thisis perfectly grammmtical, but has no overt ana-phoric pronoun in the such that clause.
(A similarex-mple is concealed elsewhere in the text of thispaper .
)Langendoen and Posta l  (1984) cons ider  sentencesl i ke  Joe was ta lk ing  about some bourbon- lover ,  butWHICH bourbon- lover  i s  unknown, and argue that  acompound noun of any length  can rep lace  the f i r s toccur rence  of bourbon- lover  prov ided the sames t r ing  i s  subst i tu ted  fo r  the second occur rence  aswe l l .
They c la im that  th i s  y ie lds  an in f in i tes t r ing  match ing language ext rac tab le  from Eng l i shthrough in tersect ion  wi th  a regu lar  se t .
gut th i sargument presupposes  that  the  e l l ips i s  in  WHICHbourbon- lover  \[Joe was ta lk ing  about\]  must f ind  i t santecedent  in the cur rent  sentence .
Th is  i s  notso.
L ingu is t i c  accounts  of anaphora have o f tenbeen over ly  f i xa ted  on the in t rasentent ia l  syntac -t i c  cond i t ions  on antecedent -anaphor  pa i r ings .Ar t i f i c ia l  in te l l igence  researchers ,  on the o therhand, have concent ra ted  more on the reso lu t ion  ofanaphora within the larger context of thediscourse.
The latter emphasis is more likely tobring to our attention that ellipsis in one sen-tence can have its resolution through material in apreceding one.
Consider the following exchange:(5)A: It looks like they're going to appointanother bourbon-hater as Chair of theLiquor Purchasing Committee.B: Yes--even though Joe nominated somebourbon- lovers ;  but WHICH bourbon-hateris still unknown.It is possible for the expression WHICH bourbon-hater in B's utterance to be understood as WHICHbourbon-hater \[they're ~oinR to appoint\] despitethe presence in the same sentence of a mention ofbourbon-lovers.
There is thus no reason to believethat Langendoen and Postal's crucial example typeis syntactically constrained to take its antecedentfrom within its own sentence, even though that isthe only interpretation that would occur to thereader when judging the sentence in isolation.Nothing known to me so far, therefore, suggeststhat English is syntactically other than a CFL;indeed, I know of no reason to think it is not adeterministic CFL.
As far as engineering is con-cerned, this means that workers in natural languageprocessing and artificial intelligence should notoverlook (as they generally do at the moment) thepossibilities inherent in the technology that hasbeen independently developed for the computer pro-cessing of CFL's, or the mathematical results con-cerning their structures and properties.From the theoretical standpoint, however, a dif-ferent issue arises: is the oontext-free-ness ofEng l i sh  jus t  an acc ident ,  much l i ke  the acc ident  i twould be i f  we found that  Chinese was regu lar?
Arethere  o ther  languages  that  genu ine ly  show non-context - f ree  proper t ies?
I devote the next  sec t ionto th i s  quest ion ,  because  some very  impor tantresu l t s  bear ing  on i t  have been repor ted  recent ly .Since these  resu l t s  have not  yet  been pub l i shed ,  lw i l l  have to st~anarize them ra ther  abst rac t ly ,  andcite forthcoming or in-preparation papers- forfurther deta i l s .5.
NON-CONTEXT-FREENESS IN NATURAL LANGUAGESSome remarkable facts recently reported byChristopher Culy suggest that the African languageBambara (Mande family, spoken in Senegal, Mali, andUpper Volta by over a million speakers) may be anon-CYL.
Culy notes that Bambara forms from nounstems compound words of the form '~oun-~-Noun" withthe meaning "whatever N".
Thus, given that wulumeans "dog", wulu-o-wulu means "whatever dog."
Hethen observes that Bambara also forms compound nounstems of arbitrary length; wulu-filela means "dog-watcher," wulu-nyinila means "dog-hunter," wulu-filela-nyinila means "dog-watcher-hunter," and soon.
From this it is clear that arbitrarily longwords like wulu-filela-nyinila-o-wulu-filela-nyinila "whatever dog-watcher-hunter '~ will be inthe language.
This is a realization of a hypothet-ical situation sketched by Langendoen (1981), inwhich reduplication applies to a class of stemsthat have no upper length bound.
Culy (forthcom-ing) attempts to provide a formal demonstrationthat this phenomenon renders Bambara non-context-free.If gambara turns out to have a reduplicationrule defined on strings of potentially unboundedlength, then so might other languages.
It would bereasonable, therefore, to investigate the case ofEngenni (another African language, in the Kwa fam-ily, spoken in Rivers State, Nigeria by about12,000 people).
Carlson (1983), citing Thomas(1978), notes that Engenni is reported to have aphrasal reduplication construction: the finalphrase of the clause is reduplicated to indicate"secondary aspect."
Carlson is correct in notingthat if there is no grammatical upper bound to thelength of a phrase that may be reduplicated, thereis a strong possibility that Engenni could be shownto be a non-CFL.116But it is not only African languages in whichrelevant evidence is being turned up.
Swiss Germanmay be another case.
In Swiss German, there isevidence of a pattern of word order in subordinateinfinitival clauses that is very similar to thatobserved in Dutch.
Dutch shows a pattern in whichan arbitrary number of noun phrases (NP's) may befollowed by a finite verb and an arbitrary numberof nonfinite verbs, and the semantic relationsbetween them exhibit a crossed serial pattern---i.e.
verbs further to the right in the string ofverbs take as their objects NP's further to theright in the string of NP's.
Bresnan et al (1982)have shown that a CF-PSG could not assign such aset of dependencies syntactically, but as Pull,-,and Gazdar (1982, section 5) show, this does notmake the stringset non-context-free.
It is asemantic problem rather than a syntactic one.
InSwiss German, however, there is a wrinkle thatrenders the phenomenon syntactic: certain verbsdemand dative rather than accusative case on theirobjects, as a matter of pure syntax.
This patternwill in general not be one that a CF-PSC candescribe.
For example, if there are two verbsand ~" and two nouns ~ and n', the set{xv I ~ is in (n, n')* and ~ is in (v, v')* andfor all ~, if the i'th member of x is n the i'thmember of y is ~}is not a CFL.
Shieber (1984) has gathered datafrom Swiss German to support a rigorously formu-lated argument along these lines that the languageis indeed not a CFL because of this construction.It is possible that other languages will haveproperties that render them non-context-free.
Onecase discussed in 1981 in unpublished work byElisabet Eugdahl and Annie Zaenen concerns Swedish.In Swedish, there are three grammatical genders,and adjectives agree in gender with the noun theydescribe.
Consider the possibility of a"respectively"-sentence with a meaning like '~heNI, N2, and N3 are respectively AI, A2, and A3,"where NI, N2, and N3 have different genders end AI,A2, and A3 are required to agree with theircorresponding nouns in gender.
If the genderagreement were t ru ly  a syntact i c  matter  (c~ntraPullum and Gazdar (1982, 500-501, note 12)) ,  therecould be an argument o be made that  Swedish (orany language with these  sor t  of fac ts )  was not aCFL.It is worth noting that arguments based on theabove sets of facts have not yet been published forgenera l  scho la r ly  sc rut iny .
Nonethe less ,  what Ihave seen convinces me that i t  is now very likelythat we shall soon see a sound published demonstra-tion that some natural language is non-context-f ree .
I t  is  time to cons ider  care fu l ly  what theimplications are if this is true.6.
CONTEXT-FREE GRAMMARS AND SEMANTIC FILTERINGWhat sort of expressive power do we obtain byallowing the definition of a language to be givenjointly by the syntax and the semantics rather thanjust by the syntax, so that the syntactic rules cangenerate strings judged ill-formed by native speak-ers provided that the semantic rules are unable toassign interpretations to them?This idea may seem to have a long history, inview of the fact that generative gr-mmsriansengaged in much feuding in the seventies over ther iva l  mer i t s  of gr - - , -a rs  that  le t  "semant ic"  fac -to rs  const ra in  syntact i c  ru les  and grammars thatd isa l lowed th i s  but al lowed " in terpret ive  ru les"  tof i l te r  the output of the syntax.
But in fac t ,  thes ter i le  d i sputes  of those days were based on a useof the term "semantic" that bore little relation toits original or current senses.
Rules thatoperated purely on representations of sentencestructure were called "semantic" virtually at whim,despite matching perfectly the normal definition of"syntactic" in that they concerned relations hold-ing among linguistic signs.
The disputes werereally about differently ornamented models of syn-tax.What I mean by semantic filtering my be illus-trated by reference to the analysis of expletiveNP's like there in Sag (1982).
It is generallytaken to be a matter of syntax that the dummy pro-noun subject there can appear as the subject insentences like There are some knives in the drawerbut not in strings like *There broke all existin2records.
Sag simply allows the syntax to generatestructures for strings like the latter.
He charac-terizes them as deviant by assigning to there adenotation (namely, an identity function on propo-sitions) that does not allow it to combine with thet rans la t ion  of ord inary  VP's l i ke  broke a l l  ex~st -in~ records .
The VP aye ~pme knives in  the draweris  ass igned by the semantic ru les  a denotat ion  thesame as that  of the sentence Some ~pives are in thedrawer, so there  combines wi th  i t  and a sentencemeaning is obtained.
But br~ke all existinzrecords  t rans la tes  as a proper ty ,  and no sentencemeaning is obtained if it is given ~here as itssubject.
This is the sort of move that I willrefer to as semantic filtering.A quest ion  that  seems never to have been con-s idered  care fu l ly  before  is  what kind of languagescan be defined by providing a CF-PSG plus a set ofsemantic rules that leave some syntactically gen-erated sentences without a sentence meaning astheir denotation.
For instance, in a system with aCF-PSG and a denotat iona l  semant ics ,  can the set  ofsentences  that  get  ass igned sentence denotat ions  benon-CF?I am grate fu l  to Len Schubert fo r  po in t ing  outto me that  the answer i s  yes,  and prov id ing thefo l lowing example; Consider the fo l lowing gra~mmr,composed of syntact i c  ru les  pa i red with semantict rans la t ion  schemata.
(6) S --> L R F(L:(R'))L --> C C"R --> C C"C --> a a"C --> b b"C --> aC G(C')c - ->  bC SCC')Assume that  there  are two bas ic  semantic types ,and B, and that  4" andS"  are constants  denot ingent i t ies  of types A and B respect ive ly .
~,  ~, andare c ross -categor la l  operators .
~(~) has thecategory of funct ions  from X-type th ings  to !
- typeth ings ,  ~(~) has the cate~ry  of funct ions  from A_-type th ings  to ~- type  th ings ,  and H(X) has the117category  of funct ions  from B- type  th ings  to X-typeth ings .
Given the semant ic  t rans la t ion  schemata,every  d i f fe rent  X const i tuent  has a unique semant icca tegory ;  the s t ruc ture  of  the s t r ing  i s  coded in tothe structure of its translation.
But the firstrule only yields a meaning for the S constituent ifL" and ~" are of the same category.
Whateversemantic category may have been built up for aninstance of ~', the F operator applies to produce afunction from things of that type to things of typeB, and the rule says that this function must beapplied to the translation of ~'.
Clearly, if R"has exact ly  the same semant ic  ca tegory  as L" th i sw i l l  succeed in y ie ld ing  a B- type denotat ion  fo r  S,and under a l l  o ther  c i rcumstances  S w i l l  fa i l  to beass igned  a denotat ion .The set  of s t r ings  of  category  S that  a reass igned  denotat ions  under these  ru les  i s  thus{xx I ~ in (A, ~)+}which is a non-CF language.
We know, therefore,that it is poss ib le  for semant ic  filtering of a setof syutactic rules to alter expressive power signi-ficantly.
We know, in fact, that it would be pos-sible to handle Bambara noun stems in this way anddesign a set of translation principles that wouldonly allow a string '~oun-~-Noun" to be assigned adenotation if the two instances of N were string-wise identical.
What we do not know is how to for-mulate with clarity a principle of linguistictheory that adjudicates on the question of whetherthe resultant description, with its infinite numberof distinct semantic categories, is permissible.Despite the efforts of Barbara Hall Partee andother scholars who have written on constraining theMoutague semantics framework over the past tenyears ,  quest ions  about permiss ib le  power insemantic apparatus are still not very wellexp lo red .One th ing  that  i s  c lear  i s  that  Gazdar and o th -e rs  who have c la imed or assumed that  NL's arecontext - f ree  never  in tended to suggest  that  theent i re  mechanism of assoc ia t ing  a sentence  w i th  ameaning could be car r ied  out by a system equ iva lentto a pushdown automaton.
Even i f  we take thenot ion  "assoc ia t ing  a sentence  w i th  a meaning" tobe fully clear, which is g rant ing  a lot in the wayof separating out pragmatic and discourse-relatedfactors, it is obvious that operations beyond thepower of a CY-PSG to define are involved.
Thingslike identifying representations to which lambda-conversion can apply, determining whether ali vari-ables are bound, checking that every indexed ana-phoric element has an antecedent with the sameindex, verifying that a structure contains no vacu-ous quantification, and so on, are obviously ofnon-CF character when regarded as language recogni-tion problems.
Indeed, in one case, that of disal-lowing vacuous quantifiers, it has been conjectured(Partee and Marsh 1984), though not yet proved,that even an indexed grammar does not have therequisite power.It therefore should not be regarded as surpris-ing that mechanisms devised to handle the sort oftasks involved in assigning meanings to sentencescan come to the rescue in cases where a given syn-tactic framework has insufficient expressive power.Nor should i t  be surprising that those  syntactictheories that build into the syntax a power thatamply suffices to achieve a suitable syntax-to-semantics mapping have no trouble accommodating allnew sets of facts that turn up.
The moment weadopt any mechanisms with greater than, say,context-free power, our problem is that we arefaced with a multiplicity of ways to handle almostany descriptive problem.7.
GRAMMARS WITH INFINITE NONTERMINAL VOCABULARIESSuppose we dec ide we want to re jec t  the  idea ofa l low ing  a souped-up semant ic  ru le  system do par tof the job of defining the membership of thelanguage.
What syntact i c  opt ions  are  reasonab leones ,  g iven  the k ind  of  non-context - f ree  languageswe think we might have to describe?There i s  a la rge  range of theor ies  of grammarde f inab le  i f  we re lax  the s tandard  requ i rement  hatthe set  N of  nontermina l  vocabu lary  of the  gr - - - ,=rshould be finite.
Since a finite parser for such agr,mm-r cannot contain an infinite list of nonter-minals, if the infinite majority of the nontermi-naIs are not to be useless symbols, the parser mustbe equipped with some way of parsing representa-tions of nonterminals, i.e.
to test arbitraryobjects for membership in N. If the tests do notguarantee results in finite time, then clearly thedevice may be of Turing-machine power, and mayde f ine  an undec idab le  language.
Two par t i cu la r lyinteresting types of grammar that do not have thisproperty are the following:Indexed 2rammars.
If members of N are built upusing sequences of indices affixed to a membersof a finite set of basic nonterminals, and rulesin  P are  ab le  to add or remove sequence- in i t ia lind ices ,  a t tached  to a g iven  bas ic  nontermina l ,the  express ive  power ach ieved i s  that  of  theindexed grammars of  Aho (1968).
These have anautomata - theoret i c  character i za t ion  in  terms ofa s tack  automaton that  can bu i ld  s tacks  ins ideo ther  s tacks  but can on ly  empty a s tack  a f te ra l l  the s tacks  w i th in  i t  have been empt ied.
Thetime complexity of the parsing problem isexponential.Unification Krannaars.
If members of N haveinternal hierarchical structure and parsingoperations are permitted to match hierarchicalrepresentations one with another globally todetermine whether they unify (roughly, whetherthere is a minimal consistent representationthat includes the distinctive properties ofboth), and if the number of parses for a givensentence is kept to a finite number by requiringthat we do not haveA ==> Afor any A, then the expressive power seems to beweakly equivalent to the grammars that JoanBresnan and Ron Kaplan have developed under thename lexical-functional ~ramar (LF___GG; se___fieBresnan, e__dd., 1982; c__ff, also the work of MartinKay on unification grammars).
The LFG languagesinclude some non-indexed languages (Kelly Roach,unpublished work), and apparently have an NP-complete parsing problem (Ron Kaplan, personalcommunication).118Systems of this sor t  have  an undeniable interest inconnection with the study of natural language.Both theories of language structure and comput-ational implementations of grammars can be usefullyexplored in such terms.
My criticism of them wouldbe that it seems to me that the expressive power ofthese systems is too extreme.
Linguistically theyare insufficiently restrictive, and computationallythey are implausibly wasteful of resources.
How-ever, rather than attempt to support this vagueprejudice with specific criticisms, I would preferto use my space here to outline an alternative thatseems to me extremely promising.8.
READ GRAMMARS AND NATURAL LANGUAGESIn his recent doctoral dissertation, Carl Pol-lard (1984) has given a detailed exposition andmotivation for a class of grammars he terms headK~ammars.
Roach (1984) has proved that thelanguages generated by head grammars constitute afull AFL, showing all the significant closure pro-perties that  character i ze  the  c lass  o f  CFL 's .
Headgrammars have a greater expressive power, in termsof weak and strong generative capacity, than theCF-PSG's, but only to a very limited extent, asshown by some subtle and suprising results due toRoach (1984).
For example, there is a head grammarfor{anbncna  n I n 2 0}but not for{anbncndna n \[ n 2 O}and there is a head grammar for{ww I w is in (a, b)*}but not for{ww J w is in (a ,  b)*}.The time complexity of the recognition problemfor head grammars is also known: a time bound pro-portional to the seventh power of the length of theinput is sufficient to allow for recognition in theworst case on a deterministic Turing machine (Pol-lard 1984).
This clearly places head grammars inthe realm of tractable linguistic formalisms.The extension Pollard makes in CF-PSG to obtainthe head gra--,ars is in essence fairly simple.First, he treats the notion '*head" as a primitive.The strings of terminals his syntactic rules defineare headed s~rings, which means they are associatedwith an indication of a designated element to beknown as the head.
Second, he adds eight new'~rapping" operations to the standard concatenationoperation on strings that a CF-PSG can define.
Fora given ordered pair <B,C> of headed strings thereare twelve ways in which strings B and C can becombined to make a constituent A. I give here thedescriptions of just two of them which I will usebelow:LCI(B,C): concatenate C onto end of B; firstargument (B) is head of the result.Mnemonic: Left Concatenation with ist asnew head.LL2(B,C): wrap B around C, with head of B to theleft of C; C is head of the result.Mnemonic: Left wrapping with head to  theRight and ~nd as new head.The full set of operations is given in the chart infigure I.A simple and linguistically motivated head gram-mar can be given for the Swiss German situationmentioned earlier.
I will not deal with it here,because in the first place it would take consider-able space, and in the second place it is very sim-ple to read off the needed account from Pollard's(1984) treatment of the corresponding situation inDutch, making the required change in the syntax ofcase-mark ing .In the next section I apply head grammar tocases like that of Bambara noun reduplication.9.
THE RIDDLE OF REDUPLICATIONI have shown in section 6 that the set of Bambaracomplex nouns of the form '~oun--~-Noun" could bedescribed using semantic filtering of a context-free grammar.
Consider now how a head grammarcould achieve a description of the same facts.Assume, to simplify the situation, just two nounstems in Bambara, represented here as ~ and b. Thefollowing head grammar generates the language {xF igure  1:  combinatory  operat ions  in  head  gra~ar\[ LC1 \[ LC2 \[ RC1 \[ RC2 \[ LL1 I LL2 \] LR1 \[ LR2 \[ RL1 \[ RL2.
.
.
.
.
.
.
.
.
.
.
.
.
I .
.
.
.
.
\[ .
.
.
.
.
I .
.
.
.
.
I .
.
.
.
.
I .
.
.
.
.
I .
.
.
.
.
I .
.
.
.
.
I .
.
.
.
.
I .
.
.
.
.
I .
.
.
.
iLeftward orRightward?Concatenate,wrap Left,wrap Right?1 or 2 ishead of theresult?LIL J R1IIC I CIII2 I 1L LRLR LR RRR1 \ [RR2 I.....
I ..... II IS I R II IR R1 2119(7) Syntax LexiconS ---> LCI(M, A)S ---> LCI(N, B)M ---> LL2(X, O)N ---> LL2(Y, O)X ---> LL2(Z, A)Y ---> LL2(Z, B)Z ---> LCI(X, A)Z ---> LCI(Y, B)A - - ->  aB ---> b0 ---> oZ ---> eThe structure this gr---,-r assigns to the stringba-o-ba is shown in figure 2 in the form of a treewith crossing branches, using asterisks to indicateheads (or s t r i c t ly ,  nodes through which the pathfrom a labe l  to the head of i t s  termina l  s t r ing)asses).Figure 2: s t ruc ture  of the s t r ing  "ba-o-ba"accord ing to the gramar in (7) .S/ \/ \I I/ I1 Ix *o/ I  I/ I  I/ I Iz *A I/ ~-.I I/ \  I/ \ I/ \ Iz *B 1l l l?
b a o b aWe know, therefore, that there are at least twooptions available to us when we consider how a caselike Bambara may be described in rigorous andcomputationally tractable terms: semantic filteringof a CF-PSG, or the use of head gr-----rs.
However,I would like to point to certain considerationssuggesting that although both of these options areuseful as existence proofs and mathematical bench-marks, neither is the right answer for the Bembaracase.
The semantic filtering account of Bembaracomplex nouns would imply that every complex nounstem in Bambara was of a different semanticcategory, for the encoding of the exact repetitionof the terminal string of the noun stem would haveto be in terms of a unique compositional structure.This seems inherent implausible; "dog-catcher-catcher-catcher" should have the same semanticcategory as "dog-catcher-catcher" (both shoulddenote properties, I would assume).
And the headgrammar account of the same facts has two peculiar-ities.
First, it predicts a peculiar structure ofword-internal crossing syntactic dependencies (forexample, that in dog-catcher-~-dog-catcher, oneconstituent is dog-dog and another is doK-catcher-~-doR) that seem unmotivated and counter-intuitive.Second, the grammar for  the set  of complex nouns isp ro f l igate  in the sense of Pullmn (1983): there  areinherent ly  and necessar i l y  more nontermina lsinvolved than te rmina ls - - -and  thus more d i f fe rentad hoc syntact i c  ca tegor ies  than there  are nounstems.
Again, th i s  seems abhor rent .What i s  the cor rec t  descr ip t ion?
My ana ly t i ca lin tu i t ion  (which of course ,  I do not ask o thers  toaccept  unquest ion ing ly )  i s  that  we need a d i rec tre fe rence  to the redup l i ca t ion  of the sur faces t r ing ,  and th i s  i s  miss ing in both accounts .Somehow I th ink  the grammatical ru les  shouldre f lec t  the not ion  " repeat  the morpheme-str ing"d i rec t ly ,  and by the same token the pars ing  processshould d i rec t ly  recogn ize  the redup l i ca t ion  of thenoun stem rather  than happen ind i rec t ly  to guaran-tee  i t .I even th ink  there  is  ev idence from Engl ish  thato f fe rs  support  fo r  such an idea .
There is  a con-s t ruc t ion  i l l us t ra ted  by phrases  l i ke  Trac 7 h i t  i tand h i t  i t  and h i t  i t .
that  was d i scussed  byBrowne (1964), an unpubl ished paper that  i s  summar-ized by Lakoff  and Peters  (1969, 121-122, note 8) .I t  invo lves  redup l i ca t ion  of a const i tuent  (here ,  averb phrase) .
One of the cur ious features  of th i sconst ruct ion  is  that  i f  the redup l i ca ted  phrase isan ad jec t ive  phrase in the comparative degree ,  theexpress ion  of the comparat ive degree must be ident -i ca l  throughout ,  down to the morphologica l  and pho-no log ica l  leve l :(8)a .
K imgot  lone l ie r  and lone l ie r  and lone l ie r .b.
Kim got more and more and more lone ly .c.
*Kim got lone l ie r  and more lone ly  andlone l ie r .This i s  a problem even under t rans format iona l  con-cept ions  of gr-----r, since at the levels where syn-tactic transformations apply, lonelier and morelonely are generally agreed to be indistinguish-able.
The symmetry must be preserved at the phono-logical level.
I suggest that again a primitivesyntact i c  operation "repeat the morpheme-string" iscalled for.
I have no idea at this stage how itwould be appropriate to formalize such an operationand give it a place in syntactic theory.10.
CONCLUSIONThe arguments originally given at the start of theera of generative grammar were correct in theirconclusion that NL's cannot be treated as simplyregular sets of strings, as some earlyinformation-theoretic models of language userswould have had it.
However, questions of whetherNL's were CFL's were dismissed rather too hastily;English was never shown to be outside the class ofCFL's or even the DCFL's (the latter question nevereven having been raised), and for other languagesthe first apparently valid arguments for non-CFLstatus are only now being framed.
If we are goingto employ supra-CFL mechanisms in the characteriz-ing and processing of NL's, there are a host ofitems in the catalog for us to choose among.
Ihave shown that semantic filtering is capable ofenhancing the power of a CF-PSG, and so, in manydifferent ways, is relaxing the finiteness condi-tion on the nonterminal vocabulary.
Both of these120moves are likely to inflate expressive power quitedramatically, it seams to me.
One of the most mod-est extensions of CF-PSG being explored isPollard's head grannnar, which has enough expressivepowe# to handle the cases that seem likely toar i se ,  but I have suggested that even so, i t  doesnot seem to be the r ight  formalism to cover thecase of the complex nouns in the lexicon of Sam-bare.
Something different is needed, and it is notquite c lear what.This is a familiar situation in linguistics.Description of facts gets easier as the expressivepower of one's mechanisms is enhanced, hut choosingamong alternatives, of course, get harder.
What Iwould offer as a closing suggestion is that untilwe are able to encode different theoretical propo-sals (head grammar, string transformations, LFG,unification grammar, definite clause grammar,indexed grammars, semantic filtering) in a single,implemented, well-understood formalism, our effortsto be sure we have shown one proposal to be betterthan another will be, in Gerald Gazdar's scathingphrase, "about as sensible as claims to the effectthat Turing machines which employ narrow grey tapeare less powerful than ones employing wide orangetape" (1982, 131).
In this connection, the aims ofthe PATE project at SRI International seem particu-larly helpful.
If the designers of PATE can demon-strate.that it has enough flexibility to encoderival descriptions of NL's like English, Bambara,Engenni, Dutch, Swedish, and Swiss German, and todo this in a neutral way, there may be some hope inthe future (as there has not been in the past, asfar as I can see) of evaluating alternativelinguistic theories and descriptions as rigorouslyas computer scientists evaluate alternative sortingalgorithms or LISP implementations.REFERENCESBermudez, Manuel (1984) Retular .Lookahead an__dd Look-balk in LR Parsers.
PhD thesis, University ofCalifornia, Santa Cruz.Bresnan, Joan W., ed.
(1982) The Mental Renresenta-tion of Grammatical Rel@tions.
MIT Press, Cam-bridge, MA.Browne, Wayles (1964) "On adjectival comparison andreduplication in English."
Unpublished paper.Carlson, Creg (1983) "Marking const i tuents , "  inPrank Heny, ed.
,  L inguist ic  Categories: Auxi-l ia r ie~ an___~dd Related Puzzles; vo__!.
~: Categories,69-98.
D. Reidel,  Dordrecht.Chomsky, Noam (1957) Syntactic Structures.
Mouton,The Hague.Church, Kenneth (1980) On Memory Limitations i._nnNatura~ Lan~uaRe Processing.
M.Sc.
thesis, MIT.Published by Indiana University LinguisticsClub, Bloomington IN.Culy, Christopher (forthcoming) '~he complexity ofthe vocabulary of Bombers.
"Daly, R. T. (1974) Avvlications of th_.fie MathematicalTheory of Linguistics.
Mouton, The Hague."
Gazdar, Gerald (1981a) '~nbounded dependencies andcoordinate structure.
Linguistic Inquiry 12,155-184.Cazdar, Gerald (1981b) "On syntactic categories.
"Philosophical Transactions of the Ro?al Society(Series B) 295, 267-283.Gazdar, Gerald (1982) "Phrase structure graluuar,"in Jacobson and Pullum, eds.
,  131-186.Gazdar, Gerald; Pullum, Geoffrey K.; and Sag, IvanA.
(1982) '~ux i l i a r ies  and re lated phenomena ina restrictive theory of gramnmr," Language 58,591-638.Hag~ge, Claude (1976) "Relative clause center-embedding and comprehensibility," LinguisticIn, uir~ 7, 198-201.Higginbotham, James (1984) "English is not acontext-free language."
Linguistic Inquiry 15,225-234.Jacobsen, Pauline, and Pullum, Geoffrey K., eds.
(1982) Th_._ee Nature of Syntactic Representation.D.
Reidel, Dordrecht, Holland.Lakoff, George, and Peters, Stanley (1969) 'Thrasalconjunction and symmetric predicates," in DavidA.
Reibel and Sanford A. Schane, eds.
,Studies _~ English.
Prent ice-Hal l ,  EnglewoodCliffs.Langendoen, D. Terence (1975) "Finite-state parsingof phrase-structure languages and the status ofreadjustment rules in grammar,"Inouir7 5, 533-554.Langendoen, D. Terence (1981) '~he generative capa-city of word-formation components," LinguisticIn,uirv 12, 320-322.Langendoen, D. Terence, and Postal, Paul M. (1984)"English and the class of context-freelanguages," unpublished paper.Levelt, W. J. M. (1974) Formal GrR-,,-rs i _ _nn~~ics an__j Psvchol in~uist ics (vol.
I I ) :  Applica-t ions in L inguist ic  Theory.
Mouton, The Hague.Marcus, Mitchel l  (1980) A Theor?
of SyntacticRecognition for Natural Langua2e.
MI_~TPress,Cambridge MA.Manaster-Ramer, Alexis (1983) '~he soft formalunderbelly of theoret ica l  syntax," in Pavers~!om the Nineteenth Regional Meeting, ChicagoL inguist ic  Society, Chicago IL.Nozick, Robert (1974) Anarchy, State,  and Utonia.Basic Books, New York.Partee, Barbara, and William Marsh (1984) '~ownon-context-free is variable binding?
"Presented at the Third Nest Coast Conference onFormal L inguist ics ,  University of Cal i fornia,Santa Cruz.Perei ra,  Fernando (1984) '~ new character izat ion ofattachment preferences,"  in D. R. Dowry, L.Karttunen, and A. M. Zwicky, ads.,  NaturalLanguage Processing: Ps?chol in~uist ic,  Comput-at ional  an_.dd Theoretical  Perspect iyes.
CambridgeUniversity Press, New York NY.Pol lard,  Carl J .
(1984) Generalized phrase Struc-ture Grammars, Head G~a----rs, and NaturalLanguaees.
Ph.D. thes is ,  Stanford Univers i ty.Pullum, Geoffrey K. (1979) Rule Interact ion and th_._.eeOrganization o_~f~ Grammar.
Garland, New York.Pullam, Geoffrey K. (1983) "Context-freeness andthe computer processing of human languages," in21st Annual Meetin~ of the Assocation f_.q/_Computational Ling,istics: Proceedings of theConference, 1-6.
ACL, Menlo Park CA.121Pullum, Geoffrey K., and Gerald Gazdar (1982)"Natural languages and context-free languages,"Linguistics an__~dPhilosoph?
4, 471-504.Rich, Elaine (1983) Artificial Intelligence.McGraw-Hill, New York NY.Roach, Kelly (1984) "Formal properties of headgr,--,=rs."
Unpublished paper, Xerox Palo AltoResearch Center, Palo Alto CA.Sag, Ivan A.
(1982) '% semantic analysis of "NP-movement" dependencies in English."
In Jacobsonand Pull,--, eds., 427-466.Shieber, Stuart (1983) "Evidence against thecontext-freeness of natural language."
Unpub-lished paper.
SRI International, Menlo Park CA,and Center for the Study of Language and Infor-mation, Stanford CA.Shieber, Stuart (1983) "Sentence disambiguation bya shift-reduce parsing technique," in 21stAnnual Meeting of the Assocation fo__E Comput-ational Linguistics: Proceedings of the Confer-ence, 113-118.
ACL, Menlo Park CA.Thomas, E. (1978) A Grammatical Description of theEngenni Lan2ua2e.
SIL Publication no.
60.
Sum-mer Institute of Linguistics, Arlington TX.122
