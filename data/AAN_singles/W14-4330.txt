Proceedings of the SIGDIAL 2014 Conference, pages 228?237,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsAspectual Properties of Conversational ActivitiesRebecca J. Passonneau and Boxuan Guan and Cho Ho Yeungbecky@ccls.columbia.edu and bg2469@columbia.edu and cy2277@columbia.eduColumbia University, New York, NY, USAYuan Duydu@fb.comFacebook, New York, NY, USAEmma Connereconner@oberlin.eduOberlin College, Oberlin, OH, USAAbstractSegmentation of spoken discourse intodistinct conversational activities has beenapplied to broadcast news, meetings,monologs, and two-party dialogs.
Thispaper considers the aspectual propertiesof discourse segments, meaning how theytranspire in time.
Classifiers were con-structed to distinguish between segmentboundaries and non-boundaries, where thesizes of utterance spans to represent datainstances were varied, and the locationsof segment boundaries relative to these in-stances.
Classifier performance was betterfor representations that included the end ofone discourse segment combined with thebeginning of the next.
In addition, classi-fication accuracy was better for segmentsin which speakers accomplish goals withdistinctive start and end points.1 IntroductionPeople engage in dialogue to address a wide rangeof goals.
It has long been observed that discoursecan be structured into units that correspond to dis-tinct goals and activities (Grosz and Sidner, 1986;Passonneau and Litman, 1997).
This is concep-tually distinct from structuring discourse into thetopical units addressed in (Hearst, 1997).
Theability to recognize where distinct activities oc-cur in spoken discourse could support offline ap-plications to spoken corpora such as search (Wardand Werner, 2013), summarization (Murray et al.,2005), and question answering.
Further, a deeperunderstanding of the relation of conversationalactivities to observable features of utterance se-quences could inform the design of interactive sys-tems for online applications such as informationgathering, service requests, tutoring, and compan-ionship.
Automatic identification of such units,however, has been difficult to achieve.
This pa-per considers the aspectual properties of speak-ers?
conversational activities, meaning how theytranspire in time.
We hypothesize that recognitionof a transition to a new conversational activity de-pends on recognizing not only the start of a newactivity but also the end of the preceding one, onthe grounds that the relative contrast between end-ings and beginnings might matter as much or morethan absolute characteristics consistent across allbeginnings or all endings.
We further hypothesizethat transitions to certain kinds of conversationalactivity may be easier to detect than others.Following Austin?s view that speech constitutesaction of different kinds (Austin, 1962), we as-sume that different kinds of communicative ac-tion have different ways of transpiring in time,just as other actions do.
Conversational activitiesthat address objective goals, for example, can havevery well-demarcated beginnings and endings, aswhen two people choose a restaurant to go tofor dinner.
Conversational participants can, how-ever, address goals that need not have a specificresolution, such as shared complaints about thelack of good Chinese restaurants.
This distinctionbetween different kinds of actions that speakersperform through their communicative behavior isanalogous to the distinction in linguistic semanticspertaining to verbal aspect, between states, pro-cesses and transition events (or accomplishmentsand achievements) (Vendler, 1957) (Dowty, 1986).States (e.g., being at a standstill) have no percep-tible change from moment to moment; processes(e.g., walking) have detectable differences in statefrom moment to moment with no clearly demar-cated change of state during the process; transitionevents (e.g., starting to walk; walking to the endof the block) involve a transition from one state orprocess to another.To investigate the aspectual properties of dis-course segments, we constructed classifiers to de-228tect discourse segment boundaries based on fea-tures of utterances.
We considered the aspec-tual properties of discourse segments in two ways.First, to investigate the relative contribution offeatures from segment endings versus beginnings,we experimented with different sizes of utter-ance sequences, and different locations of seg-ment boundaries relative to these sequences.
Sec-ond, we considered different categories of seg-ments, based on the speculation that segment tran-sitions that are easier to recognize would be as-sociated with conversational activities that havea well-demarcated event structure, in constrast toactivities that involve goals to maintain or sustainaspects of interaction.The following section describes related work inthis area, as well as the difficulties in achievinggood performance.
Most work on identification ofdiscourse segments (or other forms of discoursestructure in spoken interaction) depends on a priorphase of annotation (e.g., (Galley et al., 2003; Pas-sonneau and Litman, 1997)).
We studied a corpusof eighty-two transcribed and annotated telephonedialogues between library patrons and librariansthat had been annotated with units analogous tospeech acts, and subsequently annotated with dis-course segments comprised of these units.
The an-notation yielded eight distinct kinds of discoursesegment, where a segment results from a linearsegmentation of a discourse into strictly sequentialunits.
(While the segmentation is sequential, theunits can have hierarchical relations.)
We foundthat classifiers to detect segment boundaries per-formed best with boundaries represented by fea-tures of sequences of utterances that spanned theend of one segment and the beginning of the next.Error analysis indicated that performance was bet-ter for boundaries that initiate conversational ac-tivities with clear beginnings and endings.2 Related WorkSegmentation of spoken language interaction intodistinct discourse units has been applied to meet-ings as well as to two-party discourse using acous-tic features, lexical features, and very heteroge-neous features.
In our previous work, we useda very heterogeneous set of features to segmentmonologues into units that had been identifiedby annotators as corresonding to distinct inten-tional units (Passonneau and Litman, 1997).
Texttiling (Hearst, 1997) has been applied to segmen-tation of meetings into distinct agenda segmentsusing both prior and following context (Baner-jee and Rudnicky, 2006).
Results had high pre-cision and low recall.
We also find that recall ismore challenging than precision.
Topic modelingmethods have also been applied to the identifica-tion of topical segments in speech (Purver et al.,2006) (Eisenstein and Barzilay, 2008), with im-provements over earlier work on the ICSI meetingcorpus (Galley et al., 2003) (Malioutov and Barzi-lay, 2006).An analog of text tiling that uses acoustic pat-terns rather than lexical items has been applied tothe segmentation of speech into stories using seg-mental dynamic time warping (SDTW) (Park andGlass, 2008).
The method is based on the intuitionof aligning utterances by similar acoustic patterns,possibly representing common words and phrases.Results on TDT2 Mandarin Broadcast News cor-pus were moderately good for short episodes withF=0.71 beating the baseline for lexical text tilingof 0.66, but poor on long episodes.An alternative method of relying solely onacoustic information has been applied to impor-tance prediction at a very fine granularity (Wardand Richart-Ruiz, 2013).
Four basic classesof prosodic features derived from PCA wereused (Ward and Vega, 2012): volume, pitchheight, pitch range and speaking rate cross variouswidths of time intervals.
The data was labeled byannotators using an importance scale of 1 to 5, andlinear regression was used to predict the label forinstances consisting of frames.
The method per-formed well with a correlation of 0.82 and meanaverage error of 0.75 (5-fold cross validation).The identification of different kinds of units indiscourse is somewhat related to the notion ofgenre identification, e.g.
(Obin et al., 2010) (Rieset al., 2000).
Results from this area have been ap-plied to segmentation of conversation by a combi-nation of topic and style (Ries, 2002).3 Data and AnnotationsThe corpus consists of recordings, transcripts andannotations on the transcripts of a set of 82 callsrecorded in 2005 between patrons of the AndrewHeiskell Braille and Talking Book Library of NewYork City.1An annotation for dialog acts with a1The audio files and transcripts are available for downloadfrom the Columbia University Data Commons.
The annota-tions and raw features will be released in the near future.229reduced set of dialog act types and adjacency pairrelations (Dialogue Function Units, DFUs) wasdeveloped, originally for comparison of dialoguesacross modalities (Hu et al., 2009).
A subsequentphase of annotation at the discourse level thatmakes use of the dialog act annotation was laterapplied.
This later annotation, referred to as TaskSuccess and Cost Annotation (TSCA), was aimedat identifying individual dialog tasks analogous tothose carried out by spoken dialog systems, to fa-cilitate comparison of human-human dialog withhuman-machine dialog.
Interannotator reliabilityof both annotations was measured using Krippen-dorff?s alpha (Krippendorff, 1980) at levels of 0.66and above for individual dialogues (Passonneau etal., 2011).
The corpus consists of 24,760 words,or 302 words per dialog.Briefly, the second phase of annotation involvedgrouping DFUs into larger sequences in whichthe participants continued to pursue a single co-ordinated activity, and labeling the large discourseunits for their discourse function.
The human an-notation instructions avoided reference to overtsignals of dialog structure.
Rather, annotatorswere asked to judge the semantic and pragmaticfunctions of utterances.
The annotations have beendescribed in previous work (Hu et al., 2009; Pas-sonneau et al., 2011); the annotation guidelines areavailable online.2The location of a transition between one con-versational activity and the next is represented asoccurring between adjacent utterances.
There are9,340 utterance in the corpus, or 114 per dialog.About 10.6 percent of the utterances (994) start anew discourse unit.
Within each unit, the speak-ers establish a conversational goal explicitly or im-plicitly, and continue to address the goal until itis achieved, suspended, or abandoned.
The dis-course segments were of the following seven cate-gories, with an additional Other category for noneof the above (examples from the corpus are shownafter each segment category description; wordsin brackets represent overlapping talk of the twospeakers):?
Conventional: The participants engage inconventionalized behavior, e.g., greetings (atthe beginning of the call) or goodbyes (at theend of the call).2See links at http://www1.ccls.columbia.edu/?Loqui/resources.html for transcriptionguidelines, and annotation manuals.Librarian: andrew heiskell libraryLibrarian: how are youPatron: good morningLibrarian: good morning?
Book-Request: The participants address a pa-tron?s request for a book, which can be a spe-cific book that first needs to be identified,or which can be a non-specific request for abook fitting some criterion (e.g., a mysterythe patron has not read before).Patron: do you have any fannie flagg storiesLibrarian: flagPatron: yeahPatron: F L A <Pause>Patron: A G G I think it is?
Inform: One of the participants provides theother with general information that does notsupport a Book Request, e.g., the patron pro-vides identifying information so the librariancan pull up the patron?s record.Patron: well I?ll call him again thenPatron: and I?ll get the name [today]Librarian [talk] to him and call me backPatron: <pause> i- i?ll call himPatron: and then i?ll call you okayLibrarian: okay?
Librarian-Proposal: The participants addressthe librarian?s suggestion of a specific bookor a kind of book that might meet the patron?sdesires.Librarian: I have ellis but not bretPatron: ah wa wa what do you have by himLibrarian: by coseLibrarian: C O S ELibrarian: I have the rage of a privileged classPatron: that?s all right?
Request-Action: One of the participants asksthe other to perform an action, e.g., the pa-tron asks that certain authors be added to thepatron?s list of preferencesPatron: also <pause> uhPatron: <pause> of the favorite author listLibrarian: mmhmPatron: would you umPatron: remove t jefferson parkerLibrarian: okay230?
Information-Request: One of the participantsseeks information from the other, e.g., the pa-tron wants to know ifPatron: this is the talking books rightLibrarian: yesLibrarian: this is the library for the blind?
Sidebar: The librarian temporarily takes acall from another Patron only long enough toplace the new caller on holdLibrarian: hold on one secondLibrarian: Andrew Heiskell LibraryLibrarian: please hold?
OtherOf these seven kinds of discourse units, Book-Requests and Librarian-Proposals are the mostclearly delimited by beginning and ending points.At the beginning of a Book-Request, the patronestablishes that she wants a book, and the end isidentified by the mutual achievement of the librar-ian and patron of either a successful resolution,meaning the identification of a particular book inthe library?s collection that the patron will accept,or a failure of the current attempt, which oftenleads to a new revised book request.
Librarian-Proposals are very parallel to Book-Requests; thedifference is that the librarian makes a suggestionof a specific book or kind of book which must beidentified for the patron, and which the patron thenaccepts or rejects.4 ExperimentsThe experiments to automatically identify the lo-cations of the annotated discourse units apply ma-chine learning to instances consisting of utterancesequences that represent the two classes, presenceversus absence of a boundary.
We hyothesizethat the enormous challenges for identifying dis-course structure in human-machine dialogue canbe better addressed through complementary re-liance on semantics and interaction structure (be-havioral cues), and each can reinforce the other.The main focus of the experiments reported hereis on data representation to address the questions,what features of the context support the abilityto segment a dialogue into conversational activityunits, and how much context is necessary?A disadvantage of the dataset is its relativelysmall size, especially given the extreme skew with00112+ First utterance of segment+ Last utterance of segmentS1P0S1P1- Any other utterance- Any other utteranceS2P2S2P1S2P0 + First 2 utterances of segment - Any other sequence of 2 utterances- Any other sequence of 2 utterances- Any other sequence of 2 utterances + Last utterance of one segment, first of next+ Last 2 utterances of segmentFigure 1: Schematic representation of instancespans and labels.
Bars on the left show the num-ber of utterances (size) and position of segmentboundary (position) for five of the fourteen typesof instances.
Positive and negative labels areshown on the descriptions at the right.the positive class consisting of only 10% of the in-stances.
On the other hand, the small size madedetailed annotation feasible, and the corpus iswell-suited to our research question in that it rep-resents naturally occurring, spontaneous human-human telephone discourse.
Therefore.
the man-ner in which the dialogs evolve over time is en-tirely natural.
Our major question of interest ishow much of the time-course of the discourse isrequired for a machine learner to identify the startof a new discourse unit.
To examine this question,we vary two dimensions of the representation ofthe instances for learning.
The first is the numberof utterances around the location of the start of anew discourse unit.
The second is the set of fea-tures to represent each instance, which as we willsee below, affects to some degree how many utter-ances to include before and after the start of a newdiscourse unit.Four machine learning methods were tested us-ing the Weka toolkit (Hall et al., 2009): NaiveBayes, J48 Decision Trees, Logistic Regressionand Multilayer Perceptron.
Of these, J48 had thebest and most consistent performance, which wespeculate is due to a combination of the small sizeof the dataset, and non-linearity of the data.
Be-cause J48 is doing feature selection while buildingthe tree, it can identify different threshholds forthe same features, depending on the location in thetree.
All results reported here are for J48.4.1 Labels and Instance SpansWe refer to a sequence of utterances, and a poten-tial location of the onset of a discourse unit relativeto that sequence, as a span.
We varied the num-231ber of utterances for each span from 1 to 4, andthe location of the start of a new unit to be at thebeginning of the first utterance, at the end of thelast utterance, or between any pair of utterances inthe span.
For a single utterance, there will be twotypes of instances, as shown in Figure 1.
Each in-stance type is represented as S<N>P<M> whereN is the number of utterances in the span and M ishow many utterances there are before the bound-ary.
S1P0 denotes size 1 spans with the boundaryat position 0; positively labeled instances repre-sent the first utterance of a segment.
S1P1 denotessize 1 spans with the boundary at position 1; posi-tively labeled instances represent the last utteranceof a segment.
The experiments used all labelingsfor spans from size 1 to 4, yielding 14 types ofinstances.
For multi-utterance spans that occur atthe beginning or end of a discourse, dummy utter-ances are used to fill out the spans.4.2 FeaturesWe use three sets of features.
A set we refer toas discourse features consists of a mixed set ofacoustic features and lexicogrammatical featuresthat have been associated with discourse structure,such as discourse cue words (Hirschberg and Lit-man, 1993).
Table 1 lists the 35 discourse features.The second set is a bag-of-words (BOW) vectorrepresentation, and the third is the combination ofthe discourse and BOW features.
We used alterna-tive sets of features on the assumption that the per-formance of a machine learner across the differ-ent instance spans will vary, depending on the as-pects of the utterance that the features capture.
Wesee some expected differences in performance be-tween the discourse features and BOW, with BOWbenefitting more than the discourse features fromlonger spans.
Unexpectedly, we see no gain inperformance from the combination of both featuresets.The discourse features consist of acoustic fea-tures, pause features, word and utterance lengthfeatures, proper noun features and speaker change.The acoustic features and the (unfilled) pause lo-cation and duration features were extracted usingPraat, a cross-platform tool for speech analysis.The features pertaining to filled pauses (e.g., um,uh) were extracted from the transcripts.4.3 Conditions and EvaluationThe experimental conditions varied the feature set,the selection of training data versus testing data,and the fourteen kinds of instance spans and la-bels.
Three feature sets consisted of the discoursefeatures from Table 1 (discourse), bag-of-words(bow), and the combination of the two (combo).In all experiments, the data was randomly splitinto 75% for training, and 25% for testing, us-ing two methods to select instances.
In random-ization by dialog, all utterances from a single di-alog were kept together and 75% of the dialogswere selected for training.
In randomization byutterance, 75% of all utterances were randomlyselected for training, without regard to which di-alog they came from.
This was done to test thehypothesis that the bow representation would bemore sensitive to changes of vocabulary across di-alogs.
The three feature sets, fourteen data rep-resentations and two randomization methods yield84 experimental conditions.While N-fold cross-validation is a popularmethod to estimate a classifier?s prediction error,it is not a perfect substitute for isolating the train-ing data from the test data (Ng, 1997).
The cross-validation estimate of prediction error is relativelyunbiased, but it can be highly variable (Efron andTibshirani, 1997)(Rodriguez et al., 2010).
Toavoid the inherent risk of overfitting (Ng, 1997),one recommendation is to use cross-validation tocompare models, and to reserve a test set to verifythat a selected classifier has superior generaliza-tion (Rao and Fung, 2008).
To assess whether per-formance measures of different models are gen-uinely different requires error bounds on the result,which is not done with cross-validation.
We per-form train-test splits of the data to minimize over-fitting, and bootstrap confidence intervals for eachclassifier?s accuracy (and other metrics) in order tomeasure the variance, and thereby assess whetherthe performance error bounds of two conditionsare distinct.5 ResultsGiven that for this data, the rate of segmentboundary instances (positive labels) is about 10%,a baseline classifier that always predicts a non-segment will have about 90% overall accuracy.The baseline column in Table 2 shows the aver-age accuracy that would be achieved by this sim-ple baseline on the test data for a given run, alongwith the bootstrapped confidence interval for thisbaseline over the 50 runs.
In the 84 experiments,the baseline ranged from 90% (+/- 1%) to 89% (+/-232Interaction feature1 Speaker whether there is a speaker switch between preceding utterance and current utteranceAcoustic features2 Pitch MIN Minimum pitch of the utterance3 Pitch MAX Maximum pitch of the utterance4 Pitch MEAN Mean pitch of the utterance5 Pitch STDV Standard deviation of the pitch of the utterance6 Pitch RANGE Maximim pitch of the utterance less the minimum pitch7 Pitch CHANGE Pitch MEAN of the current utterance less the Pitch MEAN of the preceding utterance8 Intensity MIN Minimum intensity of the utterance9 Intensity MAX Maximum intensity of the utterance10 Intensity MEAN Mean intensity of the utterance11 Intensity STDV Standard deviation of the intensity of the utterance12 Intensity RANGE Intensity MAX less Intensity MIN13 Intensity CHANGE Intensity MEAN of the current utterance less Intensity MEAN of preceding utterance14 LR1 Utterance duration15 LR1 Normalized Utterance duration normalized by each speaker independentlyLexical features16 LR2 1 Word count17 LR2 2 Word count normalized by speaker18 LR3 1 Words per second19 LR3 2 Words per second by speaker20 LR4 Average word length21 LR5 Maximum word length22 LR6 1 Average frequency of characters in the utterance23 LR6 2 Number of low frequency characters24 IR Number of content words25 PN 1 Number of named entities26 PN 2 Whether the utterance contains a new named entityPause features27 Pause DURT total duration of all pauses28 Pause RATIO proportion utterance consisting of pauses29 FP1 Presence of a filled pause at the beginning of an utterance30 FP2 Presence of a filled pause at the end of an utterance31 FP3 Presence of a filled pause in the middle of an utterance32 P1 Presence of a pause tag at the beginning of an utterance33 P2 Presence of a pause tag at the end of an utterance34 P3 Presence of a pause tag in the middle of an utteranceTable 1: Discourse Features1%).
Crucially, however, the simple baseline willfail to identify any of the members of the positiveclass.
Though it is difficult to beat the baselineon overall accuracy, the question addressed hereis what level of accuracy is achieved on the pos-itive class, while remaining relatively consistentwith the baseline on overall accuracy.
It shouldbe noted that accuracy on the positive class is thesame as recall, or sensitivity (the term used in theepidemiological literature).
The worst perform-ing classifier among the 84 (disc/utterance/ S1P4)achieves 83% (+/- 1%) accuracy overall, or belowthe baseline by 6%, with 11% accuracy on the pos-itive class, 100% of which is a gain over the base-line.
By this standard, the best classifier of the 84conditions (bow/dial/S4P1) matches the baselineon overall accuracy, and achieves 50% (+/- 5%)accuracy on the positive class, which far exceedsthe baseline.
About half of the experimental con-ditions meet the baseline and achieve at least 25%accuracy on the positive class.Overall accuracy, and accuracy on the positiveclass, measure prediction error, but can be supple-mented with additional metrics that facilitate anal-ysis of the nature and cost of error types.
As a sup-plementary metric, we report average F-measure,the harmonic mean of recall and precision, due toits familiarity, and because it provides a sense ofhow often a classifier incorrectly predicts the pos-itive class.
An F-measure close to accuracy on thepositive class indicates that precision is about thesame as recall, while a relatively higher F-measureindicates that the precision is even higher than theF-measure, and the converse is true when the F-measure is lower than accuracy on the positiveclass.
Table 2 shows 32 classifiers with the high-est measures of accuracy, accuracy on the positiveclass, and F-measure.
The confidence intervals foraccuracy on the positive class and F-measure arerather wide, compared to those for overall accu-233Exp.
Baseline (sd) Acc (sd) AccPos(Recall) (sd) F (sd) >Accpos> Fbow/dial/S4P1 0.89 (+/-0.010) 0.89 (+/-0.009) 0.42 (+/-0.082) 0.28 (+/-0.054) 22 11bow/dial/S4P2 0.90 (+/-0.013) 0.89 (+/-0.010) 0.39 (+/-0.071) 0.26 (+/-0.064) 22 3bow/utterance/S1P0 0.90 (+/-0.004) 0.90 (+/-0.005) 0.51 (+/-0.051) 0.26 (+/-0.034) 30 11bow/utterance/S4P0 0.89 (+/-0.005) 0.88 (+/-0.006) 0.43 (+/-0.049) 0.26 (+/-0.040) 23 10disc/dial/S2P1 0.90 (+/-0.009) 0.87 (+/-0.009) 0.32 (+/-0.059) 0.26 (+/-0.037) 4 10bow/utterance/S4P3 0.89 (+/-0.006) 0.88 (+/-0.005) 0.41 (+/-0.050) 0.25 (+/-0.027) 22 11combo/dial/S3P2 0.89 (+/-0.011) 0.86 (+/-0.010) 0.31 (+/-0.048) 0.25 (+/-0.031) 7 10disc/dial/S4P3 0.90 (+/-0.008) 0.86 (+/-0.009) 0.30 (+/-0.041) 0.25 (+/-0.030) 4 10combo/dial/S3P1 0.89 (+/-0.010) 0.86 (+/-0.011) 0.31 (+/-0.059) 0.25 (+/-0.038) 3 10combo/dial/S4P2 0.89 (+/-0.013) 0.86 (+/-0.012) 0.30 (+/-0.044) 0.25 (+/-0.031) 4 10combo/dial/S2P1 0.89 (+/-0.012) 0.87 (+/-0.010) 0.32 (+/-0.054) 0.25 (+/-0.033) 7 10combo/dial/S4P3 0.90 (+/-0.007) 0.87 (+/-0.008) 0.29 (+/-0.044) 0.25 (+/-0.035) 4 10disc/dial/S3P2 0.90 (+/-0.008) 0.87 (+/-0.008) 0.29 (+/-0.047) 0.25 (+/-0.040) 3 10bow/utterance/S4P1 0.90 (+/-0.005) 0.89 (+/-0.004) 0.40 (+/-0.053) 0.25 (+/-0.020) 22 10bow/dial/S4P3 0.90 (+/-0.007) 0.89 (+/-0.009) 0.39 (+/-0.072) 0.25 (+/-0.035) 22 10disc/dial/S4P2 0.90 (+/-0.009) 0.86 (+/-0.009) 0.28 (+/-0.042) 0.25 (+/-0.030) 0 10bow/dial/S1P0 0.90 (+/-0.009) 0.89 (+/-0.009) 0.48 (+/-0.065) 0.24 (+/-0.045) 28 0combo/dial/S4P1 0.90 (+/-0.010) 0.86 (+/-0.010) 0.28 (+/-0.045) 0.24 (+/-0.034) 0 9disc/dial/S3P1 0.89 (+/-0.011) 0.86 (+/-0.010) 0.29 (+/-0.046) 0.24 (+/-0.033) 2 9bow/dial/S4P0 0.90 (+/-0.009) 0.88 (+/-0.011) 0.37 (+/-0.031) 0.24 (+/-0.040) 22 0disc/dial/S4P1 0.90 (+/-0.009) 0.86 (+/-0.008) 0.27 (+/-0.041) 0.23 (+/-0.032) 0 3bow/utterance/S4P2 0.89 (+/-0.007) 0.88 (+/-0.010) 0.39 (+/-0.044) 0.23 (+/-0.033) 22 0combo/utterance/S2P0 0.89 (+/-0.005) 0.86 (+/-0.009) 0.27 (+/-0.041) 0.21 (+/-0.029) 0 0disc/dial/S2P0 0.89 (+/-0.010) 0.86 (+/-0.009) 0.27 (+/-0.047) 0.20 (+/-0.027) 0 0disc/utterance/S2P0 0.90 (+/-0.006) 0.86 (+/-0.008) 0.26 (+/-0.032) 0.20 (+/-0.024) 0 0combo/utterance/S1P0 0.89 (+/-0.005) 0.88 (+/-0.006) 0.31 (+/-0.041) 0.20 (+/-0.026) 10 0combo/utterance/S3P0 0.90 (+/-0.005) 0.86 (+/-0.008) 0.25 (+/-0.038) 0.20 (+/-0.033) 0 0disc/utterance/S4P3 0.89 (+/-0.006) 0.86 (+/-0.009) 0.24 (+/-0.043) 0.20 (+/-0.033) 0 0combo/utterance/S2P1 0.89 (+/-0.006) 0.86 (+/-0.008) 0.26 (+/-0.036) 0.20 (+/-0.023) 0 0disc/utterance/S2P1 0.89 (+/-0.005) 0.86 (+/-0.007) 0.26 (+/-0.032) 0.20 (+/-0.022) 0 0combo/utterance/S4P1 0.89 (+/-0.006) 0.85 (+/-0.008) 0.24 (+/-0.033) 0.20 (+/-0.027) 0 0disc/utterance/S4P0 0.89 (+/-0.006) 0.85 (+/-0.009) 0.24 (+/-0.034) 0.20 (+/-0.024) 0 0Table 2: Classification performance (with standard deviations in parentheses) of the best 40% of 84J48 models trained on 75% of the data and tested on the remaining 25%, with bootstrapped confidenceintervals from 50 trials each.racy.
To draw comparisons among the classifiersthat take into account this variance, the two right-most columns of the table indicate for each clas-sifier how many other classifiers in the same ta-ble the current classifier surpasses on mean accu-racy of the positive class, or on mean F-measure.Here, to surpass another classifier means the lowerbound of its confidence interval surpasses the up-per bounds of other classifiers?
confidence inter-vals.Table 2 shows that there is no one classifier thatsurpasses all others on all measures.
There are,however, some clear trends.
Regarding the num-ber of utterances spanned by each data instance,the table shows that of the 32 best performing clas-sifiers, the majority (seventeen) have size 4 spans,and all but three have spans longer than a singleutterance.
This trend indicates that more contextleads to better accuracy overall and better accuracyon the positive class.
Regarding where the seg-ment boundary is located relative to the span, themajority of cases (twenty-two) locate the bound-ary within the span, meaning that the span includesone or more of the final utterances of a segmentand one or more of the initial utterances of the nextsegment.
The remaining cases involve spans thatinclude utterances only from the beginning of thesegment.
There are no cases of higher perform-ing classifiers that use spans from segment end-ings.
Among the classifiers in the top half of thetable, the best performing bow classifiers surpassa larger number of the other classifiers on accu-racy of the positive class.
The best performing dis-course or combination classifiers surpass a largernumber of other classifiers on F-measure.
Thissuggests that in general, the bow classifiers do bet-ter on recall and the classifiers with discourse fea-tures have higher precision.The combination of BOW and discourse fea-tures has a performance that differs little from thediscourse features alone, and does not do as wellas BOW S4P1.
This result was unexpected, andsuggests that the bow and discourse feature setsoften identify nearly the same set of discourse234Discourse, Rand Dial, S4P3Activity Type TP % FN %Inform 7 (0.11) 56 (0.89)Book Request 18 (0.32) 40 (0.68)Librarian Proposal 4 (0.27) 11 (0.73)Request-Action 0 (0.00) 6 (1.00)Information-Request 6 (0.11) 47 (0.89)Sidebar 1 (0.08) 11 (0.92)Conventional 5 (0.17) 25 (0.83)Total 37 (0.14) 230 (0.86)BOW, Rand Dial, S4P2Inform 7 (0.10) 70 (0.90)Book Request 14 (0.20) 57 (0.80)Librarian Proposal 1 (0.05) 20 (0.95)Request-Action 0 (0.00) 5 (1.00)Information-Request 8 (0.16) 42 (0.84)Sidebar 0 (0.00) 13 (1.00)Conventional 6 (0.23) 29 (0.77)Total 37 (0.14) 230 (0.86)Table 3: Error Analysis of the Positive Classboundaries.
Since the initial utterances of a seg-ment seem to have features with greater predictivepower than the final utterances of a segment, andsince discourse cue words tend to occur in the firstutterance or so of a segment, it could be that dis-course cue words explain the good performanceof both sets of features.
This could be tested in fu-ture work by restricting a BOW representation towords other than discourse cue words.To pursue in more detail the factors that influ-ence accuracy on the positive class (recall), wenow turn to an error analysis of the kinds of dis-course units associated with true positives versusfalse negatives of the classifier?s confusion matrix.Table 3 presents the results of an error analysis ofthe two cells of the confusion matrix for a clas-sifier?s results on the positive class, the true pos-itives and the false negatives.
We looked at thebreakdown of the seven kinds of discourse unitsto see whether there were differences in the like-lihood of a correct identification of a boundary,depending on the kind of discourse unit in ques-tion.
Results are drawn from classifiers learnedunder two conditions, S4P3 spans with discoursefeatures randomized by dialogue (disc/dial/S4P3)and S4P3 spans with BOW features, randomizedby dialogue (bow/dial/S4P3).
(Results from otherclassifiers are very similar.)
In both cases, Book-Requests have a much higher probability of be-ing among the true positives (32% for discourse,20% for BOW) than for the positive class over-all (14%).
Conventional discourse units, wherethe participants first make their greetings, or maketheir final good byes, are also correctly identifiedmore often than the overall TP rate.
Librarian Pro-posals are identified well by the model using thediscourse features, but not by the one using theBOW features.
We speculate that this is becauseLibrarian Proposals typically present informationthat is new to the discourse: often, the librarianis making a suggestion to the patron based on in-formation the librarian can see in the preferencefield of the patron?s record, or in the patron?s pastborrowing behavior.
We speculate that the vocab-ulary in Librarian Proposals may be too variableto be predictive.
Information-Request units andInform units are also relatively difficult to identifycorrectly.6 ConclusionThe problem of identification of conversational ac-tivities is a difficult one for machine processing formany reasons.
Like vision and speech, segmenta-tion of the units is difficult because the units arenot discrete, objective, components of perception,but instead are the result of abstraction.
The exper-iments presented here consider a novel explana-tion for the difficulty of the task, which is that dis-course units differ from each other regarding themanner in which they evolve in time.
The resultsshow that a data representation that includes utter-ances from both the end of one unit and the begin-ning of another improves performance.
The tran-sition between one conversational activity and an-other takes place over the course of several utter-ances, rather than occurring at an instant in time.Error analysis indicates further that discourse unitsthat correspond to conversational activities withclear end points that can be achieved have a higherprobability of being recognized correctly.ReferencesJohn L. Austin.
1962.
How to do Things with Words:The William James Lectures delivered at HarvardUniversity in 1955.
Clarendon Press, Oxford.Satanjeev Banerjee and Alexander I. Rudnicky.
2006.A texttiling based approach to topic boundary detec-tion in meetings.
Technical report, Department ofComputer Science, Carnegie Mellon University.David R. Dowty.
1986.
The effects of aspectual classon the temporal structure of discourse: semantics orpragmatics?
Linguistics and Philosophy, 9(1):37?61.Bradley Efron and Robert Tibshirani.
1997.
Im-provements on cross-validation: The .632+ boot-235strap method.
Journal of the American StatisticalAssociation, 92(438):548?560, June.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP ?08), pages 334?343.
Association for Computational Linguistics.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discoursesegmentation of multi-party conversation.
In Pro-ceedings of the 41st Annual Meeting on Associa-tion for Computational Linguistics, pages 562?569,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175?204, July.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11(1):10?18.Marti A. Hearst.
1997.
Texttiling: segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1).Julia Hirschberg and Diane Litman.
1993.
Empiricalstudies on the disambiguation of cue phrases.
Com-putational Linguistics, 19:501?530.Jun Hu, Rebecca J. Passonneau, and Owen Rambow.2009.
Contrasting the interaction structure of anemail and a telephone corpus: A machine learningapproach to annotation of dialogue function units.In Proceedings of the SIGDIAL 2009 Conference,pages 357?366, London, UK, September.
Associa-tion for Computational Linguistics.Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to Its Methodology.
Sage Publications,Beverly Hills, CA.Igor Malioutov and Regina Barzilay.
2006.
Min-imum cut model for spoken lecture segmentation.In Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th AnnualMeeting of the Association for Computational Lin-guistics, ACL-44, pages 25?32, Stroudsburg, PA,USA.
Association for Computational Linguistics.Gabriel Murray, Steve Renals, and Jean Carletta.
2005.Extractive summarization of meeting recordings.
InProceedings of the 9th European Conference onSpeech Communication and Technology, pages 593?596.Andrew Y. Ng.
1997.
Preventing overfitting ofcross-validation data.
In Proceedings of the Four-teenth International Conference on Machine Learn-ing, ICML ?97, pages 245?253.Nicolas Obin, Volker Dellwo, Anne Lacheret, andXavier Rodet.
2010.
Expectations for discoursegenre identification: aprosodic study.
In Inter-speech, pages 3070?3073.A.S.
Park and J.R. Glass.
2008.
Unsupervised patterndiscovery in speech.
Audio, Speech, and LanguageProcessing, IEEE Transactions on, 16(1):186?197,Jan.Rebecca J. Passonneau and Diane J. Litman.
1997.Discourse segmentation by human and automatedmeans.
Computational Linguistics, 23(1):103?139,March.Rebecca J. Passonneau, Irene Alvarado, Phil Crone,and Simon Jerome.
2011.
Paradise-style evaluationof a human-human library corpus.
In Proceedingsof the SIGDIAL 2011 Conference, pages 325?331,Portland, Oregon, June.
Association for Computa-tional Linguistics.Matthew Purver, Thomas L. Griffiths, and Joshua B.Kording, Konrad P. andTenenbaum.
2006.
Unsu-pervised topic modelling for multi-party spoken dis-course.
In Proceedings of the 44th annual meet-ing of the Association for Computational Linguistics(ACL-44), pages 17?24.
Association for Computa-tional Linguistics.R.
Bharat Rao and Glenn Fung.
2008.
On the dangersof cross-validation.
an experimental evaluation.
InSDM, pages 588?596.
SIAM.Klaus Ries, Lori Levin, Liza Valle, Alon Lavie, andAlex Waibel.
2000.
Shallow discourse genre an-notation in callhome spanish.
In Proceedings of In-ternational Conference on Language Resources andEvaluation (LREC).
European Language Resourcesand Evaluation (ELRA).Klaus Ries.
2002.
Segmenting conversations bytopic, initiative, and style.
In AnniR.
Coden, EricW.Brown, and Savitha Srinivasan, editors, InformationRetrieval Techniques for Speech Applications, vol-ume 2273 of Lecture Notes in Computer Science,pages 51?66.
Springer Berlin Heidelberg.J.D.
Rodriguez, A. Perez, and J.A.
Lozano.
2010.
Sen-sitivity analysis of k-fold cross validation in predic-tion error estimation.
Pattern Analysis and MachineIntelligence, IEEE Transactions on, 32(3):569?575,March.Zeno Vendler.
1957.
Verbs and times.
PhilosophicalReview, 66(2):143?160.Nigel G. Ward and Karen A. Richart-Ruiz.
2013.
Pat-terns of importance variation in spoken dialog.
InSigDial.Nigel G. Ward and Alejandro Vega.
2012.
A bottom-up exploration of the dimensions of dialog state inspoken interaction.
In SigDial.236Nigel G. Ward and Steven D. Werner.
2013.
Usingdialog-activity similarity for spoken information re-trieval.
In Interspeech.237
