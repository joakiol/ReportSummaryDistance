Proceedings of NAACL HLT 2007, Companion Volume, pages 149?152,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAgenda-Based User Simulation forBootstrapping a POMDP Dialogue SystemJost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye and Steve YoungCambridge University Engineering DepartmentTrumpington Street, Cambridge, CB21PZ, United Kingdom{js532, brmt2, kw278, hy216, sjy}@eng.cam.ac.ukAbstractThis paper investigates the problem of boot-strapping a statistical dialogue manager with-out access to training data and proposes a newprobabilistic agenda-based method for simu-lating user behaviour.
In experiments with astatistical POMDP dialogue system, the simu-lator was realistic enough to successfully testthe prototype system and train a dialogue pol-icy.
An extensive study with human subjectsshowed that the learned policy was highly com-petitive, with task completion rates above 90%.1 Background and Introduction1.1 Bootstrapping Statistical Dialogue ManagersOne of the key advantages of a statistical approach to Dia-logue Manager (DM) design is the ability to formalise de-sign criteria as objective reward functions and to learn anoptimal dialogue policy from real dialogue data.
In caseswhere a system is designed from scratch, however, it isoften the case that no suitable in-domain data is availablefor training the DM.
Collecting dialogue data without aworking prototype is problematic, leaving the developerwith a classic chicken-and-egg problem.Wizard-of-Oz (WoZ) experiments can be carried out torecord dialogues, but they are often time-consuming andthe recorded data may show characteristics of human-human conversation rather than typical human-computerdialogue.
Alternatively, human-computer dialogues canbe recorded with a handcrafted DM prototype but neitherof these two methods enables the system designer to testthe implementation of the statistical DM and the learn-ing algorithm.
Moreover, the size of the recorded corpus(typically  103 dialogues) usually falls short of the re-quirements for training a statistical DM (typically 104dialogues).1.2 User Simulation-Based TrainingIn recent years, a number of research groups have inves-tigated the use of a two-stage simulation-based setup.
Astatistical user model is first trained on a limited amountof dialogue data and the model is then used to simulatedialogues with the interactively learning DM (see Schatz-mann et al (2006) for a literature review).The simulation-based approach assumes the presenceof a small corpus of suitably annotated in-domain dia-logues or out-of-domain dialogues with a matching dia-logue format (Lemon et al, 2006).
In cases when no suchdata is available, handcrafted values can be assigned tothe model parameters given that the model is sufficientlysimple (Levin et al, 2000; Pietquin and Dutoit, 2005) butthe performance of dialogue policies learned this way hasnot been evaluated using real users.1.3 Paper OutlineThis paper presents a new probabilistic method for simu-lating user behaviour based on a compact representationof the user goal and a stack-like user agenda.
The modelprovides an elegant way of encoding the relevant dialoguehistory from a user?s point of view and has a very smallparameter set so that manually chosen priors can be usedto bootstrap the DM training and testing process.In experiments presented in this paper, the agenda-based simulator was used to train a statistical POMDP-based (Young, 2006; Young et al, 2007) DM.
Even with-out any training of its model parameters, the agenda-based simulator was able to produce dialogue behaviourrealistic enough to train a competitive dialogue policy.An extensive study1 with 40 human subjects showed thattask completion with the learned policy was above 90%despite a mix of native and non-native speakers.1This research was partly funded by the EU FP6 TALKProject.
The system evaluation was conducted in collabora-tion with O.
Lemon, K. Georgila and J. Henderson at EdinburghUniversity and their work is gratefully acknowledged.1492 Agenda-Based Simulation2.1 User Simulation at a Semantic LevelHuman-machine dialogue can be formalised on a seman-tic level as a sequence of state transitions and dialogueacts2.
At any time t, the user is in a state S, takes ac-tion au, transitions into the intermediate state S?, receivesmachine action am, and transitions into the next state S?
?where the cycle restarts.S ?
au?
S??
am?
S???
?
?
?
(1)Assuming a Markovian state representation, user be-haviour can be decomposed into three models: P (au|S)for action selection, P (S?|au, S) for the state transitioninto S?, and P (S?
?|am, S?)
for the transition into S?
?.2.2 Goal- and Agenda-Based State RepresentationInspired by agenda-based methods to dialogue manage-ment (Wei and Rudnicky, 1999) the approach describedhere factors the user state into an agenda A and a goal G.S = (A,G) and G = (C,R) (2)During the course of the dialogue, the goal G ensures thatthe user behaves in a consistent, goal-directed manner.G consists of constraints C which specify the requiredvenue, eg.
a centrally located bar serving beer, and re-quests R which specify the desired pieces of information,eg.
the name, address and phone number (cf.
Fig.
1).The user agenda A is a stack-like structure containingthe pending user dialogue acts that are needed to elicit theinformation specified in the goal.
At the start of the dia-logue a new goal is randomly generated using the systemdatabase and the agenda is initially populated by convert-ing all goal constraints into inform acts and all goal re-quests into request acts.
A bye act is added at the bottomof the agenda to close the dialogue.As the dialogue progresses the agenda and goal are dy-namically updated and acts are selected from the top ofthe agenda to form user acts au.
In response to incom-ing machine acts am, new user acts are pushed onto theagenda and no longer relevant ones are removed.
Theagenda thus serves as a convenient way of tracking theprogress of the dialogue as well as encoding the rele-vant dialogue history.
Acts can also be temporarily storedwhen actions of higher priority need to be issued first,hence providing the simulator with a simple model ofuser memory3.2In this paper, the terms dialogue act and dialogue actionare used interchangeably.
The notation act(a=x, b=y,...) is usedto represent a dialogue act of a given type act (such as informor request with items a = x, b = y, etc.3See Fig.
1, turns 1-3, for an example: System turn 1 ?Whatpricerange??
triggers the user act inform(pricerange=cheap)to be pushed onto the agenda but it is not executed until turn 3because negate(drinks=beer) is issued first.C0=[type = bardrinks = beerarea = central]R0=[name =addr =phone =]Sys 0 Hello, how may I help you?A1=??????
?inform(type = bar)inform(drinks = beer)inform(area = central)request(name)request(addr)request(phone)bye()??????
?Usr 1 I?m looking for a nice bar serving beer.Sys 1 Ok, a wine bar.
What pricerange?A2=??????
?negate(drinks = beer)inform(pricerange = cheap)inform(area = central)request(name)request(addr)request(phone)bye()??????
?Usr 2 No, beer please!Sys 2 A bar serving beer, correct?A3=??????
?affirm()inform(pricerange = cheap)inform(area = central)request(name)request(addr)request(phone)bye()??????
?Usr 3 Yeah something cheap in the town centre.Sys 3 Murphy?s on Main Square serves cheap beer.A4=[request(phone)bye()]Usr 4 Ok, and what?s the phone number?Sys 4 The number is 796 69 94.A5=[bye()]Usr 5 Thanks, goodbye!Figure 1: Sample dialogue and agenda sequence2.3 User Act SelectionAt any time during the dialogue, the updated agenda oflength N contains all dialogue acts the user intends toconvey to the system.
Since the agenda is ordered ac-cording to priority, with A[N ] denoting the top and A[1]denoting the bottom item, selecting the next user act sim-plifies to popping n items off the top of the stack.
Hence,letting au[i] denote the ith item in the user act auau[i] := A[N ?
n + i] ?i ?
[1..n], 1 ?
n ?
N. (3)Using A[N?n+1..N ] as a Matlab-like shorthand nota-150tion for the top n items on A, the action selection modelbecomes a Dirac delta functionP (au|S) = P (au|A,G) = ?
(au, A[N?
n+1..N ]) (4)where the random variable n corresponds to the levelof initiative taken by the simulated user.
In a statisticalmodel the probability distribution over integer values forn should be conditioned on A and learned from dialoguedata.
For the purposes of bootstrapping the system, n canbe assumed independent of A and any distribution P (n)that places the majority of its probability mass on smallvalues of n can be used.2.4 State Transition ModelThe factorisation of S into A and G can now be ap-plied to the state transition models P (S?|au, S) andP (S?
?|am, S?).
Letting A?
denote the agenda after select-ing au(as explained in the previous subsection) and usingN?= N ?
n to denote the size of A?, we haveA?
[i] := A[i] ?i ?
[1..N?].
(5)Using this definition of A?
and assuming that the goalremains constant when the user executes au, the first statetransition depending on ausimplifies toP (S?|au, S) = P (A?, G?|au, A,G)= ?
(A?, A[1..N?])?
(G?, G).
(6)Using S = (A,G), the chain rule of probability, and rea-sonable conditional independence assumptions, the sec-ond state transition based on amcan be decomposed intogoal update and agenda update modules:P (S?
?|am, S?
)= P (A?
?|am, A?, G??)?
??
?agenda updateP (G?
?|am, G?)?
??
?goal update.
(7)When no restrictions are placed on A??
and G?
?, the spaceof possible state transitions is vast.
The model parame-ter set is too large to be handcrafted and even substantialamounts of training data would be insufficient to obtainreliable estimates.
It can however be assumed that A??
isderived from A?
and that G??
is derived from G?
and thatin each case the transition entails only a limited numberof well-defined atomic operations.2.5 Agenda Update Model for System ActsThe agenda transition from A?
to A??
can be viewed as asequence of push-operations in which dialogue acts areadded to the top of the agenda.
In a second ?clean-up?step, duplicate dialogue acts, null() acts, and unnecessaryrequest() acts for already filled goal request slots mustbe removed but this is a deterministic procedure so that itcan be excluded in the following derivation for simplicity.Considering only the push-operations, the items 1 to N ?at the bottom of the agenda remain fixed and the updatemodel can be rewritten as follows:P (A?
?|am, A?, G??
)= P (A??[1..N??
]|am, A?[1..N?
], G??)
(8)= P (A??[N?+1..N??
]|am, G??)?
?(A??[1..N?
], A?[1..N?]).
(9)The first term on the RHS of Eq.
9 can now be furthersimplified by assuming that every dialogue act item inamtriggers one push-operation.
This assumption can bemade without loss of generality, because it is possible topush a null() act (which is later removed) or to push anact with more than one item.
The advantage of this as-sumption is that the known number M of items in amnow determines the number of push-operations.
HenceN?
?= N?+ M andP (A??[N?+1..N??
]|am, G??
)= P (A??
[N?+1..N?+M ]|am[1..M ], G??)
(10)=M?i=1P (A??
[N?+i]|am[i], G??)
(11)The expression in Eq.
11 shows that each item am[i] inthe system act triggers one push operation, and that thisoperation is conditioned on the goal.
This model is nowsimple enough to be handcrafted using heuristics.
For ex-ample, the model parameters can be set so that when theitem x=y in am[i] violates the constraints in G?
?, one ofthe following is pushed onto A??
: negate(), inform(x=z),deny(x=y, x=z), etc.2.6 Goal Update Model for System ActsThe goal update model P (G?
?|am, G?)
describes how theuser constraints C ?
and requests R?
change with a givenmachine action am.
Assuming that R??
is conditionallyindependent of C?
given C ??
it can be shown thatP (G?
?|am, G?
)= P (R?
?|am, R?, C??
)P (C?
?|am, R?, C?).
(12)To restrict the space of transitions from R?
to R??
itcan be assumed that the request slots are independent andeach slot (eg.
addr,phone,etc.)
is either filled using infor-mation in amor left unchanged.
Using R[k] to denote thek?th request slot, we approximate that the value of R??
[k]only depends on its value at the previous time step, thevalue provided by am, and M(am, C??)
which indicatesa match or mismatch between the information given inamand the goal constraints.P (R?
?|am, R?, C??
)=?kP (R??
[k]|am, R?
[k],M(am, C??)).
(13)151To simplify P (C ?
?|am, R?, C?)
we assume that C ??
isderived from C ?
by either adding a new constraint, set-ting an existing constraint slot to a different value (eg.drinks=dontcare), or by simply changing nothing.
Thechoice of transition does not need to be conditioned onthe full space of possible am, R?
and C ?.
Instead it canbe conditioned on simple boolean flags such as ?Does amask for a slot in the constraint set?
?, ?Does amsignal thatno item in the database matches the given constraints??,etc.
The model parameter set is then sufficiently small forhandcrafted values to be assigned to the probabilities.3 Evaluation3.1 Training the HIS Dialogue ManagerThe Hidden Information State (HIS) model is the firsttrainable and scalable implementation of a statisticalspoken dialog system based on the Partially-ObservableMarkov Decision Process (POMDP) model of dialogue(Young, 2006; Young et al, 2007; Williams and Young,2007).
POMDPs extend the standard Markov-Decision-Process model by maintaining a belief space, i.e.
a proba-bility distribution over dialogue states, and hence providean explicit model of the uncertainty present in human-machine communication.The HIS model uses a grid-based discretisation of thecontinuous state space and online -greedy policy iter-ation.
Fig.
2 shows a typical training run over 60,000simulated dialogues, starting with a random policy.
Usergoals are randomly generated and an (arbitrary) rewardfunction assigning 20 points for successful completionand -1 for every dialogue turn is used.
As can be seen, di-alogue performance (defined as the average reward over1000 dialogues) converges after roughly 25,000 iterationsand asymptotes to a value of approx.
14 points.Figure 2: Training a POMDP system3.2 Experimental Evaluation and ResultsA prototype HIS dialogue system with a learned policywas built for the Tourist Information Domain and exten-sively evaluated with 40 human subjects including nativeand non-native speakers.
A total of 160 dialogues with21667 words was recorded and the average Word-Error-Rate was 29.8%.
Task scenarios involved finding a spe-cific bar, hotel or restaurant in a fictitious town (eg.
theaddress of a cheap, Chinese restaurant in the west).The performance of the system was measured basedon the recommendation of a correct venue, i.e.
a venuematching all constraints specified in the given task (alltasks were designed to have a solution).
Based on thisdefinition, 145 out of 160 dialogues (90.6%) were com-pleted successfully, and the average number of turns tocompletion was 5.59 (if no correct venue was offered thefull number of turns was counted).4 Summary and Future WorkThis paper has investigated a new agenda-based user sim-ulation technique for bootstrapping a statistical dialoguemanager without access to training data.
Evaluation re-sults show that, even with manually set model parame-ters, the simulator produces dialogue behaviour realisticenough for training and testing a prototype system.
Whilethe results demonstrate that the learned policy works wellfor real users, it is not necessarily optimal.
The next stepis hence to use the recorded data to train the simulator,and to then retrain the DM policy.ReferencesO.
Lemon, K. Georgila, and J. Henderson.
2006.
EvaluatingEffectiveness and Portability of Reinforcement Learned Di-alogue Strategies with real users: the TALK TownInfo Eval-uation.
In Proc.
of IEEE/ACL SLT, Palm Beach, Aruba.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
A StochasticModel of Human-Machine Interaction for Learning DialogStrategies.
IEEE Trans.
on Speech and Audio Processing,8(1):11?23.O.
Pietquin and T. Dutoit.
2005.
A probabilistic framework fordialog simulation and optimal strategy learning.
IEEE Trans.on Speech and Audio Processing, Special Issue on Data Min-ing of Speech, Audio and Dialog.J.
Schatzmann, K. Weilhammer, M.N.
Stuttle, and S. Young.2006.
A Survey of Statistical User Simulation Tech-niques for Reinforcement-Learning of Dialogue Manage-ment Strategies.
Knowledge Engineering Review, 21(2):97?126.X Wei and AI Rudnicky.
1999.
An agenda-based dialog man-agement architecture for spoken language systems.
In Proc.of IEEE ASRU.
Seattle, WA.J.
Williams and S. Young.
2007.
Partially Observable MarkovDecision Processes for Spoken Dialog Systems.
ComputerSpeech and Language, 21(2):231?422.S.
Young, J. Schatzmann, K. Weilhammer, and H. Ye.
2007.The Hidden Information State Approach to Dialog Manage-ment.
In Proc.
of ICASSP, Honolulu, Hawaii.S.
Young.
2006.
Using POMDPs for Dialog Management.
InProc.
of IEEE/ACL SLT, Palm Beach, Aruba.152
