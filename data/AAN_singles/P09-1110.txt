Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 976?984,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPLearning Context-Dependent Mappings from Sentences to Logical FormLuke S. Zettlemoyer and Michael CollinsMIT CSAILCambridge, MA 02139{lsz,mcollins}@csail.mit.comAbstractWe consider the problem of learningcontext-dependent mappings from sen-tences to logical form.
The training ex-amples are sequences of sentences anno-tated with lambda-calculus meaning rep-resentations.
We develop an algorithm thatmaintains explicit, lambda-calculus repre-sentations of salient discourse entities anduses a context-dependent analysis pipelineto recover logical forms.
The method usesa hidden-variable variant of the percep-tion algorithm to learn a linear model usedto select the best analysis.
Experimentson context-dependent utterances from theATIS corpus show that the method recov-ers fully correct logical forms with 83.7%accuracy.1 IntroductionRecently, researchers have developed algorithmsthat learn to map natural language sentences torepresentations of their underlying meaning (Heand Young, 2006; Wong and Mooney, 2007;Zettlemoyer and Collins, 2005).
For instance, atraining example might be:Sent.
1: List flights to Boston on Friday night.LF 1: ?x.flight(x) ?
to(x, bos)?
day(x, fri) ?
during(x, night)Here the logical form (LF) is a lambda-calculusexpression defining a set of entities that are flightsto Boston departing on Friday night.Most of this work has focused on analyzing sen-tences in isolation.
In this paper, we consider theproblem of learning to interpret sentences whoseunderlying meanings can depend on the context inwhich they appear.
For example, consider an inter-action where Sent.
1 is followed by the sentence:Sent.
2: Show me the flights after 3pm.LF 2: ?x.flight(x) ?
to(x, bos)?day(x, fri) ?
depart(x) > 1500In this case, the fact that Sent.
2 describes flightsto Boston on Friday must be determined based onthe context established by the first sentence.We introduce a supervised, hidden-variable ap-proach for learning to interpret sentences in con-text.
Each training example is a sequence of sen-tences annotated with logical forms.
Figure 1shows excerpts from three training examples in theATIS corpus (Dahl et al, 1994).For context-dependent analysis, we develop anapproach that maintains explicit, lambda-calculusrepresentations of salient discourse entities anduses a two-stage pipeline to construct context-dependent logical forms.
The first stage usesa probabilistic Combinatory Categorial Grammar(CCG) parsing algorithm to produce a context-independent, underspecified meaning representa-tion.
The second stage resolves this underspecifiedmeaning representation by making a sequence ofmodifications to it that depend on the context pro-vided by previous utterances.In general, there are a large number of possi-ble context-dependent analyses for each sentence.To select the best one, we present a weighted lin-ear model that is used to make a range of parsingand context-resolution decisions.
Since the train-ing data contains only the final logical forms, wemodel these intermediate decisions as hidden vari-ables that must be estimated without explicit su-pervision.
We show that this model can be effec-tively trained with a hidden-variable variant of theperceptron algorithm.In experiments on the ATIS DEC94 test set, theapproach recovers fully correct logical forms with83.7% accuracy.2 The Learning ProblemWe assume access to a training set that consists ofn interactions D = ?I1, .
.
.
, In?.
The i?th interac-tion Ii contains ni sentences,wi,1, .
.
.
, wi,ni .
Eachsentence wi,j is paired with a lambda-calculus ex-976Example #1:(a) show me the flights from boston to philly?x.flight(x) ?
from(x, bos) ?
to(x, phi)(b) show me the ones that leave in the morning?x.flight(x) ?
from(x, bos) ?
to(x, phi)?
during(x,morning)(c) what kind of plane is used on these flights?y.
?x.flight(x) ?
from(x, bos) ?
to(x, phi)?
during(x,morning) ?
aircraft(x) = yExample #2:(a) show me flights from milwaukee to orlando?x.flight(x) ?
from(x,mil) ?
to(x, orl)(b) cheapestargmin(?x.flight(x) ?
from(x,mil) ?
to(x, orl),?y.fare(y))(c) departing wednesday after 5 o?clockargmin(?x.flight(x) ?
from(x,mil) ?
to(x, orl)?
day(x,wed) ?
depart(x) > 1700 ,?y.fare(y))Example #3:(a) show me flights from pittsburgh to la thursday evening?x.flight(x) ?
from(x, pit) ?
to(x, la)?
day(x, thur) ?
during(x, evening)(b) thursday afternoon?x.flight(x) ?
from(x, pit) ?
to(x, la)?
day(x, thur) ?
during(x, afternoon)(c) thursday after 1700 hours?x.flight(x) ?
from(x, pit) ?
to(x, la)?
day(x, thur) ?
depart(x) > 1700Figure 1: ATIS interaction excerpts.pression zi,j specifying the target logical form.Figure 1 contains example interactions.The logical forms in the training set are repre-sentations of each sentence?s underlying meaning.In most cases, context (the previous utterances andtheir interpretations) is required to recover the log-ical form for a sentence.
For instance, in Exam-ple 1(b) in Figure 1, the sentence ?show me theones that leave in the morning?
is paired with?x.flight(x) ?
from(x, bos) ?
to(x, phi)?
during(x,morning)Some parts of this logical form (from(x, bos) andto(x, phi)) depend on the context.
They have to berecovered from the previous logical forms.At step j in interaction i, we define the con-text ?zi,1, .
.
.
, zi,j?1?
to be the j ?
1 precedinglogical forms.1 Now, given the training data, wecan create training examples (xi,j , zi,j) for i =1 .
.
.
n, j = 1 .
.
.
ni.
Each xi,j is a sentence anda context, xi,j = (wi,j , ?zi,1, .
.
.
, zi,j?1?).
Giventhis set up, we have a supervised learning problemwith input xi,j and output zi,j .1In general, the context could also include the previoussentences wi,k for k < j.
In our data, we never observed anyinteractions where the choice of the correct logical form zi,jdepended on the words in the previous sentences.3 Overview of ApproachIn general, the mapping from a sentence and a con-text to a logical form can be quite complex.
In thissection, we present an overview of our learningapproach.
We assume the learning algorithm hasaccess to:?
A training set D, defined in Section 2.?
A CCG lexicon.2 See Section 4 for anoverview of CCG.
Each entry in the lexiconpairs a word (or sequence of words), witha CCG category specifying both the syntaxand semantics for that word.
One exampleCCG entry would pair flights with the cate-gory N : ?x.flight(x).Derivations A derivation for the j?th sentencein an interaction takes as input a pair x = (wj , C),where C = ?z1 .
.
.
zj?1?
is the current context.
Itproduces a logical form z.
There are two stages:?
First, the sentence wj is parsed usingthe CCG lexicon to form an intermediate,context-independent logical form pi.?
Second, in a series of steps, pi is mapped to z.These steps depend on the context C.As one sketch of a derivation, consider how wemight analyze Example 1(b) in Figure 1.
In thiscase the sentence is ?show me the ones that leavein the morning.?
The CCG parser would producethe following context-independent logical form:?x.!
?e, t?
(x) ?
during(x,morning)The subexpression !
?e, t?
results directly from thereferential phrase the ones; we discuss this in moredetail in Section 4.2, but intuitively this subexpres-sion specifies that a lambda-calculus expression oftype ?e, t?
must be recovered from the context andsubstituted in its place.In the second (contextually dependent) stage ofthe derivation, the expression?x.flight(x) ?
from(x, bos) ?
to(x, phi)is recovered from the context, and substituted forthe !
?e, t?
subexpression, producing the desired fi-nal logical form, seen in Example 1(b).2Developing algorithms that learn the CCG lexicon fromthe data described in this paper is an important area for futurework.
We could possibly extend algorithms that learn fromcontext-independent data (Zettlemoyer and Collins, 2005).977In addition to substitutions of this type, we willalso perform other types of context-dependent res-olution steps, as described in Section 5.In general, both of the stages of the derivationinvolve considerable ambiguity ?
there will be alarge number of possible context-independent log-ical forms pi for wj and many ways of modifyingeach pi to create a final logical form zj .Learning We model the problem of selectingthe best derivation as a structured prediction prob-lem (Johnson et al, 1999; Lafferty et al, 2001;Collins, 2002; Taskar et al, 2004).
We presenta linear model with features for both the parsingand context resolution stages of the derivation.
Inour setting, the choice of the context-independentlogical form pi and all of the steps that map pi tothe output z are hidden variables; these steps arenot annotated in the training data.
To estimate theparameters of the model, we use a hidden-variableversion of the perceptron algorithm.
We use an ap-proximate search procedure to find the best deriva-tion both while training the model and while ap-plying it to test examples.Evaluation We evaluate the approach on se-quences of sentences ?w1, .
.
.
, wk?.
For each wj ,the algorithm constructs an output logical form zjwhich is compared to a gold standard annotation tocheck correctness.
At step j, the context containsthe previous zi, for i < j, output by the system.4 Context-independent ParsingIn this section, we first briefly review the CCGparsing formalism.
We then define a set of ex-tensions that allow the parser to construct logicalforms containing references, such as the !
?e, t?
ex-pression from the example derivation in Section 3.4.1 Background: CCGCCG is a lexicalized, mildly context-sensitiveparsing formalism that models a wide range oflinguistic phenomena (Steedman, 1996; Steed-man, 2000).
Parses are constructed by combininglexical entries according to a small set of relativelysimple rules.
For example, consider the lexiconflights := N : ?x.flight(x)to := (N\N)/NP : ?y.?f.
?x.f(x) ?
to(x, y)boston := NP : bostonEach lexical entry consists of a word and a cat-egory.
Each category includes syntactic and se-mantic content.
For example, the first entrypairs the word flights with the category N :?x.flight(x).
This category has syntactic typeN ,and includes the lambda-calculus semantic expres-sion ?x.flight(x).
In general, syntactic types caneither be simple types such as N , NP , or S, orcan be more complex types that make use of slashnotation, for example (N\N)/NP .CCG parses construct parse trees according toa set of combinator rules.
For example, considerthe functional application combinators:3A/B : f B : g ?
A : f(g) (>)B : g A\B : f ?
A : f(g) (<)The first rule is used to combine a category withsyntactic type A/B with a category to the rightof syntactic type B to create a new category oftype A.
It also constructs a new lambda-calculusexpression by applying the function f to theexpression g. The second rule handles argumentsto the left.
Using these rules, we can parse thefollowing phrase:flights to bostonN (N\N)/NP NP?x.flight(x) ?y.?f.
?x.f(x) ?
to(x, y) boston>(N\N)?f.
?x.f(x) ?
to(x, boston)<N?x.flight(x) ?
to(x, boston)The top-most parse operations pair each word witha corresponding category from the lexicon.
Thelater steps are labeled with the rule that was ap-plied (?> for the first and ?< for the second).4.2 Parsing with ReferencesIn this section, we extend the CCG parser to intro-duce references.
We use an exclamation point fol-lowed by a type expression to specify referencesin a logical form.
For example, !e is a reference toan entity and !
?e, t?
is a reference to a function.
Asmotivated in Section 3, we introduce these expres-sions so they can later be replaced with appropriatelambda-calculus expressions from the context.Sometimes references are lexically triggered.For example, consider parsing the phrase ?showme the ones that leave in the morning?
from Ex-ample 1(b) in Figure 1.
Given the lexical entry:ones := N : ?x.!
?e, t?
(x)a CCG parser could produce the desired context-3In addition to application, we make use of composition,type raising and coordination combinators.
A full descriptionof these combinators is beyond the scope of this paper.
Steed-man (1996; 2000) presents a detailed description of CCG.978independent logical form:?x.!
?e, t?
(x) ?
during(x,morning)Our first extension is to simply introduce lexicalitems that include references into the CCG lexi-con.
They describe anaphoric words, for exampleincluding ?ones,?
?those,?
and ?it.
?In addition, we sometimes need to introducereferences when there is no explicit lexical trig-ger.
For instance, Example 2(c) in Figure 1 con-sists of the single word ?cheapest.?
This query hasthe same meaning as the longer request ?show methe cheapest one,?
but it does not include the lex-ical reference.
We add three CCG type-shiftingrules to handle these cases.The first two new rules are applicable whenthere is a category that is expecting an argumentwith type ?e, t?.
This argument is replaced with a!
?e, t?
reference:A/B : f ?
A : f(?x.!
?e, t?
(x))A\B : f ?
A : f(?x.!
?e, t?
(x))For example, using the first rule, we could producethe following parse for Example 2(c)cheapestNP/N?g.argmin(?x.g(x), ?y.fare(y))NPargmin(?x.!
?e, t?
(x), ?y.fare(y))where the final category has the desired lambda-caculus expression.The third rule is motivated by examples such as?show me nonstop flights.?
Consider this sentencebeing uttered after Example 1(a) in Figure 1.
Al-though there is a complete, context-independentmeaning, the request actually restricts the salientset of flights to include only the nonstop ones.
Toachieve this analysis, we introduce the rule:A : f ?
A : ?x.f(x) ?
!
?e, t?
(x)where f is an function of type ?e, t?.With this rule, we can construct the parsenonstop flightsN/N N?f.
?x.f(x) ?
nonstop(x) ?x.flight(x)>N?x.nonstop(x) ?
flight(x)N?x.nonstop(x) ?
flight(x) ?
!
?e, t?
(x)where the last parsing step is achieved with thenew type-shifting rule.These three new parsing rules allow significantflexibility when introducing references.
Later, wedevelop an approach that learns when to introducereferences and how to best resolve them.5 Contextual AnalysisIn this section, we first introduce the general pat-terns of context-dependent analysis that we con-sider.
We then formally define derivations thatmodel these phenomena.5.1 OverviewThis section presents an overview of the ways thatthe context C is used during the analysis.References Every reference expression (!e or!
?e, t?)
must be replaced with an expression fromthe context.
For example, in Section 3, we consid-ered the following logical form:?x.!
?e, t?
(x) ?
during(x,morning)In this case, we saw that replacing the !
?e, t?subexpression with the logical form for Exam-ple 1(a), which is directly available in C, producesthe desired final meaning.Elaborations Later statements can expand themeaning of previous ones in ways that are diffi-cult to model with references.
For example, con-sider analyzing Example 2(c) in Figure 1.
Here thephrase ?departing wednesday after 5 o?clock?
hasa context-independent logical form:4?x.day(x,wed) ?
depart(x) > 1700 (1)that must be combined with the meaning of theprevious sentence from the context C:argmin(?x.fight(x) ?
from(x,mil) ?
to(x, orl),?y.fare(y))to produce the expressionargmin(?x.fight(x) ?
from(x,mil) ?
to(x, orl)?day(x,wed) ?
depart(x) > 1700,?y.fare(y))Intuitively, the phrase ?departing wednesday af-ter 5 o?clock?
is providing new constraints for theset of flights embedded in the argmin expression.We handle examples of this type by construct-ing elaboration expressions from the zi in C. Forexample, if we constructed the following function:?f.argmin(?x.fight(x) ?
from(x,mil)?
to(x, orl) ?
f(x), (2)?y.fare(y))4Another possible option is the expression ?x.!
?e, t?
?day(x,wed)?
depart(x) > 1700.
However, there is no ob-vious way to resolve the !
?e, t?
expression that would producethe desired final meaning.979we could apply this function to Expression 1 andproduce the desired result.
The introduction of thenew variable f provides a mechanism for expand-ing the embedded subexpression.References with Deletion When resolving ref-erences, we will sometimes need to delete subpartsof the expressions that we substitute from the con-text.
For instance, consider Example 3(b) in Fig-ure 1.
The desired, final logical form is:?x.flight(x) ?
from(x, pit) ?
to(x, la)?
day(x, thur) ?
during(x, afternoon)We need to construct this from the context-independent logical form:?x.!
?e, t?
?
day(x, thur) ?
during(x, afternoon)The reference !
?e, t?
must be resolved.
The onlyexpression in the context C is the meaning fromthe previous sentence, Example 3(a):?x.flight(x) ?
from(x, pit) ?
to(x, la) (3)?
day(x, thur) ?
during(x, evening)Substituting this expression directly would pro-duce the following logical form:?x.flight(x) ?
from(x, pit) ?
to(x, la)?
day(x, thur) ?
during(x, evening)?
day(x, thur) ?
during(x, afternoon)which specifies the day twice and has two differenttime spans.We can achieve the desired analysis by deletingparts of expressions before they are substituted.For example, we could remove the day and timeconstraints from Expression 3 to create:?x.flight(x) ?
from(x, pit) ?
to(x, la)which would produce the desired final meaningwhen substituted into the original expression.Elaborations with Deletion We also allowdeletions for elaborations.
In this case, we deletesubexpressions of the elaboration expression thatis constructed from the context.5.2 DerivationsWe now formally define a derivation that maps asentence wj and a context C = {z1, .
.
.
, zj?1} toan output logical form zj .
We first introduce no-tation for expressions in C that we will use in thederivation steps.
We then present a definition ofdeletion.
Finally, we define complete derivations.Context Sets Given a context C, our algorithmconstructs three sets of expressions:?
Re(C): A set of e-type expressions that canbe used to resolve references.?
R?e,t?
(C): A set of ?e, t?-type expressionsthat can be used to resolve references.?
E(C): A set of possible elaboration expres-sions (for example, see Expression 2).We will provide the details of how these setsare defined in Section 5.3.
As an example, if Ccontains only the logical form?x.flight(x) ?
from(x, pit) ?
to(x, la)then Re(C) = {pit, la} and R?e,t?
(C) is a set thatcontains a single entry, the complete logical form.Deletion A deletion operator accepts a logicalform l and produces a new logical form l?.
It con-structs l?
by removing a single subexpression thatappears in a coordination (conjunction or disjunc-tion) in l. For example, if l is?x.flight(x) ?
from(x, pit) ?
to(x, la)there are three possible deletion operations, eachof which removes a single subexpression.Derivations We now formally define a deriva-tion to be a sequence d = (?, s1, .
.
.
, sm).
?
is aCCG parse that constructs a context-independentlogical form pi with m?
1 reference expressions.5Each si is a function that accepts as input a logi-cal form, makes some change to it, and produces anew logical form that is input to the next functionsi+1.
The initial si for i < m are reference steps.The final sm is an optional elaboration step.?
Reference Steps: A reference step is a tuple(l, l?, f, r, r1, .
.
.
, rp).
This operator selects areference f in the input logical form l andan appropriately typed expression r from ei-ther Re(C) or R?e,t?(C).
It then applies a se-quence of p deletion operators to create newexpressions r1 .
.
.
rp.
Finally, it constructsthe output logical form l?
by substituting rpfor the selected reference f in l.?
Elaboration Steps: An elaboration step is atuple (l, l?, b, b1, .
.
.
, bq).
This operator se-lects an expression b from E(C) and ap-plies q deletions to create new expressionsb1 .
.
.
bq.
The output expression l?
is bq(l).5In practice, pi rarely contains more than one reference.980In general, the space of possible derivations islarge.
In Section 6, we describe a linear modeland decoding algorithm that we use to find highscoring derivations.5.3 Context SetsFor a context C = {z1, .
.
.
, zj?1}, we define setsRe(C), R?e,t?
(C), and E(C) as follows.e-type Expressions Re(z) is a set of e-type ex-pressions extracted from a logical form z.
We de-fine Re(C) =?j?1i=1 Re(zi).Re(z) includes all e-type subexpressions of z.6For example, if z isargmin(?x.flight(x) ?
from(x,mil) ?
to(x, orl),?y.fare(y))the resulting set isRe(z) = {mil, orl, z}, where zis included because the entire argmin expressionhas type e.?e, t?-type Expressions R?e,t?
(z) is a set of?e, t?-type expressions extracted from a logicalform z.
We define R?e,t?
(C) =?j?1i=1 R?e,t?
(zi).The set R?e,t?
(z) contains all of the ?e, t?-typesubexpressions of z.
For each quantified vari-able x in z, it also contains a function ?x.g.
Theexpression g contains the subexpressions in thescope of x that do not have free variables.
Forexample, if z is?y.
?x.flight(x) ?
from(x, bos) ?
to(x, phi)?
during(x,morning) ?
aircraft(x) = yR?e,t?
(z) would contain two functions: the entireexpression z and the function?x.flight(x) ?
from(x, bos) ?
to(x, phi)?
during(x,morning)constructed from the variable x, where the subex-pression aircraft(x) = y has been removed be-cause it contains the free variable y.Elaboration Expressions Finally, E(z) is a setof elaboration expressions constructed from a log-ical form z.
We define E(C) =?j?1i=1 E(zi).E(z) is defined by enumerating the placeswhere embedded variables are found in z. Foreach logical variable x and each coordination(conjunction or disjunction) in the scope of x, anew expression is created by defining a function?f.z?
where z?
has the function f(x) added to theappropriate coordination.
This procedure would6A lambda-calculus expression can be represented as atree structure with flat branching for coordination (conjunc-tion and disjunction).
The subexpressions are the subtrees.produce the example elaboration Expression 2 andelaborations that expand other embedded expres-sions, such as the quantifier in Example 1(c).6 A Linear ModelIn general, there will be many possible derivationsd for an input sentence w in the current contextC.
In this section, we introduce a weighted lin-ear model that scores derivations and a decodingalgorithm that finds high scoring analyses.We define GEN(w;C) to be the set of possiblederivations d for an input sentence w given a con-textC, as described in Section 5.2.
Let ?
(d) ?
Rmbe an m-dimensional feature representation for aderivation d and ?
?
Rm be an m-dimensional pa-rameter vector.
The optimal derivation for a sen-tence w given context C and parameters ?
isd?
(w;C) = arg maxd?GEN(w;C)?
?
?
(d)Decoding We now describe an approximate al-gorithm for computing d?
(w;C).The CCG parser uses a CKY-style chart parsingalgorithm that prunes to the top N = 50 entriesfor each span in the chart.We use a beam search procedure to find thebest contextual derivations, with beam size N =50.
The beam is initialized to the top N logi-cal forms from the CCG parser.
The derivationsare extended with reference and elaboration steps.The only complication is selecting the sequence ofdeletions.
For each possible step, we use a greedysearch procedure that selects the sequence of dele-tions that would maximize the score of the deriva-tion after the step is applied.7 LearningFigure 2 details the complete learning algorithm.Training is online and error-driven.
Step 1 parsesthe current sentence in context.
If the optimal logi-cal form is not correct, Step 2 finds the best deriva-tion that produces the labeled logical form7 anddoes an additive, perceptron-style parameter up-date.
Step 3 updates the context.
This algorithm isa direct extension of the one introduced by Zettle-moyer and Collins (2007).
It maintains the contextbut does not have the lexical induction step thatwas previously used.7For this computation, we use a modified version of thebeam search algorithm described in Section 6, which prunesderivations that could not produce the desired logical form.981Inputs: Training examples {Ii|i = 1 .
.
.
n}.
Each Ii is asequence {(wi,j , zi,j) : j = 1 .
.
.
ni} where wi,j is asentence and zi,j is a logical form.
Number of trainingiterations T .
Initial parameters ?.Definitions: The function ?
(d) represents the features de-scribed in Section 8.
GEN(w;C) is the set of deriva-tions for sentence w in context C. GEN(w, z;C) isthe set of derivations for sentence w in context C thatproduce the final logical form z.
The function L(d)maps a derivation to its associated final logical form.Algorithm:?
For t = 1 .
.
.
T, i = 1 .
.
.
n: (Iterate interactions)?
Set C = {}.
(Reset context)?
For j = 1 .
.
.
ni: (Iterate training examples)Step 1: (Check correctness)?
Let d?
= argmaxd?GEN(wi,j ;C) ?
?
?
(d) .?
If L(d?)
= zi,j , go to Step 3.Step 2: (Update parameters)?
Let d?
= argmaxd?GEN(wi,j ,zi,j ;C) ?
?
?
(d) .?
Set ?
= ?
+ ?(d?)
?
?(d?)
.Step 3: (Update context)?
Append zi,j to the current context C.Output: Estimated parameters ?.Figure 2: An online learning algorithm.8 FeaturesWe now describe the features for both the parsingand context resolution stages of the derivation.8.1 Parsing FeaturesThe parsing features are used to score the context-independent CCG parses during the first stage ofanalysis.
We use the set developed by Zettlemoyerand Collins (2007), which includes features thatare sensitive to lexical choices and the structure ofthe logical form that is constructed.8.2 Context FeaturesThe context features are functions of the deriva-tion steps described in Section 5.2.
In a deriva-tion for sentence j of an interaction, let l be theinput logical form when considering a new step s(a reference or elaboration step).
Let c be the ex-pression that s selects from a context set Re(zi),R?e,t?
(zi), or E(zi), where zi, i < j, is an ex-pression in the current context.
Also, let r be asubexpression deleted from c. Finally, let f1 andf2 be predicates, for example from or to.Distance Features The distance features are bi-nary indicators on the distance j ?
i.
These fea-tures allow the model to, for example, favor re-solving references with lambda-calculus expres-sions recovered from recent sentences.Copy Features For each possible f1 there is afeature that tests if f1 is present in the contextexpression c but not in the current expression l.These features allow the model to learn to selectexpressions from the context that introduce ex-pected predicates.
For example, flights usuallyhave a from predicate in the current expression.Deletion Features For each pair (f1, f2) thereis a feature that tests if f1 is in the current expres-sion l and f2 is in the deleted expression r. Forexample, if f1 = f2 = days the model can favoroverriding old constraints about the departure daywith new ones introduced in the current utterance.When f1 = during and f2 = depart time thealgorithm can learn that specific constraints on thedeparture time override more general constraintsabout the period of day.9 Related WorkThere has been a significant amount of work onthe problem of learning context-independent map-pings from sentences to meaning representations.Researchers have developed approaches usingmodels and algorithms from statistical machinetranslation (Papineni et al, 1997; Ramaswamyand Kleindienst, 2000; Wong and Mooney, 2007),statistical parsing (Miller et al, 1996; Ge andMooney, 2005), inductive logic programming(Zelle and Mooney, 1996; Tang and Mooney,2000) and probabilistic push-down automata (Heand Young, 2006).There were a large number of successful hand-engineered systems developed for the originalATIS task and other related tasks (e.g., (Carbonelland Hayes, 1983; Seneff, 1992; Ward and Is-sar, 1994; Levin et al, 2000; Popescu et al,2004)).
We are only aware of one system thatlearns to construct context-dependent interpreta-tions (Miller et al, 1996).
The Miller et al (1996)approach is fully supervised and produces a fi-nal meaning representation in SQL.
It requirescomplete annotation of all of the syntactic, se-mantic, and discourse decisions required to cor-rectly analyze each training example.
In contrast,we learn from examples annotated with lambda-calculus expressions that represent only the final,context-dependent logical forms.Finally, the CCG (Steedman, 1996; Steedman,982Train Dev.
Test AllInteractions 300 99 127 526Sentences 2956 857 826 4637Table 1: Statistics of the ATIS training, development andtest (DEC94) sets, including the total number of interactionsand sentences.
Each interaction is a sequence of sentences.2000) parsing setup is closely related to previousCCG research, including work on learning parsingmodels (Clark and Curran, 2003), wide-coveragesemantic parsing (Bos et al, 2004) and grammarinduction (Watkinson and Manandhar, 1999).10 EvaluationData In this section, we present experiments inthe context-dependent ATIS domain (Dahl et al,1994).
Table 1 presents statistics for the train-ing, development, and test sets.
To facilitate com-parison with previous work, we used the standardDEC94 test set.
We randomly split the remainingdata to make training and development sets.
Wemanually converted the original SQL meaning an-notations to lambda-calculus expressions.Evaluation Metrics Miller et al (1996) reportaccuracy rates for recovering correct SQL annota-tions on the test set.
For comparison, we report ex-act accuracy rates for recovering completely cor-rect lambda-calculus expressions.We also present precision, recall and F-measurefor partial match results that test if individual at-tributes, such as the from and to cities, are cor-rectly assigned.
See the discussion by Zettlemoyerand Collins (2007) (ZC07) for the full details.Initialization and Parameters The CCG lexi-con is hand engineered.
We constructed it by run-ning the ZC07 algorithm to learn a lexicon onthe context-independent ATIS data set and makingmanual corrections to improve performance on thetraining set.
We also added lexical items with ref-erence expressions, as described in Section 4.We ran the learning algorithm for T = 4 train-ing iterations.
The parsing feature weights wereinitialized as in ZC07, the context distance fea-tures were given small negative weights, and allother feature weights were initially set to zero.Test Setup During evaluation, the context C ={z1 .
.
.
zj?1} contains the logical forms output bythe learned system for the previous sentences.
Ingeneral, errors made while constructing these ex-pressions can propogate if they are used in deriva-tions for new sentences.SystemPartial Match ExactPrec.
Rec.
F1 Acc.Full Method 95.0 96.5 95.7 83.7Miller et al ?
?
?
78.4Table 2: Performance on the ATIS DEC94 test set.Limited ContextPartial Match ExactPrec.
Rec.
F1 Acc.M = 0 96.2 57.3 71.8 45.4M = 1 94.9 91.6 93.2 79.8M = 2 94.8 93.2 94.0 81.0M = 3 94.5 94.3 94.4 82.1M = 4 94.9 92.9 93.9 81.6M = 10 94.2 94.0 94.1 81.4Table 3: Performance on the ATIS development set forvarying context window lengths M .Results Table 2 shows performance on the ATISDEC94 test set.
Our approach correctly recov-ers 83.7% of the logical forms.
This result com-pares favorably to Miller et al?s fully-supervisedapproach (1996) while requiring significantly lessannotation effort.We also evaluated performance when the con-text is limited to contain only the M most recentlogical forms.
Table 3 shows results on the devel-opment set for different values of M .
The poorperformance with no context (M = 0) demon-strates the need for context-dependent analysis.Limiting the context to the most recent statement(M = 1) significantly improves performancewhile using the last three utterances (M = 3) pro-vides the best results.Finally, we evaluated a variation where the con-text contains gold-standard logical forms duringevaluation instead of the output of the learnedmodel.
On the development set, this approachachieved 85.5% exact-match accuracy, an im-provement of approximately 3% over the standardapproach.
This result suggests that incorrect log-ical forms in the context have a relatively limitedimpact on overall performance.11 ConclusionIn this paper, we addressed the problem oflearning context-dependent mappings from sen-tences to logical form.
We developed a context-dependent analysis model and showed that it canbe effectively trained with a hidden-variable vari-ant of the perceptron algorithm.
In the experi-ments, we showed that the approach recovers fullycorrect logical forms with 83.7% accuracy.983ReferencesJohan Bos, Stephen Clark, Mark Steedman, James R.Curran, and Julia Hockenmaier.
2004.
Wide-coverage semantic representations from a CCGparser.
In Proceedings of the International Confer-ence on Computational Linguistics.Jaime G. Carbonell and Philip J. Hayes.
1983.
Re-covery strategies for parsing extragrammatical lan-guage.
American Journal of Computational Lin-guistics, 9.Stephen Clark and James R. Curran.
2003.
Log-linearmodels for wide-coverage CCG parsing.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Deborah A. Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg.
1994.
Expanding the scope of the ATIStask: the ATIS-3 corpus.
In ARPA HLT Workshop.Ruifang Ge and Raymond J. Mooney.
2005.
A statis-tical semantic parser that integrates syntax and se-mantics.
In Proceedings of the Conference on Com-putational Natural Language Learning.Yulan He and Steve Young.
2006.
Spoken languageunderstanding using the hidden vector state model.Speech Communication, 48(3-4).Mark Johnson, Stuart Geman, Steven Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Proc.of the Association for Computational Linguistics.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the InternationalConference on Machine Learning.E.
Levin, S. Narayanan, R. Pieraccini, K. Biatov,E.
Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,A.
Pokrovsky, M. Rahim, P. Ruscitti, and M.Walker.2000.
The AT&T darpa communicator mixed-initiative spoken dialogue system.
In Proceedings ofthe International Conference on Spoken LanguageProcessing.Scott Miller, David Stallard, Robert J. Bobrow, andRichard L. Schwartz.
1996.
A fully statistical ap-proach to natural language interfaces.
In Proc.
ofthe Association for Computational Linguistics.K.
A. Papineni, S. Roukos, and T. R. Ward.
1997.Feature-based language understanding.
In Proceed-ings of European Conference on Speech Communi-cation and Technology.Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,David Ko, and Alexander Yates.
2004.
Modernnatural language interfaces to databases: Composingstatistical parsing with semantic tractability.
In Pro-ceedings of the International Conference on Compu-tational Linguistics.Ganesh N. Ramaswamy and Jan Kleindienst.
2000.Hierarchical feature-based translation for scalablenatural language understanding.
In Proceedings ofInternational Conference on Spoken Language Pro-cessing.Stephanie Seneff.
1992.
Robust parsing for spokenlanguage systems.
In Proc.
of the IEEE Conferenceon Acoustics, Speech, and Signal Processing.Mark Steedman.
1996.
Surface Structure and Inter-pretation.
The MIT Press.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press.Lappoon R. Tang and Raymond J. Mooney.
2000.Automated construction of database interfaces: In-tegrating statistical and relational learning for se-mantic parsing.
In Proceedings of the Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Very Large Corpora.Ben Taskar, Dan Klein, Michael Collins, DaphneKoller, and Christopher Manning.
2004.
Max-margin parsing.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing.Wayne Ward and Sunil Issar.
1994.
Recent improve-ments in the CMU spoken language understandingsystem.
In Proceedings of the workshop on HumanLanguage Technology.Stephen Watkinson and Suresh Manandhar.
1999.
Un-supervised lexical learning with categorial gram-mars using the LLL corpus.
In Proceedings of the1st Workshop on Learning Language in Logic.Yuk Wah Wong and Raymond Mooney.
2007.
Learn-ing synchronous grammars for semantic parsingwith lambda calculus.
In Proceedings of the Asso-ciation for Computational Linguistics.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the National Con-ference on Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proceedings of the Conference on Un-certainty in Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In Proc.
of the Joint Conference onEmpirical Methods in Natural Language Processingand Computational Natural Language Learning.984
