Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 815?824,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA Hybrid Hierarchical Model for Multi-Document SummarizationAsli CelikyilmazComputer Science DepartmentUniversity of California, Berkeleyasli@eecs.berkeley.eduDilek Hakkani-TurInternational Computer Science InstituteBerkeley, CAdilek@icsi.berkeley.eduAbstractScoring sentences in documents given ab-stract summaries created by humans is im-portant in extractive multi-document sum-marization.
In this paper, we formulate ex-tractive summarization as a two step learn-ing problem building a generative modelfor pattern discovery and a regressionmodel for inference.
We calculate scoresfor sentences in document clusters basedon their latent characteristics using a hi-erarchical topic model.
Then, using thesescores, we train a regression model basedon the lexical and structural characteris-tics of the sentences, and use the model toscore sentences of new documents to forma summary.
Our system advances currentstate-of-the-art improving ROUGE scoresby ?7%.
Generated summaries are lessredundant and more coherent based uponmanual quality evaluations.1 IntroductionExtractive approach to multi-document summa-rization (MDS) produces a summary by select-ing sentences from original documents.
Doc-ument Understanding Conferences (DUC), nowTAC, fosters the effort on building MDS systems,which take document clusters (documents on asame topic) and description of the desired sum-mary focus as input and output a word length lim-ited summary.
Human summaries are provided fortraining summarization models and measuring theperformance of machine generated summaries.Extractive summarization methods can be clas-sified into two groups: supervised methods thatrely on provided document-summary pairs, andunsupervised methods based upon properties de-rived from document clusters.
Supervised meth-ods treat the summarization task as a classifica-tion/regression problem, e.g., (Shen et al, 2007;Yeh et al, 2005).
Each candidate sentence isclassified as summary or non-summary based onthe features that they pose and those with high-est scores are selected.
Unsupervised methodsaim to score sentences based on semantic group-ings extracted from documents, e.g., (Daume?IIIand Marcu, 2006; Titov and McDonald, 2008;Tang et al, 2009; Haghighi and Vanderwende,2009; Radev et al, 2004; Branavan et al, 2009),etc.
Such models can yield comparable or bet-ter performance on DUC and other evaluations,since representing documents as topic distribu-tions rather than bags of words diminishes the ef-fect of lexical variability.
To the best of our knowl-edge, there is no previous research which utilizesthe best features of both approaches for MDS aspresented in this paper.In this paper, we present a novel approach thatformulates MDS as a prediction problem basedon a two-step hybrid model: a generative modelfor hierarchical topic discovery and a regressionmodel for inference.
We investigate if a hierarchi-cal model can be adopted to discover salient char-acteristics of sentences organized into hierarchiesutilizing human generated summary text.We present a probabilistic topic model on sen-tence level building on hierarchical Latent Dirich-let Allocation (hLDA) (Blei et al, 2003a), whichis a generalization of LDA (Blei et al, 2003b).
Weconstruct a hybrid learning algorithm by extract-ing salient features to characterize summary sen-tences, and implement a regression model for in-ference (Fig.3).
Contributions of this work are:?
construction of hierarchical probabilistic modeldesigned to discover the topic structures of all sen-tences.
Our focus is on identifying similarities ofcandidate sentences to summary sentences using anovel tree based sentence scoring algorithm, con-cerning topic distributions at different levels of thediscovered hierarchy as described in ?
3 and ?
4,?
representation of sentences by meta-features to815characterize their candidacy for inclusion in sum-mary text.
Our aim is to find features that can bestrepresent summary sentences as described in ?
5,?
implementation of a feasible inference methodbased on a regression model to enable scoring ofsentences in test document clusters without re-training, (which has not been investigated in gen-erative summarization models) described in ?
5.2.We show in ?
6 that our hybrid summarizerachieves comparable (if not better) ROUGE scoreon the challenging task of extracting the sum-maries of multiple newswire documents.
The hu-man evaluations confirm that our hybrid model canproduce coherent and non-redundant summaries.2 Background and MotivationThere are many studies on the principles govern-ing multi-document summarization to produce co-herent and semantically relevant summaries.
Pre-vious work (Nenkova and Vanderwende, 2005;Conroy et al, 2006), focused on the fact that fre-quency of words plays an important factor.
While,earlier work on summarization depend on a wordscore function, which is used to measure sentencerank scores based on (semi-)supervised learn-ing methods, recent trend of purely data-drivenmethods, (Barzilay and Lee, 2004; Daume?III andMarcu, 2006; Tang et al, 2009; Haghighi andVanderwende, 2009), have shown remarkable im-provements.
Our work builds on both methods byconstructing a hybrid approach to summarization.Our objective is to discover from documentclusters, the latent topics that are organized into hi-erarchies following (Haghighi and Vanderwende,2009).
A hierarchical model is particularly ap-pealing to summarization than a ?flat?
model, e.g.LDA (Blei et al, 2003b), in that one can discover?abstract?
and ?specific?
topics.
For instance, dis-covering that ?baseball?
and ?football?
are bothcontained in an abstract class ?sports?
can help toidentify summary sentences.
It follows that sum-mary topics are commonly shared by many docu-ments, while specific topics are more likely to bementioned in rather a small subset of documents.Feature based learning approaches to summa-rization methods discover salient features by mea-suring similarity between candidate sentences andsummary sentences (Nenkova and Vanderwende,2005; Conroy et al, 2006).
While such methodsare effective in extractive summarization, the factthat some of these methods are based on greedyalgorithms can limit the application areas.
More-over, using information on the hidden semanticstructure of document clusters would improve theperformance of these methods.Recent studies focused on the discovery of la-tent topics of document sets in extracting sum-maries.
In these models, the challenges of infer-ring topics of test documents are not addressedin detail.
One of the challenges of using a pre-viously trained topic model is that the new docu-ment might have a totally new vocabulary or mayinclude many other specific topics, which may ormay not exist in the trained model.
A commonmethod is to re-build a topic model for new setsof documents (Haghighi and Vanderwende, 2009),which has proven to produce coherent summaries.An alternative yet feasible solution, presented inthis work, is building a model that can summa-rize new document clusters using characteristicsof topic distributions of training documents.
Ourapproach differs from the early work, in that, wecombine a generative hierarchical model and re-gression model to score sentences in new docu-ments, eliminating the need for building a genera-tive model for new document clusters.3 Summary-Focused Hierarchical ModelOur MDS system, hybrid hierarchical summa-rizer, HybHSum, is based on an hybrid learn-ing approach to extract sentences for generatingsummary.
We discover hidden topic distributionsof sentences in a given document cluster alongwith provided summary sentences based on hLDAdescribed in (Blei et al, 2003a)1.
We build asummary-focused hierarchical probabilistic topicmodel, sumHLDA, for each document cluster atsentence level, because it enables capturing ex-pected topic distributions in given sentences di-rectly from the model.
Besides, document clusterscontain a relatively small number of documents,which may limit the variability of topics if they areevaluated on the document level.
As described in ?4, we present a new method for scoring candidatesentences from this hierarchical structure.Let a given document cluster D be representedwith sentences O={om}|O|m=1 and its correspondinghuman summary be represented with sentencesS={sn}|S|n=1.
All sentences are comprised of wordsV ={w1, w2, ..w|V |}in {O ?
S}.1Please refer to (Blei et al, 2003b) and (Blei et al, 2003a)for details and demonstrations of topic models.816Summary hLDA (sumHLDA): The hLDArepresents distribution of topics in sentences byorganizing topics into a tree of a fixed depth L(Fig.1.a).
Each candidate sentence om is assignedto a path com in the tree and each word wi in agiven sentence is assigned to a hidden topic zomat a level l of com .
Each node is associated with atopic distribution over words.
The sampler methodalternates between choosing a new path for eachsentence through the tree and assigning each wordin each sentence to a topic along that path.
Thestructure of tree is learnt along with the topics us-ing a nested Chinese restaurant process (nCRP)(Blei et al, 2003a), which is used as a prior.The nCRP is a stochastic process, which as-signs probability distributions to infinitely branch-ing and infinitely deep trees.
In our model, nCRPspecifies a distribution of words into paths in anL-level tree.
The assignments of sentences topaths are sampled sequentially: The first sentencetakes the initial L-level path, starting with a sin-gle branch tree.
Later, mth subsequent sentence isassigned to a path drawn from the distribution:p(pathold, c|m,mc) =mc?+m?1p(pathnew, c|m,mc) =?
?+m?1(1)pathold and pathnew represent an existing andnovel (branch) path consecutively, mc is the num-ber of previous sentences assigned to path c, m isthe total number of sentences seen so far, and ?
isa hyper-parameter which controls the probabilityof creating new paths.
Based on this probabilityeach node can branch out a different number ofchild nodes proportional to ?.
Small values of ?suppress the number of branches.Summary sentences generally comprise abstractconcepts of the content.
With sumHLDA we wantto capture these abstract concepts in candidate sen-tences.
The idea is to represent each path sharedby similar candidate sentences with representativesummary sentence(s).
We let summary sentencesshare existing paths generated by similar candi-date sentences instead of sampling new paths andinfluence the tree structure by introducing two sep-arate hyper-parameters for nCRP prior:?
if a summary sentence is sampled, use ?
= ?s,?
if a candidate sentence is sampled, use ?
= ?o.At each node, we let summary sentences samplea path by choosing only from the existing childrenof that node with a probability proportional to thenumber of other sentences assigned to that child.This can be achieved by using a small value for ?s(0 < ?s ?
1).
We only let candidate sentencesto have an option of creating a new child nodewith a probability proportional to ?o.
By choos-ing ?s ?
?o we suppress the generation of newbranches for summary sentences and modify the?
of nCRP prior in Eq.
(1) using ?s and ?o hyper-parameters for different sentence types.
In the ex-periments, we discuss the effects of this modifica-tion on the hierarchical topic tree.The following is the generative process forsumHLDA used in our HybHSum :(1) For each topic k ?
T , sample a distribution?k v Dirichlet(?).
(2) For each sentence d ?
{O ?
S},(a) if d ?
O, draw a path cd v nCRP(?o),else if d ?
S, draw a path cd v nCRP(?s).
(b) Sample L-vector ?d mixing weights fromDirichlet distribution ?d ?
Dir(?).
(c) For each word n, choose: (i) level zd,n|?dand (ii) word wd,n| {zd,n, cd, ?
}Given sentence d, ?d is a vector of topic pro-portions from L dimensional Dirichlet parameter-ized by ?
(distribution over levels in the tree.)
Thenth word of d is sampled by first choosing a levelzd,n = l from the discrete distribution ?d withprobability ?d,l.
Dirichlet parameter ?
and ?o con-trol the size of tree effecting the number of topics.
(Small values of ?s do not effect the tree.)
Largevalues of ?
favor more topics (Blei et al, 2003a).Model Learning: Gibbs sampling is a commonmethod to fit the hLDA models.
The aim is to ob-tain the following samples from the posterior of:(i) the latent tree T , (ii) the level assignment z forall words, (iii) the path assignments c for all sen-tences conditioned on the observed words w.Given the assignment of words w to levels z andassignments of sentences to paths c, the expectedposterior probability of a particular word w at agiven topic z=l of a path c=c is proportional to thenumber of times w was generated by that topic:p(w|z, c,w, ?)
?
n(z=l,c=c,w=w) + ?
(2)Similarly, posterior probability of a particulartopic z in a given sentence d is proportional tonumber of times z was generated by that sentence:p(z|z, c, ?)
?
n(c=cd,z=l) + ?
(3)n(.)
is the count of elements of an array satisfy-ing the condition.
Note from Eq.
(3) that two sen-tences d1 and d2 on the same path c would have817different words, and hence different posterior topicprobabilities.
Posterior probabilities are normal-ized with total counts and their hyperparameters.4 Tree-Based Sentence ScoringThe sumHLDA constructs a hierarchical treestructure of candidate sentences (per documentcluster) by positioning summary sentences on thetree.
Each sentence is represented by a path in thetree, and each path can be shared by many sen-tences.
The assumption is that sentences sharingthe same path should be more similar to each otherbecause they share the same topics.
Moreover, ifa path includes a summary sentence, then candi-date sentences on that path are more likely to beselected for summary text.
In particular, the sim-ilarity of a candidate sentence om to a summarysentence sn sharing the same path is a measureof strength, indicating how likely om is to be in-cluded in the generated summary (Algorithm 1):Let com be the path for a given om.
We findsummary sentences that share the same path withom via: M = {sn ?
S|csn = com}.
The score ofeach sentence is calculated by similarity to the bestmatching summary sentence in M :score(om) = maxsn?M sim(om, sn) (4)If M=?, then score(om)=?.
The efficiency of oursimilarity measure in identifying the best match-ing summary sentence, is tied to how expressivethe extracted topics of our sumHLDA models are.Given path com , we calculate the similarity of omto each sn, n=1..|M | by measuring similarities on:?
sparse unigram distributions (sim1) at eachtopic l on com : similarity between p(wom,l|zom =l, com , vl) and p(wsn,l|zsn = l, com , vl)??
distributions of topic proportions (sim2);similarity between p(zom |com) and p(zsn |com).?
sim1: We define two sparse (discrete) un-igram distributions for candidate om and sum-mary sn at each node l on a vocabulary iden-tified with words generated by the topic at thatnode, vl ?
V .
Given wom ={w1, ..., w|om|},let wom,l ?
wom be the set of words in om thatare generated from topic zom at level l on pathcom .
The discrete unigram distribution poml =p(wom,l|zom = l, com , vl) represents the probabil-ity over all words vl assigned to topic zom at levell, by sampling only for words in wom,l.
Similarly,psn,l = p(wsn,l|zsn , com , vl) is the probability ofwords wsn in sn of the same topic.
The proba-bility of each word in pom,l and psn,l are obtainedusing Eq.
(2) and then normalized (see Fig.1.b).Algorithm 1 Tree-Based Sentence Scoring1: Given tree T from sumHLDA, candidate and summarysentences: O = {o1, ..., om} , S = {s1, ..., sn}2: for sentences m?
1, ..., |O| do3: - Find path com on tree T and summary sentences4: on path com : M = {sn ?
S|csn = com}5: for summary sentences n?
1, ..., |M | do6: - Find score(om)=maxsn sim(om, sn),7: where sim(om, sn) = sim1 ?
sim28: using Eq.
(7) and Eq.
(8)9: end for10: end for11: Obtain scores Y = {score(om)}|O|m=1The similarity between pom,l and psn,l isobtained by first calculating the divergencewith information radius- IR based on Kullback-Liebler(KL) divergence, p=pom,l, q=psn,l :IRcom ,l(pom,l, psn,l)=KL(p||p+q2 )+KL(q||p+q2 ) (5)where, KL(p||q)=Pi pi logpiqi.
Then the divergenceis transformed into a similarity measure (Manningand Schuetze, 1999):Wcom,l(pom,l, psn,l) = 10?IRcom,l(pom,l,psn,l)(6)IR is a measure of total divergence from the av-erage, representing how much information is lostwhen two distributions p and q are described interms of average distributions.
We opted for IRinstead of the commonly used KL because withIR there is no problem with infinite values sincepi+qi2 6=0 if either pi 6=0 or qi 6=0.
Moreover, un-like KL, IR is symmetric, i.e., KL(p,q) 6=KL(q,p).Finally sim1 is obtained by average similarity ofsentences using Eq.
(6) at each level of com by:sim1(om, sn) = 1L?Ll=1 Wcom ,l(pom,l, psn,l) ?
l(7)The similarity between pom,l and psn,l at each levelis weighted proportional to the level l because thesimilarity between sentences should be rewardedif there is a specific word overlap at child nodes.
?sim2: We introduce another measure basedon sentence-topic mixing proportions to calculatethe concept-based similarities between om and sn.We calculate the topic proportions of om and sn,represented by pzom = p(zom |com) and pzsn =p(zsn |com) via Eq.(3).
The similarity between thedistributions is then measured with transformed IR818(a) Snapshot of Hierarchical Topic Structure of adocument cluster on ?global warming?.
(Duc06)z1z2z3zz1z2z3zPosterior TopicDistributionsvz1z3..........w5z2w8........w2.z1w5.......w7w1Posterior Topic-Word Distributionscandidate omsummary sn(b) Magnified view of sample path c [z1,z2,z3] showingom={w1,w2,w3,w4,w5} and sn={w1,w2,w6,w7,w8}......z1zK-1zKz4z2z3humanwarmingincidenceresearchglobalpredicthealthchangediseaseforecasttemperatureslowmalariasneezestarvingmiddle-eastsiberiaom: ?Global1warming2may rise3incidence4of malaria5.?sn:?Global1warming2effects6human7health8.
?level:3level:1level:2vz1vz2vz2vz3vz3w1w5w6w7....w2w8....w5....w5....w6w1w5w6w7.....w2w8.....pomzpsnzp(w|z1, c   )sn,1snp(w|z1, c   )om,1omp(w|z2, c   )sn,2snp(w|z2, c   )om,2omp(w|z3, c   )sn,3snp(w|z3, c   )om,3omFigure 1: (a) A sample 3-level tree using sumHLDA.
Each sentence is associated with a path c through the hierarchy, whereeach node zl,c is associated with a distribution over terms (Most probable terms are illustrated).
(b) magnified view of a path(darker nodes) in (a).
Distribution of words in given two sentences, a candidate (om) and a summary (sn) using sub-vocabularyof words at each topic vzl .
Discrete distributions on the left are topic mixtures for each sentence, pzom and pzsn .as in Eq.
(6) by:sim2 (om, sn) = 10?IRcom (pzom ,pzsn ) (8)sim1 provides information about the similaritybetween two sentences, om and sn based on topic-word distributions.
Similarly, sim2 provides in-formation on the similarity between the weights ofthe topics in each sentence.
They jointly effect thesentence score and are combined in one measure:sim(om, sn) = sim1(om, sn) ?
sim2 (om, sn) (9)The final score for a given om is calculated fromEq.(4).
Fig.1.b depicts a sample path illustratingsparse unigram distributions of om and sm at eachlevel as well as their topic proportions, pzom , andpzsn .
In experiment 3, we discuss the effect of ourtree-based scoring on summarization performancein comparison to a classical scoring method pre-sented as our baseline model.5 Regression ModelEach candidate sentence om, m = 1..|O| is rep-resented with a multi-dimensional vector of q fea-tures fm = {fm1, ..., fmq}.
We build a regressionmodel using sentence scores as output and selectedsalient features as input variables described below:5.1 Feature ExtractionWe compile our training dataset using sentencesfrom different document clusters, which do notnecessarily share vocabularies.
Thus, we create n-gram meta-features to represent sentences insteadof word n-gram frequencies:(I) nGram Meta-Features (NMF): For eachdocument cluster D, we identify most fre-quent (non-stop word) unigrams, i.e., vfreq ={wi}ri=1 ?
V , where r is a model param-eter of number of most frequent unigram fea-tures.
We measure observed unigram proba-bilities for each wi ?
vfreq with pD(wi) =nD(wi)/?|V |j=1 nD(wj), where nD(wi) is thenumber of times wi appears in D and |V | is thetotal number of unigrams.
For any ith feature, thevalue is fmi = 0, if given sentence does not con-tain wi, otherwise fmi = pD(wi).
These featurescan be extended for any n-grams.
We similarlyinclude bigram features in the experiments.
(II) Document Word Frequency Meta-Features (DMF): The characteristics of sentencesat the document level can be important in sum-mary generation.
DMF identify whether a wordin a given sentence is specific to the documentin consideration or it is commonly used in thedocument cluster.
This is important becausesummary sentences usually contain abstract termsrather than specific terms.To characterize this feature, we re-use the rmost frequent unigrams, i.e., wi ?
vfreq.
Givensentence om, let d be the document that om be-longs to, i.e., om ?
d. We measure unigram prob-abilities for each wi by p(wi ?
om) = nd(wi ?om)/nD(wi), where nd(wi ?
om) is the numberof timeswi appears in d and nD(wi) is the numberof times wi appears in D. For any ith feature, thevalue is fmi = 0, if given sentence does not con-tain wi, otherwise fmi = p(wi ?
om).
We alsoinclude bigram extensions of DMF features.819(III) Other Features (OF): Term frequency ofsentences such as SUMBASIC are proven to begood predictors in sentence scoring (Nenkova andVanderwende, 2005).
We measure the averageunigram probability of a sentence by: p(om) =Pw?om1|om|PD(w), where PD(w) is the observedunigram probability in the document collection Dand |om| is the total number of words in om.
Weuse sentence bigram frequency, sentence rank ina document, and sentence size as additional fea-tures.5.2 Predicting Scores for New SentencesDue to the large feature space to explore, we choseto work with support vector regression (SVR)(Drucker et al, 1997) as the learning algorithmto predict sentence scores.
Given training sen-tences {fm, ym}|O|m=1, where fm = {fm1, ..., fmq}is a multi-dimensional vector of features andym=score(om)?
R are their scores obtained viaEq.
(4), we train a regression model.
In experi-ments we use non-linear Gaussian kernel for SVR.Once the SVR model is trained, we use it to predictthe scores of ntest number of sentences in test (un-seen) document clusters, Otest ={o1, ...o|Otest|}.Our HybHSum captures the sentence character-istics with a regression model using sentences indifferent document clusters.
At test time, this valu-able information is used to score testing sentences.Redundancy Elimination: To eliminate redun-dant sentences in the generated summary, we in-crementally add onto the summary the highestranked sentence om and check if om significantlyrepeats the information already included in thesummary until the algorithm reaches word countlimit.
We use a word overlap measure betweensentences normalized to sentence length.
A om isdiscarded if its similarity to any of the previouslyselected sentences is greater than a threshold iden-tified by a greedy search on the training dataset.6 Experiments and DiscussionsIn this section we describe a number of experi-ments using our hybrid model on 100 documentclusters each containing 25 news articles fromDUC2005-2006 tasks.
We evaluate the perfor-mance of HybHSum using 45 document clusterseach containing 25 news articles from DUC2007task.
From these sets, we collected v80K andv25K sentences to compile training and testingdata respectively.
The task is to create max.
250word long summary for each document cluster.We use Gibbs sampling for inference in hLDAand sumHLDA.
The hLDA is used to capture ab-straction and specificity of words in documents(Blei et al, 2009).
Contrary to typical hLDA mod-els, to efficiently represent sentences in summa-rization task, we set ascending values for Dirichlethyper-parameter ?
as the level increases, encour-aging mid to low level distributions to generate asmany words as in higher levels, e.g., for a tree ofdepth=3, ?
= {0.125, 0.5, 1}.
This causes sen-tences share paths only when they include similarconcepts, starting higher level topics of the tree.For SVR, we set  = 0.1 using the default choice,which is the inverse of the average of ?(f)T?
(f)(Joachims, 1999), dot product of kernelized inputvectors.
We use greedy optimization during train-ing based on ROUGE scores to find best regular-izer C ={10?1..102}using the Gaussian kernel.We applied feature extraction of ?
5.1 to com-pile the training and testing datasets.
ROUGEis used for performance measure (Lin and Hovy,2003; Lin, 2004), which evaluates summariesbased on the maxium number of overlapping unitsbetween generated summary text and a set of hu-man summaries.
We use R-1 (recall against uni-grams), R-2 (recall against bigrams), and R-SU4(recall against skip-4 bigrams).Experiment 1: sumHLDA Parameter Analy-sis: In sumHLDA we introduce a prior differentthan the standard nested CRP (nCRP).
Here, weillustrate that this prior is practical in learning hi-erarchical topics for summarization task.We use sentences from the human generatedsummaries during the discovery of hierarchicaltopics of sentences in document clusters.
Sincesummary sentences generally contain abstractwords, they are indicative of sentences in docu-ments and should produce minimal amount of newtopics (if not none).
To implement this, in nCRPprior of sumHLDA, we use dual hyper-parametersand choose a very small value for summary sen-tences, ?s = 10e?4  ?o.
We compare the re-sults to hLDA (Blei et al, 2003a) with nCRP priorwhich uses only one free parameter, ?.
To ana-lyze this prior, we generate a corpus ofv1300 sen-tences of a document cluster in DUC2005.
We re-peated the experiment for 9 other clusters of sim-ilar size and averaged the total number of gener-ated topics.
We show results for different valuesof ?
and ?o hyper-parameters and tree depths.820?
= ?o 0.1 1 10depth 3 5 8 3 5 8 3 5 8hLDA 3 5 8 41 267 1509 1522 4080 8015sumHLDA 3 5 8 27 162 671 1207 3598 7050Table 1: Average # of topics per document cluster fromsumHLDA and hLDA for different ?
and ?o and tree depths.
?s = 10e?4 is used for sumHLDA for each depth.Features Baseline HybHSumR-1 R-2 R-SU4 R-1 R-2 R-SU4NMF (1) 40.3 7.8 13.7 41.6 8.4 12.3DMF (2) 41.3 7.5 14.3 41.3 8.0 13.9OF (3) 40.3 7.4 13.7 42.4 8.0 14.4(1+2) 41.5 7.9 14.0 41.8 8.5 14.5(1+3) 40.8 7.5 13.8 41.6 8.2 14.1(2+3) 40.7 7.4 13.8 42.7 8.7 14.9(1+2+3) 41.4 8.1 13.7 43.0 9.1 15.1Table 2: ROUGE results (with stop-words) on DUC2006for different features and methods.
Results in bold show sta-tistical significance over baseline in corresponding metric.As shown in Table 1, the nCRP prior forsumHLDA is more effective than hLDA prior inthe summarization task.
Less number of top-ics(nodes) in sumHLDA suggests that summarysentences share pre-existing paths and no newpaths or nodes are sampled for them.
We alsoobserve that using ?o = 0.1 causes the modelto generate minimum number of topics (# of top-ics=depth), while setting ?o = 10 creates exces-sive amount of topics.
?0 = 1 gives reasonablenumber of topics, thus we use this value for therest of the experiments.
In experiment 3, we useboth nCRP priors in HybHSum to analyze whetherthere is any performance gain with the new prior.Experiment 2: Feature Selection AnalysisHere we test individual contribution of each setof features on our HybHSum (using sumHLDA).We use a Baseline by replacing the scoring algo-rithm of HybHSum with a simple cosine distancemeasure.
The score of a candidate sentence is thecosine similarity to the maximum matching sum-mary sentence.
Later, we build a regression modelwith the same features as our HybHSum to createa summary.
We train models with DUC2005 andevaluate performance on DUC2006 documents fordifferent parameter values as shown in Table 2.As presented in ?
5, NMF is the bundle of fre-quency based meta-features on document clusterlevel, DMF is a bundle of frequency based meta-features on individual document level and OF rep-resents sentence term frequency, location, and sizefeatures.
In comparison to the baseline, OF has asignificant effect on the ROUGE scores.
In addi-tion, DMF together with OF has shown to improveall scores, in comparison to baseline, on averageby 10%.
Although the NMF have minimal indi-vidual improvement, all these features can statis-tically improve R-2 without stop words by 12%(significance is measured by t-test statistics).Experiment 3: ROUGE EvaluationsWe use the following multi-document summariza-tion models along with the Baseline presented inExperiment 2 to evaluate HybSumm.?
PYTHY : (Toutanova et al, 2007) A state-of-the-art supervised summarization system thatranked first in overall ROUGE evaluations inDUC2007.
Similar to HybHSum, human gener-ated summaries are used to train a sentence rank-ing system using a classifier model.?
HIERSUM : (Haghighi and Vanderwende,2009) A generative summarization method basedon topic models, which uses sentences as an addi-tional level.
Using an approximation for inference,sentences are greedily added to a summary so longas they decrease KL-divergence.?
HybFSum (Hybrid Flat Summarizer): Toinvestigate the performance of hierarchical topicmodel, we build another hybrid model using flatLDA (Blei et al, 2003b).
In LDA each sentenceis a superposition of all K topics with sentencespecific weights, there is no hierarchical relationbetween topics.
We keep the parameters and thefeatures of the regression model of hierarchicalHybHSum intact for consistency.
We only changethe sentence scoring method.
Instead of the newtree-based sentence scoring (?
4), we present asimilar method using topics from LDA on sen-tence level.
Note that in LDA the topic-word dis-tributions ?
are over entire vocabulary, and topicmixing proportions for sentences ?
are over allthe topics discovered from sentences in a docu-ment cluster.
Hence, we define sim1 and sim2measures for LDA using topic-word proportions ?
(in place of discrete topic-word distributions fromeach level in Eq.2) and topic mixing weights ?
insentences (in place of topic proportions in Eq.3)respectively.
Maximum matching score is calcu-lated as same as in HybHSum.?
HybHSum1 and HybHSum2: To analyze the ef-fect of the new nCRP prior of sumHLDA on sum-821ROUGE w/o stop words w/ stop wordsR-1 R-2 R-4 R-1 R-2 R-4Baseline 32.4 7.4 10.6 41.0 9.3 15.2PYTHY 35.7 8.9 12.1 42.6 11.9 16.8HIERSUM 33.8 9.3 11.6 42.4 11.8 16.7HybFSum 34.5 8.6 10.9 43.6 9.5 15.7HybHSum1 34.0 7.9 11.5 44.8 11.0 16.7HybHSum2 35.1 8.3 11.8 45.6 11.4 17.2Table 3: ROUGE results of the best systems onDUC2007 dataset (best results are bolded.
)marization model performance, we build two dif-ferent versions of our hybrid model: HybHSum1using standard hLDA (Blei et al, 2003a) andHybHSum2 using our sumHLDA.The ROUGE results are shown in Table 3.
TheHybHSum2 achieves the best performance on R-1 and R-4 and comparable on R-2.
When stopwords are used the HybHSum2 outperforms state-of-the-art by 2.5-7% except R-2 (with statisticalsignificance).
Note that R-2 is a measure of bi-gram recall and sumHLDA of HybHSum2 is builton unigrams rather than bigrams.
Compared tothe HybFSum built on LDA, both HybHSum1&2yield better performance indicating the effective-ness of using hierarchical topic model in summa-rization task.
HybHSum2 appear to be less re-dundant than HybFSum capturing not only com-mon terms but also specific words in Fig.
2, dueto the new hierarchical tree-based sentence scor-ing which characterizes sentences on deeper level.Similarly, HybHSum1&2 far exceeds baseline builton simple classifier.
The results justify the per-formance gain by using our novel tree-based scor-ing method.
Although the ROUGE scores forHybHSum1 and HybHSum2 are not significantlydifferent, the sumHLDA is more suitable for sum-marization tasks than hLDA.HybHSum2 is comparable to (if not better than)fully generative HIERSUM.
This indicates thatwith our regression model built on training data,summaries can be efficiently generated for testdocuments (suitable for online systems).Experiment 4: Manual EvaluationsHere, we manually evaluate quality of summaries,a common DUC task.
Human annotators are giventwo sets of summary text for each document set,generated from two approaches: best hierarchi-cal hybrid HybHSum2 and flat hybrid HybFSummodels, and are asked to mark the better summaryNew federal  rules for organicfood will assure consumers thatthe products are grown andprocessed to the same standardsnationwide.
But as  sales grewmore than 20 percent a yearthrough the 1990s, organic foodcame to account for $1 of every$100 spent  on food, and in 1997t h e a g e n c y t o o k n o t i c e ,proposing national organicstandards for all food.By the year 2001, organicproducts are projected tocommand 5 percent of total foodsales in the United  States.
Thesale of organics rose by about 30percent  last year, driven byconcerns over food safety, theenvironment  and a fear ofgenetically engineered food.
U.S.sales of organic foods havegrown by 20 percent annually  forthe last seven years.
(c) HybFSum Output(b) HybHSum2OutputThe Agriculture Departmentbegan to propose standards forall  organic foods in the late1990's  because their sale hadgrown more than 20 per cent ayear in that decade.
In January1999 the USDA approved a"certified organic" label formeats and poultry that wereraised without growth hormones,pesticide-treated feed, andantibiotics.
(a) Ref.
Outputwordorganic 6 6 6genetic 2 4 3allow 2 2 1agriculture 1 1 1standard 5 7 0sludge 1 1 0federal 1 1 0bar 1 1 0certified 1 1 0specificHybHSum2HybFSumRefFigure 2: Example summary text generated by systemscompared in Experiment 3.
(Id:D0744 in DUC2007).
Ref.is the human generated summary.Criteria HybFSum HybHSum2 TieNon-redundancy 26 44 22Coherence 24 56 12Focus 24 56 12Responsiveness 30 50 12Overall 24 66 2Table 4: Frequency results of manual quality evaluations.Results are statistically significant based on t-test.
T ie indi-cates evaluations where two summaries are rated equal.according to five criteria: non-redundancy (whichsummary is less redundant), coherence (whichsummary is more coherent), focus and readabil-ity (content and not include unnecessary details),responsiveness and overall performance.We asked 4 annotators to rate DUC2007 pre-dicted summaries (45 summary pairs per anno-tator).
A total of 92 pairs are judged and eval-uation results in frequencies are shown in Table4.
The participants rated HybHSum2 generatedsummaries more coherent and focused comparedto HybFSum.
All results in Table 4 are statis-tically significant (based on t-test on 95% con-fidence level.)
indicating that HybHSum2 sum-maries are rated significantly better.822...Document Cluster1...Document Cluster2...Document Clustern......f1f2f3fqf-input features...f1f2f3fqf-input features...f1f2f3fqf-input featuresh(f,y) : regression model for sentence ranking...........zzKzzzzsumHLDA........zzKzzzzsumHLDA........zzKzzzzsumHLDA......y-outputcandidate sentence scores0.020.010.0..y-outputcandidate sentence scores0.350.090.01..y-outputcandidate sentence scores0.430.200.03..Figure 3: Flow diagram for Hybrid Learning Algorithm for Multi-Document Summarization.7 ConclusionIn this paper, we presented a hybrid model formulti-document summarization.
We demonstratedthat implementation of a summary focused hierar-chical topic model to discover sentence structuresas well as construction of a discriminative methodfor inference can benefit summarization quality onmanual and automatic evaluation metrics.AcknowledgementResearch supported in part by ONR N00014-02-1-0294, BT Grant CT1080028046, Azerbaijan Min-istry of Communications and Information Tech-nology Grant, Azerbaijan University of Azerbai-jan Republic and the BISC Program of UC Berke-ley.ReferencesR.
Barzilay and L. Lee.
Catching the drift: Proba-bilistic content models with applications to gen-eration and summarization.
In In Proc.
HLT-NAACL?04, 2004.D.
Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.Hierarchical topic models and the nested chi-nese restaurant process.
In In Neural Informa-tion Processing Systems [NIPS], 2003a.D.
Blei, T. Griffiths, and M. Jordan.
The nestedchinese restaurant process and bayesian non-parametric inference of topic hierarchies.
InJournal of ACM, 2009.D.
M. Blei, A. Ng, and M. Jordan.
Latent dirichletallocation.
In Jrnl.
Machine Learning Research,3:993-1022, 2003b.S.R.K.
Branavan, H. Chen, J. Eisenstein, andR.
Barzilay.
Learning document-level seman-tic properties from free-text annotations.
InJournal of Artificial Intelligence Research, vol-ume 34, 2009.J.M.
Conroy, J.D.
Schlesinger, and D.P.
O?Leary.Topic focused multi-cument summarization us-ing an approximate oracle score.
In In Proc.ACL?06, 2006.H.
Daume?III and D. Marcu.
Bayesian query fo-cused summarization.
In Proc.
ACL-06, 2006.H.
Drucker, C.J.C.
Burger, L. Kaufman, A. Smola,and V. Vapnik.
Support vector regression ma-chines.
In NIPS 9, 1997.A.
Haghighi and L. Vanderwende.
Exploring con-tent models for multi-document summarization.In NAACL HLT-09, 2009.T.
Joachims.
Making large-scale svm learningpractical.
In In Advances in Kernel Methods -Support Vector Learning.
MIT Press., 1999.C.-Y.
Lin.
Rouge: A package for automatic evalu-ation of summaries.
In In Proc.
ACL Workshopon Text Summarization Branches Out, 2004.823C.-Y.
Lin and E.H. Hovy.
Automatic evaluationof summaries using n-gram co-occurance statis-tics.
In Proc.
HLT-NAACL, Edmonton, Canada,2003.C.
Manning and H. Schuetze.
Foundations of sta-tistical natural language processing.
In MITPress.
Cambridge, MA, 1999.A.
Nenkova and L. Vanderwende.
The impact offrequency on summarization.
In Tech.
ReportMSR-TR-2005-101, Microsoft Research, Red-wood, Washington, 2005.D.R.
Radev, H. Jing, M. Stys, and D. Tam.Centroid-based summarization for multipledocuments.
In In Int.
Jrnl.
Information Process-ing and Management, 2004.D.
Shen, J.T.
Sun, H. Li, Q. Yang, and Z. Chen.Document summarization using conditionalrandom fields.
In Proc.
IJCAI?07, 2007.J.
Tang, L. Yao, and D. Chens.
Multi-topic basedquery-oriented summarization.
In SIAM Inter-national Conference Data Mining, 2009.I.
Titov and R. McDonald.
A joint model of textand aspect ratings for sentiment summarization.In ACL-08:HLT, 2008.K.
Toutanova, C. Brockett, M. Gamon, J. Jagarla-mudi, H. Suzuki, and L. Vanderwende.
The ph-thy summarization system: Microsoft researchat duc 2007.
In Proc.
DUC, 2007.J.Y.
Yeh, H.-R. Ke, W.P.
Yang, and I-H. Meng.Text summarization using a trainable summa-rizer and latent semantic analysis.
In Informa-tion Processing and Management, 2005.824
