Proceedings of the SIGDIAL 2014 Conference, pages 218?227,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsLearning to Re-rank for Interactive Problem Resolution and QueryRefinementRashmi GangadharaiahIBM Research,India Research Lab,Bangalore, KA, Indiarashgang@in.ibm.comBalakrishnan Narayanaswamy and Charles ElkanDepartment of CSE,University of California, San DiegoLa Jolla, CA, USA{muralib, elkan}@cs.ucsd.eduAbstractWe study the design of an information re-trieval (IR) system that assists customerservice agents while they interact withend-users.
The type of IR needed isdifficult because of the large lexical gapbetween problems as described by cus-tomers, and solutions.
We describe anapproach that bridges this lexical gap bylearning semantic relatedness using tensorrepresentations.
Queries that are short andvague, which are common in practice, re-sult in a large number of documents be-ing retrieved, and a high cognitive loadfor customer service agents.
We showhow to reduce this burden by providingsuggestions that are selected based on thelearned measures of semantic relatedness.Experiments show that the approach offerssubstantial benefit compared to the use ofstandard lexical similarity.1 IntroductionInformation retrieval systems help businesses andindividuals make decisions by automatically ex-tracting actionable intelligence from large (un-structured) data (Musen et al., 2006; AntonioPalma-dos Reis, 1999).
This paper focuses on theapplication of retrieval systems in a contact cen-ters where the system assists agents while they arehelping customers with problem resolution.Currently, most contact center information re-trieval use (web based) front-ends to search en-gines indexed with knowledge sources (Holland,2005).
Agents enter queries to retrieve documentsrelated to the customer?s problem.
These sourcesare often incomplete as it is unlikely that all pos-sible customer problems can be identified beforeproduct release.
This is particularly true for re-cently released and frequently updated products.One approach, which we build on here, is to mineproblems and resolutions from online discussionforums Yahoo!
Answers1Ubuntu Forums2andApple Support Communities3.
While these oftenprovide useful solutions within hours or days ofa problem surfacing, they are semantically noisy(Gangadharaiah and Narayanaswamy, 2013).Most contact centers and agents are evaluatedbased on the number of calls they handle over aperiod (Pinedo et al., 2000).
As a result, queriesentered by agents into the search engine are usu-ally underspecified.
This, together with noise inthe database, results in a large number of docu-ments being retrieved as relevant documents.
Thisin turn, increases the cognitive load on agents, andreduces the effectiveness of the search system andthe efficiency of the contact center.
Our first taskin this paper is to automatically make candidatesuggestions that reduce the search space of rel-evant documents in a contact center application.The agent/user then interacts with the system byselecting one of the suggestions.
This is used toexpand the original query and the process can berepeated.
We show that even one round of inter-action, with a small set of suggestions, can lead tohigh quality solutions to user problems.In query expansion, the classical approach is toautomatically find suggestions either in the formof words, phrases or similar queries (Kelly et al.,2009; Feuer et al., 2007; Leung et al., 2008).These can be obtained either from query logs orbased on their representativeness of the initial re-trieved documents (Guo et al., 2008; Baeza-yateset al., 2004).
The suggestions are then ranked ei-ther based on their frequencies or based on theirsimilarity to the original query (Kelly et al., 2009;Leung et al., 2008).
For example, if suggestionsand queries are represented as term vectors (e.g.1http://answers.yahoo.com/2http://ubuntuforums.org/3https://discussions.apple.com/218term frequency-inverse document frequency or tf-idf) their similarity may be determined using simi-larity measures such as cosine similarity or inverseof euclidean distance (Salton and McGill, 1983).However, in question-answering and problem-resolution domains, and in contrast to traditionalInformation Retrieval, most often the query andthe suggestions do not have many overlappingwords.
This leads to low similarity scores, evenwhen the suggestion is highly relevant.
Considerthe representative example in Table 1, taken fromour crawled dataset.
Although the suggestions,?does not support file transfer?, ?connection notstable?, ?pairing failed?
are highly relevant for theproblem of ?Bluetooth not working?, their lexi-cal similarity score is zero.
The second task thatthis paper addresses is how to bridge this lexicalchasm between the query and the suggestions.
Forthis, we learn a measure of semantic-relatednessbetween the query and the suggestions rather thandefining closeness based on lexical similarity.Query Bluetooth not working .Suggestions devices not discovered,bluetooth greyed out,bluetooth device did not respond,does not support file transfer,connection not stable,pairing failedTable 1: Suggestions for the Query or customer?sproblem, ?Bluetooth not working?.The primary contributions of this paper are that:?
We show how tensor methods can be usedto learn measures of question-answer orproblem-resolution similarity.
In addition,we show that these learned measures canbe used directly with well studied classifica-tion techniques like Support Vector Machines(SVMs) and Logistic Classifiers to classifywhether suggestions are relevant.
This resultsin substantially improved performance overusing conventional similarity metrics.?
We show that along with the learned similar-ity metric, a data dependent Information Gain(which incorporates knowledge about the setof documents in the database) can be used asa feature to further boost accuracy.?
We demonstrate the efficacy of our approachon a complete end-to-end problem-resolutionsystem, which includes crawled data fromonline forums and gold standard user inter-action annotations.2 System outlineAs discussed in the Introduction, online discus-sion forums form a rich source of problems andtheir corresponding resolutions.
Thread initiatorsor users of a product facing problems with theirproduct post in these forums.
Other users postpossible solutions to the problem.
At the sametime, there is noise due to unstructured content,off-topic replies and other factors.
Our interac-tion system has two phases, as shown in Figure1.
The offline phase attempts to reduce noise inthe database, while the online phase assists usersdeal with the cognitive overload caused by a largeset of retrieved documents.
In our paper, threadsform the documents indexed by the system.The goals of the offline phase are two-fold.First, to reduce the aforementioned noise in thedatabase, we succinctly represent each document(i.e., a thread in online discussion forums) by itssignature, which is composed of units extractedfrom the first post of the underlying thread thatbest describe the problem discussed in the thread.Second, the system makes use of click-throughdata, where users clicked on relevant suggestionsfor their queries to build a relevancy model.
Asmentioned before, the primary challenge is tobuild a model that can identify units that are se-mantically similar to a given query.In the online phase, the agent who acts as themediator between the user and the Search Engineenters the user?s/customer?s query to retrieve rele-vant documents.
From these retrieved documents,the system then obtains candidate suggestions andranks these suggestions using the relevancy modelbuilt in the offline phase to further better under-stand the query and thereby reduce the space ofdocuments retrieved.
The user then selects thesuggestion that is most relevant to his query.
Theretrieved documents are then filtered displayingonly those documents that contain the selectedsuggestion in their signatures.
The process con-tinues until the user quits.2.1 Signatures of documentsIn the offline phase, every document (correspond-ing to a thread in online discussion forums) isrepresented by units that best describe a problem.We adopt the approach suggested in (Gangadhara-219iah and Narayanaswamy, 2013) to automaticallygenerate these signatures from each discussionthread.
We assume that the first post describesthe user?s problem, something we have found tobe true in practice.
From the dependency parsetrees of the first posts, we extract three types ofunits (i) phrases (e.g., sync server), (ii) attribute-values (e.g., iOS, 4) and (iii) action-attribute tuples(e.g., sync server: failed).
Phrases form good baseproblem descriptors.
Attribute-value pairs provideconfigurational contexts to the problem.
Action-attribute tuples, as suggested in (Gangadharaiahand Narayanaswamy, 2013), capture segments ofthe first post that indicate user wanting to performan action (?I cannot hear notifications on blue-tooth?)
or the problems caused by a user?s action(?working great before I updated?).
These makethem particularly valuable features for problem-resolution and question-answering.2.2 Representation of Queries andSuggestionsQueries are represented as term vectors using theterm frequency-inverse document frequency (tf-idf) representation forming the query space.
Theterm frequency is defined as the frequency withwhich word appears in the query and the inversedocument frequency for a word is defined as thefrequency of queries in which the word appeared.Similarly, units are represented as tf-idf term vec-tors from the suggestion space.
Term frequency inthe unit space is defined as the number of timesa word appears in the unit and its inverse docu-ment frequency is defined in terms of the numberof units in which the word appeared.
Since thevocabulary used in the queries and documents aredifferent, the representations for queries and unitsbelong to different spaces of different dimensions.For every query-unit pair, we learn a measureof similarity as explained in Section 4.
Addi-tionally, we use similarity features based on co-sine similarity between the query and the unit un-der consideration.
We also consider an additionalfeature based on information gain (Gangadhara-iah and Narayanaswamy, 2013).
In particular, ifS represents the set all retrieved documents, S1isa subset of S (S1?
S) containing a unit unitiandS2is a subset of S that does not contain uniti,information gain with unitiis,Gain(S, uniti) = E(S)?|S1||S|E(S1)?|S2||S|E(S2) (1)E(S) =?k=1,...|S|?p(dock)log2p(dock).
(2)The probability for each document is based on itsrank in the retrieved of results,p(docj) =1rank(docj)?k=1,...|S|1rank(dock).
(3)We crawled posts and threads from online forumsfor the products of interest, as detailed in Sec-tion 5.1, and these form the documents.
We usedtrial interactions and retrievals to collect the click-though data, which we used as labeled data forsimilarity metric learning.
In particular, labels in-dicate which candidate units were selected as rel-evant suggestions by a human annotator.
We nowexplain our training (offline) and testing (online)phases that use this data in more detail.2.3 TrainingThe labeled (click-through) data for training therelevance model is collected as follows.
Anno-tators were given pairs of queries.
Each pair iscomposed of an underspecified query and a spe-cific query (Section 5.1 provides more informa-tion on the creation of these queries).
An un-derspecified query is a query that reflects what auser/agent typically enters into the system, and thecorresponding specific query is full-specified ver-sion of the underspecified query.
Annotators werefirst asked to query the search engine with eachunderspecified query.
We use the Lemur searchengine (Strohman et al., 2004).
From the resultingset of retrieved documents, the system uses the in-formation gain criteria (as given in (1) below) torank and display to the annotators the candidatesuggestions (i.e., the units that appear in the signa-tures of the retrieved documents).
Thus, our sys-tem is bootstrapped using the information gain cri-terion.
The annotators then selects the candidatesuggestion that is most relevant to the correspond-ing specific query.
The interaction with the systemcontinues until the annotators quit.We then provide a class label for each unit basedon the collected click-through information.
In par-ticular, if a unit s ?
S(x) was clicked by a user forhis query x, from the list S we provide a + la-bel to indicate that the unit is relevant suggestionfor the query.
Similarly, for all other units that arenever clicked by users for x are labeled as?.
Thisforms the training data for the system.
Details on220Forum Discussion  Threads Unit Extraction Suggestion units  for first postsSearch Engine query results Interaction Module Finds suggestionsCandidate  SuggestionsUser clicks on  (units, query) Learn Relevance Model !
OfflineOnlineFigure 1: Outline of our interactive query refine-ment system for problem resolutionthe feature extraction and how the model is createdis given in Section 3.2.4 TestingIn the online phase, the search engine retrievesdocuments for the user?s query x?.
Signatures forthe retrieved documents form the initial space ofcandidate units.
As done in training, for every pairof x?and unit the label is predicted using the modelbuilt in the training phase.
Units that are predictedas + are shown to the user.
When a user clickson his most relevant suggestion, the retrieved re-sults are filtered to show only those documents thatcontain the suggestion (i.e., in its signature).
Thisprocess continues until the user quits.3 ModelWe consider underspecified queries x ?
Rxdandunits y ?
Ryd.
Given an underspecified query xwe pass it through a search engine, resulting in alist of results S(x).As explained in Section 2.3, our training dataconsists of labels r(x, y) ?
+1,?1 for eachunder-specified query, y ?
S(x).
r(x, y) = +1if the unit is labeled a relevant suggestion andr(x, y) = ?1 if it is not labeled relevant.
Unitsare relevant or not based on the final query, andnot just y, a distinction we expand upon below.At each time step, our system proposes a listZ(x) of possible query refinement suggestions zto the user.
The user can select one or none ofthese suggestions.
If the user selects z, only thosedocuments that contain the suggestion (i.e., in itssignature) are shown to the user, resulting in a fil-tered set of results, S(x+ z).This process can be repeated until a stoppingcriterion is reached.
Stopping criterion include thesize of the returned list is smaller than some num-ber |S(x + z)| < N , in which case all remain-ing documents are returned.
Special cases includewhen only one document is returned N = 1.
Wewill design query suggestions so that |S(x+z)| >0.
Another criterion we use is to return all remain-ing documents after a certain maximum number ofinteractions or until the user quits.4 Our ApproachWe specify our algorithm using a tensor notation.We do this since tensors appear to subsume mostof the methods applied in practice, where differentalgorithms use slightly different costs, losses andconstraints.
These ideas are strongly motivated by,but generalize to some extent, suggestions for thisproblem presented in (Elkan, 2010).For our purposes, we consider tensors as multi-dimensional arrays, with the number of dimen-sions defined as the order of the tensor.
An Morder tensor X ?
RI1?I2...IM.
As such tensorssubsume vectors (1st order tensors) and matrices(2nd order tensors).
The vectorization of a ten-sor of order M is obtained by stacking elementsfrom the M dimensions into a vector of lengthI1?
I2?
.
.
.?
IMin the natural way.The inner product of two tensors is defined as?X,W?
=I1?i1I2?i2.
.
.IM?iMxi1wi1xi2wi2.
.
.
xiMwiM(4)Analogous to the definition for vectors, the(Kharti-Rao) outer product A = X ?W of twotensors X and W has Aij= XiWjwhere i and jrun over all elements of X and W .
Thus, if X isof order MXand W of order MW, A is of orderMA= MX+MW.The particular tensor we are interested in is a2-D tensor (matrix) X which is the outer productof query and unit pairs (Feats).
In particular, for aquery x and unit y, Xi,j= xiyj.Given this representation, standard classifica-tion and regression methods from the machinelearning literature can often be extended to dealwith tensors.
In our work we consider two clas-sifiers that have been successful in many applica-tions, logistic regression and support vector ma-chines (SVMs) (Bishop, 2006).221In the case of logistic regression, the conditionalprobability of a reward signal r(X) = r(x, y) is,p(r(X) = +1) =11 + exp(?
?X,W?+ b)(5)The parameters W and b can be obtained by min-imizing the log loss Lregon the training data DLreg(W, b) = (6)?
(X,r(X))?Dlog(1 + exp(?r(X)?X,W?+ b)For SVMs with the hinge loss we select param-eters to minimize Lhinge,Lhinge(W, b) = ||X||2F+ (7)??
(X,r(X))?Dmax[0, 1?
(r(X)?X,W?+ b)]where ||X||Fis the Frobenius norm of tensor X.Given the number of parameters in our system(W, b) to limit overfitting, we have to regularizethese parameters.
We use regularizers of the form?
(W, b) = ?W||W||F(8)such regularizes have been successful in manylarge scale machine learning tasks includinglearning of high dimensional graphical models(Ravikumar et al., 2010) and link prediction(Menon and Elkan, 2011).Thus, the final optimization problem we arefaced with is of the formminW,bL(W, b) + ?
(W, b) (9)where L is Lregor Lhingeas appropriate.
Otherlosses, classifiers and regularizers may be used.The advantage of tensors over their vectorizedcounterparts, that may be lost in the notation, isthat they do not lose the information that the dif-ferent dimensions can (and in our case do) lie indifferent spaces.
In particular, in our case we usedifferent features to represent queries and units (asdiscussed in Section 2.2) which are not of the samelength, and as a result trivially do not lie in thesame space.Tensor methods also allow us to regularize thecomponents of queries and units separately in dif-ferent ways.
This can be done for example by,i) forcing W = Q1Q2, where Q1and Q2areconstrained to be of fixed rank s ii) using trace orFrobenius norms on Q1and Q2for separate regu-larization as proxies for the rank iii) using differentsparsity promoting norms on the rows of Q1andQ2iv) weighing these penalties differently for thetwo matrices in the final loss function.
Note thatby analogy to the vector case, we directly obtaingeneralization error guarantees for our methods.We also discuss the advantage of the tensorrepresentation above over a natural representationX = [x; y] i.e.
X is the column vector obtainedby stacking the query and unit representations.Note that in this representation, for logistic regres-sion, while a change in the query x can changethe probability for a unit P (r(X) = 1) it can-not change the relative probability of two differentunits.
Thus, the ordering of all unit remains thesame for all queries.
This flaw has been pointedout in the literature in (Vert and Jacob, 2008) and(Bai et al., 2009), but was brought to our attentionby (Elkan, 2010).Finally, we note that by normalizing the queryand unit vectors (x and y), and selecting W = I(the identity matrix) we can recover the cosinesimilarity metric (Elkan, 2010).
Thus, our rep-resentation is atleast as accurate and we showthat learning the diagonal and off-diagonal com-ponents of W can substantially improve accuracy.Additionally, for every (query,unit) we alsocompute information gain (IG) as given in (1), andthe lexical similarity (Sim) in terms of cosine sim-ilarity between the query and the unit as additionalfeatures in the feature vectors.5 Results and DiscussionTo evaluate our system, we built and simulateda contact center information retrieval system foriPhone problem resolution.5.1 Description of the DatasetWe collected data by crawling forum discussionthreads from the Apple Discussion Forum, createdduring the period 2007-2011, resulting in about147,000 discussion threads.
The underspecifiedqueries and specific queries were created as fol-lows.
Discussion threads were first clustered treat-ing each discussion thread as a data point using atf-idf representation.
The thread nearest the cen-troid of the 60 largest clusters were marked as the?most common?
problems.The first post is used as a proxy for the problemdescription.
An annotator was asked to then create222Underspecified query ?Safari not working?1.
safari:crashes2.
safari:cannot find:server3.
server:stopped responding4.
phone:freezes5.
update:failedTable 2: Specific Queries generated with the un-derspecified Query, ?Safari not working?.a short query (underspecified) from the first postof each of the 60 selected threads.
These querieswere given to the Lemur search engine (Strohmanet al., 2004) to retrieve the 50 most similar threadsfrom an index built on the entire set of 147,000threads.
The annotator manually analyzed the firstposts of the retrieved threads to create contexts,resulting in a total 200 specific queries.We give an example to illustrate the data cre-ation in Table 2.
From an under-specified query?Safari not working?, the annotator found 5 spe-cific queries.
Two other annotators, were giventhese specific queries with the search engine?sresults from the corresponding under-specifiedquery.
They were asked to choose the most rel-evant results for the specific queries.
The intersec-tion of the choices of the annotators formed our?gold standard?
of relevant documents.5.2 ResultsWe simulated a contact center retrieval systems (asin Figure 1) to evaluate the approach proposed inthis paper.
To evaluate the generality of our ap-proach we conduct experiments with both SVMsand Logistic Regression.
Due to lack of space weillustrate each result for only one kind of classifier.5.2.1 Evaluating the Relevance ModelTo measure the performance of the relevancemodel for predicting the class labels or for findingthe most relevant units towards making the user?sunderspecified query more specific, we performedthe following experiment.
4000 random query-unit pairs were picked from the training data, col-lected as explained in Section 2.
Since most unitsare not relevant for a query, 90% of the pairs be-longed to the ?
class.
On average, every spe-cific query gave rise to 2.4 suggestions.
Hence,predicting ?
for all pairs still achieves an errorrate of 10%.
This data was then split into vary-ing sizes of training and test sets.
The relevancymodel was then built on the training half and theclassifiers were used to predict labels on the test0 500 1000 1500 2000 2500 3000 3500 400000.010.020.030.040.050.06Number of training query?suggestion pairsError rateSimFeats?IG?SimFeats+IG+SimFigure 2: Performance with Logistic Regressionusing different features and various sizes of Train-ing and Test sets.
Feats-IG-Sim does not use co-sine similarity (Sim) and information gain (IG).Feats+IG+Sim considers Sim and IG.set.
Figure 2 shows error rate obtained with logis-tic regression (a similar trend was observed withSVMs) on various sizes of the training data andtest data.
The plot shows that the model (Feats-IG-Sim and Feats+IG+Sim) performs significantlybetter at predicting the relevancy of units for un-derspecified queries when compared to just us-ing cosine similarity (Sim) as a feature.
Feats-IG-Sim does not make use of cosine similarityas a feature or the information gain feature whileFeats+IG+Sim uses both these features for train-ing the relevancy model and for predicting the rel-evancy of units.
As expected the performance ofthe classifier improves as the size of the trainingdata is increased.5.2.2 Evaluating the Interaction EngineWe evaluate a complete system with both the user(the agent) and the search engine in the loop.
Wemeasure the value of the interactions by an analy-sis of which results ?rise to the top?.
Users weregiven a specific query and its underspecified queryalong with the results obtained when the under-specified query was input to the search engine.They were presented with suggestions that werepredicted + for the underspecified query usingSVMs.
The user was asked to select the most ap-propriate suggestion that made the underspecifiedquery more specific.
This process continues untilthe user quits either because he is satisfied with theretrieved results or does not obtain relevant sug-gestions from the system.
For example, for theunderspecified query in Table 2, one of the pre-dicted suggestions was, ?server:stopped respond-2231 2 3 4 5 6 7 8 9 1000.511.522.5Size of retrieved listMeanAverage PrecisionBaselineFeats?IG?SimFeats+IG+SimFigure 3: Comparison of the proposed approachwith respect to the Baseline that does not involveinteraction in terms of MAP at N.ing?.
If the user finds the suggestion relevant, heclicks on it.
The selected suggestion then reducesthe number of retrieved results.
We then measuredthe relevance of the reduced result, with respectto the gold standard for that specific query, usingmetrics used in IR - MRR, Mean Average Preci-sion (MAP) and Success at rank N.Figures 3, 4 and Table 3 evaluate the results ob-tained with the interaction engine using Feats-IG-Sim and Feats+IG+Sim.
We compared the per-formance of our algorithms with a Baseline thatdoes not perform any interaction and is evaluatedbased on the retrieved results obtained with the un-derspecified queries.
The models for each of thesystems were trained using query-suggestion pairscollected from 100 specific queries (data collectedas explained in Section 2).
The remaining 100 spe-cific queries were used for testing.
We see that thesuggestions predicted by the classifiers using therelevancy model indeed improves the performanceof the baseline.
Also, adding the IG and Sim fea-ture further boosts the performance of the system.Systems MRRBaseline 0.4218Feats-IG-Sim 0.9449Feats+IG+Sim 0.9968Table 3: Comparison of the proposed approachwith respect to the Baseline that does not involveinteraction in terms of MRR.5.3 Related WorkLearning affinities between queries an documentsis a well studied area.
(Liu, 2009) provides an ex-cellent survey of these approaches.
In these meth-1 2 3 4 5 6 7 8 9 100.20.30.40.50.60.70.80.91Size of retrieved listSuccess atBaselineFeats?IG?SimFeats+IG+SimFigure 4: Comparison of the proposed approachwith respect to the Baseline that does not involveinteraction in terms of Success at N.ods, there is a fixed feature function ?
(x, y) de-fined between any query-document pair.
Thesefeatures are then used, along with labeled train-ing data, to learn the parameters of a model thatcan then be used to predict the relevance r(x, y)of a new query-document pair.
The output of themodel can also be used to re-rank the results of asearch engine.
In contrast to this class of methods,we define and parameterize the ?
function andjointly optimize the parameters of the feature map-ping and the machine learning re-ranking model.Latent tensor methods for regression and clas-sification have recently become popular in the im-age and signal processing domain.
Most of thesemethods solve an optimization problem similar toour own (9), but add additional constraints limit-ing the rank of the learned matrix W either ex-plicitly or implicit by defining W = Q1QT2, anddefining Q1?
Rdx?dand Q2?
Rdy?d.
This ap-proach is used for example in (Pirsiavash et al.,2009) and more recently in (Tan et al., 2013) (Guoet al., 2012).
While this reduces the number of pa-rameters to be learned from dxdyto d(dx+ dy) itmakes the problem non-convex and introduces anadditional parameter d that must be selected.This approach of restricting the rank was re-cently suggested for information retrieval in (Wuet al., 2013).
They look at a regression problem,using click-through rates as the reward functionr(x, y).
In addition, (Wu et al., 2013) does notuse an initial search engine and hence must learnan affinity function between all query-documentpairs.
In contrast to this, we learn a classificationfunction that discriminates between the true andfalse positive documents that are deemed similar224by the search engine.
This has three beneficial ef-fects : (i) it reduces the amount of labeled trainingdata required and the imbalance between the posi-tive and negative classes which can make learningdifficult (He and Garcia, 2009) and (ii) allows usto build on the strengths of fast and strong existingsearch engines increasing accuracy and decreas-ing retrieval time and (iii) allows the learnt modelto focus learning on the query-document pairs thatare most problematic for the search engine.Bilinear forms of tensor models without therank restriction have recently been studied for linkprediction (Menon and Elkan, 2011) and imageprocessing (Kobayashi and Otsu, 2012).
Sincethe applications are different, there is no prelimi-nary search engine which retrieves results, makingthem ranking methods and ours a re-ranking ap-proach.
Related work in text IR includes (Beefer-man and Berger, 2000), where two queries areconsidered semantically similar if their clicks leadto the same page.
However, the probability thatdifferent queries lead to common clicks of thesame URLs is very small, again increasing thetraining data required.
Approaches in the pasthave also proposed techniques to automaticallyfind suggestions either in the form of words,phrases (Kelly et al., 2009; Feuer et al., 2007;Baeza-yates et al., 2004) or similar queries (Leunget al., 2008) from query logs (Guo et al., 2008;Baeza-yates et al., 2004) or based on their prob-ability of representing the initial retrieved doc-uments (Kelly et al., 2009; Feuer et al., 2007).These suggestions are then ranked either based ontheir frequencies or based on their closeness to thequery.
Closeness is defined in terms of lexical sim-ilarity to the query.
However, most often the queryand the suggestions do not have any co-occurringwords leading to low similarity scores, even whenthe suggestion is relevant.
(Gangadharaiah and Narayanaswamy, 2013)use information gain to rank candidate sugges-tions.
However, the relevancy of the suggestionshighly depends on the relevancy of the initial re-trieved documents.
Our work here addresses thequestion of how to bridge this lexical chasm be-tween the query and the suggestions.
For this, weuse semantic-relatedness between the query andthe suggestions as a measure of closeness ratherthan defining closeness based on lexical similar-ity.
A related approach to handle this lexical gapby applying alignment techniques from StatisticalMachine translation (Brown et al., 1993), in par-ticular by building translation models for infor-mation retrieval (Berger and Lafferty, 1999; Rie-zler et al., 2007).
These approaches require train-ing data in the form of question-answer pairs, areagain limited to words or phrases and are not in-tended for understanding the user?s problem betterthrough interaction, which is our focus.6 Conclusions, Discussions and FutureWorkWe studied the problem of designing InformationRetrieval systems for interactive problem resolu-tion.
We developed a system for bridging thelarge lexical gap between short, incomplete prob-lem queries and documents in a database of reso-lutions.
We showed that tensor representations area useful tool to learn measures of semantic relat-edness, beyond the cosine similarity metric.
Ourresults show that with interaction, suggestions canbe effective in pruning large sets of retrieved doc-uments.
We showed that our approach offers sub-stantial improvement over systems that only uselexical similarities for retrieval and re-ranking, inan end-to-end problem-resolution domain.In addition to the classification losses consid-ered in this paper, we can also use another lossterm based on ideas from recommender systems,in particular (Menon and Elkan, 2011).
Considerthe matrix T with all training queries as rows andall units as the columns.
If we view the queryrefinement problem as a matrix completion prob-lem, it is natural to assume that this matrix has lowrank, so that T can be written as T = U?VT,where ?
is a diagonal matrix and parameter of ouroptimization.
These can then be incorporated intothe training process by appropriate changes to thecost and regularization terms.Another benefit of the tensor representation isthat it can easily be extended to incorporate othermeta-information that may be available.
For ex-ample, if context sensitive features, like the iden-tity of the agent, are available these can be incor-porated as another dimension in the tensor.
Whileoptimization over these higher dimensional ten-sors may be more computationally complex, theproblems are still convex and can be solved ef-ficiently.
This is a direction of future researchwe are pursuing.
Finally, exploring the power ofinformation gain type features in larger databasesystems is of interest.225ReferencesFatemeh Zahedi Antonio Palma-dos Reis.
1999.
De-signing personalized intelligent financial decisionsupport systems.Ricardo Baeza-yates, Carlos Hurtado, and MarceloMendoza.
2004.
Query recommendation us-ing query logs in search engines.
In In Interna-tional Workshop on Clustering Information over theWeb (ClustWeb, in conjunction with EDBT), Creete,pages 588?596.
Springer.Bing Bai, Jason Weston, David Grangier, Ronan Col-lobert, Kunihiko Sadamasa, Yanjun Qi, CorinnaCortes, and Mehryar Mohri.
2009.
Polynomial se-mantic indexing.
In NIPS, pages 64?72.Doug Beeferman and Adam Berger.
2000.
Agglomer-ative clustering of a search engine query log.
In Pro-ceedings of the Sixth ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, KDD ?00, pages 407?416, New York, NY, USA.ACM.Adam Berger and John Lafferty.
1999.
Informationretrieval as statistical translation.
In Proceedings ofthe 22Nd Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR ?99, pages 222?229, New York,NY, USA.
ACM.Christopher M Bishop.
2006.
Pattern recognition andmachine learning.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Comput.
Linguist., 19(2):263?311, June.Charles Elkan.
2010.
Learning affinity with biliearmodels.
Unpublished Notes.Alan Feuer, Stefan Savev, and Javed A. Aslam.
2007.Evaluation of phrasal query suggestions.
In Pro-ceedings of the Sixteenth ACM Conference on Con-ference on Information and Knowledge Manage-ment, CIKM ?07, pages 841?848, New York, NY,USA.
ACM.Rashmi Gangadharaiah and BalakrishnanNarayanaswamy.
2013.
Natural language queryrefinement for problem resolution from crowd-sourced semi-structured data.
In Proceedings ofthe Sixth International Joint Conference on NaturalLanguage Processing, pages 243?251, Nagoya,Japan, October.
Asian Federation of NaturalLanguage Processing.Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng.
2008.A unified and discriminative model for query refine-ment.
In Sung-Hyon Myaeng, Douglas W. Oard,Fabrizio Sebastiani, Tat-Seng Chua, and Mun-KewLeong, editors, SIGIR, pages 379?386.
ACM.Weiwei Guo, Irene Kotsia, and Ioannis Patras.
2012.Tensor learning for regression.
Image Processing,IEEE Transactions on, 21(2):816?827.Haibo He and Edwardo A Garcia.
2009.
Learningfrom imbalanced data.
Knowledge and Data Engi-neering, IEEE Transactions on, 21(9):1263?1284.Alexander Holland.
2005.
Modeling uncertainty indecision support systems for customer call center.In Computational Intelligence, Theory and Applica-tions, pages 763?770.
Springer.Diane Kelly, Karl Gyllstrom, and Earl W. Bailey.
2009.A comparison of query and term suggestion fea-tures for interactive searching.
In Proceedings of the32Nd International ACM SIGIR Conference on Re-search and Development in Information Retrieval,SIGIR ?09, pages 371?378, New York, NY, USA.ACM.Takumi Kobayashi and Nobuyuki Otsu.
2012.
Effi-cient optimization for low-rank integrated bilinearclassifiers.
In Computer Vision?ECCV 2012, pages474?487.
Springer.Kenneth Wai-Ting Leung, Wilfred Ng, and Dik LunLee.
2008.
Personalized concept-based clusteringof search engine queries.
IEEE Trans.
on Knowl.and Data Eng., 20(11):1505?1518, November.Tie-Yan Liu.
2009.
Learning to rank for informationretrieval.
Foundations and Trends in InformationRetrieval, 3(3):225?331.Aditya Krishna Menon and Charles Elkan.
2011.
Linkprediction via matrix factorization.
In MachineLearning and Knowledge Discovery in Databases,pages 437?452.
Springer.Mark A Musen, Yuval Shahar, and Edward H Short-liffe.
2006.
Clinical decision-support systems.Michael Pinedo, Sridhar Seshadri, and J George Shan-thikumar.
2000.
Call centers in financial services:strategies, technologies, and operations.
In Cre-ating Value in Financial Services, pages 357?388.Springer.Hamed Pirsiavash, Deva Ramanan, and CharlessFowlkes.
2009.
Bilinear classifiers for visual recog-nition.
In NIPS, pages 1482?1490.Pradeep Ravikumar, Martin J Wainwright, and John DLafferty.
2010.
High-dimensional ising model se-lection using 1-regularized logistic regression.
TheAnnals of Statistics, 38(3):1287?1319.Stefan Riezler, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal, and Yi Liu.
2007.Statistical Machine Translation for Query Expan-sion in Answer Retrieval.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 464?471, Prague, CzechRepublic, June.
Association for ComputationalLinguistics.226Gerard Salton and Michael J McGill.
1983.
Introduc-tion to modern information retrieval.T.
Strohman, D. Metzler, H. Turtle, and W. B. Croft.2004.
Indri: A language model-based search enginefor complex queries.
Proceedings of the Interna-tional Conference on Intelligence Analysis.Xu Tan, Yin Zhang, Siliang Tang, Jian Shao, Fei Wu,and Yueting Zhuang.
2013.
Logistic tensor re-gression for classification.
In Intelligent Scienceand Intelligent Data Engineering, pages 573?581.Springer.Jean-Philippe Vert and Laurent Jacob.
2008.
Machinelearning for in silico virtual screening and chemicalgenomics: new strategies.
Combinatorial chemistry& high throughput screening, 11(8):677.Wei Wu, Zhengdong Lu, and Hang Li.
2013.
Learn-ing bilinear model for matching queries and docu-ments.
The Journal of Machine Learning Research,14(1):2519?2548.227
