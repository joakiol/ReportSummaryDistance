2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 367?371,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsG2P Conversion of Proper Names Using Word Origin InformationSonjia Waxmonsky and Sravana ReddyDepartment of Computer ScienceThe University of ChicagoChicago, IL 60637{wax, sravana}@cs.uchicago.eduAbstractMotivated by the fact that the pronuncia-tion of a name may be influenced by itslanguage of origin, we present methods toimprove pronunciation prediction of propernames using word origin information.
Wetrain grapheme-to-phoneme (G2P) models onlanguage-specific data sets and interpolate theoutputs.
We perform experiments on US sur-names, a data set where word origin variationoccurs naturally.
Our methods can be usedwith any G2P algorithm that outputs poste-rior probabilities of phoneme sequences for agiven word.1 IntroductionSpeakers can often associate proper names with theirlanguage of origin, even when the words have notbeen seen before.
For example, many English speak-ers will recognize that Makowski and Masiello arePolish and Italian respectively, without prior knowl-edge of either name.
Such recognition is importantfor language processing tasks since the pronuncia-tions of out-of-vocabulary (OOV) words may de-pend on the language of origin.
For example, asnoted by Llitjo?s (2001), ?sch?
is likely to be pro-nounced as /sh/ for German-origin names (Schoe-nenberg) and /sk/ for Italian-origin words (Schi-avone).In this work, we apply word origin recognitionto grapheme-to-phoneme (G2P) conversion, the taskof predicting the phonemic representation of a wordgiven its written form.
We specifically study G2Pconversion for personal surnames, a domain whereOOVs are common and expected.Our goal is to show how word origin informationcan be used to train language-specific G2P models,and how output from these models can be combinedto improve prediction of the best pronunciation of aname.
We deal with data sparsity in rare languageclasses by re-weighting the output of the language-specific and language-independent models.2 Previous WorkLlitjo?s (2001) applies word origin information topronunciation modeling for speech synthesis.
Here,a CART decision tree system is presented for G2Pconversion that maps letters to phonemes using localcontext.
Experiments use a data set of US surnamesthat naturally draws from a diverse set of origin lan-guages, and show that the inclusion of word originfeatures in the model improves pronunciation accu-racy.
We use similar data, as described in ?4.1.Some works on lexical modeling for speechrecognition also make use of word origin.
Here,the focus is on expanding the vocabulary of an ASRsystem rather than choosing a single best pronunci-ation.
Maison et al (2003) train language-specificG2P models for eight languages and output pronun-ciations to augment a baseline lexicon.
This aug-mented lexicon outperforms a handcrafted lexiconin ASR experiments; error reduction is highest forforeign names spoken by native speakers of the ori-gin language.
Cremelie and ten Bosch (2001) carryout a similar lexicon augmentation, and make use ofpenalty weighting, with different penalties for pro-nunciations generated by the language-specific andlanguage-independent G2P models.The problem of machine transliteration is closelyrelated to grapheme-to-phoneme conversion.
Many367transliteration systems (Khapra and Bhattacharyya,2009; Bose and Sarkar, 2009; Bhargava and Kon-drak, 2010) use word origin information.
Themethod described by Hagiwara and Sekine (2011)is similar to our work, except that (a) we use a dataset where multiple languages of origin occur nat-urally, rather than creating language-specific listsand merging them into a single set, and (b) weconsider methods of smoothing against a language-independent model to overcome the problems ofdata sparsity and errors in word origin recognition.3 Language-Aware G2POur methods are designed to be used with anystatistical G2P system that produces the posteriorprobability Pr(??|g?)
of a phoneme sequence ??
fora word (grapheme sequence) g?
(or a score thatcan be normalized to give a probability).
Themost likely pronunciation of a word is taken to bearg max??
Pr(??|g?
).Our baseline is a single G2P model that is trainedon all available training data.
We train additionalmodels on language-specific training subsets and in-corporate the output of these models to re-estimatePr(??|g?
), which involves the following steps:1.
Train a supervised word origin classifier to pre-dict Pr(l|w) for all l ?
L, the set of languagesin our hand-labeled word origin training set.2.
Train G2P models for each l ?
L. Each modelml is trained on words with Pr(l|w) greaterthan some threshold ?.
Here, we use ?
= 0.7.3.
For each word w in the test set, generate can-didate transcriptions from model ml for eachlanguage with nonzero Pr(l|w).
Re-estimatePr(??|g?)
by interpolating the outputs of thelanguage-specific models.
We may also use theoutput of the language-independent model.We elaborate on our approaches to Steps 1 and 3.3.1 Step 1: Word origin modelingWe apply a sequential conditional model to predictPr(l|w), the probability of a language class giventhe word.
A similar Maximum Entropy model ispresented by Chen and Maison (2003), where fea-tures are the presence or absence of a given charac-ter n-gram in w. In our approach, feature functionsare defined at character positions rather than over theentire word.
Specifically, for word wj composed ofcharacter sequence c1 .
.
.
cm of length m (includingstart and end symbols), binary features test for thepresence or absence of an n-gram context at eachposition m. A context is the presence of a charac-ter n-gram starting or ending at position m. Modelfeatures are represented as:fi(w,m, lk) =????
?1, if lang(w) = lk and contexti is present at position m0, otherwise(1)Then, for wj = ci .
.
.
cm:Pr(lk|wj) =exp?m?i ?ifi(cm, lk)Z(2)where Z =?j exp?m?i ?ifi(cm, lk) is a nor-malization factor.
In practice, we can implement thismodel as a CRF, where a language label is appliedat each character position rather than for the word.While all the language labels in a sequence neednot be the same, we find only a handful of wordswhere a transition occurs from one language label toanother within a word.
For these cases, we take thelabel of the last character in the word as the languageof origin.
Experiments comparing this sequentialMaximum Entropy method with other word originclassifiers are described by Waxmonsky (2011).3.2 Step 3: Re-weighting of G2P outputWe test two methods of re-weighting Pr(??|g?)
us-ing the word origin estimation and the output oflanguage-specific G2P models.Method A uses only language-specific models:P?r(??|g?)
=?l?LPr(?
?|g?, l) Pr(l|g) (3)where Pr(?
?|g?, l) is estimated by model ml.Method B With the previous method, names frominfrequent classes suffer from data sparsity.
Wetherefore smooth with the output PI of the baselinelanguage-independent model.P?r(??|g?)
= ?PrI(??|g?)+(1??)?l?LPr(?
?|g?, l) Pr(l|g)(4)The factor ?
is tuned on a development set.368Language Train Test Base (A) (B)Class Count Count -lineBritish 16.1k 2111 71.8 73.1 73.9German 8360 1109 75.8 74.2 78.2Italian 3358 447 61.7 66.2 65.1Slavic 1658 232 50.9 49.6 51.7Spanish 1460 246 44.7 41.5 48.0French 1143 177 42.9 42.4 45.2Dutch 468 82 70.7 52.4 68.3Scandin.
393 61 77.1 60.7 72.1Japanese 116 23 73.9 52.2 78.3Arabic 68 18 33.3 11.1 38.9Portug.
34 4 25.0 25.0 50.0Hungarian 28 3 100.0 66.7 100.0Other 431 72 55.6 54.2 59.7All 67.8 67.4 70.0Table 1: G2P word accuracy for various weighting meth-ods using a character-based word origin model.4 Experiments4.1 DataWe assemble a data set of surnames that occur fre-quently in the United States.
Since surnames areoften ?Americanized?
in their written and phone-mic forms, our goal is to model how a name ismost likely to be pronounced in standard US Englishrather than in its language of origin.We consider the 50,000 most frequent surnamesin the 1990 census1, and extract those entries thatalso appear in the CMU Pronouncing Dictionary2,giving us a set of 45,841 surnames with theirphoneme representations transcribed in the Arpabetsymbol set.
We divide this data 80/10/10 into train,test, and development sets.To build a word origin classification training set,we randomly select 3,000 surnames from the samecensus lists, and label by hand the most likely lan-guage of origin of each name when it occurs in theUS.
Labeling was done primarily using the Dictio-nary of American Family Names (Hanks, 2003) andEllis Island immigration records.3 We find that, inmany cases, a surname cannot be attributed to a sin-gle language but can be assigned to a set of lan-1http://www.census.gov/genealogy/names/2http://www.speech.cs.cmu.edu/cgi-bin/cmudict3http://www.ellisisland.orgguages related by geography and language family.For example, we discovered several surnames thatcould be ambiguously labeled as English, Scottish,or Irish in origin.
For languages that are frequentlyconfusable, we create a single language group to beused as a class label.
Here, we use groups for BritishIsles, Slavic, and Scandinavian languages.
Namesof undetermined origin are removed, leaving a finaltraining set of 2,795 labeled surnames and 33 dif-ferent language classes.
We have made this anno-tated word origin data publicly available for futureresearch.4In these experiments, we use surnames from the12 language classes that contain at least 10 hand-labeled words, and merge the remaining languagesinto an ?Other?
class.
Table 1 shows the final lan-guage classes used.
Unlike the training sets, we donot remove names with ambiguous or unknown ori-gin from the test set, so our G2P system is also eval-uated on the ambiguous names.4.2 ResultsThe Sequitur G2P algorithm (Bisani and Ney, 2008)is used for all our experiments.We use the CMU Dictionary as the gold stan-dard, with the assumption that it contains the stan-dard pronunciations in US English.
While surnamesmay have multiple valid pronunciations, we makethe simplifying assumption that a name has one bestpronunciation.
Evaluation is done on the test set of4,585 names from the CMU Dictionary.Table 1 shows G2P accuracy for the baseline sys-tem and Methods A and B.
Test data is partitionedby the most likely language of origin.We see that Method A, which uses only language-specific G2P models, has lower overall accuracythan the baseline.
We attribute this to data spar-sity introduced by dividing the training set by lan-guage.
With the exception of British and German,language-specific training set sizes are less than 10%the size of the baseline training set of 37k names.Another cause of the lowered performance is likelydue to errors made by our word origin model.Examining results for individual language classesfor Method A, we see that Italian and British are4The data may be downloaded from http://people.cs.uchicago.edu/?wax/wordorigin/.369Language Surname Baseline Method BCarcione K AA R S IY OW N IY K AA R CH OW N IYCuttino K AH T IY N OW K UW T IY N OWItalian Lubrano L AH B R AA N OW L UW B R AA N OWPesola P EH S AH L AH P EH S OW L AHKotula K OW T UW L AH K AH T UW L AHSlavic Jaworowski JH AH W ER AO F S K IY Y AH W ER AO F S K IYLisak L IY S AH K L IH S AH KWasik W AA S IH K V AA S IH KBencivenga B EH N S IH V IH N G AH B EH N CH IY V EH NG G AHSpanish Vivona V IH V OW N AH V IY V OW N AHZavadil Z AA V AA D AH L Z AA V AA D IY LTable 2: Sample G2P output from the Baseline (language-independent) and Method B systems.
Language labelsshown here are the arg maxl P (l|w) using the character-based word origin model.
Phoneme symbols are from anArpabet-based alphabet, as used in the CMU Pronouncing Dictionary.the only language classes where accuracy improves.For Italian, we attribute this to two factors: highdivergence in pronunciation from US English, andthe availability of enough training data to build asuccessful language-specific model.
In the case ofBritish, a language-specific model removes foreignwords but leaves enough training data to model thelanguage sufficiently.Method B shows accuracy gains of 2.2%, withgains for almost all language classes except Dutchand Scandinavian.
This is probably because namesin these two classes have almost standard US En-glish pronunciations, and are already well-modeledby a language-independent model.We next look at some sample outputs from ourG2P systems.
Table 2 shows names where MethodB generated the gold standard pronunciation and thebaseline system did not.
For the Italian and Span-ish sets, we see that the letter-to-phoneme mappingsproduced by Method B are indicative of the lan-guage of origin: (c ?
/CH/) in Carcione, (u ?/UW/) in Cuttino, (o ?
/OW/) in Pesola, and (i ?/IY/) in Zavadil and Vivona.
Interestingly, the nameBencivenga is categorized as Spanish but appearswith the letter-to-phoneme mapping (c ?
/CH/),which corresponds to Italian as the language of ori-gin.
We found other examples of the (c ?
/CH/)mappings, indicating that Italian-origin names havebeen folded into Spanish data.
This is not surprisingsince Spanish and Italian names have high confusionwith each other.
Effectively, our word origin modelproduced a noisy Spanish G2P training set, but there-weighted G2P system is robust to these errors.We see examples in the Slavic set where the goldstandard dictionary pronunciation is partially but notcompletely Americanized.
In Jaworowski, we havethe mappings (j?
/Y/) and (w?
/F/), both of whichare derived from the original Polish pronunciation.But for the same name, we also have (w ?
/W/)rather than (w?
/V/), although the latter is truer tothe original Polish.
This illustrates one of the goalsof our project, which is is to capture these patternsof Americanization as they occur in the data.5 ConclusionWe apply word origin modeling to grapheme-to-phoneme conversion, interpolating betweenlanguage-independent and language-specific proba-bilistic grapheme-to-phoneme models.
We find thatour system outperforms the baseline in predictingAmericanized surname pronunciations and capturesseveral letter-to-phoneme features that are specificto the language of origin.Our method operates as a wrapper around G2Poutput without modifying the underlying algorithm,and therefore can be applied to any state-of-the-artG2P system that outputs posterior probabilities ofphoneme sequences for a word.Future work will consider unsupervised or semi-supervised approaches to word origin recognitionfor this task, and methods to tune the smoothingweights ?
at the language rather than the globallevel.370ReferencesAditya Bhargava and Grzegorz Kondrak.
2010.
Lan-guage identification of names with SVMs.
In Proceed-ings of NAACL.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication.Dipankar Bose and Sudeshna Sarkar.
2009.
Learningmulti character alignment rules and classification oftraining data for transliteration.
In Proceedings of theACL Named Entities Workshop.Stanley F. Chen and Beno?
?t Maison.
2003.
Using placename data to train language identification models.
InProceedings of Eurospeech.Nick Cremelie and Louis ten Bosch.
2001.
Improv-ing the recognition of foreign names and non-nativespeech by combining multiple grapheme-to-phonemeconverters.
In Proceedings of ITRW on AdaptationMethods for Speech Recognition.Masato Hagiwara and Satoshi Sekine.
2011.
Latent classtransliteration based on source language origin.
InProceedings of ACL.Patrick Hanks.
2003.
Dictionary of American familynames.
New York : Oxford University Press.Mitesh M. Khapra and Pushpak Bhattacharyya.
2009.Improving transliteration accuracy using word-origindetection and lexicon lookup.
In Proceedings of theACL Named Entities Workshop.Ariadna Font Llitjo?s.
2001.
Improving pronunciationaccuracy of proper names with language origin classes.Master?s thesis, Carnegie Mellon University.Beno?
?t Maison, Stanley F. Chen, and Paul S. Cohen.2003.
Pronunciation modeling for names of foreignorigin.
In Proceedings of ASRU.Sonjia Waxmonsky.
2011.
Natural language process-ing for named entities with word-internal information.Ph.D.
thesis, University of Chicago.371
