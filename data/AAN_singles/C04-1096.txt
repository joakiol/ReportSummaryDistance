Generation of Relative Referring Expressions based on Perceptual GroupingKotaro FUNAKOSHIDepartment of Computer ScienceTokyo Institute of TechnologyMeguro O?okayama 2-12-1,Tokyo 152-8552, Japankoh@cl.cs.titech.ac.jpSatoru WATANABEDepartment of Computer ScienceTokyo Institute of TechnologyMeguro O?okayama 2-12-1,Tokyo 152-8552, Japansatoru w@cl.cs.titech.ac.jpNaoko KURIYAMADepartment of Human System ScienceTokyo Institute of TechnologyMeguro O?okayama 2-12-1,Tokyo 152-8552, Japankuriyama@hum.titech.ac.jpTakenobu TOKUNAGADepartment of Computer ScienceTokyo Institute of TechnologyMeguro O?okayama 2-12-1,Tokyo 152-8552, Japantake@cl.cs.titech.ac.jpAbstractPast work of generating referring expressionsmainly utilized attributes of objects and bi-nary relations between objects.
However, suchan approach does not work well when thereis no distinctive attribute among objects.
Toovercome this limitation, this paper proposes amethod utilizing the perceptual groups of ob-jects and n-ary relations among them.
The keyis to identify groups of objects that are natu-rally recognized by humans.
We conducted psy-chological experiments with 42 subjects to col-lect referring expressions in such situations, andbuilt a generation algorithm based on the re-sults.
The evaluation using another 23 subjectsshowed that the proposed method could effec-tively generate proper referring expressions.1 IntroductionIn the last two decades, many researchers have stud-ied the generation of referring expressions to enablecomputers to communicate with humans about con-crete objects in the world.For that purpose, most past work (Appelt, 1985;Dale and Haddock, 1991; Dale, 1992; Dale andReiter, 1995; Heeman and Hirst, 1995; Horacek,1997; Krahmer and Theune, 2002; van Deemter,2002; Krahmer et al, 2003) makes use of attributesof an intended object (the target) and binary rela-tions between the target and others (distractors) todistinguish the target from distractors.
Therefore,these methods cannot generate proper referring ex-pressions in situations where no significant surfacedifference exists between the target and distractors,and no binary relation is useful to distinguish thetarget.
Here, a proper referring expression meansa concise and natural linguistic expression enablinghearers to distinguish the target from distractors.For example, consider indicating object b to per-son P in the situation shown in Figure 1.
Note thatperson P does not share the label information suchas a and b with the speaker.
Because object b isnot distinguishable from objects a or c by means oftheir appearance, one would try to use a binary re-lation between object b and the table, i.e., ?A ballto the right of the table?.
1 However, ?to the rightof ?
is not a discriminatory relation, for objects aand c are also located to the right of the table.
Us-ing a and c as a reference object instead of the ta-ble does not make sense, since a and c cannot beuniquely identified because of the same reason thatb cannot be identified.
Such situations have neverdrawn much attention, but can occur easily and fre-quently in some domains such as object arrange-ment (Tanaka et al, 2004).van der Sluis and Krahmer (2000) proposed us-ing gestures such as pointing in situations like thoseshown in Figure 1.
However, pointing and gazingare not always available depending on the positionalrelation between the speaker and the hearer.In the situation shown in Figure 1, a speaker canindicate object b to person P with a simple expres-sion ?the front ball?
without using any gesture.
Inorder to generate such an expression, one must beable to recognize the salient perceptual group of theobjects and use the n-ary relative relations in thegroup.
2In this paper, we propose a method of generat-1In this paper, we simply assume that all participants sharethe appropriate reference frame (Levinson, 2003).
We mentionthis issue in the last section.2Although Krahmer et al claim that their method can han-dle n-ary relations (Krahmer et al, 2003), they provide no de-tails.
We think their method cannot directly handle situationswe discuss here.ing referring expressions that utilizes n-ary relationsamong members of a group.
Our method recognizesgroups by using Tho?risson?s algorithm (Tho?risson,1994).
As the first step of our research project, wedeal with the limited situations where only homoge-neous objects are randomly arranged (see Figure 2).Therefore, we handle positional n-ary relation only,and other types of n-ary relation such as size, e.g.,?the biggest one?, are not mentioned.Speakers often refer to multiple groups in thecourse of referring to the target.
In these cases, wecan observe two types of relations: the intra-grouprelation such as ?the front two among the five nearthe desk?, and the inter-group relation such as ?thetwo to the right of the five?.
We define that a sub-sumption relation between two groups is an intra-group relation.In what follows, Section 2 explains the exper-iments conducted to collect expressions in whichperceptual groups are used.
The proposed method isdescribed and evaluated in Section 3.
In Section 4,we examine a possibility to predict the adequacy ofan expression in terms of perceptual grouping.
Fi-nally, we conclude the paper in Section 5.PabcTableFigure 1: An example of problematic situations2 Data CollectionWe conducted a psychological experiment with 42Japanese undergraduate students to collect referringexpressions in which perceptual groups are used.
Inorder to evaluate the collected expressions, we con-ducted another experiment with a different group of44 Japanese undergraduate students.
There is nooverlap between the subjects of those two experi-ments.
Details of this experiment are described inthe following subsections.2.1 Collecting Referring ExpressionsMethod Subjects were presented 2-dimensionalbird?s-eye images in which several objects of thesame color and the same size were arranged and thesubjects were requested to convey a target object tothe third person drawn in the same image.
We used12 images of arrangements.
In each image, threeto nine objects were arranged manually so that theobjects distributes non-uniformly.
An example ofimages presented to subjects is shown in Figure 2.Labels a, .
.
.
, f, x in the image are assigned for pur-poses of illustration and are not assigned in the ac-tual images presented to the subjects.
Each subjectwas asked to describe a command so that the personin the image picks a target object that is enclosedwith dotted lines.
When a subject could not thinkof a proper expression, she/he was allowed to aban-don that arrangement and proceed to the next one.Referring expressions designating the target objectwere collected from these subjects?
commands.Pabefc dxFigure 2: A visual stimulus of the experimentAnalysis We presented 12 arrangements to 42subjects and obtained 476 referring expressions.Twenty eight judgments were abandoned in the ex-periment.
Observing the collected expressions, wefound that starting from a group with all of the ob-jects, subjects generally narrow down the group toa singleton group that has the target object.
There-fore, a referring expression can be formalized as asequence of groups (SOG) reflecting the subject?snarrowing down process.The following example shows an observed ex-pression describing the target x in Figure 2 with thecorresponding SOG representation below it.
?hidari oku ni aru mittu no tama no uti noitiban migi no tama.?
(the rightmost ball among the three ballsat the back left)SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}] 3where{a, b, c, d, e, f, x} denotes all objects inthe image (total set),{a, b, x} denotes the three objects at theback left, and{x} denotes the target.3We denote an SOG representation by enclosing groupswith square brackets.Since narrowing down starts from the total set,the SOG representation starts with a set of all ob-jects and ends with a singleton group with the tar-get.
Translating the collected referring expressionsinto the SOG representation enables us to abstractand classify the expressions.
On average, we ob-tained about 40 expressions for each arrangement,and classified them into 8.4 different SOG represen-tations.Although there are two types of relations be-tween groups as we mentioned in Section 1, the ex-pressions using only intra-group relations made upabout 70% of the total.2.2 Evaluating the Collected ExpressionsMethod Subjects were presented expressions col-lected in the experiment described in Section 2.1 to-gether with the corresponding images, and were re-quested to indicate objects referred to by the expres-sions.
The presented images are the same as thoseused in the previous experiment except that there areno marks on the targets.
At the same time, subjectswere requested to express their confidence in select-ing the target, and evaluate the conciseness, and thenaturalness of the given expressions on a scale of 1to 8.Because the number of expressions that we couldevaluate with subjects was limited, we chose a max-imum of 10 frequent expressions for each arrange-ment.
The expressions were chosen so that as manydifferent SOG representations were included as pos-sible.
If an arrangement had SOGs less than 10,several expressions that had the same SOG but dif-ferent surface realizations were chosen.
The resul-tant 117 expressions were evaluated by 49 subjects.Each subject evaluated about 29.5 expressions.Analysis Discarding incomplete answers, we ob-tained 1,429 evaluations in total.
12.2 evaluationswere obtained for each expression on average.We measured the quality of each expression interms of an evaluation value that is defined in (1).This measure is used to analyze what kind of ex-pressions are preferred and to set up a scoring func-tion (6) for machine-generated expressions as de-scribed in Section 3.1.
(evaluation value)= (accuracy)?
(confidence)?
(naturalness) + (conciseness)2(1)According to our analysis, the expressions withonly intra-group relations (84 samples) obtainedhigh accuracies (Ave. 79.3%) and high evaluationvalues (Ave. 33.1), while the expressions with inter-group relations (33 samples) obtained lower accura-cies (Ave. 69.1%) and lower evaluation values (Ave.19.7).The expressions with only intra-group relationsare observed more than double as many as the ex-pressions with inter-group relations.
We provide acouple of example expressions indicating object xin Figure 2 to contrast those two types of expres-sions below.?
without inter-group relations?
?the rightmost ball among the three ballsat the back left??
with inter-group relations?
?the ball behind the two front balls?In addition, expressions explicitly mentioning allthe objects obtained lower evaluation values.
Con-sidering these observations, we built a generationalgorithm using only intra-group relations and didnot mention all the objects explicitly.Among these expressions, we selected those withwhich the subjects successfully identified the targetwith more than 90% accuracy.
These expressionsare used to extract parameters of our generation al-gorithm in the following sections.3 Generating Referring Expressions3.1 Generation AlgorithmGiven an arrangement of objects and a target, our al-gorithm generates referring expressions by the fol-lowing three steps:Step 1: enumerate perceptual groups based on theproximity between objectsStep 2: generate the SOG representations by com-bining the groupsStep 3: translate the SOG representations into lin-guistic expressionsIn the rest of this section, we illustrate how thesethree steps generate referring expressions in the sit-uation shown in Figure 2.Step 1: Enumerating Perceptual Groups.To generate perceptual groups from an arrangement,Tho?risson?s algorithm (Tho?risson, 1994) is adopted.Given a list of objects in an arrangement, the al-gorithm generates groups based on the proximity ofthe objects and returns a list of groups.
Only groupscontaining the target, that is x, are chosen becauseSOG: [{a, b, c, d, e, f, x}, {a, b, x}, {x}]?
E(R({a, b, c, d, e, f, x}, {a, b, x})) + E({a, b, x}) + E(R({a, b, x}, {x})) + E({x})?
?hidari oku no?+?mittu no tama?+?no uti no migihasi no?+?tama?
(at the back left) (three balls) (rightmost .
.
.
among) (ball)Figure 3: An example of surface realizationwe handle intra-group relations only as mentionedbefore, and that implies that all groups mentionedin an expression must include the target.
Then, thegroups are sorted in descending order of the groupsize.
Finally a singleton group consisting of the tar-get is added to the end of the list if such a group ismissing in the list.
The resultant group list, GL, isthe output of Step 1.For example, the algorithm recognizes the fol-lowing groups given the arrangement shown in Fig-ure 2:{{a, b, c, d, e, f, x}, {a, b, c, d, x},{a, b, x}, {c, d}, {e, f}}.After filtering out the groups without the target andadding a singleton group with the target, we obtainthe following list:{{a, b, c, d, e, f, x}, {a, b, c, d, x}, {a, b, x}, {x}}.
(2)Step 2: Generating the SOG Representations.In this step, the SOG representations introduced inSection 2 are generated from the GL of Step 1,which generally has a form like (3), where Gide-notes a group, and G0is a group of all the objects.Here, we narrow down the objects starting from thetotal set (G0) to the target ({x}).
[G0, G1, .
.
.
, Gm?2, {x}] (3)Given a group list GL, all possible SOGs are gen-erated.
From a group list of size m, 2m?2 SOGrepresentations can be generated since G0and {x}should be included in the SOG representation.
Forexample, from a group list of {G0, G1, G2, {x}},we obtain four SOGs: [G0, {x}], [G0, G1, {x}],[G0, G2, {x}], and [G0, G1, G2, {x}].For example, one of the SOG representationsgenerated from list (2) is[{a, b, c, d, e, f, x}, {a, b, x}, {x}].
(4)Note that any two groups Giand Gjin a list ofgroups generated by Tho?risson?s algorithm with re-gard to one feature, e.g., proximity in this paper, aremutually disjoint (Gi?Gj= ?
), otherwise one sub-sumes the other (Gi?
Gjor Gj?
Gi).
No inter-secting groups without a subsumption relation aregenerated.Step 3: Generating Linguistic Expressions.In the last step, the SOG representations are trans-lated into linguistic expressions.
Since Japanese isa head-final language, the order of linguistic ex-pressions for groups are retained in the final lin-guistic expression for the SOG representation.
Thatis, an SOG representation [G0, G1, .
.
.
, Gn?2, {x}]can be realized as shown in (5), where E(X) de-notes a linguistic expression for X, R(X,Y ) de-notes a relation between X and Y , and ?+?
is astring concatenation operator.E(G0) + E(R(G0, G1)) + E(G1) + .
.
.+E(R(Gn?2, {x})) + E({x}) (5)As described in Section 2.2, expressions that ex-plicitly mention all the objects obtain lower evalu-ation values, and expressions using intra-group re-lations obtain high evaluation values.
Consideringthese observations, our algorithm does not use thelinguistic expression corresponding to all the ob-jects, that is E(G0), and only uses intra-group re-lations for R(X,Y ).Possible expressions of X are collected from theexperimental data in Section 2.1, and the first ap-plicable expression is selected when realizing a lin-guistic expression for X, i.e., E(X).
Therefore,this algorithm produces one linguistic expressionfor each SOG even though there are some other pos-sible expressions.For example, the SOG representation (4) is real-ized as shown in Figure 3.Note that there is no mention of all the objects,{a, b, c, d, e, f, x}, in the linguistic expression.3.2 Evaluation of Generated ExpressionsWe implemented the algorithm described in Sec-tion 3.1, and evaluated the output with 23 under-graduate students.
The subjects were different fromthose of the previous experiments but were of thesame age group, and the experimental environmentAccuracy (%) Naturalness Conciseness Confidence Eval.
val.Human-12-all 87.3 4.82 5.27 6.14 29.3Human-12-90 97.9 5.20 5.62 6.50 35.0Human-12-100 100 5.36 5.73 6.65 37.2System-12 91.0 5.60 6.25 6.32 40.1System-20 88.4 5.09 5.65 6.25 35.2System-Average 89.2 5.24 5.82 6.27 36.6Table 1: Summary of evaluationwas the same.
The evaluation of the output was per-formed in the same manner as that of Section 2.2.The results are shown in Table 1.
?Human-12-all?
shows the average values of all expres-sions collected from humans with 12 arrangementsas described in Section 2.2.
?Human-12-90?
and?Human-12-100?
show the average values of ex-pressions by humans that gained more than 90%and 100% in accuracy in the same evaluation ex-periment respectively.?System-12?
shows the average values of expres-sions generated by the algorithm for the 12 arrange-ments used in the data collection experiment de-scribed in Section 2.1.
The algorithm generated 18expressions for the 12 arrangements, which werepresented to each subject in random order for eval-uation.?System-20?
shows the average values of expres-sions generated by the algorithm for 20 randomlygenerated arrangements that generate at least twolinguistic expressions each.
The algorithm gen-erated 48 expressions for these 20 arrangements,which were evaluated in the same manner as thatof ?System-12?.?System-Average?
shows the micro average ofexpressions of both ?System-12?
and ?System-20?.?Accuracy?
shows the rates at which the sub-jects could identify the correct target objects fromthe given expressions.
Comparing the accuracies of?Human-12-*?
and ?System-12?, we find that thealgorithm generates good expressions.
Moreover,the algorithm is superior to human in terms of ?Nat-uralness?
and ?Conciseness?.
However, this resultshould be interpreted carefully.
Further investiga-tion of the expressions revealed that humans oftensacrificed naturalness and conciseness in order todescribe the target as precisely as possible for com-plex arrangements.4 Scoring SOG RepresentationsThe algorithm presented in the previous section out-puts several possible expressions.
Therefore, wehave to choose one of the expressions by calculat-ing their scores.The scores can be computed using various mea-sures, such as complexity of expressions, andsalience of referent objects.
In this section, we in-vestigate whether the adequacies of the courses ofnarrowing down can be predicted: that is, whethermeaningful scores of SOG representations can becalculated.4.1 Method for SOG ScoringAn SOG representation has a form as stated in (3).We presumed that, when a speaker tries to narrowdown an object group from Gito Gi+1, there isan optimal ratio between the dimensions of GiandGi+1.
In other words, narrowing down a group froma very big one to a very small one might cause hear-ers to become confused.For example, consider the following two expres-sions that both indicate object x in Figure 2.
Hearerswould prefer (i) to (ii) though (ii) is simpler than (i).
(i) ?the rightmost ball among the three balls at theback left?
(ii) ?the fourth ball from the right?In fact, we found (i) among the expressions col-lected in Section 2.1, but did not find (ii) amongthem.
Our algorithm generated both (i) and (ii)in Section 3.2, and the two expressions gained theevaluation values of 44.4 and 32.1 respectively.If our presumption is correct, we can expectto choose better expressions by choosing expres-sions that have adequate dimension ratios betweengroups.Calculation FormulaThe total score of an SOG representation is calcu-lated by averaging the scores given by functions f1and f2whose parameters are dimension ratios be-tween two consecutive groups as given in (6), wheren is the number of groups in the SOG.score(SOG) = 1n ?
1{n?3?i=0f1(dim(Gi+1)dim(Gi))+ f2(dim({x})dim(Gn?2))} (6)The dimension of a group dim is defined as theaverage distance between the centroid of the groupand that of each object.
The dimension of the sin-gleton group {x} is defined as a constant value.
Be-cause of this idiosyncrasy of the singleton group{x} compared to other groups, f2was introducedseparately from f1even though both functions rep-resent the same concept, as described below.The optimal ratio between two groups, and thatfrom a group to the target were found through thequadratic regression analysis of data collected in theexperiment described in Section 2.2. f1and f2arethe two regression curves found through analysisrepresenting correlations between dimension ratiosand values calculated based on human evaluation asin formula (1).We could not find direct correlationsbetween dimension ratios and accuracies.4.2 ResultsWe checked to what extent the scores of generatedexpressions given by formula (6) conformed withthe human evaluation given by formula (1) as agree-ment.
Agreement was calculated as follows using20 randomly generated arrangements described inSection 3.2.First, the generated expressions were ordered ac-cording to the score given by formula (6) and thehuman evaluation given by formula (1).
All binaryorder relations between two expressions were ex-tracted from these two ordered lists of expressions.The agreement was defined as the ratio of the samebinary order relations among all binary order rela-tions.The agreement between scores and the humanevaluation was 45.8%.
The score did not predictSOG representations that would generate better ex-pressions very well.
Further research is required toconclusively rule out the use of dimension ratios forprediction or whether other factors are involved.5 Concluding Remarks and Future WorkThis paper proposed an algorithm that generates re-ferring expressions using perceptual groups and n-ary relations among them.
The algorithm was builton the basis of the analysis of expressions that werecollected through psychological experiments.
Theperformance of the algorithm was evaluated by 23subjects and it generated promising results.In the following, we look at future work to bedone.Recognizing salient geometric formations:Tho?risson?s algorithm (Tho?risson, 1994) cannotrecognize a linear arrangement of objects as agroup, although such arrangements are quite salientfor humans.
This is one of the reasons for thedisconformity between the evaluations given by thealgorithm and those of the humans subjects.We can enumerate most of such geometric ar-rangements salient for human subject by referring togeometric terms found in lexicons and thesauri suchas ?line?, ?circle?, ?square?
and so on.
Tho?risson?salgorithm should be extended to recognize these ar-rangements.Using relations other than positional relations:In this paper, we focused on positional relations ofperceptual groups.
Other relations such as degree ofcolor and size should be treated in the same manner.Tho?risson?s original algorithm (Tho?risson, 1994)takes into account these relations as well as posi-tional relations of objects when calculating similar-ity between objects to generate groups.
However, ifwe generate groups using multiple relations simul-taneously, the assumption used in Step 1 of our al-gorithm that any pair of groups in an output list donot intersect without a subsumption relation cannotbe held.
Therefore, the mechanism generating SOGrepresentations (Step 2 in Section 3.1) must be re-considered.Resolving reference frames and differences ofperspective: We assumed that all participants ina conversation shared the same reference frame.However, when we apply our method to conversa-tional agent systems, e.g., (Cavazza et al, 2002;Tanaka et al, 2004), reference frames must be prop-erly determined each time to generate referring ex-pressions.
Although there are many studies con-cerning reference frames, e.g., (Clark, 1973; Her-skovits, 1986; Levinson, 2003), little attention hasbeen paid to how reference frames are determined interms of the perceptual groups and their elements.In addition to reference frames, differences ofperspective also have to be taken into account toproduce proper referring expressions since humansoften view spatial relations between objects in a3-dimensional space by projecting them on a 2-dimensional plane.
In the experiments, we pre-sented the subjects with 2-dimensional bird?s-eyeimages.
The result might have been different if wehad used 3-dimensional images instead, because theprojection changes the sizes of objects and spatialrelations among them.Integration with conventional methods: In thispaper, we focused on a limited situation where in-herent attributes of objects do not serve any identi-fying function, but this is not the case in general.
Analgorithm integrating conventional attribute-basedmethods and the proposed method should be formu-lated to achieve the end goal.A possible direction would be to enhance the al-gorithm proposed by Krahmer et al (Krahmer etal., 2003).
They formalize an object arrangement(scene) as a labeled directed graph in which ver-tices model objects and edges model attributes andbinary relations, and regard content selection as asubgraph construction problem.
Their algorithmperforms searches directed by a cost function on agraph to find a unique subgraph.If we consider a perceptual group as an ordinaryobject as shown in Figure 4, their algorithm is appli-cable.
It will be able to handle not only intra-grouprelations (e.g., the edges with labels ?front?, ?mid-dle?, and ?back?
in Figure 4) but also inter-group re-lations (e.g., the edge from ?Group 1?
to ?Table?
inFigure 4).
However, introducing perceptual groupsas vertices makes it difficult to design the cost func-tion.
A well-designed cost function is indispensablefor generating concise and comprehensible expres-sions.
Otherwise, an expression like ?a ball in frontof a ball in front of a ball?
for the situation shown inFigure 1 would be generated.Group 1b c aTablefront_of front_of right_ofback_of back_of left_offrontmiddle backright_ofright_ofright_ofFigure 4: A simplified graph with a group vertex forthe situation shown in Figure 1ReferencesDouglas E. Appelt.
1985.
Planning English refer-ring expressions.
Artificial Intelligence, 26:1?33.Mark Cavazza, Fred Charles, and Steven J. Mead.2002.
Character-based interactive stroytelling.IEEE Intelligent Systems, 17(4):17?24.Herbert H. Clark.
1973.
Space, time, semantics,and the child.
In T. E. Moore, editor, Cogni-tive development and the acquisition of language,pages 65?110.
Academic Press.Robert Dale and Nicholas Haddock.
1991.
Gener-ating referring expressions involving relations.
InProceedings of the Fifth Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics(EACL?91), pages 161?166.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the gricean maxims in the gen-eration of referring expressions.
Cognitive Sci-ence, 19(2):233?263.Robert Dale.
1992.
Generating referring expres-sions: Constructing descriptions in a domain ofobjects and processes.
MIT Press, Cambridge.Peter Heeman and Graem Hirst.
1995.
Collabo-rating referring expressions.
Computational Lin-guistics, 21(3):351?382.Annette Herskovits.
1986.
Language and Spa-tial cognition: an interdisciplinary study of theprepositions in English.
Cambridge UniversityPress.Helmut Horacek.
1997.
An algorithm for gener-ating referential descriptions with flexible inter-faces.
In Proceedings of the 35th Annual Meetingof the Association for Computational Linguistics,pages 206?213.Emiel Krahmer and Marie?t Theune.
2002.
Efficientcontext-sensitive generation of descriptions.
InKees van Deemter and Rodger Kibble, editors,Information Sharing: Givenness and Newness inLanguage Processing.
CSLI Publications, Stan-ford, California.Emiel Krahmer, Sebastiaan van Erk, and Andre?Verleg.
2003.
Graph-based generation of re-ferring expressions.
Computational Linguistics,29(1):53?72.Stephen C. Levinson, editor.
2003.
Space in Lan-guage and Cognition.
Cambridge UniversityPress.Hozumi Tanaka, Takenobu Tokunaga, and YusukeShinyama.
2004.
Animated agents capable ofunderstanding natural language and perform-ing actions.
In Helmut Prendinger and MituruIshizuka, editors, Life-Like Characters, pages429?444.
Springer.Kristinn R. Tho?risson.
1994.
Simulated perceptualgrouping: An application to human-computer in-teraction.
In Proceedings of the Sixteenth An-nual Conference of the Cognitive Science Society,pages 876?881.Kees van Deemter.
2002.
Generating referring ex-pressions: Boolean extensions of the incrementalalgorithm.
Computational Linguistics, 28(1):37?52.Ielka van der Sluis and Emiel Krahmer.
2000.Generating referring expressions in a multimodalcontext: An empirically oriented approach.
Pre-sented at the CLIN meeting 2000, Tilburg.
