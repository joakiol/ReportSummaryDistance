Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 138?141,Paris, October 2009. c?2009 Association for Computational LinguisticsImproving generative statistical parsing with semi-supervised wordclusteringMarie Candito and Beno?t Crabb?Universit?
Paris 7/INRIA (Alpage), 30 rue du Ch?teau des Rentiers, 75013 ParisAbstractWe present a semi-supervised method toimprove statistical parsing performance.We focus on the well-known problem oflexical data sparseness and present exper-iments of word clustering prior to pars-ing.
We use a combination of lexicon-aided morphological clustering that pre-serves tagging ambiguity, and unsuper-vised word clustering, trained on a largeunannotated corpus.
We apply these clus-terings to the French Treebank, and wetrain a parser with the PCFG-LA unlex-icalized algorithm of (Petrov et al, 2006).We find a gain in French parsing perfor-mance: from a baseline of F1=86.76% toF1=87.37% using morphological cluster-ing, and up to F1=88.29% using furtherunsupervised clustering.
This is the bestknown score for French probabilistic pars-ing.
These preliminary results are encour-aging for statistically parsing morpholog-ically rich languages, and languages withsmall amount of annotated data.1 IntroductionLexical information is known crucial in naturallanguage parsing.
For probabilistic parsing, onemain drawback of the plain PCFG approach is tolack sensitivity to the lexicon.
The symbols acces-sible to context-free rules are part-of-speech tags,which encode generalizations that are too coarsefor many parsing decisions (for instance subcat-egorization information is generally absent fromtagsets).
The lexicalized models first proposedby Collins reintroduced words at every depth of aparse tree, insuring that attachments receive prob-abilities that take lexical information into account.On the other hand, (Matsuzaki et al, 2005) haveproposed probabilistic CFG learning with latentannotation (hereafter PCFG-LA), as a way to au-tomate symbol splitting in unlexicalized proba-bilistic parsing (cf.
adding latent annotations toa symbol is comparable to splitting this symbol).
(Petrov et al, 2006) rendered the method usable inpractice, with a tractable technique to retain onlythe beneficial splits.We know that both lexicalized parsing algo-rithm and PCFG-LA algorithm suffer from lex-ical data sparseness.
For lexicalized parsers,(Gildea, 2001) shows that bilexical dependenciesparameters are almost useless in the probabilisticscoring of parser because they are too scarce.For PCFG-LA, we have previously studied thelexicon impact on this so-called ?unlexicalized?algorithm, for French parsing (Crabb?
and Can-dito, 2008), (Candito et al, 2009).
We have testeda totally unlexicalized parser, trained on a treebankwhere words are replaced by their POS tags.
It ob-tains a parseval F1=86.28 (note that it induces per-fect tagging).
We compared it to a parser trainedwith word+tag as terminal symbols (to simulate aperfect tagging), achieving F1=87.79.
This provesthat lexical information is indeed used by the ?un-lexicalized?
PCFG-LA algorithm: some lexicalinformation percolates through parse trees via thelatent annotations.We have also reported a slight improvement(F1=88.18) when word forms are clustered on amorphological basis, into lemma+tag clusters.
SoPCFG-LA uses lexical information, but it is toosparse, hence it benefits from word clustering.
Yetthe use of lemma+tag terminals supposes taggingprior to parsing.
We propose here to apply rathera deterministic supervised morphological cluster-ing that preserves tagging ambiguities, leaving itto the parser to disambiguate POS tags.We also investigate the use of unsupervisedword clustering, obtained from unannotated text.It has been proved useful for parsing by (Koo etal., 2008) and their work directly inspired ours.They have shown that parsing improves whencluster information is used as features in a discrim-inative training method that learns dependencyparsers.
We investigate in this paper the use ofsuch clusters in a generative approach to proba-bilistic phrase-structure parsing, simply by replac-ing each token by its cluster.138We present in section 2 the treebank instanti-ation we use for our experiments, the morpho-logical clustering in section 3, and the Brown al-gorithm for unsupervised clustering in section 4.Section 5 presents our experiments, results anddiscussion.
Section 6 discusses related work.
Sec-tion 7 concludes with some ideas for future work.2 French TreebankFor our experiments, we use the French Treebank(hereafter FTB) (Abeill?
et al, 2003), containing12531 sentences of the newspaper Le Monde.
Westarted with the treebank instantiation defined in(Crabb?
and Candito, 2008), where the rich origi-nal annotation containing morphological and func-tional information is mapped to a plain phrase-structure treebank with a tagset of 28 POS tags.In the original treebank, 17% of the tokens be-long to a compound, and compounds range fromvery frozen multi word expressions like y com-pris (literally there included, meaning including)to syntactically regular entities like loi agraire(land law).
In most of the experiments with theFTB, each compound is merged into a single to-ken: (P (CL y) (A compris)) is merged as (Py_compris).
But because our experiments aim atreducing lexical sparseness but also at augmentinglexical coverage using an unannotated corpus, wefound it necessary to make the unannotated cor-pus tokenisation and the FTB tokenisation consis-tent.
To set up a robust parser, we chose to avoidrecognizing compounds that exhibit syntacticallyregular patterns.
We create a new instance of thetreebank (hereafter FTB-UC), where syntacticallyregular patterns are ?undone?
(Figure 1).
This re-duces the number of distinct compounds in thewhole treebank from 6125 to 3053.NPDl?NNUnionA?conomiqueCetAmon?taireNPDl?NUnionAPA?conomiqueCOORDCetAPAmon?taireFigure 1: A NP with a compound (left) changedinto a regular structure with simple words (right)3 Morphological clusteringThe aim of this step is to reduce lexical sparsenesscaused by inflection, without hurting parsability,and without committing ourselves as far as ambi-guity is concerned.
Hence, a morphological clus-tering using lemmas is not possible, since lemmaassignment supposes POS disambiguation.
Fur-ther, information such as mood on verbs is nec-essary to capture for instance that infinitive verbshave no overt subject, that participial clauses aresentence modifiers, etc...
This is encoded in theFTB with different projections for finite verbs(projecting sentences) versus non finite verbs (pro-jecting VPpart or VPinf).We had the intuition that the other inflectionmarks in French (gender and number for determin-ers, adjectives, pronouns and nouns, tense and per-son for verbs) are not crucial to infer the correctphrase-structure projected by a given word1.So to achieve morphological clustering, we de-signed a process of desinflection, namely of re-moving some inflection marks.
It makes use ofthe Lefff, a freely available rich morphological andsyntactic French lexicon (Sagot et al, 2006), con-taining around 116000 lemmas (simple and com-pounds) and 535000 inflected forms.
The desin-flection is as follows: for a token t to desin-flect, if it is known in the lexicon, for all the in-flected lexical entries le of t, try to get corre-sponding singular entries.
If for all the le, cor-responding singular entries exist and all have thesame form, then replace t by the correspondingsingular.
For instance for wt=entr?es (ambigu-ous between entrances and entered, fem, plural),the two lexical entries are [entr?es/N/fem/plu] and[entr?es/V/fem/plu/part/past]2 , each have a corre-sponding singular lexical entry, with form entr?e.Then the same process applies to map feminineforms to corresponding masculine forms.
Thisallows to change mang?e (eaten, fem, sing) intomang?
(eaten, masc, sing).
But for the form en-tr?e, ambiguous between N and Vpastpart entries,only the participle has a corresponding masculineentry (with form entr?).
In that case, in orderto preserve the original ambiguity, entr?e is notreplaced by entr?.
Finite verb forms, when un-ambiguous with other POS, are mapped to sec-ond person plural present indicative correspondingforms.
This choice was made in order to avoid cre-ating ambiguity: the second person plural formsend with a very typical -ez suffix, and the result-ing form is very unlikely ambiguous.
For the first1For instance, French oral comprehension does not seemto need plural marks very much, since a majority of Frenchsingular forms have their corresponding plural form pro-nounced in the same way.2This is just an example and not the real Lefff format.139token of a sentence, if unknown in the lexicon,the algorithm tries to desinflect the low case cor-responding form.This desinflection reduces the number of dis-tinct tokens in the FTB-UC from 27143 to 20268.4 Unsupervised word clusteringWe chose to use the (Brown et al, 1992) hard clus-tering algorithm, which has proven useful for var-ious NLP tasks, such as dependency parsing (Kooet al, 2008) or named entity recognition (Liang,2005).
The algorithm to obtain C clusters is asfollows: each of the C most frequent tokens ofthe corpus is assigned its own distinct cluster.
Forthe (C+1)th most frequent token, create a (C+1)thcluster.
Then for each pair among the C+1 result-ing clusters, merge the pair that minimizes the lossin the likelihood of the corpus, according to a bi-gram language model defined on the clusters.
Re-peat this operation for the (C+2)th most frequenttoken, etc...
This results in a hard clustering intoC clusters.
The process can be continued to fur-ther merge pairs of clusters among the C clusters,ending with a unique cluster for the whole vocab-ulary.
This can be traced to obtain a binary treerepresenting the merges of the C clusters.
A clus-ter can be identified by its path within this binarytree.
Hence, clusters can be used at various levelsof granularity.5 Experiments and discussionFor the Brown clustering algorithm, we used PercyLiang?s code3, run on the L?Est R?publicain cor-pus, a 125 million word journalistic corpus, freelyavailable at CNRTL4.
The corpus was tokenised5 ,segmented into sentences and desinflected usingthe process described in section 3.
We ran the clus-tering into 1000 clusters for the desinflected formsappearing at least 20 times.We tested the use of word clusters for parsingwith the Berkeley algorithm (Petrov et al, 2006).Clustering words in this case has a double advan-tage.
First, it augments the known vocabulary,which is made of all the forms of all the clus-ters appearing in the treebank.
Second, it reducessparseness for the latent annotations learning onthe lexical rules of the PCFG-LA grammar.3http://www.eecs.berkeley.edu/ pliang/software4http://www.cnrtl.fr/corpus/estrepublicain5The 200 most frequent compounds of the FTB-UC weresystematically recognized as one token.We used Petrov?s code, adapted to French by(Crabb?
and Candito, 2008), for the suffixes usedto classify unknown words, and we used the sametraining(80%)/dev(10%)/test(10%) partition.
Weused the FTB-UC treebank to train a baselineparser, and three other parsers by changing the ter-minal symbols used in training data:desinflected forms: as described in section 3clusters + cap: each desinflected form is re-placed by its cluster bit string.
If the desinflectedform has no corresponding cluster (it did not ap-pear 20 times in the unannotated corpus), a spe-cial cluster UNKC is used.
Further, a _C suffix isadded if the form starts with a capital.clusters + cap + suffixes: same as before, ex-cept that 9 additional features are used as suffixesto the cluster: if form is all digits, ends with ant,or r, or ez (cf.
this is how end desinflected formsof unambiguous finite verbs), ...We give in table 1 parsing performance in termsof labeled precision/recall/Fscore, and also themore neutral unlabeled attachment score (UAS)6.The desinflection process does help: benefitsfrom reducing data sparseness exceed the lossof agreement markers.
Yet tagging decreases alittle, and this directly impacts the dependencyscore, because the dependency extraction useshead propagation rules that are sensitive to tag-ging.
In the same way, the use of bare clustersincreases labeled recall/precision, but the taggingaccuracy decreases, and thus the UAS.
This canbe due to the coarseness of the clustering method,which sometimes groups words that have differ-ent POS (for instance among a cluster of infiniteverbs, one may find a present participle).
Thequality of the clusters is more crucial in our casethan when clusters are features, whose informativ-ity is discriminatively learnt.
This observation ledus to append a restricted set of suffixes to the clus-ters, which gives us the best results for now.6 Related workWe already mentioned that we were inspired bythe success of (Koo et al, 2008) in using wordclusters as features for the discriminative learningof dependency parsers.
Another approach to aug-ment the known vocabulary for a generative prob-6In all metrics punctuation tokens are ignored and all re-sults are for sentences of less than 40 words.
Note that weused the FTB-UC treebank.
There are mors tokens in sen-tences than in the FTB with all compounds merged, and base-line F1 scores are a little higher (86.79 versus 86.41).140terminal symbols LP LR F1 UAS Vocab.
size Tagging Acc.inflected forms (baseline) 86.94 86.65 86.79 91.00 27143 96.90desinflected forms 87.42 87.32 87.37 91.14 20268 96.81clusters + cap 88.08 87.50 87.79 91.12 1201 96.37clusters + cap + suffixes 88.43 88.14 88.29 91.68 1987 97.04Table 1: Parsing performance when training and parsing use clustered terminal symbolsabilistic parser is the one pursued in (Goldberg etal., 2009).
Within a plain PCFG, the lexical proba-bilities for words that are rare or absent in the tree-bank are taken from an external lexical probabil-ity distribution, estimated using a lexicon and theBaulm-Welch training of an HMM tagger.
This isproved useful to better parse Hebrew.7 Conclusion and future workWe have tested the very simple method of replac-ing inflected forms by clusters of forms in a gener-ative probabilistic parser.
This crude technique hassurprisingly good results and offers a very cheapand simple way to augment the vocabulary seen attraining time.
It seems interesting to try the tech-nique on other generative approaches such as lex-icalized probabilistic parsing.We plan to optimize the exact shape of termi-nal symbols to use.
Bare unsupervised clusters areunsatisfactory, and we have seen that adding sim-ple suffixes to the clusters improved performance.Learning such suffixes is a path to explore.
Also,the hierarchical organization of the clusters couldbe used, in the generative approach adopted here,by modulating the granularity of the clusters de-pending on their frequency in the treebank.We also need to check to what extent the desin-flection step helps for taking advantage of the verylocal information captured by the Brown cluster-ing.Finally, we could try using other kinds of clus-tering, such as the approach of (Lin, 1998), whichcaptures similarity between syntactic dependen-cies beared by nouns and verbs.8 AcknowledgementsThe authors truly thank Percy Liang and SlavPetrov for providing their code for respec-tively Brown clustering and PCFG-LA.
Thiswork was supported by the French NationalResearch Agency (SEQUOIA project ANR-08-EMER-013).ReferencesAnne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,2003.
Building a Treebank for French.
Kluwer,Dordrecht.Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jen-nifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Compu-tational linguistics, 18(4):467?479.Marie Candito, Benoit Crabb?, and Djam?
Seddah.2009.
On statistical parsing of french with super-vised and semi-supervised strategies.
In EACL 2009Workshop Grammatical inference for Computa-tional Linguistics, Athens, Greece.Benoit Crabb?
and Marie Candito.
2008.
Exp?riencesd?analyse syntaxique statistique du fran?ais.
InActes de la 15?me Conf?rence sur le Traitement Au-tomatique des Langues Naturelles (TALN?08), pages45?54, Avignon, France.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proc.
of EMNLP?01, pages 167?202,Pittsburgh, USA.Yoav Goldberg, Reut Tsarfaty, Meni Adler, andMichael Elhadad.
2009.
Enhancing unlexicalizedparsing performance using a wide coverage lexicon,fuzzy tag-set mapping, and EM-HMM-based lexicalprobabilities.
In Proc.
of EACL-09, pages 327?335,Athens, Greece.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proc.
of ACL-08, Columbus, USA.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
In MIT Master?s thesis, Cambridge,USA.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proc.
of ACL-98, pages 768?774, Montreal, Canada.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic cfg with latent annotations.
InProc.
of ACL-05, pages 75?82, Ann Arbor, USA.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of ACL-06, Syd-ney, Australia.Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de LaClergerie, and Pierre Boullier.
2006.
The Lefff 2syntactic lexicon for french: architecture, acquisi-tion, use.
In Proc.
of LREC?06, Genova, Italy.141
