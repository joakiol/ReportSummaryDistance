Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 702?706,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAnalyzing Stemming Approaches for Turkish Multi-DocumentSummarizationMuhammed Yavuz Nuzumlal?
Arzucan?Ozg?urDepartment of Computer EngineeringBo?gazic?i UniversityTR-34342, Bebek,?Istanbul, Turkey{yavuz.nuzumlali,arzucan.ozgur}@boun.edu.trAbstractIn this study, we analyzed the effects of ap-plying different levels of stemming approachessuch as fixed-length word truncation and mor-phological analysis for multi-document sum-marization (MDS) on Turkish, which is an ag-glutinative and morphologically rich language.We constructed a manually annotated MDSdata set, and to our best knowledge, reportedthe first results on Turkish MDS.
Our resultsshow that a simple fixed-length word trun-cation approach performs slightly better thanno stemming, whereas applying complex mor-phological analysis does not improve TurkishMDS.1 IntroductionAutomatic text summarization has gained more impor-tance with the enormous growth and easy availability ofthe Internet.
It is now possible to reach extensive andcontinuously growing amount of resources.
However,this situation brings its own challenges such as findingthe relevant documents, and absorbing a large quan-tity of relevant information (Gupta and Lehal, 2010).The goal of multi-document summarization (MDS) isto automatically create a summary of a set of docu-ments about the same topic without losing the impor-tant information.
Several approaches for MDS havebeen proposed in the last decade.
However, most ofthem have only been applied to a relatively small setof languages, mostly English, and recently also to lan-guages like Chinese, Romanian, Arabic, and Spanish(Giannakopoulos, 2013).Previous studies have shown that methods proposedfor languages like English do not generally work wellfor morphologically rich languages like Finnish, Turk-ish, and Czech, and additional methods considering themorphological structures of these languages are needed(Eryi?git et al., 2008).
For instance, Turkish is an ag-glutinative language where root words can take manyderivational and inflectional affixes.
This feature re-sults in a very high number of different word surfaceforms, and eventually leads to the data sparseness prob-lem.
Hakkani-T?ur et al.
(2000) analyzed the number ofunique terms for Turkish and English and showed thatthe term count for Turkish is three times more than En-glish for a corpus of 1M words.There are only a few studies for text summariza-tion on Turkish, all of which are about single-documentsummarization (Altan, 2004; C??
?g?r et al., 2009;?Ozsoyet al., 2010; G?uran et al., 2010; G?uran et al., 2011).Some of these studies applied morphological analysismethods, but none of them analyzed their effects in de-tail.To our best knowledge, this paper reports the firstmulti-document summarization study for Turkish.
Weused LexRank as the main summarization algorithm(Erkan and Radev, 2004), applied and analyzed differ-ent levels of stemming methods such as complex mor-phological analysis and fixed-length word truncation.We also created the first manually annotated MDS dataset for Turkish, which has been made publicly availablefor future studies.The rest of the paper is organized as follows.
Sec-tion 2 presents the related work on MDS, as well ason the applications of morphological analysis on Turk-ish for different Natural Language Processing (NLP)and Information Retrieval (IR) problems.
In Section3, we provide a very brief introduction to the Turkishmorphology and present the stemming methods that weevaluated.
The details about the created data set andour experimental setup are presented in Section 4.
Wepresent and discuss the results in Section 5, and con-clude in Section 6.2 Related WorkA large number of methods have been proposed formulti-document summarization in the last 10-15 years(e.g.
(Erkan and Radev, 2004; Shen and Li, 2010;Christensen et al., 2013)).
While most of these ap-proaches have only been applied to English, summa-rization data sets and systems for other languages likeChinese, Romanian, and Arabic have also been pro-posed in the recent years (Giannakopoulos, 2013).Previous studies on automatic summarization forTurkish only tackled the problem of single-documentsummarization (SDS).
Altan (2004) and C??
?g?r et al.
(2009) proposed feature-based approaches for Turk-ish SDS, whereas?Ozsoy et al.
(2010) and G?uran etal.
(2010) used Latent Semantic Analysis (LSA) basedmethods.
G?uran et al.
(2011) applied non-negative ma-702Word Analysisg?oren (the one who sees) g?or+en(DB)g?or?ulen (the one which is seen) g?or+?ul(DB)+en(DB)g?or?us?
(opinion) g?or+?us?(DB)g?or?us?
?un (your opinion) g?or+?us?
(DB)+?ung?or?us?ler (opinions) g?or+?us?
(DB)+lerg?or?us?me (negotiation) g?or+?us?
(DB)+me(DB)g?or?us?melerin (of negotiations) g?or+?us?
(DB)+me(DB)+ler+inTable 1: Different word forms and their morphological analysis for the stem ?g?or?
(to see).
The derivationalboundaries are marked with (DB).trix factorization (NMF) and used consecutive wordsdetection as a preprocessing step.The effect of morphological analysis for Turkishwas analyzed in detail for Information Retrieval (Canet al., 2008) and Text Categorization (Akkus?
andC?ak?c?, 2013).
Can et al.
(2008) showed that us-ing fixed-length truncation methods perform similarlyto lemmatization-based stemming for information re-trieval.
Akkus?
and C?ak?c?
(2013) obtained better resultsfor text categorization with fixed-length word trunca-tion rather than complex morphological analysis, butthe difference was not significant.
For other morpho-logically rich languages, there is a case study on Greekby Galiotou et al.
(2013).
They applied different stem-ming algorithms and showed that stemming on Greektexts improves the summarization performance.3 MethodologyThis section contains detailed information about the ap-plication of different levels of morphological featuresduring the summarization process.
Before diving intothe details, we provide a very brief description of themorphological structure of the Turkish language.3.1 Turkish MorphologyTurkish is an agglutinative language with a productivemorphology.
Root words can take one or more deriva-tional and inflectional affixes; therefore, a root can beseen in a large number of different word forms.
An-other issue is the morphological ambiguity, where aword can have more than one morphological parse.Table 1 shows an example list of different wordforms for the stem ?g?or?
(to see).
All the words in thetable have the same root, but the different suffixes leadto different surface forms which may have similar ordifferent meanings.
When the surface forms of thesewords are used in a summarization system, they will beregarded as totally different words.
However, if a mor-phological analysis method is applied to the sentencesbefore giving them to the summarization system, wordswith similar meanings can match during the sentencesimilarity calculations.3.2 Stemming PoliciesIn this section, we explain the different stemming meth-ods that we investigated.Raw: In this method, we take the surface forms ofwords, without applying any stemming.Root: This method takes the most simple unit of aword, namely the root form.
For example, in Table 1,the words ?g?oren?, ?g?or?us?
?un?, and ?g?or?us?melerin?have the same root (g?or), so they will match during sen-tence similarity calculations.Deriv: Using the Root method may oversimplifywords because some words that are derived from thesame root may have irrelevant meanings.
In the aboveexample, ?g?or?us?ler?
and ?g?oren?
have different mean-ings, but they have the same root (g?or).
In order to solvethis oversimplification issue, we propose to preservederivational affixes, and only remove the inflectionalaffixes from the words.
In this method, ?g?or?us?ler?and ?g?oren?
will not match because when we removeonly the inflectional affixes, they become ?g?or?us??
and?g?oren?.
On the other hand, the words ?g?or?us?ler?
and?g?or?us??un?
will match because their Deriv forms are thesame, which is ?g?or?us?
?.Prefix: In Turkish, affixes almost always occur assuffixes, not prefixes.
Additionally, applying morpho-logical analysis methods is a time consuming process,and may become an overhead for online applications.Therefore, a fixed-length simplification method is alsotried, since it is both a fast method and can help matchsimilar words by taking the first n characters of wordswhich have lengths larger than n.As the summarization algorithm, we used LexRank(Erkan and Radev, 2004), which is a salient graph-based method that achieves promising results for MDS.In LexRank, first a sentence connectivity graph is con-structed based on the cosine similarities between sen-tences, and then the PageRank (Page et al., 1999) algo-rithm is used to find the most important sentences.4 Experimental Setup4.1 Data SetOne of the greatest challenges for MDS studies in Turk-ish is that there does not exist a manually annotateddata set.
In this study, we have collected and manually703annotated a Turkish MDS data set, which is publiclyavailable for future studies1.In order to match the standards for MDS data sets,we tried to follow the specifications of the DUC 2004data set.
Our data set consists of 21 clusters, each con-sisting of around 10 documents.
We selected 21 differ-ent topics from different domains (e.g., politics, eco-nomics, sports, social, daily, and technology), and se-lected 10 documents on average for each topic.
Thedocuments were obtained from the websites of variousnews sources.
The average number of words per doc-ument is 337, and the average number of letters in aword is 6.84.For manual annotation, we divided the 21 clustersinto three groups and sent them to three annotators dif-ferent from the authors.
We required the human sum-maries not to exceed 120 words for the summary ofeach cluster.4.2 Tools4.2.1 Turkish Morphological AnalysisIn order to perform different levels of morphologicalanalysis on documents, we used a two-level morpho-logical analyzer (Oflazer, 1994) and a perceptron-basedmorphological disambiguator (Sak et al., 2007), whichis trained with a corpus of about 750, 000 tokens fromnews articles.
The accuracy of the disambiguator hasbeen reported as 96% (Sak et al., 2007).
The Root andDeriv forms of words were generated from the disam-biguator output.4.2.2 MEAD Summarization ToolkitWe used MEAD (Radev et al., 2004), which is an open-source toolkit created for extractive MDS, in our exper-iments.
MEAD handles all the necessary processes togenerate a summary document (e.g., sentence ranking,selection, re-ordering, and etc.
).We used the LexRank implementation that comeswith MEAD as a feature, together with the Cen-troid and Position features (each feature is equallyweighted).
We forced the generated summaries not toexceed 120 words.
However, we define the followingexception in order to preserve the readability and thegrammaticality of the generated summary.
For a can-didate sentence S having n words, if the absolute dif-ference between the threshold (which is 120) and thesummary length including sentence S (say Nw) is lessthan the absolute difference between the threshold andthe summary length excluding sentence S (say Nwo),and if Nwis less than 132 (which is 120?1.1), we allowthe summary to exceed the threshold and add sentenceS as the last summary sentence.We used term frequency (tf) based cosine similarityas the similarity measure during the sentence selectionstep.
We also required sentence length to be between1The data set can be retrieved from the following githubrepository: https://github.com/manuyavuz/TurkishMDSDataSet_alpha6 and 50 words (which we found empirically) in or-der to increase the readability of the summaries.
Thereason behind applying this filtering is that very shortsentences generally do not contain much informationto become a summary sentence, whereas very long sen-tences decrease the readability and fill a significant per-centage of the summary limit.4.2.3 ROUGEFor evaluation, we used ROUGE, which is a standardmetric for automated evaluation of summaries basedon n-gram co-occurrence.
We used ROUGE-1 (basedon uni-grams), ROUGE-2 (based on bi-grams), andROUGE-W (based on longest common sub-sequenceweighted by length) in our experiments.
Among these,ROUGE-1 has been shown to agree with human judgesthe most (Lin and Hovy, 2003), so we give importanceto it while interpreting the results.5 Evaluation and ResultsWe ran MEAD with the proposed stemming policiesusing different levels of cosine similarity threshold val-ues to analyze the effect of the similarity threshold onthe summarization performance.
After the sentencesare ranked using the LexRank method, the similaritythreshold is used to decide whether to include a sen-tence to the summary.
A sentence is not included to thesummary, if its similarity to a previously picked sen-tence is larger than the similarity threshold.In our preliminary experiments, we used the defaultsimilarity threshold 0.7, which was found empiricallyby the MEAD developers for English.
However, it pro-duced poor results on the Turkish data set.Policy ROUGE-1 ROUGE-2 ROUGE-WPrefix10 0.438 0.194 0.197Prefix12 0.433 0.197 0.195Prefix9 0.432 0.194 0.194Prefix4 0.432 0.178 0.190Prefix7 0.431 0.189 0.190Prefix5 0.431 0.183 0.190Prefix6 0.430 0.185 0.189Raw 0.428 0.189 0.191Deriv 0.428 0.178 0.188Prefix8 0.427 0.187 0.188Prefix11 0.427 0.190 0.193Root 0.420 0.186 0.185Table 2: Best scores for different policiesFigure 1 shows the F-1 scores for the ROUGE-1metric for policies with different thresholds.
After thethreshold exceeds 0.5, the performances for all poli-cies start to decrease, so we don?t report the valueshere to make the chart more readable.
In general, Rawand Prefix10 (taking the first 10 letters of the words)achieve better performances with lower threshold val-ues, whereas Root and Deriv operate better with rel-atively higher threshold values.
As we stated earlier,in Turkish, words with similar meanings can occur in7040.32?0.34?0.36?0.38?0.4?0.42?0.44?0.46?0.15?
0.2?
0.25?
0.3?
0.35?
0.4?
0.45?
0.5?ROUGE-??1?F-??1?Scores?Similarity?Thresholds?Raw?
Deriv?
Root?
Prefix?10?Figure 1: F-1 scores for different similarity threshold valuestext with different surface forms due to their inflec-tions.
Such words can not be matched during similar-ity computation if morphological analysis is not per-formed.
Therefore, using higher similarity thresholdvalues cause very similar sentences to occur togetherin the summaries, and eventually, result in poor scores.Table 2 shows the best scores obtained by each pol-icy.
The Prefix policy generally outperforms the Rawpolicy.
The Prefix10 policy achieves the best ROUGE-1 score.
On the other hand, the policies that apply com-plex morphological analysis (i.e.
Root and Deriv) arenot able to outperform the simple Prefix and Raw poli-cies.
The Deriv policy performs similarly to the Rawand Prefix policies, whereas the Root policy obtains thelowest ROUGE-1 score.5.1 DiscussionThe results show that using a simple fixed-length pre-fix policy outperforms all other methods, and apply-ing complex morphological analysis does not improveTurkish MDS.
The poor performance of the Root pol-icy is somewhat expected due to the fact that, if we pre-serve only the roots of the words, we lose the semanticdifferences among the surface forms provided by thederivational affixes.
On the other hand, the reason be-hind the observation that Deriv and Raw obtain similarperformances is not obvious.In order to further analyze this observation, weused an entropy based measure, which is calculated asshown below, to quantify the homogeneity of the clus-ters in the data set in terms of the variety of the surfaceforms corresponding to the Deriv forms of each wordin the cluster.
We first compute the entropy for eachDeriv form in a cluster.
The entropy of a Deriv formis lower, if it occurs with fewer different surface formsin the cluster.
The entropy of a cluster is computed bysumming the entropies of the Deriv forms in the clus-ter and dividing the sum by the number of words in thecluster (i.e.
N).DDerivi= {t | t inflected fromDeriv i}H(Derivi) =?t?DDerivip(t) log p(t)H(C) =?iH(Derivi)NTo compare with the data set clusters, we generatedrandom document clusters by randomly selecting 10different clusters and then randomly selecting one doc-ument from each selected cluster.
The average entropyvalue for the data set clusters and the random clusterswere 4.99 and 7.58, respectively.
Due to this signifi-cant difference, we can hypothesize that the documentsabout the same topic show a more homogeneous struc-ture.
In other words, a Deriv form is usually seen in thesame surface form in a cluster of documents which areabout the same topic.
Therefore, the Deriv policy andthe Raw policy achieve similar results for summarizingdocuments about the same topic.During evaluation, we ran ROUGE with the Derivversions of the human summaries and the system sum-maries in order to match semantically similar wordshaving different surface forms.
We also experimentedwith ROUGE using the Raw versions, but the resultsfollowed very similar patterns, so those results were notreported.6 ConclusionIn this paper, we reported the first steps for a multi-document summarization system for Turkish.
A manu-ally annotated data set has been constructed from news705articles, and made publicly available for future stud-ies.
We utilized the LexRank summarization algorithm,and analyzed the effects of different stemming poli-cies for Turkish MDS.
Our results show that simplefixed-length truncation methods with high limits (suchas taking the first 10 letters) improves summarizationscores.
In contrast to our expectation, using morpho-logical analysis does not enhance Turkish MDS, possi-bly due to the homogeneousness of the documents in acluster to be summarized.
As future work, we plan toextend the data set with more clusters and more refer-ence summaries, as well as to develop sentence com-pression methods for Turkish MDS.AcknowledgmentsWe would like to thank Ferhat Ayd?n for his contri-butions during the data set corpus collection and an-notation process.
We would also like to thank BurakSivrikaya and Serkan Bugur for their help in generat-ing the human summaries for the data set.ReferencesBurak Kerim Akkus?
and Ruket C?ak?c?.
2013.
Catego-rization of turkish news documents with morpholog-ical analysis.
In ACL (Student Research Workshop),pages 1?8.
The Association for Computer Linguis-tics.Zeynep Altan.
2004.
A turkish automatic text sum-marization system.
In Proceedings of the IASTEDInternational Conference Artificial Intelligence andApplications, pages 74?83.Fazl?
Can, Seyit Koc?berber, Erman Balc?
?k, Cihan Kay-nak, H Ca?gdas?
?Ocalan, and Onur M Vursavas?.
2008.Information retrieval on turkish texts.
Journal of theAmerican Society for Information Science and Tech-nology, 59(3):407?421.Janara Christensen, Stephen Soderland Mausam, andOren Etzioni.
2013.
Towards coherent multi-document summarization.
In Proceedings ofNAACL-HLT, pages 1163?1173.G?unes?
Erkan and Dragomir R. Radev.
2004.
Lex-pagerank: Prestige in multi-document text summa-rization.
In EMNLP, pages 365?371.
ACL.G?uls?en Eryi?git, Joakim Nivre, and Kemal Oflazer.2008.
Dependency parsing of turkish.
Computa-tional Linguistics, 34(3):357?389.Eleni Galiotou, Nikitas Karanikolas, and Christodou-los Tsoulloftas.
2013.
On the effect of stemming al-gorithms on extractive summarization: a case study.In Panayiotis H. Ketikidis, Konstantinos G. Margari-tis, Ioannis P. Vlahavas, Alexander Chatzigeorgiou,George Eleftherakis, and Ioannis Stamelos, editors,Panhellenic Conference on Informatics, pages 300?304.
ACM.George Giannakopoulos.
2013.
Multi-document mul-tilingual summarization and evaluation tracks in acl2013 multiling workshop.
MultiLing 2013, page 20.Vishal Gupta and Gurpreet Singh Lehal.
2010.
Asurvey of text summarization extractive techniques.Journal of Emerging Technologies in Web Intelli-gence, 2(3).Aysun G?uran, Eren Bekar, and S Akyokus?.
2010.A comparison of feature and semantic-based sum-marization algorithms for turkish.
In InternationalSymposium on Innovations in Intelligent Systemsand Applications.
Citeseer.A G?uran, NG Bayaz?t, and E Bekar.
2011.
Au-tomatic summarization of turkish documents usingnon-negative matrix factorization.
In Innovations inIntelligent Systems and Applications (INISTA), 2011International Symposium on, pages 480?484.
IEEE.Dilek Z. Hakkani-T?ur, Kemal Oflazer, and G?okhan T?ur.2000.
Statistical morphological disambiguation foragglutinative languages.
In COLING, pages 285?291.
Morgan Kaufmann.Chin-Yew Lin and Eduard H. Hovy.
2003.
Au-tomatic evaluation of summaries using n-gram co-occurrence statistics.
In HLT-NAACL.Kemal Oflazer.
1994.
Two-level description of turk-ish morphology.
Literary and linguistic computing,9(2):137?148.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The pagerank citation rank-ing: Bringing order to the web.
Stanford InfoLab.Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda Celebi, Stanko Dim-itrov, Elliott Drabek, Ali Hakim, Wai Lam, DanyuLiu, et al.
2004.
Mead-a platform for multidocu-ment multilingual text summarization.
Proceedingsof the 4th International Conference on Language Re-sources and Evaluation (LREC 2004).Has?im Sak, Tunga G?ung?or, and Murat Sarac?lar.
2007.Morphological disambiguation of turkish text withperceptron algorithm.
In Alexander F. Gelbukh, edi-tor, CICLing, volume 4394 of Lecture Notes in Com-puter Science, pages 107?118.
Springer.Chao Shen and Tao Li.
2010.
Multi-document sum-marization via the minimum dominating set.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics, pages 984?992.
Associ-ation for Computational Linguistics.Celal C??
?g?r, M?ucahid Kutlu, and?Ilyas C?ic?ekli.
2009.Generic text summarization for turkish.
In ISCIS,pages 224?229.
IEEE.Makbule G?ulc?in?Ozsoy,?Ilyas C?ic?ekli, and Ferda NurAlpaslan.
2010.
Text summarization of turkish textsusing latent semantic analysis.
In Chu-Ren Huangand Dan Jurafsky, editors, COLING, pages 869?876.Tsinghua University Press.706
