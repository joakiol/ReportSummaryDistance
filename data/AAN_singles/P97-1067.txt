Choosing the Word Most Typical in ContextUsing a Lexical Co-occurrence NetworkPhilip EdmondsDepar tment  o f  Computer  Sc ience,  Un ivers i ty  of  TorontoToronto ,  Canada,  M5S 3G4pedmonds?cs, toronto, eduAbstractThis paper presents apartial solution to a com-ponent of the problem of lexical choice: choos-ing the synonym most typical, or expected, incontext.
We apply a new statistical approachto representing the context of a word throughlexical co-occurrence networks.
The imple-mentation was trained and evaluated on a largecorpus, and results show that the inclusion ofsecond-order co-occurrence r lations improvesthe performance of our implemented lexicalchoice program.1 IntroductionRecent work views lexical choice as the process of map-ping fi'om a set of concepts (in some representation ofknowledge) to a word or phrase (Elhadad, 1992; Stede,1996).
When the same concept admits more than onelexicalization, it is often difficult to choose which ofthese 'synonyms' is the most appropriate for achievingthe desired pragmatic goals: but this is necessary for high-quality machine translation and natural language genera-tion.Knowledge-based approaches to representing the po-tentially subtle differences between synonyms have suf-fered from a serious lexical acquisition bottleneck (Di-Marco, Hirst, and Stede, 1993; Hirst, 1995).
Statisticalapproaches, which have sought to explicitly representdifferences between pairs of synonyms with respect otheir occurrence with other specific words (Church et al,1994), are inefficient in time and space.This paper presents a new statistical approach to mod-eling context hat provides a preliminary solution to animportant sub-problem, that of determining the near-synonym that is most typical, or expected, if any, in agiven context.
Although weaker than full lexical choice,because it doesn't choose the 'best' word, we believe thatit is a necessary first step, because it would allow oneto determine the effects of choosing a non-typical wordin place of the typical word.
The approach relies on ageneralization of lexical co-occurrence that allows for animplicit representation f the differences between two (ormore) words with respect o any actual context.For example, our implemented lexical choice programselects mistake as most typical for the 'gap' in sen-tence (1), and error in (2).
(1) However, such a move also would run the riskof cutting deeply into U.S. economic growth,which is why some economists think it would bea big {error I mistake \[ oversight}.
(2) The {error I mistake t oversight} was magnifiedwhen the Army failed to charge the standardpercentage rate for packing and handling.2 Genera l i z ing  Lex ica l  Co-occurrence2.1 Evidence-based Models of ContextEvidence-based models represent context as a set of fea-tures, say words, that are observed to co-occur with, andthereby predict, a word (Yarowsky, 1992; Golding andSchabes, 1996; Karow and Edelman, 1996; Ng and Lee,1996).
But, if we use just the context surrounding a word,we might not be able to build up a representation satisfac-tory to uncover the subtle differences between synonyms,because of the massive volume of text that would be re-quired.Now, observe that even though a word might not co-occur significantly with another given word, it might nev-ertheless predict he use of that word if the two words aremutually related to a third word.
That is, we can treatlexical co-occurrence as though it were moderately tran-sitive.
For example, in (3), learn provides evidence fortask because it co-occurs (in other contexts) with difficult,which in turn co-occurs with task (in other contexts), eventhough learn is not seen to co-occur significantly withtask.
(3) The team's most urgent ask was to learn whetherChernobyl would suggest any safety flaws atKWU-designed plants.So, by augmenting the contextual representation of aword with such second-order (and higher) co-occurrencerelations, we stand to have greater predictive power, as-suming that we assign less weight to them in accordancewith their lower information content.
And as our resultswill show, this generalization of co-occurrence is neces-sary.507I ~"~yms I ~aw~SSS I I r~1.36Figure 1: A fragment of the lexical co-occurrence net-work for task.
The dashed line is a second-order relationimplied by the network.We can represent hese relations in a lexical co-occurrence network, as in figure 1, that connects lexi-cal items by just their first-order co-occurrence r lations.Second-order and higher relations are then implied bytransitivity.2.2 Building Co-occurrence NetworksWe build a lexical co-occurrence network as follows:Given a root word, connect it to all the words that sig-nificantly co-occur with it in the training corpus; 1 then,recursively connect hese words to their significant co-occurring words up to some specified epth.We use the intersection of two well-known measuresof significance, mutual information scores and t-scores(Church et al, 1994), to determine if a (first-order) co-occurrence relation should be included in the network;however, we use just the t-scores in computing signifi-cance scores for all the relations.
Given two words, w0and wd, in a co-occurrence relation of order d, and ashortest path P(wo, wd) = (wo .
.
.
.
.
wd) between them, thesignificance score is1 t(Wi-1, wi)sig(wo, wa) = -~ E iwiEP(w| ,wd)This formula ensures that significance is inversely pro-portional to the order of the relation.
For example, in thenetwork of figure 1, sig(task, learn) = It(task, difficult) +?t(difficult, learn)\]18 = 0.41.A single network can be quite large.
For instance,the complete network for task (see figure 1) up to thethird-order has 8998 nodes and 37,548 edges.2.3 Choosing the Most Typical WordThe amount of evidence that a given sentence provides forchoosing a candidate word is the sum of the significancescores of each co-occurrence of the candidate with a wordIOur training corpus was the part-of-speech-tagged 1989Wall Street Journal, which consists of N = 2, 709,659 tokens.No lemrnatization r sense disambiguation was done.
Stopwords were numbers, symbols, proper nouns, and any tokenwith a raw frequency greater than F = 800.Set POS Synonyms (with training corpus frequency)1 JJ difficult (352), hard (348), tough (230)2 I~ error (64), mistake (61), oversight (37)3 ~n~ job (418), task (123), duty (48)4 NN responsibility (142), commitment (122),obligation (96), burden (81)5 r~N material (177), stuff (79), substance (45)6 VB give (624), provide (501), offer (302)7 VB settle (126), resolve (79)Table 1: The sets of synonyms for our experiment.in the sentence.
So, given a gap in a sentence S, we findthe candidate c for the gap that maximizesM(c, S) = ~_, sig(c, w)wESFor example, given S as sentence (3), above, and thenetwork of figure 1, M(task, S) = 4.40.
However, job(using its own network) matches best with a score of5.52; duty places third with a score of 2.21.3 Results and EvaluationTo evaluate the lexical choice program, we selected sev-eral sets of near-synonyms, shown in table 1, that havelow polysemy in the corpus, and that occur with similarfrequencies.
This is to reduce the confounding effects oflexical ambiguity.For each set, we collected all sentences from the yet-unseen 1987 Wall Street Journal (part-of-speech-tagged)that contained any of the members of the set, ignoringword sense.
We replaced each occurrence by a 'gap' thatthe program then had to fill.
We compared the 'correct-ness' of the choices made by our program to the baselineof always choosing the most frequent synonym accordingto the training corpus.But what are the 'correct' responses?
Ideally, theyshould be chosen by a credible human informant.
Butregrettably, we are not in a position to undertake a studyof how humans judge typical usage, so we will turn in-stead to a less ideal source: the authors of the Wall StreetJournal.
The problem is, of course, that authors aren'talways typical.
A particular word might occur in a 'pat-tern' in which another synonym was seen more often,making it the typical choice.
Thus, we cannot expectperfect accuracy in this evaluation.Table 2 shows the results for all seven sets of synonymsunder different versions of the program.
We varied twoparameters: (1) the window size used during the construc-tion of the network: either narrow (4-4 words), medium(4- 10 words), or wide (4- 50 words); (2) the maximumorder of co-occurrence r lation allowed: 1, 2, or 3.The results show that at least second-order co-occurrences are necessary to achieve better than baselineaccuracy in this task; regular co-occurrence r lations areinsufficient.
This justifies our assumption that we need508Set 1 2 3 4 5 6 7Size 6665 1030 5402 3138 1828 10204 1568Baseline 40.1% 33.5% 74.2% 36.6% 62.8% 45.7% 62.2%1 31.3% 18.7% 34.5% 27.7% 28.8% 33.2% 41.3%Narrow 2 47.2% 44.5% 66.2% 43.9% 61.9% a 48.1% 62.8% a3 47.9% 48.9% 68.9% 44.3% 64.6% a 48.6% 65.9%1 24.0% 25.0% 26.4% 29.3% 28.8% 20.6% 44.2%Medium2 42.5% 47.1% 55.3% 45.3% 61.5% a 44.3% 63.6% a3 42.5% 47.0% 53.6% .
.
.
.Wide 1 9.2% 20.6% 17.5% 20.7% 21.2% 4.1% 26.5%2 39.9% a 46.2% 47.1% 43.2% 52.7% 37.7% 58.6%=Difference from baseline not significant.Table 2: Accuracy of several different versions of the iexical choice program.
The best score for each set is in boldface.Size refers to the size of the sample collection.
All differences from baseline are significant at the 5% level accordingto Pearson's X2 test, unless indicated.more than the surrounding context to build adequate con-textual representations.Also, the narrow window gives consistently higher ac-curacy than the other sizes.
This can be explained, per-haps, by the fact that differences between ear-synonymsoften involve differences in short-distance ollocationswith neighboring words, e.g., face the task.There are two reasons why the approach doesn't doas well as an automatic approach ought to.
First, asmentioned above, our method of evaluation is not ideal;it may make our results just seem poor.
Perhaps ourresults actually show the level of 'typical usage' in thenewspaper.Second, lexical ambiguity is a major problem, affectingboth evaluation and the construction of the co-occurrencenetwork.
For example, in sentence (3), above, it turns outthat the program uses safety as evidence for choosing job(because job safety is a frequent collocation), but this isthe wrong sense of job.
Syntactic and collocational redherrings can add noise too.4 ConclusionWe introduced the problem of choosing the most typicalsynonym in context, and gave a solution that relies on ageneralization flexical co-occurrence.
The results howthat a narrow window of training context (-t-4 words)works best for this task, and that at least second-orderco-occurrence r lations are necessary.
We are planningto extend the model to account for more structure in thenarrow window of context.AcknowledgementsFor comments and advice, I thank Graeme Hirst, EduardHovy, and Stephen Green.
This work is financially sup-ported by the Natural Sciences and Engineering Councilof Canada.ReferencesChurch, Kenneth Ward, William Gale, Patrick Hanks, DonaldHindle, and Rosamund Moon.
1994.
Lexical substitutability.In B.T.S.
Atkins and A. ZampoUi, editors, ComputationalApproaches to the Lexicon.
Oxford University Press, pages153-177.DiMarco, Chrysanne, Graeme Hirst, and Manfred Stede.
1993.The semantic and stylistic differentiation f synonyms andnear-synonyms.
In AAAI Spring Symposium on BuildingLexicons for Machine Translation, pages 114--121, Stanford,CA, March.Elhadad, Michael.
1992.
Using Argumentation to ControlLexical Choice: A Functional Unification Implementation.Ph.D.
thesis, Columbia University.Golding, Andrew R. and Yves Schabes.
1996.
Combin-ing trigram-based and feature-based methods for context-sensitive spelling correction.
In Proceedings of the 34thAnnual Meeting of the Association for Computational Lin-guistics.Hirst, Graeme.
1995.
Near-synonymy and the structure oflexical knowledge.
In AAAI Symposium on Representationand Acquisition of Lexical Knowledge: Polysemy, Ambiguity,and Generativity, pages 51-56, Stanford, CA, March.Karow, Yael and Shimon Edelman.
1996.
Learning similarity-based word sense disambiguation from sparse data.
In Pro-ceedings of the Fourth Workshop on Very Large Corpora,Copenhagen, August.Ng, Hwee Tou and Hian Beng Lee.
1996.
Integrating multiplesources to disambiguate word sense: An exemplar-basedapproach.
In Proceedings of the 34th Annual Meeting of theAssociation for Computational Linguistics.Stede, Manfred.
1996.
Lexical Semantics and Knowledge Rep-resentation i  Multilingual Sentence Generation.
Ph.D. the-sis, University of Toronto.Yarowsky, David.
1992.
Word-sense disambiguation usingstatistical models of Roget's categories trained on large cor-pora.
In Proceedings ofthe14th lnternational Conference onComputational Linguistics (COLING-92), pages 4~ a. a.50.509
