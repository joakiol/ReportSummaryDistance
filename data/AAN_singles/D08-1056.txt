Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 533?542,Honolulu, October 2008. c?2008 Association for Computational LinguisticsOnline Word Games for Semantic Data CollectionDavid Vickrey Aaron Bronzan William Choi Aman KumarJason Turner-Maier Arthur Wang Daphne KollerStanford UniversityStanford, CA 94305-9010{dvickrey,abronzan,aman,arthurex,koller}@cs.stanford.edu{wchoi25,jasonptm}@stanford.eduAbstractObtaining labeled data is a significant obstaclefor many NLP tasks.
Recently, online gameshave been proposed as a new way of obtain-ing labeled data; games attract users by be-ing fun to play.
In this paper, we consider theapplication of this idea to collecting seman-tic relations between words, such as hyper-nym/hyponym relationships.
We built threeonline games, inspired by the real-life gamesof ScattergoriesTM and TabooTM.
As of June2008, players have entered nearly 800,000data instances, in two categories.
The firsttype of data consists of category/answer pairs(?Types of vehicle?,?car?
), while the secondis essentially free association data (?subma-rine?,?underwater?).
We analyze both typesof data in detail and discuss potential uses ofthe data.
We show that we can extract fromour data set a significant number of new hy-pernym/hyponym pairs not already found inWordNet.1 IntroductionOne of the main difficulties in natural language pro-cessing is the lack of labeled data.
Typically, obtain-ing labeled data requires hiring human annotators.Recently, building online games has been suggestedan alternative to hiring annotators.
For example, vonAhn and Dabbish (2004) built the ESP Game1, anonline game in which players tag images with wordsthat describe them.
It is well known that there arelarge numbers of web users who will play onlinegames.
If a game is fun, there is a good chance thatsufficiently many online users will play.We have several objectives in this paper.
Thefirst is to discuss design decisions in building wordgames for collecting data, and the effects of thesedecisions.
The second is to describe the word games1www.gwap.com/gwap/gamesPreview/espgamethat we implemented and the kinds of data they aredesigned to collect.
As of June 2008, our gameshave been online for nearly a year, and have col-lected nearly 800,000 data instances.
The third goalis to analyze the resulting data and demonstrate thatthe data collected from our games is potentially use-ful in linguistic applications.
As an example appli-cation, we show that the data we have collected canbe used to augment WordNet (Fellbaum, 1998) witha significant number of new hypernyms.2 General Design GuidelinesOur primary goal is to produce a large amount ofclean, useful data.
Each of these three objectives(?large?, ?clean?, and ?useful?)
has important im-plications for the design of our games.First, in order to collect large amounts of data,the game must be attractive to users.
If the gameis not fun, people will not play it.
This requirementis perhaps the most significant factor to take into ac-count when designing a game.
For one thing, it tendsto discourage extremely complicated labeling tasks,since these are more likely to be viewed as work.
Itwould certainly be a challenge (although not neces-sarily impossible) to design a game that yields la-beled parse data, for example.In this paper, we assume that if people play agame in real life, there is a good chance they willplay it online as well.
To this end, we built on-line versions of two popular ?real-world?
games:ScattergoriesTM and TabooTM.
Not only are thesegames fun, but there is also a preexisting demandfor online versions of these games, driving searchtraffic to our site.
We will go into more detail aboutthese games in the next section.An important characteristic of these games is thatthey involve more than one player.
Interacting withanother player increases the sense of fun.
Anotherimportant feature these games share is that they are533timed.
Timing has several advantages.
First, tim-ing helps make the games feel more ?game-like?, byadding a sense of urgency.
Without timing, it risksfeeling more like a labeling task than a game.The next requirement is that the data be clean.First, the players must be capable of producing high-quality annotations.
Second, the game should en-courage users to enter relevant data.
We awardpoints as a motivating factor, but this can lead play-ers to enter irrelevant data, or collude with otherplayers, in order to get a higher score.
In particu-lar, collusion is more likely when players can freelycommunicate.
An excellent technique for producinggood data, used effectively in the ESP game, is torequire the players to match on their inputs.
Requir-ing players to match their partner?s hidden answersdiscourages off-topic answers and makes it quite dif-ficult to collude (requiring outside communication).We use this technique in all of our games.Finally, the data must be useful.
Ideally, it wouldbe directly applicable to an NLP task.
This require-ment can come into conflict with the other goals.There are certainly many kinds of data that wouldbe useful for NLP tasks (such as labeled parses), butdesigning a game to collect this data that people willplay and that produces clean data is difficult.In this paper, we focus on a particular kind of lin-guistic data: semantic relationships between pairs ofwords and/or phrases.
We do this for several rea-sons.
First, this kind of data is relatively simple,leading to fun games which produce relatively cleandata.
Second, the real-world games we chose toemulate naturally produce this kind of data.
Third,there are a number of recent works which focus onextracting these kinds of relationships, e.g.
(Snowet al, 2006; Nakov & Hearst, 2008).
Our workpresents an interesting new way of extracting thistype of data.
Finally, at least one of these kinds ofrelationships, the hypernym, or ?X is a Y?
relation,has proven to be useful for a variety of NLP tasks.3 Description of Our GamesWe now describe our three games in detail.3.1 CategorillaCategorilla, inspired by ScattergoriesTM, asks play-ers to supply words or phrases which fit specific cat-egories, such as ?Things that fly?
or ?Types of fish?.In addition, each game has a specific letter which allanswers must begin with.
Thus, if the current gamehas letter ?b?, reasonable answers would be ?bird?and ?barracuda?, respectively.
In each game, a ran-domly matched pair of players are given the same10 categories; they receive points when they matchwith the other player for a particular category.
Play-ers are allowed to type as may answers for a givencategory as they wish (until a match is made for thatcategory).
After a match is made, the players getto see what word they matched on for that category.Each answer is supposed to fit into a specific cate-gory, so the data is automatically structured.Our system contains 8 types of categories, manyof which were designed to correspond to linguisticresources used in NLP applications.
Table 1 de-scribes the category types.The purpose of the first three types of categories isto extract hypernym/hyponym pairs like those foundin WordNet (e.g., ?food?
is a hypernym of ?pizza?
).In fact, the categories were automatically generatedfrom WordNet, as follows.
First, we assigned countsCs to each synset s in WordNet using the Sem-Cor2 labeled data set of word senses.
Let desc(s)be the set of descendants of s in the hypernym hi-erarchy.
Then for each pair of synsets s, d, whered ?
desc(s), we computed a conditional distribu-tion P (d|s) = CdPd?
?desc(s) Cd?, the probability thatwe choose node d from among the descendants ofs.
Finally, we computed the entropy of each node sin WordNet,?d?desc(s) P (d|s)logP (d|s).
Synsetswith many different descendants occurring in Sem-Cor will have higher entropies.
Each node with asufficiently high entropy was chosen as a category.We then turned each synset into a category by tak-ing the first word in that synset and plugging it intoone of several set phrases.
For nouns, we tried twovariants (?Types of food?
and ?Foods?).
Depend-ing on the noun, either of these may be more natu-ral (consider ?Cities?
vs. ?Types of city?).
?Typesof food?
tends to produce more adjectival answersthan ?Foods?.
We tried only one variation for verbs(?Methods of paying?).
This phrasing is not per-fect; in particular, it encourages non-verb answerslike ?credit card?.The second group of categories tries to capture se-lectional preferences of verbs ?
for example, ?ba-2Available at www.cs.unt.edu/ rada/downloads.html534Name # Description Example Good AnswerNHyp 269 Members of a class of nouns ?Vehicles?
?car?NType 269 Members of a class of nouns ?Types of vehicle?
?car?VHyp 70 Members of a class of verbs ?Methods of cutting?
?trimming?VS 1380 Subjects of a verb ?Things that eat?
?cats?VO 909 Direct objects of a verb ?Things that are abandoned?
?family?VPP 77 Preposition arguments of a verb ?Things that are accused of?
?crime?Adj 219 Things described by an adjective ?Things that are recycled?
?cans?O 105 Other; mostly ?Things found at/in ...?
?Things found in a school?
?teachers?Table 1: Summary of category types.
# indicates the number of categories of that type.nana?
makes sense as the object of ?eat?
but not asthe subject.
Our goal with these categories was toproduce data useful for automatically labeling se-mantic roles (Gildea & Jurafsky, 2002), where selec-tional preferences play an important role.
We triedthree different types of categories, corresponding tosubjects, objects, and prepositional objects.
Exam-ples are ?Things that eat?, ?Things that are eaten?,and ?Things that are eaten with?, to which goodanswers would be ?animals?, ?food?, and ?forks?.These categories were automatically generated us-ing the labeled parses in Penn Treebank (Marcuset al, 1993) and the labeled semantic roles of Prop-Bank (Kingsbury et al, 2002).
To generate theobject categories, for example, for each verb wethen counted the number of times a core argument(ARG0-ARG5) appeared as the direct object of thatverb (according to the gold-standard parses), andused all verbs with count at least 5.
This guaran-teed that all generated categories were grammati-cally correct and captured information about corearguments for that verb.
Most of the prepositionalobject categories proved to be quite confusing (e.g.,?Things that are acted as?
), so we manually removedall but the most clear.
Not surprisingly, the use ofthe Wall Street Journal had a noticeable effect on thetypes of categories extracted; they have a definite fi-nancial bias.The third group of categories only has onetype, which consists of adjective categories such as?Things that are large?.
While we did not have anyspecific task in mind for this category type, having adatabase of attributes/noun pairs seems potentiallyuseful for various NLP tasks.
To generate thesecategories, we simply took the most common ad-jectives in the SemCor data set.
Again, the result-ing set of adjectives reflect the corpus; for example,?Things that are green?
was not generated as a cate-gory, while ?Things that are corporate?
was.The final group of categories were hand-written.This group was added to make sure that a sufficientnumber of ?fun?
categories were included, sincesome of the category types, particularly the verbcategories, are somewhat confusing and difficult.Most of the hand-written categories are of the form?Things found at/in X?, where X is a location, suchas ?Japan?
or ?the ocean?.The starting letter requirement also has importantconsequences for data collection.
It was designedto increase the variety of obtained data; without thisrestriction, players might produce a smaller set of?obvious?
answers.
As we will see in the results,this restriction did indeed lead to a great diversity ofanswers, but at a severe cost to data quality.3.2 CategodzillaCategodzilla is a slightly modified version of Cat-egorilla, with the starting letter constraint relaxed.The combination of difficult categories and rare let-ters often leads to bad answers in Categorilla.
To in-crease data quality, in Categodzilla for each categorythere are three boxes.
In the first box you can typeany word you want.
Answers in the second box muststart with a given ?easy?
letter such as ?c?.
Answersin the third box must start with a given ?hard?
letter,such as ?k?.
The boxes much be matched in order;guesses typed in the first box which match either ofthe other two boxes are automatically propagated.3.3 Free AssociationFree Association, inspired by TabooTM, simply asksplayers to type words related to a given ?seed?
word.Players are not allowed to type any of several wordson a ?taboo?
list, specific to the current seed word.535As soon as a match is achieved, players move on toa new seed word.The seed words came from two sources.
The firstwas the most common words in SemCor.
The sec-ond was the Google unigram data, which lists themost common words on the web.
In both cases, wefiltered out stop words (including all prepositions).Unlike Categorilla, we found that nearly all col-lected Free Association data was of good quality,due to the considerably easier nature of the task.
Ofcourse, we do lose the structure present in Catego-rilla.
As the name suggests, the collected data is es-sentially free word association pairs.
We analyze thedata in depth to see what kinds of relations we got.4 Existing Word GamesTwo notable word games already exist for collectinglinguistic data.
The first is the Open Mind CommonSense system3 (Chklovski, 2003).
The second isVerbosity4 (von Ahn et al, 2006).
Both these gamesare designed to extract common sense facts, and thushave a different focus than our games.5 BotsThere may not always be enough players availableonline to match a human player with another humanplayer.
Therefore, one important part of designingan online game is building a bot which can func-tion in the place of a player.
The bots for all of ourgames are similar.
Each has a simple random modelwhich determines how long to wait between guesses.The bot?s guesses are drawn from past guesses madeby human players for that category/seed word (plusstarting letter in the case of Categorilla).
Just as witha human player, as soon as one of the bot?s guessesmatches one of the player?s, a match is made.If there are no past guesses, the bot instead makes?imaginary?
guesses.
For example, in Categorilla,we make the (obviously false) assumption that forevery category and every starting letter there are ex-actly 20 possible answers, and that both the player?sguesses and the bot?s imaginary guesses are drawnfrom those 20 answers.
Then, given the numberof guesses made by the player and the number ofimaginary guesses made by the bot, the probabil-ity of a match can be computed (assuming that all3http://commons.media.mit.edu/en4www.gwap.com/gwap/gamesPreview/verbosityGrla Gdza FreeGame Length 3min 3min 2minGames Played 19656 2999 15660Human-Human Games 428 45 401Categories 3298 3298 9488Guesses Collected 391804 78653 307963Guesses/Categories 119 24 32Unique Guesses 340433 56142 221874Guesses: All/Unique 1.15 1.40 1.39Guesses/Games 19.9 26.2 19.7Guesses per minute 6.6 8.7 9.9Table 2: Statistics for Categorilla, Categodzilla, and FreeAssociation.guesses are made independently).
Once this proba-bility passes a certain threshold, randomly generatedfor each category at the start of each game, the botmatches one of the player?s guesses, chosen at ran-dom.
The Free Association bot works similarly.For Free Association, the bot rarely has to resortto generating these imaginary guesses.
In Catego-rilla, due to the starting letter requirement, the bothas to make imaginary guesses much more often.Imaginary guessing can encourage poor behavior onthe part of players, since they see that matches canoccur for obviously bad answers.
They may also re-alize that they are playing against a bot.An additional complication for Categorilla andCategodzilla is that the bot has to decide which cat-egories to make guesses for, and in what order.
Ourcurrent guessing model takes into account past diffi-culty of the category and the current guessing of thehuman player to determine where to guess next.6 Users and UsageTable 2 shows statistics of each of the games, asof late June 2008.
While we have collected nearly800,000 data instances, nearly all of the games werebetween a human and the bot.
Over the course ofa year, our site received between 40 and 100 vis-its a day; this was not enough to make it likely forhuman-human games to occur.
The fact that we stillcollected this amount of data suggests that our bot isa satisfactory substitue for a human teammate.
Wehave anecdotally found that most players do not re-alize they are playing against a bot.
While most ofthe data comes from games between a human and abot, our data set consists only of input by the humanplayers.53611010010001 2 3-5 6-1011-2021-5051-100101-200201-500501-10001001-2000Games PlayedNumberofUsersCategorillaCategodzillaFree AssociationFigure 1: Users are grouped by number of games played.Note that this graph is on a double-log scale.Our main tool for attracting traffic to our site wasGoogle.
First, we obtained $1 a day in AdWords,which pays for between 7 to 10 clicks on our ada day.
Second, our site is in the top 10 results formany relevant searches, such as ?free online scatter-gories?.Categorilla was the most popular of the games,with about 25% more games played than Free As-sociation.
Taking the longer length of Categorillagames into account (see Table 2), this correspondsto almost 90% more play time.
This is despite thefact that Free Association is the first game listed onour home page.
We hypothesize that this is becauseScattergoriesTM is a more popular game in real life,and so many people come to our site specificallylooking for an online ScattergoriesTM game.
Cat-egodzilla has been played signficantly less; it hasbeen available for less time and is listed third on thesite.
Even for Categodzilla, the least played game,we have collected on average 24 guesses per cate-gory.Several of our design decisions for the gameswere based on trying to increase the diversity of an-swers.
Categorilla has the highest answer diversity.For a given category, each answer occurred on aver-age only 1.15 times.
In general, this average shouldincrease with the amount of collected data.
How-ever, Categodzilla and Free Association have col-lected significantly fewer answers per category thanCategorilla, but still have a higher average, around1.4.
The high answer diversity of Categorilla is adirect result of the initial letter constraint.
For allthree games, the majority of category/answer pairsoccurred only once.Figure 1 shows the distribution over users of the00.020.040.060.080.10.12a b c d e f g h i j k l m n o p q r s t u v w x y z *Fraction of AnswersCategorillaCategodzillaFree AssocationFigure 2: Fraction of answers with given initial letter.
*denotes everything nonalphabetical.number of games played.
Not surprisingly, it followsthe standard Zipfian curve; there are a large numberof users who have played only a few games, and afew users who have played a lot of games.
The mid-dle of the curve is quite thick; for both Categorillaand Free Association there are more than 100 play-ers who have played between 21 and 50 games.Figure 2 shows the distribution of initial lettersof collected answers for each game.
Categorillais nearly flat over all letters besides ?q?, ?x?, and?z?
which are never chosen as the inital letter con-straint.
This means players make a similar numberof guesses even for difficult initial letters.
In con-trast, the distribution of initial letters for Free Asso-ciation data reflects the relatively frequency of initialletters in English.
Even though Categodzilla doeshave letter constraints in the 2nd and 3rd columns,its statistics over initial letter are very similar to FreeAssociation.7 Categorilla and Categodzilla DataIn our analyses, we take ALL guesses made at anytime, whether or not they actually produced a match.This greatly increases the amount of usable data, butalso increases the amount of noise in the data.The biggest question about the data collectedfrom Categorilla and Categodzilla is the quality ofthe data.
Many categories can be difficult or some-what confusing, and the initial letter constraint fur-ther increases the difficulty.To evaluate the quality of the data, we askedthree volunteer labelers to label 1000 total cate-gory/answer pairs.
Each labeler labeled every pairwith one of three labels, ?y?, ?n?, or ?k?.
?y?
meansthat the answer fit the category.
?n?
means that it537Annotator y k n#1 72 13 115#2 77 27 96#3 88 42 70Majority 76 29 95Table 3: Comparison of annotatorsData Set y k nControl 30 14 156Categorilla 76 29 95Categodzilla 144 23 33Table 4: Overall answer accuracydoes not fit.
?k?
means that it ?kind of?
fits.
This wasmostly left up to the labelers; the only suggestionwas that one use of ?k?
could be if the category was?Things that eat?
and the answer was ?sandwich.
?Here, the answer is clearly related to the category,but doesn?t actually fit.The inter-annotator agreement was reasonable,with a Fleiss?
kappa score of .49.
The main differ-ence between annotators was how permissive theywere; the percentage of answers labeled ?n?
rangedfrom 58% for the first annotator to 35% for the third.The labeled pairs were divided into 5 subgroups of200 pairs each (described below); Table 3 shows thenumber of each label for the Categorilla-Randomsubset.
We aggregated the different annotations bytaking a majority vote; if all three answers were dif-ferent, the item was labeled ?k?.
Table 3 also showsthe statistics of the majority vote on the same subset.Overall Data Quality.
We compared resultsfor three random subsets of answers, Control-Random, Categorilla-Random, and Categodzilla-Random.
Categorilla-Random was built by select-ing 200 random category/answer pairs from the Cat-egorilla data.
Note that category/answer pairs thatoccurred more than once were more likely to be se-lected.
Categodzilla-Random was built similarly.Control-Random was built by randomly selectingtwo sets of 200 category/answer pairs each (includ-ing data from both Categorilla and Categodzilla),and then combining the categories from the first setwith the answers from the second to generate a setof random category/answer pairs.Table 4 shows results for these three subsets.
Thechance for a control answer to be labeled ?y?
was15%.
Categorilla produces data that is significantlyCategory Results -- Categorilla0510152025NHyp NType VHyp VS VO VPP Adj OCategory TypenkyFigure 3: Categorilla accuracy by category typebetter than control, with 38% of answers labeled ?y?.Categodzilla, which is more relaxed about initial let-ter restrictions, is significantly better than Catego-rilla, with 72% of answers labeled ?y?.
This relax-ation has an enormous impact on the quality of thedata.
Note however that these statistics are not ad-justed for accuracy of individual players; it may bethat only more accurate players play Categodzilla.Effect of Category Type on Data Quality.Within each type of category (see Table 1), cer-tain categories appear much more often than oth-ers due to the way categories are selected (at leasttwo ?easy?
categories are guaranteed every game).To adjust for this, we built a subset of 200 cat-egory/answer pairs by selecting 25 different cate-gories randomly from each type of category.
Wethen selected an answer at random from among theanswers submitted for that category.
In addition, webuilt a control set using the same 200 categories butinstead using answers selected at random from theentire Categorilla data set.
Results for Categorilladata are shown in Figure 3; we omit the correspond-ing graph for control for lack of space.
For mostcategories, the Categorilla data is significantly bet-ter than the control.
The hand-written category type,O, has the best data quality, which is not surpris-ing because these categories allow the most possibleanswers, and thus are easiest of think of answers for.These categories also have the highest number of ?y?labels for the control.
Next best are the hypernymcategories, NType.
NType is much higher than theother noun hypernym category NHyp because the?Type of?
phrasing is generally more natural and al-lows for adjectival answers.
The VPP category type,which tries to extract prepositional objects, contains538Data Set Letters Size y k nControl Easy 127 .14 .08 .78Control Hard 72 .15 .06 .79Categorilla Easy 106 .45 .14 .41Categorilla Hard 94 .30 .15 .55Table 5: Accuracy of easy letters vs. hard letters.
Size isthe number of answers for that row.the most number of ?k?
annotations; this is becauseplayers often put answers that are subjects or ob-jects of the verb, such as ?pizza?
for ?Things thatare eaten with?.
The adjective category type, Adj,has the lowest increase over the control; this is likelydue to the nature of the extracted adjectives.Effect of Initial Letter on Data Quality.
Ingeneral, we would expect common initial letters toyield better data since there are more possible an-swers to choose from.
We did not have enough la-beled data to do letter by letter statistics.
Instead, webroke the letters into two groups, based on the em-pirical difficulty of obtaining matches when giventhat initial letter.
The easy letters were ?abcfhlmn-prst?, while the hard letters were ?degijkouvwy?.
Ta-ble 5 shows the results on Categorilla-Random andControl-Random on these two subsets.
First, notethat the results on Control-Random are the same forhard letters and easy letters.
This means that wordsstarting with common letters are not more likely tofit in a category.
For both hard letters and easy let-ters, the accuracy is considerably better on the Cat-egorilla data.
However, the increase in the numberof ?y?
labels for easy letters is twice that for hardletters.
The quality of data for hard letters is consid-erably worse than that for easy letters.8 Free Association DataIn contrast to Categorilla and even Categodzilla, wefound that the Free Association data was quite clean.However, it is also not structured; we simply getpairs of related words.
Thus, the essential questionfor this game is what kind of data we get.To analyze the types of relationships betweenwords, the authors labeled 500 randomly extractedunique pairs with a rich set of word-word relations,described in Table 6.
This set of relations was de-signed to capture the observed relationships encoun-tered in the Free Association data.
Unlike our Cat-egorilla labeled set, pairs that occurred more thanonce were NOT more likely to be selected than pairsthat occurred once (i.e., the category/answer pairswere aggregated prior to sampling).
Sampling in thisway led to more diversity in the pairs extracted.To label each pair, the authors found a sequenceof relationships which connected the two words.
Inmany cases, this was a single link.
For example,?dragon?
and ?wing?
are connected by a single link,?wing?
IS PART OF ?dragon?.
In others, multiplelinks were required.
For the seed word ?dispute?
andanswer ?arbitrator?, we can connect using two links:?dispute?
IS OBJECT OF ?resolve?, ?arbitrator?
ISSUBJECT OF ?resolve?.
There were two other pos-sible ways to label a pair.
First, they might be totallyunrelated (i.e., a bad answer).
Second, they mightbe related, but not connectable using our set of basicrelations.
For example, ?echo?
is clearly related to?valley?, but in a complicated way.The quality of the data is considerably higher thanCategorilla and Categodzilla; under 10% of wordsare unrelated.
Slightly over 20% of the pairs are la-beled Misc, i.e., the words are related but in a com-plicated way.
3% of the pairs can be linked with achain of two simple relations.
The remaining 67%of all pairs were linked with a single simple relation.The category Desc deserves some discussion.This category included both simple adjective de-scriptions, such as ?creek?
and ?noisy?, and alsoqualifiers, such as ?epidemic?
and ?typhoid?, whereone word specifies what kind of thing the other is.The distinction between Desc and Phrase was sim-ply based on to what extent the combination of thetwo words was a set phrase (such as ?east?
and ?Ger-many?
).Schulte im Walde et al (2008) address very sim-ilar issues to those discussed in this section.
Theybuilt a free association data set containing about200,000 German word pairs using a combination ofonline and offline volunteers (but not a game).
Theythen analyze the resulting associations by comparingthe resulting pairs to a large-scale lexical resource,GermaNet (the German counterpart of WordNet).Our data analysis was by hand, making it compar-atively small scale but more detailed.
It would beinteresting to compare the data sets to see whetherthe use of a game affects the resulting data.9 Filtering Bad DataIn this section, we consider a simple heuristic forfiltering bad data: only retaining answers that were539Name # Description ExampleMisc 103 Words related, but in a complicated way ?echo?, ?valley?Desc 76 One of the words describes the other ?cards?, ?business?None 47 Words are not related ?congress?,?store?Syn 46 The words are synonyms ?downturn?, ?dip?Obj 33 One word is the object of the other ?exhale?,?emission?Hyp 30 One word is an example of the other ?cabinet?,?furniture?
?Syn 29 The words are ?approximate?
synonyms ?maverick?,?outcast?Cousin 21 The words share a common hypernym (is-a) relation ?meter?,?foot?Has 18 One word ?has?
the other ?supermarket?,?carrots?2-Chain 15 Words are linked by a chain of two simple relations ?arbitrator?,?dispute?Phrase 13 Words make a phrase; similar to Desc ?East?, ?Germany?Part 11 One is a part of the other ?dragon?,?wings?At 10 One is found at the other ?harbor?, ?lake?Subj 8 One is the subject of the other ?actor?, ?pretend?Form 7 One is a form of the other ?revere?,?reverence?Def 7 One defines the other ?blind?,?unable to see?Opp 7 The two are opposites ?positive?,?negative?Sound 6 The two words sound similar ?boutique?,?antique?Sub 5 One is a subword of the other ?outlet?, ?out?Unit 2 One is a unit of the other ?reel?,?film?Made 2 One is made of the other ?knee?,?bone?Table 6: Relation types for 500 hand-labeled examples.
# indicates the number of pairs with that label.guessed some minimum number of times.
Note thatin this section all answers were stemmed in order tocombine counts across plurals and verb tenses.For the Categorilla data, filtering out cate-gory/answer pairs that only occurred once fromCategorilla-Random left a total of 64 answers (froman original 200), of which 36 were labeled ?y?
and 8were labeled ?k?.
The fraction of ?y?
labels in thereduced set is 56%, up from 38% in the originalset.
This gain in quality comes at the cost of losingslightly over two-thirds of the data.For Categodzilla-Random, a similar filter left 88(out of 200), with 79 labeled ?y?
and 7 labeled ?k?.For the hand-labeled Free Association data, apply-ing this filter yielded a total of 123 pairs (out of anoriginal 500), with only 2 having no relation5.
Inthese two games, this filter eliminates nearly all baddata while keeping a reasonable fraction of the data.Clearly, this filter is less effective for Catego-rilla than the other two games.
One of the mainreasons for this is that the letter constraints cause5The higher fraction of lost pairs for Free Association is pri-marily due to the method of sampling pairs for evaluation, asdiscussed in Section 8.people to try to fit words starting with that letterinto all categories that they even vaguely relate to,rather than thinking of words that really fit that cat-egory.
Examples include {?Art supplies?,?jacket?
},{?Things found in Chicago?,?king?}
and {?Thingsthat are African?,?yak?}.
Of course, we can furtherincrease the quality of the data by making the fil-ter more restrictive, at the cost of losing more data.For example, removing answers occuring fewer than5 times from Categorilla-Random leaves only 8 an-swers (out of 200), 7 labeled ?y?
and 1 labeled ?n?.There are other ways we could filter the data.
Forexample, suppose we are given an outside databaseof pairs of words which are known to be semanti-cally related.
We could apply the following heuris-tic: if an answer to a particular category is similar tomany other answers for that category, then that an-swer is likely to be a good one.
Preliminary experi-ments using distributional similarity of words as thesimilarity metric suggest that this heuristic capturescomplimentary information to the guess frequencyheuristic.
We leave as future work a full integrationof the two heuristics into a single improved filter.540Classified Type # ExampleReal hypernyms 96 ?equipment?,?racquet?Compound hypernyms 32 ?arrangement?,?flower?Adjectives 25 ?building?,?old?Sort-of hypernyms 14 ?vegetable?,?salad?Not hypernyms 33 ?profession?,?money?Table 7: Breakdown of potential hypernym pairs10 Using the DataCategorilla and Categodzilla produce structured datawhich is already in a usable or nearly usable form.For example, the NHyp and NType categories pro-duce lists of hypernyms, which could be used to aug-ment WordNet.
We looked at this particular applica-tion in some detail.First, in order to remove noisy data, we usedonly Categodzilla data and removed answers whichoccurred only once.
We took all category/answerpairs where the category was of type either NHyp orNType, and where the answer was a noun.
This re-sulted in 1604 potential hypernym/hyponym pairs.Of these, 733 (or 46%) were already in WordNet.The remaining 871 were not found in WordNet.
Wethen hand-labeled a random subset of 200 of the 871to determine how many of them were real hyper-nym/hyponym pairs.
The results are shown in Ta-ble 7.
Counting compound hyponyms, nearly two-thirds of the pairs are real hypernym/hyponym pairs.These new pairs could directly augment WordNet.For example, for the word ?crime?, WordNet hasas hyponyms ?burglary?
and ?fraud?.
However,it doesn?t have ?arson?, ?homicide?, or ?murder?,which are among the 871 new pairs.
WordNet lists?wedding?
as being an ?event?, but not ?birthday?.The verb subject, object, and prepositional objectcategories were designed to collect data about theselectional preferences of verbs.
These categoriesturned out to be problematic for several reasons.First, statistics about selectional preferences of verbsare not too difficult to extract from the web (althoughin some cases they might be somewhat noisy).
Thus,the motivation for extracting this data using a gameis not as apparent.
Second, providing arguments ofverbs out of the context of a sentence may be too dif-ficult.
For example, for the category ?Things that areaccumulated?, there a couple of obvious answers,such as ?wealth?
or ?money?, but beyond these itbecomes more difficult.
In the context of an actualdocument, quite a lot of things can accumulate, butoutside of that context it is difficult to think of them.One solution to this problem would be to providecontext.
For example, the category ?Things that ac-cumulate in your body?
is both easier to think ofanswers for and probably collects more useful data.However, automatically creating categories with theright level of specificity is not a trivial task; our ini-tial experiments suggested that it is easy to gener-ate too much context, creating an uninteresting cat-egory.The Free Association game produces a lot of veryclean data, but does not classify the relationships be-tween the words.
While a web of relationships mightbe useful by itself, classifying the pairs by relationtype would clearly be valuable.
Snow et al (2006)and Nakov and Hearst (2008), among others, look atusing a large amount of unlabeled data to classifyrelations between words.
One issue with extract-ing new relations from text, for example meronyms(part-of relationships), is that they tend to occurfairly rarely.
Thus, it is very easy to get a large num-ber of spurious pairs.
Using our data as a set of can-didate pairs for relation extraction could greatly re-duce the resulting noise.
We believe that applicationof existing techniques to the data from the Free As-sociation game could lead to a clean, classified set ofword-word relations, but leave this as future work.11 Discussion and Future WorkOne way to extend Categorilla and Categodzillawould be to add additional types of categories.
Forexample, a meronym category type (e.g.
?Parts of acar?)
would work well.
Further developing the verbcategories (e.g., ?Things that accumulate in yourbody?)
is another challenging but interesting direc-tion; these categories would produce phrase-wordrelationships rather than word-word relationships.Probably the most interesting direction for futurework is trying to increase the complexity of the datacollected from a game.
There are two significant dif-ficulties: keeping the game fun, and making sure thecollected data is not too noisy.
One interesting ques-tion for future research is whether different game ar-chitectures might be better suited to certain kindsof data.
For example, a ?telephone?
style game,where players relay a phrase or sentence throughsome noisy channel, might be an interesting way toobtain paraphrase data.541ReferencesChklovski, T. (2003).
Using analogy to acquire com-monsense knowledge from human contributors.Thesis.Fellbaum, C.
(Ed.).
(1998).
Wordnet: An electroniclexical database.
MIT Press.Gildea, D., & Jurafsky, D. (2002).
Automatic label-ing of semantic roles.
Computational Linguistics.Kingsbury, P., Palmer, M., & Marcus, M. (2002).Adding semantic annotation to the penn treebank.Proceedings of the Human Language TechnologyConference (HLT?02).Marcus, M., Marcinkiewicz, M., & Santorini, B.(1993).
Building a large annotated corpus of en-glish: the penn treebank.
Computational Linguis-tics.Nakov, P., & Hearst, M. (2008).
Solving relationalsimilarity problems using the web as a corpus.Proceedings of ACL.Schulte imWalde, S., Melinger, A., Roth, M., &We-ber, A.
(2008).
An empirical characterisation ofresponse types in german association norms.
Toappear, Research on Language and Computation.Snow, R., Jurafsky, D., & Ng, A.
(2006).
Semantictaxonomy induction from heterogenous evidence.Proceedings of COLING/ACL.von Ahn, L., & Dabbish, L. (2004).
Labeling imageswith a computer game.
ACM CHI.von Ahn, L., Kedia, M., & Blum, M. (2006).
Ver-bosity: a game for collecting common-sense facts.Proceedings of the SIGCHI conference on HumanFactors in computing systems.542
