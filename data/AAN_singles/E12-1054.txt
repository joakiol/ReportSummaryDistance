Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529?538,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsMeasuring Contextual Fitness Using Error Contexts Extracted from theWikipedia Revision HistoryTorsten ZeschUbiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Information, FrankfurtUbiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer Science, Technische Universita?t Darmstadthttp://www.ukp.tu-darmstadt.deAbstractWe evaluate measures of contextual fitnesson the task of detecting real-word spellingerrors.
For that purpose, we extract nat-urally occurring errors and their contextsfrom the Wikipedia revision history.
Weshow that such natural errors are bettersuited for evaluation than the previouslyused artificially created errors.
In partic-ular, the precision of statistical methodshas been largely over-estimated, while theprecision of knowledge-based approacheshas been under-estimated.
Additionally, weshow that knowledge-based approaches canbe improved by using semantic relatednessmeasures that make use of knowledge be-yond classical taxonomic relations.
Finally,we show that statistical and knowledge-based methods can be combined for in-creased performance.1 IntroductionMeasuring the contextual fitness of a term in itscontext is a key component in different NLP ap-plications like speech recognition (Inkpen andDe?silets, 2005), optical character recognition(Wick et al 2007), co-reference resolution (Beanand Riloff, 2004), or malapropism detection (Bol-shakov and Gelbukh, 2003).
The main idea is al-ways to test what fits better into the current con-text: the actual term or a possible replacement thatis phonetically, structurally, or semantically simi-lar.
We are going to focus on malapropism detec-tion as it allows evaluating measures of contex-tual fitness in a more direct way than evaluatingin a complex application which always entails in-fluence from other components, e.g.
the quality ofthe optical character recognition module (Walkeret al 2010).A malapropism or real-word spelling error oc-curs when a word is replaced with another cor-rectly spelled word which does not suit the con-text, e.g.
?People with lots of honey usuallylive in big houses.
?, where ?money?
was replacedwith ?honey?.
Besides typing mistakes, a majorsource of such errors is the failed attempt of au-tomatic spelling correctors to correct a misspelledword (Hirst and Budanitsky, 2005).
A real-wordspelling error is hard to detect, as the erroneousword is not misspelled and fits syntactically intothe sentence.
Thus, measures of contextual fitnessare required to detect words that do not fit theircontexts.Existing measures of contextual fitness can becategorized into knowledge-based (Hirst and Bu-danitsky, 2005) and statistical methods (Mays etal., 1991; Wilcox-OHearn et al 2008).
Bothtest the lexical cohesion of a word with its con-text.
For that purpose, knowledge-based ap-proaches employ the structural knowledge en-coded in lexical-semantic networks like WordNet(Fellbaum, 1998), while statistical approachesrely on co-occurrence counts collected from largecorpora, e.g.
the Google Web1T corpus (Brantsand Franz, 2006).So far, evaluation of contextual fitness mea-sures relied on artificial datasets (Mays et al1991; Hirst and Budanitsky, 2005) which are cre-ated by taking a sentence that is known to be cor-rect, and replacing a word with a similar wordfrom the vocabulary.
This has a couple of dis-advantages: (i) the replacement might be a syn-onym of the original word and perfectly valid inthe given context, (ii) the generated error might529be very unlikely to be made by a human, and(iii) inserting artificial errors often leads to un-natural sentences that are quite easy to correct,e.g.
if the word class has changed.
However,even if the word class is unchanged, the origi-nal word and its replacement might still be vari-ants of the same lemma, e.g.
a noun in singu-lar and plural, or a verb in present and past form.This usually leads to a sentence where the errorcan be easily detected using syntactical or statis-tical methods, but is almost impossible to detectfor knowledge-based measures of contextual fit-ness, as the meaning of the word stays more orless unchanged.
To estimate the impact of this is-sue, we randomly sampled 1,000 artificially cre-ated real-word spelling errors1 and found 387 sin-gular/plural pairs and 57 pairs which were in an-other direct relation (e.g.
adjective/adverb).
Thismeans that almost half of the artificially createderrors are not suited for an evaluation targeted atfinding optimal measures of contextual fitness, asthey over-estimate the performance of statisticalmeasures while underestimating the potential ofsemantic measures.
In order to investigate thisissue, we present a framework for mining natu-rally occurring errors and their contexts from theWikipedia revision history.
We use the resultingEnglish and German datasets to evaluate statisti-cal and knowledge-based measures.We make the full experimental framework pub-licly available2 which will allow reproducing ourexperiments as well as conducting follow-up ex-periments.
The framework contains (i) methodsto extract natural errors from Wikipedia, (ii) ref-erence implementations of the knowledge-basedand the statistical methods, and (iii) the evalua-tion datasets described in this paper.2 Mining Errors from WikipediaMeasures of contextual fitness have previouslybeen evaluated using artificially created datasets,as there are very few sources of sentences withnaturally occurring errors and their corrections.Recently, the revision history of Wikipedia hasbeen introduced as a valuable knowledge sourcefor NLP (Nelken and Yamangil, 2008; Yatskar etal., 2010).
It is also a possible source of naturalerrors, as it is likely that Wikipedia editors make1The same artificial data as described in Section 3.2.2http://code.google.com/p/dkpro-spelling-asl/real-word spelling errors at some point, whichare then corrected in subsequent revisions of thesame article.
The challenge lies in discriminatingreal-word spelling errors from all sorts of otherchanges, including non-word spelling errors, re-formulations, or the correction of wrong facts.For that purpose, we apply a set of precision-oriented heuristics narrowing down the numberof possible error candidates.
Such an approachis feasible, as the high number of revisions inWikipedia allows to be extremely selective.2.1 Accessing the Revision DataWe access the Wikipedia revision data usingthe freely available Wikipedia Revision Toolkit(Ferschke et al 2011) together with the JWPLWikipedia API (Zesch et al 2008a).3 The APIoutputs plain text converted from Wiki-Markup,but the text still contains a small portion of left-over markup and other artifacts.
Thus, we per-form additional cleaning steps removing (i) to-kens with more than 30 characters (often URLs),(ii) sentences with less than 5 or more than 200tokens, and (iii) sentences containing a high frac-tion of special characters like ?:?
usually indicat-ing Wikipedia-specific artifacts like lists of lan-guage links.
The remaining sentences are part-of-speech tagged and lemmatized using TreeTagger(Schmid, 2004).
Using these cleaned and anno-tated articles, we form pairs of adjacent article re-visions (ri and ri+1).2.2 Sentence AlignmentFully aligning all sentences of the adjacent revi-sions is a quite costly operation, as sentences canbe split, joined, replaced, or moved in the arti-cle.
However, we are only looking for sentencepairs which are almost identical except for thereal-word spelling error and its correction.
Thus,we form all sentence pairs and then apply an ag-gressive but cheap filter that rules out all sentenceswhich (i) are equal, or (ii) whose lengths differmore than a small number of characters.
For theresulting much smaller subset of sentence pairs,we compute the Jaro distance (Jaro, 1995) be-tween each pair.
If the distance exceeds a cer-tain threshold tsim (0.05 in this case), we do notfurther consider the pair.
The small amount of re-maining sentence pairs is passed to the sentencepair filter for in-depth inspection.3http://code.google.com/p/jwpl/5302.3 Sentence Pair FilteringThe sentence pair filter further reduces the num-ber of remaining sentence pairs by applying a setof heuristics including surface level and semanticlevel filters.
Surface level filters include:Replaced Token Sentences need to consist ofidentical tokens, except for one replaced token.No Numbers The replaced token may not be anumber.UPPER CASE The replaced token may not bein upper case.Case Change The change should not only in-volve case changes, e.g.
changing ?english?
into?English?.Edit Distance The edit distance between thereplaced token and its correction need to be be-low a certain threshold.After applying the surface level filters, the re-maining sentence pairs are well-formed and con-tain exactly one changed token at the same posi-tion in the sentence.
However, the change doesnot need to characterize a real-word spelling er-ror, but could also be a normal spelling error or asemantically motivated change.
Thus, we apply aset of semantic filters:Vocabulary The replaced token needs to occurin the vocabulary.
We found that even quite com-prehensive word lists discarded too many validerrors as Wikipedia contains articles from a verywide range of domains.
Thus, we use a frequencyfilter based on the Google Web1T n-gram counts(Brants and Franz, 2006).
We filter all sentenceswhere the replaced token has a very low unigramcount.
We experimented with different values andfound 25,000 for English and 10,000 for Germanto yield good results.Same Lemma The original token and the re-placed token may not have the same lemma, e.g.?car?
and ?cars?
would not pass this filter.Stopwords The replaced token should not be ina short list of stopwords (mostly function words).Named Entity The replaced token should notbe part of a named entity.
For this purpose, weapplied the Stanford NER (Finkel et al 2005).Normal Spelling Error We apply the Jazzyspelling detector4 and rule out all cases in whichit is able to detect the error.Semantic Relation If the original token and thereplaced token are in a close lexical-semantic rela-4http://jazzy.sourceforge.net/tions, the change is likely to be semantically mo-tivated, e.g.
if ?house?
was replaced with ?hut?.Thus, we do not consider cases, where we detecta direct semantic relation between the original andthe replaced term.
For this purpose, we use Word-Net (Fellbaum, 1998) for English and GermaNet(Lemnitzer and Kunze, 2002) for German.3 Resulting Datasets3.1 Natural Error DatasetsUsing our framework for mining real-wordspelling errors in context, we extracted an En-glish dataset5, and a German dataset6.
Althoughthe output generally was of high quality, man-ual post-processing was necessary7, as (i) forsome pairs the available context did not provideenough information to decide which form wascorrect, and (ii) a problem that might be spe-cific to Wikipedia ?
vandalism.
The revisions arefull of cases where words are replaced with simi-lar sounding but greasy alternatives.
A relativelymild example is ?In romantic comedies, there isa love story about a man and a woman who fallin love, along with silly or funny comedy farts.
?,where ?parts?
was replaced with ?farts?
only to bechanged back shortly afterwards by a Wikipediavandalism hunter.
We removed all cases that re-sulted from obvious vandalism.
For further ex-periments, a small list of offensive terms could beadded to the stopword list to facilitate this pro-cess.A connected problem is correct words that getfalsely corrected by Wikipedia editors (withoutthe malicious intend from the previous examples,but with similar consequences).
For example, theinitially correct sentence ?Dung beetles roll it intoa ball, sometimes being up to 50 times their ownweight.?
was ?corrected?
by exchanging weightwith wait.
We manually removed such obviousmistakes, but are still left with some borderlinecases.
In the sentence ?By the 1780s the goalsof England were so full that convicts were oftenchained up in rotting old ships.?
the obvious error5Using a revision dump from April 5, 2011.6Using a revision dump from August 13, 2010.7The most efficient and precise way of finding real-wordspelling errors would of course be to apply measures of con-textual fitness.
However, the resulting dataset would thenonly contain errors that are detectable by the measures wewant to evaluate ?
a clearly unacceptable bias.
Thus, a cer-tain amount of manual validation is inevitable.531?goal?
was changed by some Wikipedia editor to?jail?.
However, actually it should have been theold English form for jail ?gaol?
which can be de-duced when looking at the full context and laterversions of the article.
We decided to not removethese rare cases, because ?jail?
is a valid correctionin this context.After manual inspection, we are left with 466English and 200 German errors.
Given that werestricted our experiment to 5 million English andGerman revisions, much larger datasets can be ex-tracted if the whole revision history is taken intoaccount.
Our snapshot of the English Wikipediacontains 305?106 revisions.
Even if not all of themcorrespond to article revisions, it is safe to assumethat more than 10,000 real-word spelling errorscan be extracted from this version of Wikipedia.Using the same amount of source revisions, wefound significantly more English than German er-rors.
This might be due to (i) English having moreshort nouns or verbs than German that are morelikely to be confused with each other, and (ii) theEnglish Wikipedia being known to attract a largeramount of non-native editors which might lead tohigher rates of real-word spelling errors.
How-ever, this issue needs to be further investigatede.g.
based on comparable corpora build on the ba-sis of different language editions of Wikipedia.Further refining the identification of real-word er-rors in Wikipedia would allow evaluating how fre-quent such errors actually occur, and how longit takes the Wikipedia editors to detect them.
Iferrors persist over a long time, using measuresof contextual fitness for detection would be evenmore important.Another interesting observation is that the av-erage edit distance is around 1.4 for both datasets.This means that a substantial proportion of errorsinvolve more than one edit operation.
Given thatmany measures of contextual fitness allow at mostone edit, many naturally occurring errors will notbe detected.
However, allowing a larger edit dis-tance enormously increases the search space re-sulting in increased run-time and possibly de-creased detection precision due to more false pos-itives.3.2 Artificial Error DatasetsIn contrast to the quite challenging process ofmining naturally occurring errors, creating artifi-cial errors is relatively straightforward.
From acorpus that is known to be free of spelling errors,sentences are randomly sampled.
For each sen-tence, a random word is selected and all stringswith edit distance smaller than a given threshold(2 in our case) are generated.
If one of those gen-erated strings is a known word from the vocabu-lary, it is picked as the artificial error.Previous work on evaluating real-word spellingcorrection (Hirst and Budanitsky, 2005; Wilcox-OHearn et al 2008; Islam and Inkpen, 2009)used a dataset sampled from the Wall Street Jour-nal corpus which is not freely available.
Thus, wecreated a comparable English dataset of 1,000 ar-tificial errors based on the easily available Browncorpus (Francis W. Nelson and Kuc?era, 1964).8Additionally, we created a German dataset with1,000 artificial errors based on the TIGER cor-pus.94 Measuring Contextual FitnessThere are two main approaches for measuring thecontextual fitness of a word in its context: thestatistical (Mays et al 1991) and the knowledge-based approach (Hirst and Budanitsky, 2005).4.1 Statistical ApproachMays et al(1991) introduced an approach basedon the noisy-channel model.
The model assumesthat the correct sentence s is transmitted througha noisy channel adding ?noise?
which results in aword w being replaced by an error e leading thewrong sentence s?
which we observe.
The prob-ability of the correct word w given that we ob-serve the error e can be computed as P (w|e) =P (w) ?
P (e|w).
The channel model P (e|w) de-scribes how likely the typist is to make an error.This is modeled by the parameter ?.10 The re-maining probability mass (1 ?
?)
is distributedequally among all words in the vocabulary withinan edit distance of 1 (edits(w)):P (e|w) ={?
if e = w(1?
?
)/|edits(w)| if e 6= wThe source model P (w) is estimated using atrigram language model, i.e.
the probability of the8http://www.archive.org/details/BrownCorpus (CC-by-na).9http://www.ims.uni-stuttgart.de/projekte/TIGER/The corpus contains 50,000 sentences of German newspapertext, and is freely available under a non-commercial license.10We optimize ?
on a held-out development set of errors.532intended word wi is computed as the conditionalprobability P (wi|wi?1wi?2).
Hence, the proba-bility of the correct sentence s = w1 .
.
.
wn canbe estimated asP (s) =n+2?i=1P (wi|wi?1wi?2)The set of candidate sentences Sc contains all ver-sions of the observed sentence s?
derived by re-placing one word with a word from edits(w),while all other words in the sentence remainunchanged.
The correct sentence s is thosesentence from Sc that maximizes P (s|s?)
=argmaxs?Sc P (s) ?
P (s?|s).4.2 Knowledge Based ApproachHirst and Budanitsky (2005) introduced aknowledge-based approach that detects real-wordspelling errors by checking the semantic relationsof a target word with its context.
For this pur-pose, they apply WordNet as the source of lexical-semantic knowledge.The algorithm flags all words as error can-didates and then applies filters to remove thosewords from further consideration that are unlikelyto be errors.
First, the algorithm removes allclosed-class word candidates as well as candi-dates which cannot be found in the vocabulary.Candidates are then tested for having lexical co-hesion with their context, by (i) checking whetherthe same surface form or lemma appears again inthe context, or (ii) a semantically related conceptis found in the context.
In both cases, the candi-date is removed from the list of candidates.
Foreach remaining possible real-word spelling error,edits are generated by inserting, deleting, or re-placing characters up to a certain edit distance(usually 1).
Each edit is then tested for lexicalcohesion with the context.
If at least one of it fitsinto the context, the candidate is selected as a real-word error.Hirst and Budanitsky (2005) use two additionalfilters: First, they remove candidates that are?common non-topical words?.
It is unclear howthe list of such words was compiled.
Their listof examples contains words like ?find?
or ?world?which we consider to be perfectly valid candi-dates.
Second, they also applied a filter using alist of known multi-words, as the probability forwords to accidentally form multi-words is low.Dataset P R FArtificial-English .77 .50 .60Natural-English .54 .26 .35Artificial-German .90 .49 .63Natural-German .77 .20 .32Table 1: Performance of the statistical approach usinga trigram model based on Google Web1T.It is unclear which list was used.
We could usemulti-words from WordNet, but coverage wouldbe rather limited.
We decided not to use both fil-ters in order to better assess the influence of theunderlying semantic relatedness measure on theoverall performance.The knowledge based approach uses semanticrelatedness measures to determine the cohesionbetween a candidate and its context.
In the exper-iments by Budanitsky and Hirst (2006), the mea-sure by (Jiang and Conrath, 1997) yields the bestresults.
However, a wide range of other measureshave been proposed, cf.
(Zesch and Gurevych,2010).
Some measures using a wider defini-tion of semantic relatedness (Gabrilovich andMarkovitch, 2007; Zesch et al 2008b) insteadof only using taxonomic relations in a knowledgesource.As semantic relatedness measures usually re-turn a numeric value, we need to determine athreshold ?
in order to come up with a binaryrelated/unrelated decision.
Budanitsky and Hirst(2006) used a characteristic gap in the stan-dard evaluation dataset by Rubenstein and Good-enough (1965) that separates unrelated from re-lated word pairs.
We do not follow this approach,but optimize the threshold on a held-out develop-ment set of real-word spelling errors.5 Results & DiscussionIn this section, we report on the results obtainedin our evaluation of contextual fitness measuresusing artificial and natural errors in English andGerman.5.1 Statistical ApproachTable 1 summarizes the results obtained by thestatistical approach using a trigram model basedon the Google Web1T data (Brants and Franz,2006).
On the English artificial errors, we ob-serve a quite high F-measure of .60 that drops to533Dataset N-gram model Size P R FArt-EnGoogle Web7 ?
1011 .77 .50 .607 ?
1010 .78 .48 .597 ?
109 .76 .42 .54Wikipedia 2 ?
109 .72 .37 .49Nat-EnGoogle Web7 ?
1011 .54 .26 .357 ?
1010 .51 .23 .317 ?
109 .46 .19 .27Wikipedia 2 ?
109 .49 .19 .27Art-DeGoogle Web8 ?
1010 .90 .49 .638 ?
109 .90 .47 .618 ?
108 .88 .36 .51Wikipedia 7 ?
108 .90 .37 .52Nat-DeGoogle Web8 ?
1010 .77 .20 .328 ?
109 .68 .14 .238 ?
108 .65 .10 .17Wikipedia 7 ?
108 .70 .13 .22Table 2: Influence of the n-gram model on the perfor-mance of the statistical approach..35 when switching to the naturally occurring er-rors which we extracted from Wikipedia.
On theGerman dataset, we observe almost the same per-formance drop (from .63 to .32).These observations correspond to our earlieranalysis where we showed that the artificial datacontains many cases that are quite easy to correctusing a statistical model, e.g.
where a plural formof a noun is replaced with its singular form (orvice versa) as in ?I bought a car.?
vs. ?I boughta cars.?.
The naturally occurring errors often con-tain much harder contexts, as shown in the fol-lowing example: ?Through the open window theyheard sounds below in the street: cartwheels, atired horse?s plodding step, vices.?
where ?vices?should be corrected to ?voices?.
While the lemma?voice?
is clearly semantically related to otherwords in the context like ?hear?
or ?sound?, theposition at the end of the sentence is especiallydifficult for the trigram-based statistical approach.The only trigram that connects the error to thecontext is (?step?, ?,?, vices/voices) which willprobably yield a low frequency count even forvery large trigram models.
Higher order n-grammodels would help, but suffer from the usual data-sparseness problems.Influence of the N-gram Model For buildingthe trigram model, we used the Google Web1Tdata, which has some known quality issues and isDataset P R FArtificial-English .26 .15 .19Natural-English .29 .18 .23Artificial-German .47 .16 .24Natural-German .40 .13 .19Table 3: Performance of the knowledge-based ap-proach using the JiangConrath semantic relatednessmeasure.not targeted towards the Wikipedia articles fromwhich we sampled the natural errors.
Thus, wealso tested a trigram model based on Wikipedia.However, it is much smaller than the Web model,which leads us to additionally testing smaller Webmodels.
Table 2 summarizes the results.We observe that ?more data is better data?
stillholds, as the largest Web model always outper-forms the Wikipedia model in terms of recall.
Ifwe reduce the size of the Web model to the sameorder of magnitude as the Wikipedia model, theperformance of the two models is comparable.We would have expected to see better results forthe Wikipedia model in this setting, but its higherquality does not lead to a significant difference.Even if statistical approaches quite reliably de-tect real-word spelling errors, the size of the re-quired n-gram models remains a serious obstaclefor use in real-world applications.
The EnglishWeb1T trigram model is about 25GB, which cur-rently is not suited for being applied in settingswith limited storage capacities e.g.
for intelligentinput assistance in mobile devices.
As we haveseen above, using smaller models will decreaserecall to a point where hardly any error will be de-tected anymore.
Thus, we will now have a look onknowledge-based approaches which are less de-manding in terms of the required resources.5.2 Knowledge-based ApproachTable 3 shows the results for the knowledge-basedmeasure.
In contrast to the statistical approach,the results on the artificial errors are not higherthan on the natural errors, but almost equal forGerman and even lower for English; another pieceof evidence supporting our view that the proper-ties of artificial datasets over-estimate the perfor-mance of statistical measures.Influence of the Relatedness Measure As waspointed out before, Budanitsky and Hirst (2006)534Dataset Measure ?
P R FArt-EnJiangConrath 0.5 .26 .15 .19Lin 0.5 .22 .17 .19Lesk 0.5 .19 .16 .17ESA-Wikipedia 0.05 .43 .13 .20ESA-Wiktionary 0.05 .35 .20 .25ESA-Wordnet 0.05 .33 .15 .21Nat-EnJiangConrath 0.5 .29 .18 .23Lin 0.5 .26 .21 .23Lesk 0.5 .19 .19 .19ESA-Wikipedia 0.05 .48 .14 .22ESA-Wiktionary 0.05 .39 .21 .27ESA-Wordnet 0.05 .36 .15 .21Table 4: Performance of knowledge-based approachusing different relatedness measures.show that the measure by Jiang and Conrath(1997) yields the best results in their experi-ments on malapropism detection.
In addition, wetest another path-based measure by Lin (1998),the gloss-based measure by Lesk (1986), andthe ESA measure (Gabrilovich and Markovitch,2007) based on concept vectors from Wikipedia,Wiktionary, and WordNet.
Table 4 summarizesthe results.
In contrast to the findings of Budanit-sky and Hirst (2006), JiangConrath is not the bestpath-based measure, as Lin provides equal or bet-ter performance.
Even more importantly, other(non path-based) measures yield better perfor-mance than both path-based measures.
EspeciallyESA based on Wiktionary provides a good over-all performance, while ESA based on Wikipediaprovides excellent precision.
The advantage ofESA over the other measure types can be ex-plained with its ability to incorporate semantic re-lationships beyond classical taxonomic relations(as used by path-based measures).5.3 Combining the ApproachesThe statistical and the knowledge-based approachuse quite different methods to assess the con-textual fitness of a word in its context.
Thismakes it worthwhile trying to combine both ap-proaches.
We ran the statistical method (using thefull Wikipedia trigram model) and the knowledge-based method (using the ESA-Wiktionary related-ness measure) in parallel and then combined theresulting detections using two strategies: (i) wemerge the detections of both approaches in orderto obtain higher recall (?Union?
), and (ii) we onlyDataset Comb.-Strategy P R FArtificial-EnglishBest-Single .77 .50 .60Union .52 .55 .54Intersection .91 .15 .25Natural-EnglishBest-Single .54 .26 .35Union .40 .36 .38Intersection .82 .11 .19Table 5: Results obtained by a combination of the beststatistical and knowledge-based configuration.
?Best-Single?
is the best precision or recall obtained by a sin-gle measure.
?Union?
merges the detections of bothapproaches.
?Intersection?
only detects an error if bothmethods agree on a detection.count an error as detected if both methods agreeon a detection (?Intersection?).
When compar-ing the combined results in Table 5 with the bestprecision or recall obtained by a single measure(?Best-Single?
), we observe that precision can besignificantly improved using the ?Union?
strategy,while recall is only moderately improved usingthe ?Intersect?
strategy.
This means that (i) a largesubset of errors is detected by both approachesthat due to their different sources of knowledgemutually reinforce the detection leading to in-creased precision, and (ii) a small but otherwiseundetectable subset of errors requires consideringdetections made by one approach only.6 Related WorkTo our knowledge, we are the first to create adataset of naturally occurring errors based on therevision history of Wikipedia.
Max and Wis-niewski (2010) used similar techniques to createa dataset of errors from the French Wikipedia.However, they target a wider class of errors in-cluding non-word spelling errors, and their classof real-word errors conflates malapropisms aswell as other types of changes like reformulations.Thus, their dataset cannot be easily used for ourpurposes and is only available in French, whileour framework allows creating datasets for all ma-jor languages with minimal manual effort.Another possible source of real-word spellingerrors are learner corpora (Granger, 2002), e.g.the Cambridge Learner Corpus (Nicholls, 1999).However, annotation of errors is difficult andcostly (Rozovskaya and Roth, 2010), only a smallfraction of observed errors will be real-wordspelling errors, and learners are likely to make dif-535ferent mistakes than proficient language users.Islam and Inkpen (2009) presented another sta-tistical approach using the Google Web1T data(Brants and Franz, 2006) to create the n-grammodel.
It slightly outperformed the approach byMays et al(1991) when evaluated on a corpus ofartificial errors based on the WSJ corpus.
How-ever, the results are not directly comparable, asMays et al(1991) used a much smaller n-grammodel and our results in Section 5.1 show thatthe size of the n-gram model has a large influenceon the results.
Eventually, we decided to use theMays et al(1991) approach in our study, as it iseasier to adapt and augment.In a re-evaluation of the statistical model byMays et al(1991), Wilcox-OHearn et al(2008)found that it outperformed the knowledge-basedmethod by Hirst and Budanitsky (2005) whenevaluated on a corpus of artificial errors based onthe WSJ corpus.
This is consistent with our find-ings on the artificial errors based on the Browncorpus, but - as we have seen in the previous sec-tion - evaluation on the naturally occurring errorsshows a different picture.
They also tried to im-prove the model by permitting multiple correc-tions and using fixed-length context windows in-stead of sentences, but obtained discouraging re-sults.All previously discussed methods are unsuper-vised in a way that they do not rely on any trainingdata with annotated errors.
However, real-wordspelling correction has also been tackled by su-pervised approaches (Golding and Schabes, 1996;Jones and Martin, 1997; Carlson et al 2001).Those methods rely on predefined confusion-sets,i.e.
sets of words that are often confounded e.g.
{peace, piece} or {weather, whether}.
For eachset, the methods learn a model of the context inwhich one or the other alternative is more proba-ble.
This yields very high precision, but only forthe limited number of previously defined confu-sion sets.
Our framework for extracting naturalerrors could be used to increase the number ofknown confusion sets.7 Conclusions and Future WorkIn this paper, we evaluated two main approachesfor measuring the contextual fitness of terms: thestatistical approach by Mays et al(1991) andthe knowledge-based approach by Hirst and Bu-danitsky (2005) on the task of detecting real-word spelling errors.
For that purpose, we ex-tracted a dataset with naturally occurring errorsand their contexts from the Wikipedia revisionhistory.
We show that evaluating measures of con-textual fitness on this dataset provides a more re-alistic picture of task performance.
In particular,using artificial datasets over-estimates the perfor-mance of the statistical approach, while it under-estimates the performance of the knowledge-based approach.We show that n-gram models targeted towardsthe domain from which the errors are sampleddo not improve the performance of the statisti-cal approach if larger n-gram models are avail-able.
We further show that the performance ofthe knowledge-based approach can be improvedby using semantic relatedness measures that in-corporate knowledge beyond the taxonomic rela-tions in a classical lexical-semantic resource likeWordNet.
Finally, by combining both approaches,significant increases in precision or recall can beachieved.In future work, we want to evaluate a widerrange of contextual fitness measures, and learnhow to combine them using more sophisticatedcombination strategies.
Both - the statistical aswell as the knowledge-based approach - will ben-efit from a better model of the typist, as not alledit operations are equally likely (Kernighan etal., 1990).
On the side of the error extraction, weare going to further improve the extraction pro-cess by incorporating more knowledge about therevisions.
For example, vandalism is often re-verted very quickly, which can be detected whenlooking at the full set of revisions of an article.We hope that making the experimental frame-work publicly available will foster future researchin this field, as our results on the natural errorsshow that the problem is still quite challenging.AcknowledgmentsThis work has been supported by the Volk-swagen Foundation as part of the Lichtenberg-Professorship Program under grant No.
I/82806.We Andreas Kellner and Tristan Miller for check-ing the datasets, and the anonymous reviewers fortheir helpful feedback.536ReferencesDavid Bean and Ellen Riloff.
2004.
Unsupervisedlearning of contextual role knowledge for corefer-ence resolution.
In Proc.
of HLT/NAACL, pages297?304.Igor A. Bolshakov and Alexander Gelbukh.
2003.
OnDetection of Malapropisms by Multistage Colloca-tion Testing.
In Proceedings of NLDB-2003, 8thInternational Workshop on Applications of NaturalLanguage to Information Systems, number Cic.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram Version 1.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Andrew J Carlson, Jeffrey Rosen, and Dan Roth.2001.
Scaling Up Context-Sensitive Text Correc-tion.
In Proceedings of IAAI.C Fellbaum.
1998.
WordNet An Electronic LexicalDatabase.
MIT Press, Cambridge, MA.Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.2011.
Wikipedia Revision Toolkit: EfficientlyAccessing Wikipedia?s Edit History.
In Proceed-ings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human Lan-guage Technologies.
System Demonstrations, pages97?102, Portland, OR, USA.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics -ACL ?05, pages 363?370, Morristown, NJ, USA.Association for Computational Linguistics.Francis W. Nelson and Henry Kuc?era.
1964.
Manualof information to accompany a standard corpus ofpresent-day edited American English, for use withdigital computers.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.
In Proceedingsof the 20th International Joint Conference on Arti-ficial Intelligence, pages 1606?1611.Andrew R. Golding and Yves Schabes.
1996.
Com-bining Trigram-based and feature-based methodsfor context-sensitive spelling correction.
In Pro-ceedings of the 34th annual meeting on Associationfor Computational Linguistics -, pages 71?78, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Sylviane Granger, 2002.
A birds-eye view of learnercorpus research, pages 3?33.
John Benjamins Pub-lishing Company.Graeme Hirst and Alexander Budanitsky.
2005.
Cor-recting real-word spelling errors by restoring lex-ical cohesion.
Natural Language Engineering,11(1):87?111, March.Diana Inkpen and Alain De?silets.
2005.
Semanticsimilarity for detecting recognition errors in auto-matic speech transcripts.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing -HLT ?05, number October, pages 49?56, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Aminul Islam and Diana Inkpen.
2009.
Real-wordspelling correction using Google Web IT 3-grams.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing Vol-ume 3 - EMNLP ?09, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.M A Jaro.
1995.
Probabilistic linkage of large publichealth data file.
Statistics in Medicine, 14:491?498.Jay J Jiang and David W Conrath.
1997.
Seman-tic Similarity Based on Corpus Statistics and Lex-ical Taxonomy.
In Proceedings of the 10th Inter-national Conference on Research in ComputationalLinguistics, Taipei, Taiwan.Michael P Jones and James H Martin.
1997.
Contex-tual spelling correction using latent semantic analy-sis.
In Proceedings of the fifth conference on Ap-plied natural language processing -, pages 166?173, Morristown, NJ, USA.
Association for Com-putational Linguistics.Mark D Kernighan, Kenneth W Church, andWilliam A Gale.
1990.
A Spelling Correc-tion Program Based on a Noisy Channel Model.In Proceedings of the 13th International Confer-ence on Computational Linguistics, pages 205?210,Helsinki, Finland.Lothar Lemnitzer and Claudia Kunze.
2002.
Ger-maNet - Representation, Visualization, Application.In Proceedings of the 3rd International Conferenceon Language Resources and Evaluation (LREC),pages 1485?1491.M Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: how to tell a pinecone from an ice cream cone.
Proceedings of the5th annual international conference, pages 24?26.Dekang Lin.
1998.
An Information-Theoretic Defini-tion of Similarity.
In Proceedings of InternationalConference on Machine Learning, pages 296?304,Madison, Wisconsin.Aurelien Max and Guillaume Wisniewski.
2010.Mining Naturally-occurring Corrections and Para-phrases from Wikipedias Revision History.
In Pro-ceedings of the Seventh conference on InternationalLanguage Resources and Evaluation (LREC?10),pages 3143?3148.Eric Mays, Fred.
J Damerau, and Robert L Mercer.1991.
Context based spelling correction.
Informa-tion Processing & Management, 27(5):517?522.537Rani Nelken and Elif Yamangil.
2008.
MiningWikipedia?s Article Revision History for Train-ing Computational Linguistics Algorithms.
InProceedings of the AAAI Workshop on Wikipediaand Artificial Intelligence: An Evolving Synergy(WikiAI), WikiAI08.Diane Nicholls.
1999.
The Cambridge Learner Cor-pus - Error Coding and Analysis for Lexicographyand ELT.
In Summer Workshop on Learner Cor-pora, Tokyo, Japan.Alla Rozovskaya and Dan Roth.
2010.
AnnotatingESL Errors: Challenges and Rewards.
In The 5thWorkshop on Innovative Use of NLP for BuildingEducational Applications (NAACL-HLT).H Rubenstein and J B Goodenough.
1965.
ContextualCorrelates of Synonymy.
Communications of theACM, 8(10):627?633.Helmut Schmid.
2004.
Efficient Parsing of HighlyAmbiguous Context-Free Grammars with Bit Vec-tors.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COL-ING 2004), Geneva, Switzerland.Daniel D. Walker, William B. Lund, and Eric K. Ring-ger.
2010.
Evaluating Models of Latent DocumentSemantics in the Presence of OCR Errors.
Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, (October):240?250.M.
Wick, M. Ross, and E. Learned-Miller.
2007.Context-sensitive error correction: Using topicmodels to improve OCR.
In Ninth InternationalConference on Document Analysis and Recogni-tion (ICDAR 2007) Vol 2, pages 1168?1172.
Ieee,September.Amber Wilcox-OHearn, Graeme Hirst, and AlexanderBudanitsky.
2008.
Real-word spelling correctionwith trigrams: A reconsideration of the Mays, Dam-erau, and Mercer model.
In Proceedings of the 9thinternational conference on Computational linguis-tics and intelligent text processing (CICLing).Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: unsupervised extraction of lexical simplifi-cations from Wikipedia.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, HLT ?10, pages 365?368.Torsten Zesch and Iryna Gurevych.
2010.
Wisdomof Crowds versus Wisdom of Linguists - Measur-ing the Semantic Relatedness of Words.
Journal ofNatural Language Engineering, 16(1):25?59.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008a.
Extracting Lexical Semantic Knowledgefrom Wikipedia and Wiktionary.
In Proceedings ofthe Conference on Language Resources and Evalu-ation (LREC).Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008b.
Using wiktionary for computing semanticrelatedness.
In Proceedings of the 23rd AAAI Con-ference on Artificial Intelligence, pages 861?867,Chicago, IL, USA, Jul.538
