Guiding a Wel l -Founded Parser with Corpus Statist icsAmon Seagu l l  and Lenhar t  Schuber tDepartment  of Computer  ScienceUniversity of RochesterRochester, NY 14627{seagull, schubert }@cs.
rochester, eduAbst rac tWe present a parsing system built from a hand-written lexicon ~ and grammar, and trained ona selection of the Brown Corpus.
On the sen-tences it can parse, the parser performs as wellas purely corpus-based parsers.
Its advantagelies in the fact that its syntactic analyses read-ily support semantic interpretation.
Moreover,the system's hand-written foundation allows fora more fully lexicalized probabilistic model, i.e.one sensitive to co-occurrence of lexical headsof phrase constituents.1 In t roduct ionStatistical approaches to parsing have receiveda great deal of attention over recent years.The availability of large tagged and syntac-tically bracketed corpora make the program-matic extraction of lexica and grammars fea-sible.
Researchers have tackled parsing by sub-stituting these automatically derived resourcesfor hand-coded ones.
While these approacheshave had som e success to date (Collins, 1997;Charniak, 1997a), their usability as parsers insystems for natural language understanding issuspect.
1 The 'reconstruction f Treebank-stylebracketings does not serve as an adequate basisfor semantic interpretation.
The phrase struc-ture rules are too numerous, and the analy-ses too coarse (especially at the lower levels)to allow association of deterministic semanticrules with ph~:ase structure rules.
Chaxniakhimself (1997b) notes that most of the parsesconstructed by a "wide-coverage" grammar axe"pretty senseless".1Collins, Ch~niak, etc.
make no claims about theirprograms being Well suited as parsers for language un-derstanding applications.
Certainly, this type of parsinghas had success t'o-date in applications such as Informa-tion Retrieval.As an example, consider the fiat NP structuresthat are in the Penn Treebank (Marcus et al,1993).
Nouns, determiners, and adjectives areall sisters of each other in the syntactic anno-tation, e.g.
(NP (DT the)  (J J  mechanical)(NN engineering) (NN industry)).
A parserwhich constructs structures such as this fails tosolve an ambiguity problem that has generallybeen considered syntactic: Are we talking aboutthe industry of mechanical engineering, or isthe entire engineering industry perceived as me-chanical?
If our goal is language understanding,including semantic interpretation, the Treebankbracketings must be considered underspecified.We describe here a system which combineshand-coded linguistic resources with corpus-derived probabilistic information to enable(fairly) wide-coverage syntactic parsing.
Mostimportantly, the use of these linguistic resourcesallows for a better-informed probabilistic model.2 SetupOur lexicon is composed from two resources.COMLEX (Grishman et al, 1994) providesthe syntactic and morphological information for39,000 lemmas.
WordNet (Fellbaum, 1998) pro-vides the semantic information.
In addition, weadd to our lexicon approximately 47,000 "multi-word" nouns found in WordNet.SAPIR, the parser we are using, employs afeature-based general grammar of English thathas been in development a The Boeing Com-pany over the past fifteen years (Harrison andMaxwell, 1986).
The grammar consists of ap-proximately 500 rules.
By mapping COM-LEX's lexical entries into a format understand,able by SAPIR, we have a general purpose, well-founded, English language parser.With such a parser, we can use Penn's Tree-bank the way it was probably intended: as a set179of bracketing constraints for the syntactic anal-ysis of a sentence.
The Linguistic Data Con-sortium provides a preliminary version (1.075)of the Treebank's bracketing of the Brown Cor-pus (Ku~era and Francis, 1967).
Fortunately,SAPIR provides an interface whereby a sentenceand a partially specified parse tree can be fed tothe parser, so that only the syntactic analysesthat conform with the provided bracketing willbe pursued.So it is with this mechanism that we cre-ate our corpus.
We start with the brack-eted (but not part-of-speech-tagged) versionof the Treebank, and process the bracketingsin several ways, most notably removing quo-tation marks and "assuaging" gaps.
The lat-ter consists primarily of identifying the Tree-bank's sentential constructs which are con-sidered verb phrases by SAPIR, and makingthat transformation.
For example, a Tree-bank tree like (PP in (S (NP *) (VP going(NP home)  ) ) would be mapped to (PP in(VP going (NP home)  ).
Since this bracketingis supplied to SAPIR as a constraint, the parseris free to construct the gerundive NP containingsolely the VP.
In fact, the bracketing correspond-ing to the parse found by SAPIR is:(PP (P IN)(NP(VP (V GOZNG)(NP (N" (N HOME))))))(Note that postfix " indicates a one-bar levelphrase, as per X-theory (Jackendoff, 1977).
)Approximately 30% of the Treebank bracket-ings are parseable, after this assuagement, byour parser.
These 30% comprise our corpus.Of course, each (parseable) bracketing doesnot always yield just one parse.
SAPIR hassome hand-coded costs on syntactic rules whichhave served as its preference mechanism to-date.When SAPIR finds more than one parse for agiven bracketing, we simply choose its most pre-ferred one to use in our corpus.
While we cer-tainly do not feel that this is the best way tocreate our corpus, we would like to note thatover 25% of the parseable bracketings yield aunique parse, and over 50% have just one ortwo possible parses.
We should also note thatthe parseable bracketings are, of course, shorter(on average) than the unparseable ones.
The av-erage length of all of the sentences i  17.3 words,while the average for our corpus is 11.2.3 Language Mode lWe noted above that we would like a more com-plete lexicalization than what has been used byrecent models in statistical parsing.
To this end,we propose a generative model which is a directextension of a Probabilistic Context Free Gram-mar (PCFG).
In our model, as in a PCFG, thesentence is generated via top-down expansionof its parse tree, beginning with the root node.The crucial difference, however, is that as weexpand from a nonterminal to its children, wesimultaneously generate both the syntactic at-egory and the head word of each child.
This ex-pansion is predicated on the category and headword of the mother.We will also make the traditional assumptionthat all sentences are generated independentlyof each other.
Then, under this assumption andthe assumed model, we can write the probabilityof a particular parse tree T as the product of allits expansions, orP(T} = H P{Y,h, rulename IX,w} (I)expansion ETwhere X and w are the syntactic ategory andhead word of the mother node, and Yi and h.tare the syntactic ategory and head word of theith daughter, rulename is the identifier for therule that licenses the expansion.
(Of course, allof these terms should be indexed appropriatelyfor the expansion under consideration, but weleave that off for clarity.)
Note that rulenameusually determines X, and rulename and X to-gether always determine Y.
Also note that eachtree is assumed to be rooted at a dummy UTTnode (with a dummy WORD head word)~ whichserves as the parent for the "true root" of thetree.We can expand (1) via the chain rule:P(T) = H P(rulename IX,w)expansion 6 T?
P(hl X,w, rulename)(2)Note we have dropped Y from the equations,since as noted above, that sequence is deter-mined by rulename and X.
This is an appealing180rewriting, since the first term of (2), which wewill term the syntactic expansion probability,corresponds neatly to the theory of Lexical Pref-erence for those rules whose head constituent isa lexical category.
Consider the following sen-tences, from Ford et al (1982)(1) a. Mary wanted the dress on that rackb.
Mary PoSitioned the dress on that rackLP predicts that the preferred interpretation forthe first sentence is the (NP the dress  on thatrack} structure, while for the second, a readerwould prefer the!
flat V-NP-PP structure.
Thisfollows from the~ theory of Lexical Preference,which stipulates that the head word of a phraseselects for its "most preferred" syntactic expan-sion.
This is exactly what is modeled by the firstterm of Equation (2).
"Lexical preference" hasbeen around a long time, and the (correspond-ing) syntactic expansion probability we use hasbeen used by many researchers in parsing, in-cluding many of those mentioned in this article.The difficulty iwith this model, and perhapsthe reason it has not been pursued to-date, isthe intense data  sparsity problem encounteredin estimating the second term of equation (2),the lexical introduction probability.
Much workin statistical parsing limits all probabilities to"binary" lexical statistics, where for any proba-bility of the form P(X1,... ,Xrt I Y1, ... ,Yr~), atmost one of the X random variables and one ofthe Y random variables is lexical.
By allowing"vt-ary" lexical statistics, we allow an explosionof the probability space.Nevertheless, we suggest hat human parsingis responsive to 'the familiarity (or otherwise)of particular head patterns in rules.
To com-bat the data sparsity, we have used WordNetto "back off" to more general semantic lasseswhen statistics on a word are unavailable.
Toback off semantically, however, we need to bedealing with word senses, not just word forms.It is a simple refinement of our model to replaceall instances of "head word" with "head sense".Additionally, a semantic oncordance of a sub-set of the Brown.
Corpus and WordNet senses isavailable (Lande's et al, 1998).
Thus, our cor-pus, collected as described in Section 2, can beaugmented to use statistics on word senses insyntactic onstructs, after alignment of this se-mantic concordance (of Brown) with treebank'slabeled bracketing (of Brown).
Moreover, a lan-guage model that distinguishes word senses willtend to reflect the semantic as well as syntac-tic and lexical patterns in language, and thusshould be advantageous both in training themodel and in using it for parsing.4 Es t imat ionWe have used WordNet senses o that we mightcombat the data spaxsity we encounter whentrying to calculate the probabilities in Equa-tion (2).
Specifically, we have employed a "se-mantic backoff" in estimating the probabilities,where the backing off is done by ascending inthe WordNet hierarchy (following its hypernymrelations).
When attempting to calculate theprobability of a syntactic expansion - -  the prob-ability of a category X with head sense w ex-panding as rulename - -  we search, breadth-first, for the first hypernym of w in Word-Net which occurred with X at least t times inour training data, where t is some thresholdvalue.
So the probability P(rulename I X,w)P(rulename \[ X,p(w)), where p(w) denotesthe hypernym we found.Similarly, for the probability of lexical intro-duction, we abstract he tuple (X,w, rulename)to a tuple (X, p~(w), rulename) which occurredsufficiently often.
Once this is found, we searchupward, again breadth-first, 2 for some abstrac-tion ~(h) of h which appeared at least once inthe context of the adequate conditioning infor-mation.
Each a(h4) is some parent of the wordsense h4.
The probability of each original wordh4 is then conditioned on the appropriate hyper-nym of the found abstraction.
So we approxi-mate:P(hl  X,w, rulename) ,~P(d.(~t) I (X,p'(w), rulename))x \ ] - I  P(h~l a(~)) (3)Note that p(w) in the first estimation maynot equal p~(w) in the second.
Also note thatbacking off completely, to the TOP of the ontol-ogy, for the word in the conditioning informa-tion, is equivalent o dropping it from the condi-tioning information.
Backing off completely in2Breadth-f i rst  is a first approx imat ion as the searchmechanism; we intend to pursue this  issue in future work.181search for the abstraction when calculating theprobability of lexical introduction effectively re-duces that probability to I-It P(ht) .35 Exper imenta l  Resu l tsWe sequestered 421 sentences from our corpusof 4892 sentences (with trees and sense-tags),and used the balance for training the probabil-ities in equation (2).
These 4892 are the parse-able segment of the 16,374 trees for which wewere able to "match up" the Treebank syntac-tic annotation with the semantic oncordance.
(Random errors and inconsistencies seem to ac-count for why not all 19,843 trees align.
In fact,these 19,843 themselves exclude all trees whichappear to be headlines or some other irregulartext.
We do not, however, exclude any treeson the basis of the type of their root category.The corpus contains entences as well as verbphrases, noun phrases, etc..)We then tested the parser varying two binaryparameters:?
whether or not the semantic backoff pro-cedure was used - -  If not, an unobservedconditioning event would immediately haveus drop the lexical information.
For exam-ple, (X,w / would immediately be backedoff to simply (X).?
whether or not we simply estimated thejoint probability P(~tl X,w, rulename) asI-~i y (kt \] X, w, rulename ).
This we will callthe "binary" assumption, as opposed to"rt-ary".
Effectively, it means that eachdaughter's head word sense is introducedindependently of the others.Tables 1 and 2 display the results for thefour different settings, along with the results fora straight PCFG model (as a baseline).
Notethat t, our threshold parameter from above, wasset to 10 for these experiments.
Labeled pre-cision and recall (Table 1) are the same as inother reports on statistical parsing: they mea-sure how often a particular syntactic ategorywas correctly calculated to span a particularportion of the input.
Recall that our corpusaWe actually stop short of this in our estimations.
Wesearch upward for the top-most nodes in WordNet, butwe do not continue to the synthetic T0P node.
Instead,we drop the lexeme from the conditioning informationand restart he search.was derived using a hand-crafted grammar.
Itmakes sense, then, to add an additional crite-rion for correctness: we can check the actual ex-pansions (rulenames) used and see if they werecorrect.
This metric speaks to an issue raisedby Charniak (1997b) when he notes that therule NP -> NP NP has (at least) two differentinterpretations: one for appositive NPs and onefor "unit" phrases like "5 dollars a share".4 Ahand-written grammar will differentiate thesetwo constructions.
Thus Table 2 shows preci-sion and recall figures for this more strict crite-rion, for the four models in question plus PCFGagain as a baseline.
Note also that since Table 2is for syntactic expansions, it does not includelexical evel bracketings.Sem backoff No sem backoffbinary 91.2/87.1 90.6/86.4rt-ary 91.3/87.8 90.1/86.2PCFGTable 1:Results78.9/80.3Labeled Bracketing Precision/Recallbinaryn-aryPCFGSere backoff No sem backoff82.5/78.8 81.3/77.682.7/79.6 80.6/77.365.5/66.3Table 2: Syntactic Expansion Precision/RecallResultsFirst note that the degree of improvementover baseline of even the most minimal modelis approximately what other researchers, usingpurely corpus-driven techniques, have reported(Charniak, 1997a).Also note that the "full" model, using both rL-ary lexical statistics and semantic backoff, per-forms (statistically) significantly better than bothof the models which do not use semantic back-off.
The lone exception is that the precision ofthe labeled bracketings i not significantly dif-ferent for the "full" model and the "minimal"model.
54In fact there should be syntactic differences for thesetwo constructions, since phrases like "the dollars theshare" are syntactically ill-formed unit noun phrases.~Two-sided tests were used, with o?
= 0.05.182Interestingly, the "minimal" model is not sig-nificantly different from either of the two mod-els gotten by adding one o/ rt-axy statistics orsemantic backoff.
The improvement is only sig-nificant when both features are added.Our results for word sense disambiguation (ob-tained as a by-product of parsing) are shown inTable 3.
Clearly, using WordNet to back offsemantically enables the parser to do a betterjob at getting Senses right.
The sense recallfigures for the two models which use semanticbackoff are significantly better than for thosemodels which do not.
Additionally, the im-provement over baseline is significantly betterfor those models which use semantic backoff (11percentage points improvement) than for thosewhich do not (4 points better).
6Sem backoff No sem backoffbinary 40.8/51.6 40.8/45.0u-ary 41'.1/52.1 40.8/45.1Table 3: Sense Recall: baseline/model.
Thebaseline results:are gotten by choosing the mostfrequent sense for the word, given the part ofspeech assigned by the parser.
(Hence it may bedifferent across,different models for the parser.
)6 Re la ted  WorkAs our framework and corpus are rather differ-ent from other work on parsing and sense dis-ambiguation, it is difficult to make quantitativecomparisons.
Many researchers have achievedsense disambiguation rates above 90% (e.g.
Galeet al (1992)),: but this work has typically fo-cussed on disambiguating a few polysemous wordswith "coarse" sense distinctions using a largecorpus.
Here, we are disambiguating all wordswith WordNet isenses and not very much data.Ng and Lee (1996) report results on disambiguat-?6The improvement gotten for moving from binary tort-ary relations, when using WordNet, is not significant.This is most likely due to the small percentage of expan-sions which are likely to be helped by rt-ary statistics - -less than 1%.
In fact, there were only seven instances,over the 421-sentence t st set, where an n-ary rule wascorrectly selectedl by the parser and the head of thatphrase was also 'correctly selected.
Given such smallnumbers, we would not expect to see a significant im-provement, when using n-ary statistics, for word sensedisambiguation.ing among WordNet senses for the most fre-quent 191 nouns and verbs (together, they ac-count for 20% of all nouns and verbs we ex-pect to encounter in a random selection of text).They get an improvement of 6.9 percentage points(54.0 over 47.1 percent) in disambiguating in-stances of these words in the Brown Corpus.Since the most frequent words are typically themost polysemous, the ambiguity problem is moresevere for this subset, but there is also moredata: we have about 24,000 instances of 10,000distinct senses in our corpus, and Ng and Lee(1996) use 192,800 occurrences of their 191 words.Carroll et al (1998) report results on a par-ser, similarly based on linguistically well-foundedresources, using corpus-derived subcategoriza-tion probabilities (the first term in Equation (2)).They report a significant increase in parsing ac-curacy, measured using a system of grammat-ical relations.
Their corpus is annotated withgrammatical relations like subj and ccomp, andthe parser can then output these relations asa component of a parse.
Carroll et al (1998)argue that these relations enable a more accu-rate metric for parsing than labeled bracketingand recall.
Our evaluation of phrase structurerules used in a parse is a crude attempt at thishigher-level valuation.As mentioned above, much recent work onlexicalizing parsers has focused on binary lexi-cal relations, specifically head-head relations ofmother and daughter constituents e.g.
(Carrolland Rooth, 1998; Collins, 1996).
Some haveused word classes to combat he sparsity prob-lem (Charniak, 1997a).
Link grammars allowfor a probabilistic model with ternary head-head-head relations (Lafferty et al, 1992).
The linkgrammar website reports that, on a test of theirparser on 100 sentences (average l ngth 25 words)of Wall Street Journal text, over 82% of the la-beled constituents were correctly calculated/Some limited work has been done using u-arylexical statistics.
Hogenhout and Matsumoto(1996) describe a lexicalization of context freegrammars very similar to ours, but without pre-senting a generative model.
The probabilitiesused, as a result, ignore valuable conditioninginformation, such as the head word of constituenthelping to predict its syntactic expansion.
Nev-ertheless, they are able to achieve approximately7http://bobo.link.cs.cmu.edu/link/improvements.html18395% labeled bracketing precision and recall ontheir corpus.
Note that they use a small fi-nite number of word classes, rather than lexicalitems, in their statistics.Utsuro and Matsumoto (1997) present a veryinteresting mechanism for learning semantic caseframes for Japanese verbs: each case frame is atuple of independent component frames (eachof which may have an n-tuple of slots).
More-over, they use an ontology rather than simplyword classes when finding the case frames.
Inthis way, the work is essentially a generaliza-tion of the work of Resnik (1993).
They reportresults on disambiguating whether a nominalargument in a complex Japanese sentence be-longs to the subordinate clause verb or the ma-trix clause verb.
Their evaluation covers threeJapanese verbs, and achieves accuracy of 96%on this disambiguation task.Chang et al (1992) describe a model for ma-chine translation which can accommodate n-arylexical statistics.
They report no improvementin parsing accuracy for n~ > 2.
Their resultsmost likely suffer from sparse data (they hadonly about 1000 sentences), although they diduse semantic lasses rather than lexical items.They report that their total sentence accuracy(percent of test sentences whose calculated brack-eting is completely correct) is approximately 58%.7 Future  WorkThere are many directions to take this work.One advantage ofour well-founded framework isthat it allows more linguistic information, e.g.features like tense and agreement, o be usedin the language model, s For example, a verbphrase in the imperfect may often be modifiedby an adjunctive, durative PP:for.
We wouldlike to use the techniques of corpus-based pars-ing to extract these statistical patterns auto-matically.
The model easily extends to incor-porate a host of syntactic features (Seagull andSchubert, 1998).SNore that these particular features are in theoryavailable to a purely corpus-based parser, as part-of-speech tags in the Penn Treebank are marked for tenseand agreement.
But that information is not availableto the phrase-level constituent unless a notion of headsand feature passing is added to the mechanism.
It seemsthat foot features, unless explicitly realized at the phraselevel (e.g.
WHPP) would be even more difficult to percolatewithout an a priori notion of features and grammar.Currently the parser uses a pruning schemethat filters as it creates the parse bottom-up.The filtering is done based on the probability ofthe individual nodes, irrespective of the globalcontext.
The pruning procedure needs refine-ment, as our full model was not able to arriveat a parse for eight of the 421 sentences in thetest set.We would certainly like to expand our cor-pus by increasing the coverage of our grammar.Also, adding a constituent size/distance effect,as described by Schubert (1986) and as usedby some researchers in parsing (e.g.
Lesmo andTorasso (1985) and Collins (1997)) would al-most certainly improve parsing.Most likely, WordNet senses are more fine-grained than we need for syntactic disambigua-tion.
We may investigate methods of automat-ically collapsing senses which are similar.
Also,we may use more data on word sense frequen-cies, outside of the data we get from our "parse-able bracketings'.
We used WordNet for theseexperiments both because WordNet provides anontology, and because there was an extant cor-pus which was annotated with both syntacticand word sense information.
Using a corpusthat is tagged with "coarser" senses will almostcertainly ield better esults, on both sense dis-ambiguation and parsing.8 Conc lus ionThis work suggests that despite their low fre-quency, vt-ary lexical statistics can be combinedwith an ontology, such as WordNet, to be usedto aid parsing and word sense disambiguation.More interestingly, results from our small cor-pus indicate that WordNet (or some ontology) isnecessary for n-ary statistics to be useful.
In ad-dition, these results can be obtained within theframework of a well-founded grammar and lexi-con.
All of this together yields a broad-coverageparser that lends itself to applications requiringnatural anguage understanding.
In the future,we hope to improve our model and expand ourcorpus, and thus to improve our parsing accu-racy further.8.1 AcknowledgmentsThe grammar and parser we are using are gener-ously supplied by The Boeing Company.
Manythanks to Phil Harrison at Boeing for answering184all our questions about SAPIR, and for help- ambiguating word senses in a large corpus.ing to make it available in the first place.
This Computers and the Humanities, 26:415-439,material is based upon work supported by NSF December.grantsIRI-9503312, IRI-9623665, andIRI-9711009.
Ralph Grishman, Catherine Macleod, andAdam Meyers.
1994.
COMLEX syntax:Re ferencesGlenn Carroll and Mats Rooth.
1998.
Valenceinduction witch a head-lexicalized PCFG.
InProceedings df the Third Conference on Em-pirical Methods in Natural Language Process-ing, Granadal Spain.
ACL SIDGAT.John Carroll, Guido Minnen, and Ted Briscoe.1998.
Can subcategorisation probabilitieshelp a statistical parser?
In Proceedingsof the 6th ACL/SIGDAT Workshop on VeryLarge Corpora, pages 1-9, Montreal, Canada,August.Jing-Shin Chang, Yih-Fen Luo, and Keh-YihSu.
1992.
GPSM: A generalized probabilis-tic semantic model for ambiguity resolution.In Proceedings of the 30th Annual Meeting ofthe Association for Computational Linguis-tics, pages 177-184.Eugene Charni:ak.
1997a.
Statistical parsingwith a context-free grammar and word statis-tics.
In Proceedings of the Fourteenth Na-tional Conference on Artificial Intelligence.Eugene Charniak.
1997b.
Statistical techniquesfor natural language parsing.
AI Magazine,Winter.Michael John Collins.
1996.
A new statisticalparser based on bigram lexical dependencies.In Proceedings of the 3~th Annual Meeting ofthe Association for Computational Linguis-tics, pages 184-191.iMichael John Collins.
1997.
Three genera-tive, lexicali~ed models for statistical pars-ing.
In Proceedings of the 35th Annual Meet-ing of the Association for Computational Lin-guistics and i8th Conference of the EuropeanChapter of the Association for ComputationalLinguistics, pages 16-23.Christiane Fellbaum, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press.Marilyn Ford, Joan Bresnan, and Ronald M.Kaplan.
1982.
A competence-based theoryof syntactic Closure.
In Joan Bresnan, editor,The Mental Representation of GrammaticalRelations.
MIT Press.William A. Gale, Kenneth W. Church, andDavid Yarowsky.
1992.
A method for dis-Building a computational lexicon.
In Proceed-ings of the 15th International Conference onComputational Linguistics, Kyoto.Philip Harrison and Michael Maxwell.
1986.
Anew implementation for GPSG.
In Proc.
ofthe Can.
Soc.
for Computational Studies ofIntelligence (CSCSI-86), pages 78-83, Que-bec.Wide R. Hogenhout and Yuki Matsumoto,1996.
Connectionist, Statistical, and Sym-bolic Approaches to Learning for Natural Lan-guage Processing, chapter Training StochasticGrammars on Semantical Categories, pages160-172.
Springer, NY.Ray S. Jackendoff.
1977.
X Syntax: A Studyof Phrase Structure.
The MIT Press, Cam-bridge, MA.Henry Ku~era and W. Nelson Francis.
1967.Computational Analysis of Present-dayAmerican English.
Brown University Press,Providence, R.I.John Lafferty, Daniel Sleator, and Davy Tem-perley.
1992.
Grammatical trigams: Aprobabilistic model of link grammar.
InAAAI Fall Symposium on Probablistic Ap-proaches to Natural Language.Shari Landes, Claudia Leacock, and Randee I.Tengi.
1998.
Building semantic condor-dances.
In Christiane Fellbaum, editor,WordNet: An Electronic Lexical Database,chapter 8, pages 199-216.
MIT Press.Leonardo Lesmo and Pietro Torasso.
1985.Weighted interaction of syntax and semanticsin natural anguage analysis.
In Proceedingsof the Fourth National Conference on Artifi-cial Intelligence, pages 772-778.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Macinkiewicz.
1993.
Buildinga large annotated corpus of English: thePenn Treebank.
Computational Linguistics,19:313-330.Hwee Tou Ng and Hian Beng Lee.
1996.
Inte-grating multiple knowledge sources to disam-biguate word sense: An exemplar-based ap-proach.
In Proceedings of the 34th AnnualMeeting of the Association for Computational185Linguistics, pages 40-47, Santa Cruz, Califor-nia, June.Philip Stuart Resnik.
1993.
Selection and In-formation: A Class-Based Approach to Lexi-cal Relationships.
Ph.D. thesis, University ofPennsylvania.Lenhart K. Schubert.
1986.
Are there prefer-ence trade-offs in attachment decisions?
InProceedings of the Fifth National Conferenceon Artificial Intelligence, pages 601-605.Amon B. Seagull and Lenhart K. Schubert.1998.
Smarter corpus-based syntactic disam-biguation.
Technical Report 693, Universityof Rochester, November.Takehito Utsuro and Yuji Matsumoto.
1997.Learning probabilistic subcategorization pref-erence by identifying case dependencies andoptimal noun class generalization level.
InProceedings of the Fifth Applied Natural Lan-guage Processing Conference, pages 364-371,Washington, D.C., April.186
