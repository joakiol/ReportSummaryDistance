Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 540?551,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsExploring Supervised LDA Models for Assigning Attributes toAdjective-Noun PhrasesMatthias Hartung and Anette FrankComputational Linguistics DepartmentHeidelberg University{hartung,frank}@cl.uni-heidelberg.deAbstractThis paper introduces an attribute selectiontask as a way to characterize the inherent mea-ning of property-denoting adjectives in adjec-tive-noun phrases, such as e.g.
hot in hot sum-mer denoting the attribute TEMPERATURE,rather than TASTE.
We formulate this taskin a vector space model that represents adjec-tives and nouns as vectors in a semantic spacedefined over possible attributes.
The vectorsincorporate latent semantic information ob-tained from two variants of LDA topic mod-els.
Our LDA models outperform previous ap-proaches on a small set of 10 attributes withconsiderable gains on sparse representations,which highlights the strong smoothing powerof LDA models.
For the first time, we extendthe attribute selection task to a new data setwith more than 200 classes.
We observe thatlarge-scale attribute selection is a hard prob-lem, but a subset of attributes performs ro-bustly on the large scale as well.
Again, theLDA models outperform the VSM baseline.1 IntroductionCorpus-based statistical modeling of semantics isgaining increased attention in computational linguis-tics.
This field of research includes distributionalvector space models (VSMs), i.e., models that rep-resent the semantics of words or phrases as vectorsover high-dimensional cooccurrence data (Turneyand Pantel, 2010; Baroni and Lenci, 2010, i.a.
), aswell as latent variable models (LVMs) which aggre-gate distributional observations in ?hidden?, or latentvariables, thereby reducing the dimensionality of thedata.
An example of the latter are topic models (Bleiet al, 2003), which have recently been applied tomodeling selectional preferences of verbs (Ritter etal., 2010; ?O Se?aghdha, 2010), or word sense disam-biguation (Li et al, 2010).A topic that is increasingly studied in distribu-tional semantics is the semantics of adjectives, bothin isolation (Almuhareb, 2006) and in compositionaladjective-noun phrases (Hartung and Frank, 2010;Guevara, 2010; Baroni and Zamparelli, 2010).In this paper, we propose a new approach to aproblem we denote as attribute selection: The task isto predict the hidden attribute meaning expressed bya property-denoting adjective in composition witha noun.
The adjective hot, e.g., may denote at-tributes such as TEMPERATURE, TASTE or EMO-TIONALITY.
These adjective meanings can be com-bined with nouns such as tea, soup or debate, whichcan be characterized in terms of attributes as well.The goal of the task is to determine the hidden at-tribute meaning predicated over the noun in a givenadjective-noun phrase, as illustrated in (1).
(1) a. a hotvalue summerconceptb.
TEMPERATURE(summer) = hotIt is by way of the composition of adjective andnoun that specific attributes are selected from the ad-jective?s space of possible attribute meanings, andtypically lead to a disambiguation of the adjectiveand possibly the noun.
Hartung and Frank (2010)were the first to model this insight in a VSM by rep-resenting the meaning of adjectives and nouns in se-mantic vectors defined over attributes.
The meaningof adjective-noun phrases is computed by means of540COLORDIRECTIONDURATIONSHAPESIZESMELLSPEEDTASTETEMPERATUREWEIGHT~e 1 1 0 1 45 0 4 0 0 21~b 14 38 2 20 26 0 45 0 0 20~e ?~b 14 38 0 20 1170 0 180 0 0 420~e+~b 15 39 2 21 71 0 49 0 0 41Figure 1: Vectors for enormous (~e) and ball (~b)vector composition, such that the ?hidden?
attributemeaning of the phrase can be ?selected?
as a promi-nent component from the composed vector.
This isillustrated in Fig.
1 for the adjective enormous (~e)in combination with the noun ball (~b), with alter-native composition operations: vector multiplication(?)
and addition (+).1 Both yield SIZE as the mostprominent component in the composed vector.In the present paper we offer a new approach tothis formalization of the compositional meaning ofadjectives and nouns that owes to both distributionalVSMs and LVMs.
Through this combination, weattempt to improve on earlier work in Almuhareb(2006) and Hartung and Frank (2010), which areboth embedded in a purely distributional setting.Specifically, we use Latent Dirichlet Allocation(LDA; Blei et al (2003)) to train an attribute modelthat captures semantic information encoded in ad-jectives and nouns independently of one another.Following Hartung and Frank (2010), this model isembedded into a VSM that employs vector com-position to combine the meaning of adjectives andnouns.
We present two variants of LDA that differin the way attributes are associated with the inducedLDA topics: Controled LDA (C-LDA) and LabeledLDA (L-LDA; Ramage et al (2009)).
Both will bepresented in detail in Section 3.Our aims in this paper are two-fold: (i) We inves-tigate LDA as a modeling framework in the attributeselection task, as its use of topics as latent variablesmay alleviate inherent sparsity problems faced byprior work using pattern-based (Almuhareb, 2006)or vector space models (Hartung and Frank, 2010).
(ii) While these prior approaches were restricted toa confined set of 10 attributes, we will we apply our1The figure is adopted from the distributional setting of Har-tung and Frank (2010), with component values defined by pat-tern frequency counts for the chosen attribute nouns.models on a much larger space of attributes, to probetheir capacity on a more realistic data set.The remainder of this paper is divided as fol-lows.
Section 2 reviews related work on distribu-tional models of adjective semantics, and introducesthe two frameworks in which we ground our ap-proach: LVMs and VSMs.
In Section 3 we introducetwo LDA models for attribute selection: C-LDA andL-LDA.
Section 4 describes the settings for two ex-periments: In the first experiment, we perform at-tribute selection confined to a space of 10 attributesto compare against prior work.
In the second settingwe perform attribute selection on a large scale, using206 attributes.
Section 5 presents and discusses theresults.
Section 6 concludes.2 Related WorkDistributional models of adjective semantics.Almuhareb (2006) aims at capturing the relationshipbetween adjectives and attributes based on lexico-syntactic patterns, such as the ATTR of the * is ADJ.Apart from inherent sparsity issues, his approachdoes not account for the compositional nature of theproblem, as the contextual information contributedby a noun is neglected: For instance, his model isunable to predict that hot is unlikely to denote TASTEin the context of summer, other than in hot meal.Compositionality of adjective-noun phrases andhow it can be adequately modeled in VSMs isthe main concern in Baroni and Zamparelli (2010)and Guevara (2010), who are in search of thebest composition operator for combining adjectivewith noun meanings.
While these works adhereto a purely latent representation of meaning, Har-tung and Frank (2010) include attributes as sym-bolic ?hidden?
meanings of adjectives, nouns andadjective-noun phrases in a distributional VSM.Finally, a large body of work dealing with com-positionality in distributional frameworks is not con-fined to the special case of adjective-noun composi-tion (Mitchell and Lapata (2008), Rudolph and Gies-brecht (2010), i.a.).
All these approaches regardcomposition as a process combining vectors (or ma-trices, resp.)
to yield a new, contextualized vectorrepresentation within the same semantic space.Latent Dirichlet Allocation, aka.
Topic Models(TMs).
LDA is a generative probabilistic model541for document collections.
Each document is repre-sented as a mixture over latent topics, where eachtopic is a probability distribution over words (Blei etal., 2003).
These topics can be used as dense fea-tures for, e.g., document clustering.
Depending onthe number of topics, which has to be pre-specified,the dimensionality of the document representationcan be considerably reduced in comparison to sim-ple bag-of-words models.
The remainder of this pa-per will assume some familiarity with LDA and theLDA terminology as introduced in Blei et al (2003).Recent work investigates ways of accommodatingsupervision with LDA, e.g.
supervised topic models(Blei and McAuliffe, 2007), Labeled LDA (L-LDA)(Ramage et al, 2009) or DiscLDA (Lacoste-Julienet al, 2008).
We will discuss L-LDA in Section 3.Distributional VSMs and TMs.
The idea to inte-grate topic models and VSMs goes back to Mitchelland Lapata (2009) who build a distributional modelwith dimensions set to topics over bag-of-words fea-tures.
In their setting, LDA merely serves the pur-pose of dimensionality reduction, whereas our par-ticular motivation is to use topics as probabilisticindicators for the prediction of attributes as seman-tic target categories in adjective-noun composition.Mitchell and Lapata (2010) compare VSMs definedover bags of context words vs. latent topics in a sim-ilarity judgement task.
Their results indicate that amultiplicative setting works best for vector compo-sition in word-based models, while vector additionis better suited for topic vectors.3 Topic Models for Attribute Selection3.1 Using LDA for modeling lexical semanticsRecently, LDA has been used for problems in lexicalsemantics, where the primary goal is not documentmodeling but the induction of semantic knowledgefrom high-dimensional co-occurrence data.
Ritter etal.
(2010) and ?O Se?aghdha (2010) model selectionalrestrictions of verbs by inducing topic distributionsthat characterize ?mixtures of topics?
observed inverb argument positions.
As a basis for LDA mod-eling, they collect pseudo-documents, i.e.
bags ofwords that co-occur in syntactic argument positions.We apply a similar idea to the attribute selectionproblem: we collect pseudo-documents that char-acterize attributes by adjectives and nouns that co-occur with the attribute nouns in local contextual re-lations.
The topic distributions obtained from fittingan LDA model to the collection of these pseudo-documents can then be injected into semantic vectorrepresentations for adjectives and nouns.In its original statement, LDA is a fully unsuper-vised process (apart from the desired number of top-ics which has to be specified in advance) that es-timates topic distributions over documents ?d andtopic-word distributions ?t with topics representedas latent variables.
Estimating these parameters on adocument collection yields topic proportions P (t|d)and topic distributions P (w|t) that can be used tocompute a smooth distribution P (w|d) as in (2),where t denotes a latent topic, w a word and d adocument in the corpus.P (w|d) =?tP (w|t)P (t|d) (2)Being designed for exploratory rather than dis-criminative analysis, LDA does not intend condi-tioning of words or topics on external categories.That is, the resulting topics cannot be related to pre-viously defined target categories.
For attribute se-lection, the LDA-inferred topics need to be linkedto semantic attributes.
Therefore, we apply two ex-tensions of standard LDA that are capable of takingsupervised category information into account, eitherimplicitly or directly, by including an additional ob-servable variable into the generative process.In general, LVMs can be expected to overcomesparsity issues that are frequently encountered indistributional models.
This positive smoothing ef-fect is achieved by marginalization over the latentvariables (cf.
Prescher et al (2000)).
For instance, itis unlikely to observe a dependency path linking theadjective mature to the attribute MATURITY.
Sucha relation is more likely for young, for example.
Ifyoung co-occurs with mature in a different pseudo-document (AGE might be a candidate), this results ina situation where (i) young and mature share one ormore latent topics and (ii) the topic proportions forthe attributes MATURITY and AGE will become sim-ilar to the extent of common words in their pseudo-documents.
Consequently, the final attribute modelis expected to assign a (small) positive probability tothe relation between mature and MATURITY withoutobserving it in the training data.5423.2 Controled LDAThe generative story behind C-LDA is equivalent tostandard LDA.
However, the collection of pseudo-documents used as input to C-LDA is structured ina controled way such that each document conveyssemantic information that specifically characterizesthe individual categories of interest (attributes, inour case).
In line with the distributional hypothesis(Harris, 1968), we consider the pseudo-documentsconstructed in this way as distributional fingerprintsof the meaning of the corresponding attribute.The contents of the pseudo-documents are se-lected along syntactic dependency paths linkingeach attribute noun to meaningful context words (ad-jectives and nouns).2 A corpus consisting of the twosentences in (3), e.g., yields a pseudo-document forthe attribute noun SPEED containing car and fast.
(3) What is the speed of this car?
The machineruns at a very fast speed.Though we are ultimately interested in triples ofattributes, adjectives and nouns that define the com-positional semantics of adjective-noun phrases (cf.
(1)), C-LDA is only exposed to binary tuples be-tween attributes and adjectives or nouns, respec-tively.
This is in line with Hartung and Frank(2010), who obtained substantial performance im-provements by splitting the ternary relation into twobinary relations.Presenting LDA with pseudo-documents that cha-racterize individual target attributes imports super-vision into the LDA process in two respects: theestimated topic proportions P (t|d) will be highlyattribute-specific, and similarly so for the topic dis-tributions P (w|t).
This makes the model more ex-pressive for the ultimate labeling task.
Moreover,since C-LDA collects pseudo-documents focused onindividual target attributes, we are able to link exter-nal categories to the generative process by heuristi-cally labeling pseudo-documents with their respec-tive attribute as target category.
Thus, we approx-imate P (w|a), the probability of a word given anattribute, by P (w|d) as obtained from LDA:2The dependency paths, together with the set of attributenouns of interest, have to be manually specified.
See the sup-plementary material for the full list of dependency paths used.1 For each topic k ?
{1, .
.
.
, K}:2 Generate ?k = (?k,1, .
.
.
, ?k,V )T ?
Dir(?
| ?
)3 For each document d:4 For each topic k ?
{1, .
.
.
,K}5 Generate ?
(d)k ?
{0, 1} ?
Bernoulli(?
| ?k)6 Generate ?
(d) = L(d) ?
?7 Generate ?
(d) = (?l1 , .
.
.
, ?lMd )T ?
Dir(?
| ?
(d))8 For each i in {1, .
.
.
, Nd}:9 Generate zi ?
{?
(d)1 , .
.
.
, ?
(d)Md} ?
Mult(?
| ?
(d))10 Generate wi ?
{1, .
.
.
, V } ?
Mult(?
| ?zi)Figure 2: L-LDA generative process (Ramage et al 2009)P (w|a) ?
P (w|d) =?tP (w|t)P (t|d) (4)3.3 Labeled LDAL-LDA (Ramage et al, 2009) extends standard LDAto include supervision for specific target categories,yet in a different way: (i) The generative processincludes a second observed variable, i.e.
each doc-ument is explicitly labeled with a target category.A document may be labeled with an arbitrary num-ber of categories; unlabeled documents are also pos-sible.
However, L-LDA permits only binary as-signments of categories to documents; probabilisticweights over categories are not intended.
(ii) Con-trary to LDA, where the number of topics has to bespecified in advance, L-LDA sets this parameter tothe number of unique target categories.
Moreover,the model is constrained such that documents maybe assigned only those topics that correspond to theirobservable category label(s).
That is, latent topicst in the standard formulation of LDA (2) are con-strained to correspond to explicit labels a.More specifically, L-LDA extends the generativeprocess of LDA by constraining the topic distribu-tions over documents ?
(d) to only those topics thatcorrespond to the document?s set of labels ?(d).
Thisis done by projecting the parameter vector of theDirichlet topic prior ?
to a lower-dimensional vec-tor ?
(d) whose topic dimensions correspond to thedocument labels.This extension is integrated in steps 5 and 6 ofFig.
2: First, in step 5, the document?s labels ?
(d)are generated for each topic k. The resulting vectorof document?s labels ?
(d) = {k | ?
(d)k = 1} is usedto define a document-specific label projection matrix543L(d)|?
(d)|?K , such that L(d)ij = 1 if ?
(d)i = j, and 0 oth-erwise.
This matrix is used in step 6 to project theDirichlet topic prior ?
to a lower-dimensional vec-tor ?
(d), whose topic dimensions correspond to thedocument labels.
Topic proportions are then, in step7, generated for this reduced parameter space.In our instantiation of L-LDA, we collect pseudo-documents for attributes exactly as for C-LDA.
Doc-uments are labeled with exactly one category, the at-tribute noun.
Note that, even though the relationshipbetween documents and topics is fixed, the one be-tween topics and words is not.
Any word occurringin more than one document will be assigned a non-zero probability for each corresponding topic.Thus, with regard to attribute modeling, C-LDAand L-LDA build an interesting pair of opposites:The L-LDA model assumes that attributes are se-mantically primitive in the sense that they cannotbe decomposed into smaller topical units, whereaswords may be associated with several attributes atthe same time.
C-LDA, at the other end of the spec-trum, licenses semantic variability on both the at-tribute and the word level.
Particularly, a word mightbe associated with some of the topics underlying anattribute, but not with all of them, and an attributecan be characterized by multiple topics.3.4 Vector Space FrameworkFor integrating the information obtained from C-LDA or L-LDA into a distributional VSM, we fol-low Hartung and Frank (2010): Adjectives andnouns are modeled as independent semantic vectorsalong their relationship to attributes; the most promi-nent attribute(s) that represent the hidden meaningof adjective-noun phrases are selected from theircomposition (cf.
Fig.
1).The dimensions of the VSM are set to the pre-selected attributes.
Semantic vectors are computedfor all adjectives and nouns occurring at least fivetimes in the pseudo-documents.
Vector componentvalues v?w,a?
are derived from the C-LDA and L-LDA models in different ways: with C-LDA weobtain P (w|a) by approximation from P (w|d) (cf.equation (4)), while in L-LDA we obtain P (w|a) di-rectly from the induced topic-word distribution ?t,through labeled topics t = a (cf.
equation (2)).Vector composition is defined as vector multipli-cation (?)
or vector addition (+).For attribute selection on the composed vector, weuse two methods we found to perform best in Har-tung and Frank (2010): Entropy Selection (ESel)and Most Prominent Component (MPC).
ESel mea-sures entropy over the vector components to identifycomponents that encode a high amount of informa-tion.
It selects all attributes that lead to an increase ofentropy when suppressed from the vector represen-tation.
If no informative components can be detectedin a vector due to a very broad, flat distribution ofthe probability mass (cf.
~b in Fig.
1), ESel yields anempty list.
MPC always chooses exactly one vectorcomponent, i.e.
the one with the highest value.4 Experimental SettingsAttribute selection over small and large semanticspaces.
We evaluate the performance of the VSMsbased on C-LDA and L-LDA in two experimentalsettings, contrasting the problem of attribute selec-tion on semantic spaces of radically different dimen-sionality, using sets of 10 vs. 206 attributes.Evaluation measures.
We evaluate against twogold standards consisting of adjective-noun phrases(or adjective-noun pairs) and their associated at-tribute meanings.
We report precision, recall andf1-score.
Where appropriate, we test differences inthe performance of various model configurations forstatistical significance in a randomized permutationtest (Yeh, 2000), using the sigf tool (Pado?, 2006).Baselines.
We compare our models against twobaselines, PATTVSM and DEPVSM.
PATTSVM isreconstructed from Hartung and Frank (2010).
It isgrounded in a selection of lexical patterns that iden-tify the target elements (adjectives and nouns) forthe vector basis elements (i.e., the attribute nouns)in a local context window.
The component valuesare defined using raw frequency counts over the ex-tracted patterns.
DEPVSM is similar to PATTVSM;however, it relies on dependency paths that connectthe target elements and attributes in local contexts.The paths are identical to the ones used for con-structing pseudo-documents in C-LDA and L-LDA.As in PATTVSM, the vector components are set toraw frequencies over extracted paths.Implementations.
To implement our models, werely on MALLET (McCallum, 2002) for C-LDA and544the Stanford Topic Modeling Toolbox3 for L-LDA.In both cases, we run 1000 iterations of Gibbs sam-pling, using default values for all hyperparameters.Data set for attribute selection over 10 attributes.The first experiment is conducted on the data setused in Hartung and Frank (2010).
It consists of100 adjective-noun pairs manually annotated forten attributes: COLOR, DIRECTION, DURATION,SHAPE, SIZE, SMELL, SPEED, TASTE, TEMPER-ATURE, WEIGHT.
To enable comparison, the di-mensions of our models are set to exactly these at-tributes.Data set for attribute selection over a large se-mantic space (206 attributes).
In the second ex-periment, we max out the attribute selection taskto a much larger set of attributes in order to an-alyze the difficulty of the task on more represen-tative data.
We automatically construct a data setof adjective-noun phrases labeled with appropriateattributes from WordNet 3.0 (Fellbaum, 1998), re-lying on the assumption that examples given inglosses correspond to the respective word sense ofthe adjective.
We first extract all adjectives thatare linked to at least one attribute synset by theattribute relation.
Next, we run the glosses ofthese adjectives (3592 in number) through TreeTag-ger (Schmid, 1994) to find examples of adjectivesmodifying nouns in attributive constructions.
Theresulting adjective-noun phrases are labeled with theattribute label linked to the given adjective sense.This method yields 7901 labeled adjective-nounphrases.
They are divided into development and testdata according to a sampling procedure that respectsthe following criteria: (i) Both sets must containall attributes with an equal number of phrases foreach attribute; (ii) phrases with both elements con-tained in CoreWordNet4 are preferred, while othersare only considered if necessary to satisfy the firstcriterion.
This procedure yields 496/345 phrasesin the development/test set, distributed over 206 at-tributes5.3http://nlp.stanford.edu/software/tmt/.4A subset of WordNet restricted to the 5000 most fre-quently used word senses.
Available from: http://wordnetcode.princeton.edu/standoff-files/core-wordnet.txt5If an attribute provides only one example, this was addedto the development set.
Therefore, the test set only comprisesTraining data.
The pseudo-documents are collec-ted from dependency paths obtained from section 2of the parsed pukWaC corpus (Baroni et al, 2009).5 Discussion of Results5.1 Experiment 1In Experiment 1, we evaluate the performanceof C-LDA and L-LDA on the attribute selectiontask over 10 attributes against the pattern-basedand dependency-based models PATTVSM and DE-PVSM as competitive baselines.
Besides a com-parison to standard VSMs, we are especially in-terested in the relative performance of the LDAmodels.
Given that C-LDA and L-LDA estimateattribute-specific topic distributions in the structuredpseudo-documents under different assumptions re-garding the correspondence of attributes and topics(cf.
Sec.
3.2 and 3.3), we expect the two LDA vari-ants to differ in their capability to capture the topicdistributions in the labeled pseudo-documents.5.1.1 Attribute Selection for 10 AttributesTables 1 and 2 summarize the results for at-tribute selection over 10 attributes against the la-beled adjective-noun pairs in the test set, using ESeland MPC as selection functions on vectors com-posed by multiplication (Table 1) and addition (Ta-ble 2).
The results reported for C-LDA correspondto the best performing model (with number of top-ics set to 42, as this setting yields the best and mostconstant results over both composition operators).C-LDA shows highest f-scores and recall over allsettings, and highest precision with vector addition.6In line with Mitchell and Lapata (2010) (cf.
Sec.
2),we obtain the best overall results with vector addi-tion (ESel: P: 0.55, R: 0.66, F: 0.61; MPC: P: 0.59,R: 0.71, F: 0.64).
The difference between C-LDAand L-LDA is small but significant for vector mul-tiplication; for vector addition, it is not significant.Compared to the LDA models, the VSM baselines206 attributes, while all models were trained on 262 attributesobtained from WordNet in the first extraction step.6In Tables 1 and 2, statistical significance of the differencesbetween the models is marked by the superscripts L, D and P,denoting a significant difference over L-LDA, DepVSM andPattVSM, respectively.
All differences reported are significantat p < 0.05, except for the difference between C-LDA and L-LDA in Table 3 (p < 0.1).545ESel MPCP R F P R FC-LDA 0.58 0.65 0.61L,P 0.57 0.64 0.60L-LDA 0.68 0.54 0.60D 0.55 0.61 0.58DDepVSM 0.48 0.58 0.53P 0.57 0.60 0.58PattVSM 0.63 0.46 0.54 0.60 0.58 0.59Table 1: Attribute selection over 10 attributes (?
)ESel MPCP R F P R FC-LDA 0.55 0.66 0.61D,P 0.59 0.71 0.64L-LDA 0.53 0.57 0.55D,P 0.50 0.45 0.47D,PDepVSM 0.38 0.65 0.48P 0.57 0.60 0.58PattVSM 0.71 0.35 0.47 0.47 0.56 0.51Table 2: Attribute selection over 10 attributes (+)are competitive, but tend to perform lower.
This ef-fect is statistically significant for ESel with vectormultiplication: each of the LDA models statisticallysignificantly outperforms one of the VSM models,DEPVSM and PATTVSM.
With ESel and vectoraddition, both LDA models outperform both VSMmodels statistically significantly.
The LDAESel,+models outperform the PATTVSMESel,+ model ofHartung and Frank (2010) by a high margin inf-score: +0.14 for C-LDA; +0.08 for L-LDA.Compared to the stronger multiplicative settingsPATTVSMESel,?
and PATTVSMMPC,?
this stillrepresents a plus of +0.07 and +0.02 in f-score, re-spectively.
We further observe a clear improvementof the LDA models over the VSM models in terms ofrecall (+0.20, C-LDAESel,+ vs.
PATTVSMESel,?
),at the expense of some loss in precision (-0.08, C-LDAESel,+ vs.
PATTVSMESel,?).
This clearly con-firms a stronger generalization power of LDA com-pared to VSM models.With regard to selection functions, we observethat MPC tends to perform better for the VSM mod-els, while ESel is more suitable in the LDA models.Figures 3 and 4 display the overall performancecurve ranging over different topic numbers for C-LDAESel,+ and C-LDAESel,?
?
compared to theremaining models that are not dependent on topicsize.
For topic numbers smaller than the attribute setsize, C-LDA underperforms, for obvious reasons.Increasing ranges of topic numbers to 60 does notshow a linear effect on performance.
Parameter set-tings with performance drops below the VSM base-lines are rare, which holds particularly for vector ad-0.20.250.30.350.40.450.50.550.60.650  10  20  30  40  50  60F-ScoreNum.
TopicsC-LDAL-LDADepVSMPattVSMFigure 3: Performance of C-LDAESel,?
for differenttopic numbers, compared against all other models0.20.250.30.350.40.450.50.550.60.650  10  20  30  40  50  60F-ScoreNum.
TopicsC-LDAL-LDADepVSMPattVSMFigure 4: Performance of C-LDAESel,+ for differenttopic numbers, compared against all other modelsdition at topic ranges larger than 10.
With vectoraddition, C-LDA outperforms L-LDA in almost allconfigurations, yet at an overall lower performancelevel of L-LDA (0.55 with addition vs. 0.6 with mul-tiplication).
Note that in the multiplicative setting,C-LDA reaches the performance of L-LDA only inits best configurations, while with vector addition itobtains high performance that exceeds L-LDA?s topf-score of 0.6 for topic ranges between 10 and 20.Based on these observations, vector additionseems to offer the more robust setting for C-LDA,the model that is less strict with regard to topic-attribute correspondences.
Vector multiplication, onthe other hand, is more suitable for L-LDA and itsstricter association of topics with class labels.5.1.2 Smoothing Power of LDA ModelsOur hypothesis was that LDA models should bebetter suited for dealing with sparse data, compared546ESel MPCP R F P R FC-LDA 0.39 0.31 0.35 0.37 0.27 0.32L-LDA 0.30 0.18 0.23 0.20 0.18 0.19DepVSM 0.20 0.10 0.13 0.37 0.26 0.30PattVSM 0.00 0.00 0.00 0.00 0.00 0.00Table 3: Performance figures on sparse vectors (?
)ESel MPCP R F P R FC-LDA 0.43 0.33 0.38 0.44 0.28 0.34L-LDA 0.34 0.16 0.22 0.37 0.18 0.24DepVSM 0.16 0.17 0.17 0.36 0.21 0.27PattVSM 0.13 0.04 0.06 0.17 0.25 0.20Table 4: Performance figures on sparse vectors (+)to pattern-based or purely distributional approaches.While this is broadly confirmed in the above resultsby global gains in recall, we conduct a special evalu-ation focused on those pairs in the test set that sufferfrom sparse data.
We selected all adjective and nounvectors that did not yield any positive componentvalues in the PATTVSM model.
The 22 adjective-noun pairs in the test set affected by these ?zero vec-tors?
were evaluated using the remaining models.The results in Tables 3 and 4 yield a very clear pic-ture: C-LDA obtains highest precision, recall andf-score across all settings, followed by L-LDA andDEPVSMESel, while their ranks are reversed whenusing MPC.
Again, MPC works better for the VSMmodels, ESel for the LDA models.
Vector additionperforms best for C-LDA with f-scores of 0.38 and0.34 ?
outperforming the pattern-based results onsparse vectors by orders of magnitude.5.2 Experiment 2Experiment 2 is designed to max out the space ofattributes to be modeled, to assess the capacity ofboth LDA models and the DEPVSM baseline modelin the attribute selection task on a large attributespace.7 In contrast to Experiment 1, with its con-fined semantic space of 10 target attributes, this rep-resents a huge undertaking.5.2.1 Large-scale Attribute SelectionTable 5 (column all) displays the performance ofall models on attribute selection over a range of 2067We did not apply PATTVSM to this large-scale experiment,as only poor performance can be expected.all property?
+ ?
+C-LDA 0.04 0.02 0.18L,D 0.10DL-LDA 0.03 0.04 0.15 0.15DepVSM 0.02 0.02 0.12 0.07Table 5: Performance figures (in f-score) of C-LDAESelon 206 (all) and 73 property attributes (property)all propertyP R F P R FWIDTH 0.67 1.00 0.80 1.00 0.50 0.67WEIGHT 0.80 0.57 0.67 0.50 0.57 0.53MAGNETISM 0.50 1.00 0.67SPEED 0.50 0.50 0.50 1.00 0.50 0.67TEXTURE 0.33 1.00 0.50 0.33 1.00 0.50DURATION 0.50 0.50 0.50 1.00 1.00 1.00TEMPERATURE 0.30 0.75 0.43 0.43 0.75 0.55AGE 0.33 0.50 0.40THICKNESS 1.00 0.25 0.40 0.50 0.13 0.20DEGREE 1.00 0.20 0.33LENGTH 0.17 1.00 0.29 0.50 1.00 0.67DEPTH 1.00 0.14 0.25 1.00 0.86 0.92ACTION 0.17 0.50 0.25LIGHT 0.33 0.17 0.22 0.20 0.17 0.18POSITION 0.14 0.25 0.18 0.20 0.25 0.22SHARPNESS 1.00 1.00 1.00SERIOUSNESS 0.50 1.00 0.67COLOR 0.13 0.25 0.17 0.29 0.50 0.36LOYALTY 1.00 1.00 1.00average 0.49 0.54 0.51 0.63 0.63 0.63Table 6: Attribute selection on 206 attributes (all) and 73property attributes (property); performance figures of C-LDAESel,?
for best attributes (F>0)dimensions, contrasting vector addition and multi-plication.
The number of topics was set to 400.
Asthe overall performance is close to 0 for both com-position methods, no parameter setting can be iden-tified as particularly suited for this large-scale at-tribute selection task.
The differences between thethree models are very small and not significant8.5.2.2 Focused Evaluation and Data AnalysisTo gain a deeper insight into the modeling capac-ity of the LDA models for this large-scale selectiontask, Table 6 (column all) presents a partial evalua-tion of attributes that could be assigned to adjective-noun pairs with an f-score >0 by C-LDAESel,?.Despite the disappointing overall performance of8Again, statistically significant differences are marked bysuperscripts (cf.
footnote 6).
All differences reported are sig-nificant at ?
< 0.05.547prediction correctthin layer THICKNESS THICKNESSheavy load WEIGHT WEIGHTshallow water DEPTH DEPTHshort holiday DURATION DURATIONattractive force MAGNETISM MAGNETISMshort hair LENGTH LENGTHserious book DIFFICULTY MINDblue line COLOR UNIONweak president POSITION POWERfluid society REPUTE CHANGEABLENESSshort flight DISTANCE DURATIONrough bark TEXTURE EVENNESSfaint heart CONSTANCY COWARDICETable 7: Sample of correct and false predictions of C-LDAESel,?
in Experiment 2the LDA models on this large attribute space, it isremarkable that C-LDA is able to induce distinctivetopic distributions for a number of attributes with upto 0.51 f-score with balanced precision and recall,a moderate drop of only -0.10 relative to the corre-sponding model induced over 10 attributes.Raising the attribute selection task from 10 to 206attributes poses a true challenge to our models, bythe sheer size and diversity of the semantic spaceconsidered.
Table 7 gives an insight into the natureof the data and the difficulty of the task, by listingcorrect and false preditions of C-LDA for a smallsample of adjective-noun pairs.
Possible explana-tions for false predictions are manifold, among themnear misses (e.g.
serious book, weak president, shortflight, rough bark), idiomatic expressions (e.g.
faintheart, blue line) or questionable labels provided byWordNet (e.g.
serious book).As seen above, C-LDA achieves relatively highperformance figures on selected attributes (cf.
Table6, col. all).
In order to identify what makes theseattributes different from others that resist success-ful modeling, we investigated three factors: (i) theamount of training data available for each attribute,(ii) the ambiguity rate per attribute, and (iii) theirontological subtype.
(i) Measuring the dependence between trainingdata size and f-score per attribute shows that a largeamount of training data is generally helpful, but notthe decisive factor (Pearson?s r = 0.19, p < 0.01).
(ii) The ambiguity rate ARattr per attribute attris computed by averaging over all test pairs TPattrlabeled with attr, counting the total number of at-tributes attr?
that are associated with each adjectivein pairs ?adj, n?
?
TPattr in WordNet:ARattr =?attr???adj,n?
?TPattr |?adj, attr?
?WN ||TPattr |Correlating this figure with the performance per at-tribute in terms of f-score yields only a small pos-itive correlation (Pearson?s r = 0.23, p < 0.01).In fact, the qualitative analysis in Table 7 shows thatC-LDA is capable of assigning meaningful attributesto adjective-noun phrases not only in easy, but alsoambiguous cases (cf.
shallow water, where DEPTHis the only attribute provided for shallow in Word-Net vs. short holiday, short hair or short flight).
(iii) Although the 206 attributes used in Exp.
2 arerather diverse, including concepts such as HEIGHT,KINDNESS or INDIVIDUALITY, we observe a highnumber of attributes from Exp.
1 that are success-fully modeled in Exp.
2 (5 out of 10, cf.
columnall in Table 6).
Given that they are categorized intothe property class in WordNet9, we presume that thevarying performance across attributes might be in-fluenced by their ontological subtype.
This hypoth-esis is validated in a replication of Exp.
2, with train-ing data limited to the 73 attributes pertaining to theproperty subtype in WordNet.
The test set was re-stricted accordingly, resulting in 112 pairs that arelinked to a property attribute.The overall performance of the models in this ex-periment is shown in Table 5 (column property):With vector multiplication, the best-performing op-eration across all models, all models benefit consid-erably (+0.10 or more).
C-LDA shows the largestimprovement, significantly outperforming both L-LDA and DEPVSM.
With vector addition, the per-formance gains are slightly lower in general.
Inthis setting, L-LDA shows higher f-score than C-LDA, though this difference is not statistically sig-nificant.
Still, C-LDA significantly outranges DE-PVSM.
Note that we can not show a significant dif-ference between C-LDAESel,?
and L-LDAESel,+,so the comparison between these models remains in-conclusive here.
Note further that the affinity of C-LDA with vector addition and L-LDA with vectormultiplication, respectively, is inverted in the large-scale experiment (cf.
Table 5).9WordNet separates attributes into properties, qualities andstates, among several others.548While these overall results are far from satisfac-tory, they still clearly indicate that the LDA modelswork effectively for at least a subset of attributes,and outperform the VSM baseline.Again, a more detailed analysis is given in Ta-ble 6 (column property), showing the performanceof the best individual property attributes (F>0) inthe restricted experiment.
Average performance ofthe best property attributes with F>0, individually,amounts to F=0.6310.
In comparison to the unres-tricted setting (cf.
column all), nearly all propertyattributes benefit from model training on selectivedata.
Exceptions are WIDTH, WEIGHT, THICKNESS,AGE, DEGREE and LIGHT.
Thus, apparently, someof the adjectives associated with non-property at-tributes in the full set provide some discriminativepower that is helpful to distinguish property types.In a qualitative analysis of the 133 non-propertyattributes filtered out in this experiment, we find thatthe WordNet-SUMO mapping (Niles, 2003) doesnot provide differentiating definitions for about 60%of these attributes, linking them instead to a singlesubjective assessment attribute.
This suggests thatin many cases the distinctions drawn by WordNetare too subtle even for humans to reproduce.6 ConclusionThis paper explored the use of LDA topic modelsin a semantic labeling task that predicts attributesas ?hidden?
meanings in the compositional seman-tics of adjective-noun phrases.
LDA topic modelsare expected to alleviate sparsity problems of dis-tributional VSMs as encountered in prior work, byincorporating latent semantic information about at-tribute nouns.
We investigated two variants of LDAthat employ different degrees of supervision for as-sociating topics with attributes.Our contributions are as follows.
We proposedtwo LDA models for the attribute selection task thatimport supervision for a target category parameterin different ways: L-LDA (Ramage et al, 2009)embeds the target categories into the LDA process,by defining a 1:1 correspondence of topics and tar-get categories.
C-LDA, by contrast, does not af-fect the LDA generative process.
Here, we heuris-10In comparison, L-LDAESel,?
yields an average f-score of0.47 for attributes with F>0 in the property setting.tically equate pseudo-documents with target cate-gories, to approximate category-specific word-topicdistributions.
By adhering to standard LDA, C-LDAaccommodates a greater variety in the distributionsof topics to attribute-specific documents and words,as compared to L-LDA.
Combining standard LDAtopic modeling with a means of interpreting the in-duced topics relative to a set of external categories,C-LDA offers greater flexibility and expressiveness.Our experimental results show that modeling at-tributes as latent or explicit topics with C-LDA andL-LDA, respectively, outperforms the purely distri-butional baseline model DEPVSM and PATTVSMof prior work.
Targeted evaluation on sparse datapoints confirms that LDA models help to overcomeinherent sparsity effects of VSMs.
C-LDA and L-LDA are close in performance in Experiment 1.
C-LDA outperforms L-LDA only with optimal topicparameter settings.Finally, we probed the modeling capacity of LDAand VSM models on a vast space of 206 attributes.This task proved to be extremely difficult.
However,we obtain respectable results on a subset of attributesdenoting properties, where C-LDA performs best inquantitative performance measures.
It yields high-est f-scores in full and partial evaluation ?
both withthe full-size attribute model, and when training andtesting is restricted to property attributes.
The differ-ences are small, but statistically significant betweenthe LDA models and the VSM baseline in a settingrestricted to property attributes.Data analysis indicates that our models performmore robustly on concrete attributes in contrast toabstract attribute types that lack clear categorization.This suggests that our approach to attribute selec-tion is most appropriate for detecting attributes thatreflect clear ontological distinctions.However, there is ample space for improvement.In Hartung and Frank (2011), we show that thequality of the noun vectors lags behind the adjec-tive vectors.
This clearly affects the performanceof our models in cases where the semantic contri-bution of the noun is decisive for disambiguation.Future work will focus on ways to enhance the nounvector representations through additional contextualfeatures, to make them denser and more articulatedin structure.549ReferencesAbdulrahman Almuhareb.
2006.
Attributes in LexicalAcquisition.
Ph.D. Dissertation, Department of Com-puter Science, University of Essex.Marco Baroni and Alessandro Lenci.
2010.
Distri-butional Memory.
A General Framework for Corpus-based Semantics.
Computational Linguistics, 36:673?721.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, EastStroudsburg, PA, pages 1183?1193.M.
Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.2009.
The WaCky Wide Web: A Collection of VeryLarge Linguistically Processed Web-Crawled Corpora.Language Resources and Evaluation, 43:209?226.D.
Blei and J. McAuliffe.
2007.
Supervised topic mod-els.
Neural Information Processing Systems, 21.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
JMLR, 3:993?1022.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Mass.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the 2010 Workshop onGEometrical Models of Natural Language Semantics,Stroudsburg, PA. Association for Computational Lin-guistics.Zellig Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley.Matthias Hartung and Anette Frank.
2010.
A StructuredVector Space Model for Hidden Attribute Meaning inAdjective-Noun Phrases.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (COLING), Beijing, China, August.Matthias Hartung and Anette Frank.
2011.
Assessing in-terpretable, attribute-related meaning representationsfor adjective-noun phrases in a similarity predictiontask.
In Proceedings of GEometrical Models of Nat-ural Language Semantics (GEMS-2011), Edinburgh,UK.Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.2008.
DiscLDA: Discriminative Learning for Dimen-sionality Reduction and Classification.
In NIPS, vol-ume 22.Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010.Topic models for word sense disambiguation andtoken-based idiom detection.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, ACL ?10, pages 1138?1147, Upp-sala, Sweden.Andrew Kachites McCallum.
2002.
MAL-LET: A machine learning for language toolkit.http://mallet.cs.umass.edu.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedModels of Semantic Composition.
In Proceedings ofACL-08: HLT, pages 236?244, Columbus, Ohio, June.Jeff Mitchell and Mirella Lapata.
2009.
Language Mod-els Based on Semantic Composition.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, Singapore, August 2009,pages 430?439, Singapore, August.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34:1388?1429.Ian Niles.
2003.
Mapping WordNet to the SUMO Ontol-ogy.
In Proceedings of the IEEE International Knowl-edge Engineering conference, pages 23?26, June.Diarmuid ?O Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 435?444, Uppsala, Sweden, July.Association for Computational Linguistics.Sebastian Pado?, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.D.
Prescher, S. Riezler, and M. Rooth.
2000.
Using aprobabilistic class-based lexicon for lexical ambiguityresolution.
In Proceedings of the 18th COLING, pages649?655.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: Asupervised topic model for credit attribution in multi-labeled corpora.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, Singapore, August 2009, pages 248?256.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A La-tent Dirichlet Allocation Method for Selectional Pref-erences.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 424?434, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Sebastian Rudolph and Eugenie Giesbrecht.
2010.
Com-positional matrix-space models of language.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 907?916.
Asso-ciation for Computational Linguistics, July.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing (NeMLaP).
Manchester, U.K., 14?16 September 1994.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37:141?188.550Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proceed-ings of the Fourth Conference on Computational Lan-guage Learning (CoNLL-2000) and the Second Learn-ing Language in Logic Workshop, Lisbon, Portugal,pages 947?953.551
