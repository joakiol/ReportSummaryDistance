Proceedings of the Workshop on Statistical Machine Translation, pages 154?157,New York City, June 2006. c?2006 Association for Computational LinguisticsConstraining the Phrase-Based, Joint Probability Statistical TranslationModelAlexandra Birch Chris Callison-Burch Miles Osborne Philipp KoehnSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LW, UKa.c.birch-mayne@sms.ed.ac.ukAbstractThe joint probability model proposed byMarcu and Wong (2002) provides a strongprobabilistic framework for phrase-basedstatistical machine translation (SMT).
Themodel?s usefulness is, however, limited bythe computational complexity of estimat-ing parameters at the phrase level.
Wepresent the first model to use word align-ments for constraining the space of phrasalalignments searched during ExpectationMaximization (EM) training.
Constrain-ing the joint model improves performance,showing results that are very close to state-of-the-art phrase-based models.
It also al-lows it to scale up to larger corpora andtherefore be more widely applicable.1 IntroductionMachine translation is a hard problem because ofthe highly complex, irregular and diverse natureof natural languages.
It is impossible to accuratelymodel all the linguistic rules that shape the trans-lation process, and therefore a principled approachuses statistical methods to make optimal decisionsgiven incomplete data.The original IBM Models (Brown et al, 1993)learn word-to-word alignment probabilities whichmakes it computationally feasible to estimatemodel parameters from large amounts of train-ing data.
Phrase-based SMT models, such as thealignment template model (Och, 2003), improveon word-based models because phrases providelocal context which leads to better lexical choiceand more reliable local reordering.
However, mostphrase-based models extract their phrase pairsfrom previously word-aligned corpora using ad-hoc heuristics.
These models perform no searchfor optimal phrasal alignments.
Even though thisis an efficient strategy, it is a departure from therigorous statistical framework of the IBM Models.Marcu and Wong (2002) proposed the jointprobability model which directly estimates thephrase translation probabilities from the corpus ina theoretically governed way.
This model neitherrelies on potentially sub-optimal word alignmentsnor on heuristics for phrase extraction.
Instead, itsearches the phrasal alignment space, simultane-ously learning translation lexicons for both wordsand phrases.
The joint model has been shown tooutperform standard models on restricted data setssuch as the small data track for Chinese-English inthe 2004 NIST MT Evaluation (Przybocki, 2004).However, considering all possible phrases andall their possible alignments vastly increases thecomputational complexity of the joint model whencompared to its word-based counterpart.
In thispaper, we propose a method of constraining thesearch space of the joint model to areas wheremost of the unpromising phrasal alignments areeliminated and yet as many potentially usefulalignments as possible are still explored.
Thejoint model is constrained to phrasal alignmentswhich do not contradict a set high confidence wordalignments for each sentence.
These high con-fidence alignments could incorporate informationfrom both statistical and linguistic sources.
In thispaper we use the points of high confidence fromthe intersection of the bi-directional Viterbi wordalignments to constrain the model, increasing per-formance and decreasing complexity.1542 Translation Models2.1 Standard Phrase-based ModelMost phrase-based translation models (Och, 2003;Koehn et al, 2003; Vogel et al, 2003) rely ona pre-existing set of word-based alignments fromwhich they induce their parameters.
In this projectwe use the model described by Koehn et al (2003)which extracts its phrase alignments from a corpusthat has been word aligned.
From now on we re-fer to this phrase-based translation model as thestandard model.
The standard model decomposesthe foreign input sentence F into a sequence ofI phrases f1, .
.
.
, f I .
Each foreign phrase fi istranslated to an English phrase ei using the prob-ability distribution ?
(f i|ei).
English phrases maybe reordered using a relative distortion probability.This model performs no search for optimalphrase pairs.
Instead, it extracts phrase pairs(f i, ei) in the following manner.
First, it uses theIBM Models to learn the most likely word-levelViterbi alignments for English to Foreign and For-eign to English.
It then uses a heuristic to recon-cile the two alignments, starting from the pointsof high confidence in the intersection of the twoViterbi alignments and growing towards the pointsin the union.
Points from the union are selected ifthey are adjacent to points from the intersectionand their words are previously unaligned.Phrases are then extracted by selecting phrasepairs which are ?consistent?
with the symmetrizedalignment, which means that all words within thesource language phrase are only aligned to thewords of the target language phrase and vice versa.Finally the phrase translation probability distribu-tion is estimated using the relative frequencies ofthe extracted phrase pairs.This approach to phrase extraction means thatphrasal alignments are locked into the sym-metrized alignment.
This is problematic becausethe symmetrization process will grow an align-ment based on arbitrary decisions about adjacentwords and because word alignments inadequatelyrepresent the real dependencies between transla-tions.2.2 Joint Probability ModelThe joint model (Marcu and Wong, 2002), doesnot rely on a pre-existing set of word-level align-ments.
Like the IBM Models, it uses EM to alignand estimate the probabilities for sub-sententialunits in a parallel corpus.
Unlike the IBM Mod-els, it does not constrain the alignments to beingsingle words.The joint model creates phrases from words andcommonly occurring sequences of words.
A con-cept, ci, is defined as a pair of aligned phrases< ei, f i >.
A set of concepts which completelycovers the sentence pair is denoted by C. Phrasesare restricted to being sequences of words whichoccur above a certain frequency in the corpus.Commonly occurring phrases are more likely tolead to the creation of useful phrase pairs, andwithout this restriction the search space would bemuch larger.The probability of a sentence and its translationis the sum of all possible alignments C, each ofwhich is defined as the product of the probabilityof all individual concepts:p(F,E) =?C?C?<ei,f i>?Cp(< ei, f i >) (1)The model is trained by initializing the trans-lation table using Stirling numbers of the secondkind to efficiently estimate p(< ei, f i >) by cal-culating the proportion of alignments which con-tain p(< ei, f i >) compared to the total numberof alignments in the sentence (Marcu and Wong,2002).
EM is then performed by first discoveringan initial phrasal alignments using a greedy algo-rithm similar to the competitive linking algorithm(Melamed, 1997).
The highest probability phrasepairs are iteratively selected until all phrases areare linked.
Then hill-climbing is performed bysearching once for each iteration for all merges,splits, moves and swaps that improve the proba-bility of the initial phrasal alignment.
Fractionalcounts are collected for all alignments visited.Training the IBM models is computationallychallenging, but the joint model is much more de-manding.
Considering all possible segmentationsof phrases and all their possible alignments vastlyincreases the number of possible alignments thatcan be formed between two sentences.
This num-ber is exponential with relation to the length of theshorter sentence.3 Constraining the Joint ModelThe joint model requires a strategy for restrictingthe search for phrasal alignments to areas of thealignment space which contain most of the proba-bility mass.
We propose a method which examines155phrase pairs that are consistent with a set of highconfidence word alignments defined for the sen-tence.
The set of alignments are taken from the in-tersection of the bi-directional Viterbi alignments.This strategy for extracting phrase pairs is simi-lar to that of the standard phrase-based model andthe definition of ?consistent?
is the same.
How-ever, the constrained joint model does not lockthe search into a heuristically derived symmetrizedalignment.
Joint model phrases must also occurabove a certain frequency in the corpus to be con-sidered.The constraints on the model are binding duringthe initialization phase of training.
During EM,inconsistent phrase pairs are given a small, non-zero probability and are thus not considered un-less unaligned words remain after linking togetherhigh probability phrase pairs.
All words must bealigned, there is no NULL alignment like in theIBM models.By using the IBM Models to constrain the jointmodel, we are searching areas in the phrasal align-ment space where both models overlap.
We com-bine the advantage of prior knowledge about likelyword alignments with the ability to perform aprobabilistic search around them.4 ExperimentsAll data and software used was from the NAACL2006 Statistical Machine Translation workshopunless otherwise indicated.4.1 ConstraintsThe unconstrained joint model becomes in-tractable with very small amounts of training data.On a machine with 2 Gb of memory, we wereonly able to train 10,000 sentences of the German-English Europarl corpora.
Beyond this, pruning isrequired to keep the model in memory during EM.Table 1 shows that the application of the word con-straints considerably reduces the size of the spaceof phrasal alignments that is searched.
It also im-proves the BLEU score of the model, by guiding itto explore the more promising areas of the searchspace.4.2 ScalabilityEven though the constrained joint model reducescomplexity, pruning is still needed in order to scaleup to larger corpora.
After the initialization phaseof the training, all phrase pairs with counts lessUnconstrained ConstrainedNo.
Concepts 6,178k 1,457kBLEU 19.93 22.13Time(min) 299 169Table 1.
The impact of constraining the joint modeltrained on 10,000 sentences of the German-EnglishEuroparl corpora and tested with the Europarl test setused in Koehn et al (2003)than 10 million times that of the phrase pair withthe highest count, are pruned from the phrase ta-ble.
The model is also parallelized in order tospeed up training.The translation models are included within alog-linear model (Och and Ney, 2002) which al-lows a weighted combination of features func-tions.
For the comparison of the basic systemsin Table 2 only three features were used for boththe joint and the standard model: p(e|f), p(f |e)and the language model, and they were given equalweights.The results in Table 2 show that the joint modelis capable of training on large data sets, with areasonable performance compared to the standardmodel.
However, here it seems that the standardmodel has a slight advantage.
This is almost cer-tainly related to the fact that the joint model resultsin a much smaller phrase table.
Pruning eliminatesmany phrase pairs, but further investigations indi-cate that this has little impact on BLEU scores.BLEU SizeJoint Model 25.49 2.28Standard Model 26.15 19.04Table 2.
Basic system comparisons: BLEU scoresand model size in millions of phrase pairs for Spanish-EnglishThe results in Table 3 compare the joint and thestandard model with more features.
Apart fromincluding all Pharaoh?s default features, we usetwo new features for both the standard and jointmodels: a 5-gram language model and a lexical-ized reordering model as described in Koehn et al(2005).
The weights of the feature functions, ormodel components, are set by minimum error ratetraining provided by David Chiang from the Uni-versity of Maryland.On smaller data sets (Koehn et al, 2003) thejoint model shows performance comparable to thestandard model, however the joint model doesnot reach the level of performance of the stan-156EN-ES ES-ENJoint3-gram, dl4 20.51 26.645-gram, dl6 26.34 27.17+ lex.
reordering 26.82 27.80Standard Model5-gram, dl6+ lex.
reordering 31.18 31.86Table 3.
Bleu scores for the joint model and the stan-dard model showing the effect of the 5-gram languagemodel, distortion length of 6 (dl) and the addition oflexical reordering for the English-Spanish and Spanish-English tasks.dard model for this larger data set.
This couldbe due to the fact that the joint model results ina much smaller phrase table.
During EM onlyphrase pairs that occur in an alignment visited dur-ing hill-climbing are retained.
Only a very smallproportion of the alignment space can be searchedand this reduces the chances of finding optimumparameters.
The small number of alignments vis-ited would lead to data sparseness and over-fitting.Another factor could be efficiency trade-offs likethe fast but not optimal competitive linking searchfor phrasal alignments.4.3 German-English submissionWe also submitted a German-English system usingthe standard approach to phrase extraction.
Thepurpose of this submission was to validate the syn-tactic reordering method that we previously pro-posed (Collins et al, 2005).
We parse the Ger-man training and test corpus and reorder it accord-ing to a set of manually devised rules.
Then, weuse our phrase-based system with standard phrase-extraction, lexicalized reordering, lexical scoring,5-gram LM, and the Pharaoh decoder.On the development test set, the syntactic re-ordering improved performance from 26.86 to27.70.
The best submission in last year?s sharedtask achieved a score of 24.77 on this set.5 ConclusionWe presented the first attempt at creating a system-atic framework which uses word alignment con-straints to guide phrase-based EM training.
Thisshows competitive results, to within 0.66 BLEUpoints for the basic systems, suggesting that arigorous probabilistic framework is preferable toheuristics for extracting phrase pairs and theirprobabilities.By introducing constraints to the alignmentspace we can reduce the complexity of the jointmodel and increase its performance, allowing it totrain on larger corpora and making the model morewidely applicable.For the future, the joint model would benefitfrom lexical weighting like that used in the stan-dard model (Koehn et al, 2003).
Using IBMModel 1 to extract a lexical alignment weight foreach phrase pair would decrease the impact of datasparseness, and other kinds smoothing techniqueswill be investigated.
Better search algorithms forViterbi phrasal alignments during EM would in-crease the number and quality of model parame-ters.This work was supported in part under theGALE program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0011-06-C-0022.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof machine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005.Clause restructuring for statistical machine translation.
InProceedings of ACL.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofHLT/NAACL, pages 127?133.Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,and Chris Callison-Burch.
2005.
Edinburgh system de-scription.
In IWSLT Speech Translation Evaluation.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine translation.In Proceedings of EMNLP.Dan Melamed.
1997.
A word-to-word model of translationalequivalence.
In Proceedings of ACL.Franz Josef Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statistical ma-chine translation.
In ACL.Franz Josef Och.
2003.
Statistical Machine Translation:From Single-Word Models to Alignment Templates.
Ph.D.thesis, RWTH Aachen Department of Computer Science,Aachen, Germany.Mark Przybocki.
2004.
NIST 2004 machine translation eval-uation results.
Confidential e-mail to workshop partici-pants, May.Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003.The CMU statistical machine translation system.
In Ma-chine Translation Summit.157
