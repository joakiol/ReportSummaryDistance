Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 88?95,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPAutomatic Extraction of Citation Contexts for Research PaperSummarization: A Coreference-chain based ApproachDain Kaplan Ryu IidaDepartment of Computer ScienceTokyo Institute of Technology{dain,ryu-i,take}@cl.cs.titech.ac.jpTakenobu TokunagaAbstractThis paper proposes a new method basedon coreference-chains for extracting cita-tions from research papers.
To evaluateour method we created a corpus of cita-tions comprised of citing papers for 4 citedpapers.
We analyze some phenomena ofcitations that are present in our corpus,and then evaluate our method against acue-phrase-based technique.
Our methoddemonstrates higher precision by 7?10%.1 IntroductionReview and comprehension of existing research isfundamental to the ongoing process of conductingresearch; however, the ever increasing volume ofresearch papers makes accomplishing this task in-creasingly more difficult.
To mitigate this problemof information overload, a form of knowledge re-duction may be necessary.Past research (Garfield et al, 1964; Small,1973) has shown that citations contain a plethoraof latent information available and that muchcan be gained by exploiting it.
Indeed, thereis a wealth of literature on topic-clustering, e.g.bibliographic coupling (Kessler, 1963), or co-citation analysis (Small, 1973).
Subsequent re-search demonstrated that citations could be clus-tered on their quality, using keywords that ap-peared in the running-text of the citation (Wein-stock, 1971; Nanba et al, 2000; Nanba et al,2004; Teufel et al, 2006).Similarly, other work has shown the utility inthe IR domain of ranking the relevance of cited pa-pers by using supplementary index terms extractedfrom the content of citations in citing papers,including methods that search through a fixedcharacter-length window (O?Connor, 1982; Brad-shaw, 2003), or that focus solely on the sentencecontaining the citation (Ritchie et al, 2008) foracquiring these terms.
A prior case study (Ritchieet al, 2006) pointed out the challenges in properidentification of the full span of a citation in run-ning text and acknowledged that fixed-width win-dows have their limits.
In contrast to this, en-deavors have been made to extract the entire spanof a citation by using cue-phrases collected anddeemed salient by statistical merit (Nanba et al,2000; Nanba et al, 2004).
This has met in evalua-tions with some success.The Cite-Sum system (Kaplan and Tokunaga,2008) also aims at knowledge reduction throughuse of citations.
It receives a paper title as a queryand attempts to generate a summary of the paperby finding citing papers1 and extracting citationsin the running-text that refer to the paper.
Beforeoutputting a summary, it also classifies extractedcitation text, and removes citations with redun-dant content.
Another similar study (Qazvinianand Radev, 2008) aims at using the content of ci-tations within citing papers to generate summariesof fields of research.It is clear that merit exists behind extractionof citations in running text.
This paper proposesa new method for performing this task based oncoreference-chains.
To evaluate our method wecreated a corpus of citations comprised of citingpapers for 4 cited papers.
We also analyze somephenomena of citations that are present in our cor-pus.The paper organization is as follows.
We firstdefine terminology, discuss the construction of ourcorpus and the results found through its analysis,and then move on to our proposed method us-ing coreference-chains.
We evaluate the proposedmethod by using the constructed corpus, and thenconclude the paper.1Papers are downloaded automatically from the web.882 TerminologySo that we may dispense with convoluted explana-tions for the rest of this paper, we introduce severalterms.An anchor is the string of characters that marksthe occurrence of a citation in the running-text of apaper, such as ?
(Fakeman 2007)?
or ?
[57]?.2 Thesentence that this anchor resides within is then theanchor sentence.
The citation continues from be-fore and after this anchor as long as the text con-tinues to refer to the cited work; this block of textmay span more than a single sentence.
We intro-duce the citation-site, or c-site for short, to rep-resent this block of text that discusses the citedwork.
Since more than once sentence may discussthe cited work, each of these sentences is called ac-site sentence.
For clarity will also call the an-chor the c-site anchor henceforth.
A citing papercontains the c-site that refers to the cited paper.Finally, the reference at the end of the paper pro-vides details about a c-site anchor (and the c-site).Figure 1 shows a sample c-site with the c-siteanchor wavy-underlined, and the c-site itself itali-cized; the non-italicized text is unrelated to the c-site.
The reference for this c-site is also providedbelow the dotted line.
In all subsequent examples,the c-site will be in italics and the current place ofemphasis wavy-underlined.?.
.
.
Our area of interest is plant growth.
In pastresearch (:::::::Fakeman::et:::al.,::::2001), the relationshipbetween sunlight and plant growth was shown todirectly correlate.
It was also shown to adhereto simple equations for deducing this relation-ship, the equation varying by plant.
We proposea method that .
.
.
?.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.J.
Fakeman: Changing Plant Growth Factorsduring Global Warming.
In: Proceedings ofSCANLP 2001.Figure 1: A sample c-site and its reference3 Corpus Construction and AnalysisWe created a corpus comprised of 38 papers citing4 (cited) papers taken from Computational Lin-guistics: Special Issue on the Web as Corpus, Vol-ume 29, Number 3, 2003 as our data set and pre-processed it to automatically mark c-site anchors2In practice the anchor does not include brackets, thoughthe brackets do signal the start/end of the anchor.
This is be-cause multiple anchors may be present at once, e.g.
(Fakeman2007; Noman 2008).to facilitate the annotation process.
The citing pa-pers were downloaded from CiteSeer-X;3 see Ta-ble 1 for details.We then proceeded to manually annotate thecorpus using SLAT (Noguchi et al, 2008), abrowser-based multi-purpose annotation tool.
Wedevised the following guidelines for annotation.Since the tool allows for two types of annotation,namely segments that demarcate a region of text,and links, that allow an annotator to assign rela-tionships between them, we created four segmenttypes and three link types.
Segments were usedto mark c-site anchors, c-sites, background infor-mation (explained presently), and references.
Weused the term background information to refer toany running-text that elaborates on a c-site but isnot strictly part of the c-site itself (refer to Fig-ure 2 for an example).
Even during annotation,however, we encountered situations that felt am-biguous, making this a rather contentious issue.Our corpus had a limited number of backgroundinformation annotations, or we would likely haveexperienced more issues.
That being said, it is atleast important to recognize that such kinds of sup-plementary content exist (that may not be part ofthe c-site but is still beneficial to be included), andneeds to be considered more in the future.We then linked each c-site to its anchor, each an-chor to its reference, and any background informa-tion to the c-site supplemented.
We also decidedon annotating entire sentences, even if only partof a sentence referred to the cited paper.
Table 1outlines our corpus.Table 1: Corpus compositionPaper ID 1 2 3 4 TotalCiting papers 2 14 15 7 38C-sites 3 17 18 12 50C-site sentences 6 27 33 28 94To our knowledge, this is the first corpus con-structed in the context of paper summarization re-lated to collections of citing papers.4Analysis of the corpus provided some interest-ing insights, though a larger corpus is required toconfirm the frequency and validity of such phe-nomena.
The more salient discoveries are item-ized below.
These phenomena may also co-occur.3http://citeseerx.ist.psu.edu4Though not specific to the task of summarization throughuse of c-sites, citation corpora have been constructed in thepast, e.g.
(Teufel et al, 2006).89Background Information Though not strictlypart of a c-site, background information may needto be included for the citation to be comprehensi-ble.
Take Figure 2 for example (background infor-mation is wavy-underlined) for the c-site anchor?
(Resnik & Smith 2003)?.
The authors insert theirown research into the c-site (illustrated with wavy-underlines); this information is important for un-derstanding the following c-site sentence, but isnot strictly discussing the cited paper.
Backgroundinformation is thus a form of ?meta-information?about the c-site.In well written papers, often the flow of contentis gradual, which can make distinguishing back-ground information difficult.?.
.
.Resnik and his colleagues (Resnik & Smith2003) proposed a new approach, STRAND,.
.
.
The databases for parallel texts in several lan-guages with download tools are available fromthe STRAND webpage.
Recently they also ap-plied the same technique for collecting a set oflinks to monolingual pages identified as Russianby http://www.archive.org, and Internet archiv-ing service.::We:::::have:::::::::evaluated:::the:::::::Russian:::::::database::::::::produced::by:::this:::::::method:::and::::::::identified:a:::::::number::of::::::serious::::::::problems:::::with::it.
First, itdoes not identify the time when the page wasdownloaded and stored in the Internet archive.
.
.
?Figure 2: A non-contiguous c-site w/ backgroundinformation (from (Sharoff, 2006))Contiguity C-sites are not necessarily contigu-ous.
We found in fact that authors tend to in-sert opinions or comments related to their ownwork with sentences/clauses in between actual c-site sentences/clauses, that would be best omittedfrom the c-site.
In Figure 2 the wavy-underlinedtext shows the author?s opinion portion.
This cre-ates problems for cue-phrase based techniques, asthough they detect the sentence following it, theyfail on the opinion sentence.
Incorporation of a le-niency for a gap in such techniques may be pos-sible, but seems more problematic and likely tomisidentify c-site sentences altogether.Related/Itemization Authors often list severalworks (namely, insert several c-site anchors) in thesame sentence using connectives.
The works maylikely be related, and though this may be usefulinformation for certain tasks, it is important to dif-ferentiate which material is related to the c-site,and which is the c-site itself.In Figure 3 the second sentence discusses bothc-site anchors (and should be included in boththeir c-sites); the first sentence, however, containstwo main clauses connected with a connective,each clause a different c-site (one with the anchor?[3]?
and one with ?[4]?).
Sub-clausal analysis isnecessary for resolving issues such as these.
Forour current task, however, we annotated only sen-tences, and so in this example the second c-siteanchor is included in the first.?.
.
.
STRAND system [4] searches the web forparallel text:::and:::[3]:::::::extracts::::::::::translations::::pairs:::::among::::::anchor::::texts:::::::pointing:::::::together::to:::the::::same:::::::webpage.
However they all suffered from the lackof such bilingual resources available on the web.
.
.
?Figure 3: Itemized c-sites partially overlapping(from (Zhang et al, 2005))Nesting C-sites may be nested.
In Figure 4the nested citation (?
[Lafferty and Zhai 2001,Lavrenko and Croft 2001]?)
should be included inthe parent one (?
[Kraaij et al 2002]?).
The wavy-underlined portion shows the sentence needed forfull comprehension of the c-site.?.
.
.::In:::::recent:::::years,:::the:::use::of::::::::language::::::models::in::IR:::has::::been::a::::great::::::success::::::::[Lafferty:::and::::Zhai::::2001,::::::::Lavrenko::::and:::::Croft::::::2001].
It is possibleto extend the approach to CLIR by integrating atranslation model.
This is the approach proposedin [Kraaij et al 2002] .
.
.
?Figure 4: Separate c-site anchors does not meanseparate c-sites (from (Nie, 2002))Aliases Figure 5 demonstrates another issue:aliasing.
The author redefines how they cite thepaper, in this case using the acronym ?K&L?.?.
.
.
To address the data-sparsity issue, we em-ployed the technique used in Keller and Lapata(2003, K&L) to get a more robust approxima-tion of predicate-argument counts.
::::K&L use thistechnique to obtain frequencies for predicate-argument bigrams that were unseen in a givencorpus, showing that the massive size of the weboutweighs the noisy and unbalanced nature ofsearches performed on it to produce statisticsthat correlate well with corpus data .
.
.
?Figure 5: C-Site with Aliasing for anchor ?Kellerand Lapata (2003, K&L)?
(from (Kehler, 2004))4 Coreference Chain-based ExtractionSome of the issues found in our corpus, namelyidentification of background information, non-contiguous c-sites, and aliases, show promise of90Table 2: Evaluation results for coreference resolution against the MUC-7 formal corpus.MUC-7 Task Sentence Eval.System Setting R P F R P FAll Features 35.71 74.71 48.33 36.27 80.49 50.00w/o SOON STR MATCH 48.35 83.81 61.32 48.35 88.00 62.41w/o COSINE SIMILARITY 46.70 82.52 59.65 46.70 86.73 60.71resolution with coreference-chains.
This is be-cause coreference-chains match noun phrases thatappear with other noun phrases to which they re-fer, a characteristic present in these three cate-gories.
On the other hand, cue-phrases do notdetect any c-site sentence that does not use key-words (e.g.
?In addition?).
In the following sec-tion we discuss our implementation of a corefer-ence chain-based extraction technique, and howwe then applied it to the c-site extraction task.
Ananalysis of the results then follows.4.1 Training the Coreference ResolverTo create and train our coreference resolver, weused a combination of techniques as outlined orig-inally by (Soon et al, 2001) and subsequentlyextended by (Ng and Cardie, 2002).
Mim-icking their approaches, we used the corporaprovided for the MUC-7 coreference resolutiontask (LDC2001T02, 2001), which includes sets ofnewspaper articles, annotated with coreference re-lations, for both training and testing.
They alsooutlined a list of features to extract for trainingthe resolver to recognize the coreference relations.Specifically, (Soon et al, 2001) established a listof 12 features that compare a given anaphor witha candidate antecedent, e.g.
gender agreement,number agreement, both being pronouns, both partof the same semantic class (i.e.
WordNet synsethyponyms/hypernyms), etc.For training the resolver, a corpus annotatedwith anaphors and their antecedents is processed,and pairs of anaphor and candidate antecedents arecreated so as to have only one positive instanceper anaphor (the annotated antecedent).
Negativeexamples are created by taking all occurrences ofnoun phrases that occur between the anaphor andits antecedent in the text.
The antecedent in thesesteps is also always considered to be to the left of,or preceding, the anaphor; cataphors are not ad-dressed in this technique.We implemented, at least minimally, all 12 ofthese features, with a few additions of what (Ngand Cardie, 2002) hand selected as being mostsalient for increased performance.
We also ex-tended this list by adding a cosine-similarity met-ric between two noun phrases; it uses bag-of-words to create a vector for each noun phrase(where each word is a term in the vector) to com-pute their similarity.
The intuition behind this isthat noun phrases with more similar surface formsshould be more likely to corefer.We further optimized string recognition andplurality detection for handling citation-strings.See Table 3 for the full list of our features.
Whileboth (Soon et al, 2001) and (Ng and Cardie, 2002)induced decision trees (C5 and C4.5, respectively)we opted for using an SVM-based approach in-stead (Vapnik, 1998; Joachims, 1999).
SVMs areknown for being reliable and having good perfor-mance.4.2 Evaluating the Coreference ResolverWe ran our trained SVM classifier against theMUC-7 formal evaluation corpus; the results areshown in Table 2.The results using all features listed in Table 3are inferior to those set forth by (Soon et al,2001; Ng and Cardie, 2002); likely this is dueto poorer selection of features.
Upon analysis, itseems that half of the misidentified antecedentswere still chosen within the correct sentence andmore than 10% identified the proper antecedent,but selected the entire noun phrase (when thatantecedent was marked as, for example, only itshead); the majority of these cases involved theantecedent being only one sentence away fromthe anaphor.
Since the former seemed suspect ofa partial string matching feature, we decided tore-run the tests first excluding our implementa-tion of the SOON STR MATCH feature, and thenour COSINE SIMILARITY feature.
The resultsfor this are shown in Table 2.
It can be seenthat using either of the two string comparison fea-tures works substantially better than with both ofthem in tandem, with the COSINE SIMILARITYfeature showing signs of overall better perfor-mance which is competitive to (Soon et al,91Table 3: Features used for coreference resolution.Feature Possible Values Brief Description (where necessary)ANAPHOR IS PRONOUN T/FANAPHOR IS INDEFINITE T/FANAPHOR IS DEMONSTRATIVE T/FANTECEDENT IS PRONOUN T/FANTECEDENT IS EMBEDDED T/F Boolean indicating if the candidate antecedent is within anotherNP.SOON STR MATCH T/F As per (Soon et al, 2001).
Articles and demonstrative pronounsremoved before comparing NPs.
If any part of the NP matchesbetween candidate and anaphor set to true (T); false otherwise.ALIAS MATCH T/F Creates abbreviations for organizations and proper names in anattempt to find an alias.BOTH PROPER NAMES T/FBOTH PRONOUNS T/F/?NUMBER AGREEMENT T/F/?
Basic morphological rules applied to the words to see if they areplural.COSINE SIMILARITY NUM A cosine similarity score between zero and one is applied to thehead words of each NP.GENDER AGREEMENT T/F/?
If the semantic class is Male or Female, use that gender, other-wise if a salutation is present, or lastly set to Unknown.SEMANTIC CLASS AGREEMENT T/F/?
Followed (Soon et al, 2001) specifications for using basicWordNet synsets, specifically: Female and Male belonging toPerson, Organization, Location, Date, Time, Money, Percentbelonging to Object.
Any other semantic classes mapped toUnknown.2001; Ng and Cardie, 2002).
We exclude theSOON STR MATCH feature in the following ex-periments.However, the MUC-7 task measures the abilityto identity the proper antecedent from a list of can-didates; the c-site extraction task is less ambitiousin that it must only identify if a sentence containsthe antecedent, not which noun phrase it is.
Whenwe evaluate our resolver using these loosened con-ditions it is expected that it will perform better.To accomplish this we reevaluate the resultsfrom the resolver in a sentence-wise manner; wegroup the test instances by anaphor, and then bysentence.
If any noun phrase within the sentenceis marked as positive when there is in fact a pos-itive noun phrase in the sentence, the sentence ismarked as correct, and incorrect otherwise.
Theresults in Table 2 for this simplified task showan increase in recall, and subsequently F-measure.The numbers for the loosened constraints eval-uation are counted by sentence; the original iscounted by noun phrase only.Our system also generates many fewer traininginstances than the previous research, which we at-tribute to a more stringent noun phrase extractionprocedure, but have not investigated thoroughlyyet.4.3 Application to the c-site extraction taskAs outlined above, we used the resolver with theloosened constraints, namely evaluating the sen-tence a potential antecedent is in as likely or not,and not which noun phrase within the sentence isthe actual antecedent.
Using this principle as abase, we devised an algorithm for scanning sen-tences around a c-site anchor sentence to deter-mine their likelihood of being part of the c-site.The algorithm, shown in simplified form in Fig-ure 6, is described below.Starting at the beginning of a c-site anchorsentence AS, scan left-to-right; for every nounphrase encountered within AS, begin a right-to-left sentence-by-sentence search; prepend any sen-tence S containing an antecedent above a certainlikelihood THRESHOLD, until DISTANCE sen-tences have been scanned and no suitable candi-date sentences have been found.
We set the like-lihood score to 1.0, tested ad-hoc for best results,and the distance-threshold to 5 sentences, havingnoted in our corpus that no citation is discontinu-ous by more than 4.In a similar fashion, the algorithm then pro-ceeds to scan text following AS; for every nounphrase NP encountered (moving left-to-right), be-gin a right-to-left search for a suitable antecedent.If a sentence is not evaluated above THRESHOLD,92Table 4: Evaluation results for c-site extraction w/o background informationSentence (Micro-average) C-site (Macro-average)Method R P F R P FBaseline 1 (anchor sentence) 53.2 100 69.4 74.6 100 85.5Baseline 2 (random) 75.5 58.2 65.7 87.4 71.2 78.5Cue-phrases (CP) 64.9 64.9 64.9 84.0 80.9 82.4Coref-chains (CC)) 64.9 74.4 69.3 81.0 87.2 84.0CP/CC Union 74.5 58.8 65.7 88.4 75.0 81.1CP/CC Intersection 55.3 91.2 69.0 76.6 95.7 85.1set CSITE to ASpre:foreach NP in ASforeach sentence S preceding ASif DISTANCE > MAX-DIST goto postif likelihood > THRESHOLD thenset CSITE to S + CSITEreset DISTANCEendendendpost:foreach sentence S after ASforeach NP in Sforeach sentence S2 until Sif DISTANCE > MAX-DIST stopif S2 has link thenif likelihood > THRESHOLD thenset S2 has linkendendendendendFigure 6: Simplified c-site extraction algorithmusing coreference-chainsit will be ignored when the algorithm backtracksto look for candidate noun phrases for a subse-quent sentence, thus preserving the coreference-chain and preventing additional spurious chains.If more than DISTANCE sentences are scannedwithout finding a c-site sentence, the process isaborted and the collection of sentences returned.4.4 Experiment SetupTo evaluate our coreference-chain extractionmethod we compare it with a cue-phrases tech-nique (Nanba et al, 2004) and two baselines.Baseline 1 extracts only the c-site anchor sen-tence as the c-site; baseline 2 includes sentencesbefore/after the c-site anchor sentence as part ofthe c-site with a 50/50 probability ?
it tossesa coin for each consecutive sentence to decideits inclusion.
We also created two hybrid meth-ods that combine the results of the cue-phrasesand coreference-chain techniques, one the unionof their results (includes the extracted sentencesof both methods), and the other the intersection(includes sentences only for which both methodsagree), to measure their mutual compatibility.The annotated corpus provided the locations ofc-site anchors for the cited paper within the citingpaper?s running-text.
We then compared the ex-tracted c-sites of each method to the c-sites of theannotated corpus.4.5 EvaluationThe results of our experiments are presented in Ta-ble 4.
We evaluated each method as follows.
Re-call and precision were measured for a c-site basedon the number of extracted sentences; if an ex-tracted sentence was annotated as part of the c-site,it counted as correct, and if an extracted sentencewas not part of a c-site, incorrect; sentences an-notated as being part of the c-site not extracted bythe method counted as part of the total sentencesfor that c-site.
As an example, if an annotated c-site has 3 sentences (including the c-site anchorsentence), and the evaluated method extracted 2 ofthese and 1 incorrect sentence, then the recall forthis c-site using this method would be 2/3, and theprecision 2/(2 + 1).Since the evaluation is inherently sentence-based, we provide two averages in Table 4.
Themicro-average is for sentences across all c-sites;in other words, we tallied the correct and incorrectsentence count for the whole corpus and then di-vided by the total number of sentences (94).
Thisaverage provides a clearer picture on the efficacyof each method than does the macro-average.
Themacro-average was computed per c-site (as ex-plained above) and then averaged over the totalnumber of c-sites in the corpus (50).With the exception of a 3% lead in macro-average recall, coreference-chains outperformcue-phrases in every way.
We can see a substan-93tial difference in micro-average precision (74.4vs.
64.9), which results in nearly a 5% higherF-measure.
The macro-average precision is alsohigher by more than 6%.
It matches more andmisses far less.
The loss in the macro-averagerecall can be attributed to the coreference-chainmethod missing one of two sentences for severalc-sites, which would lower its overall recall score;keep in mind that since in the macro-average all c-sites are treated equally, even large c-sites in whichthe coreference-chain method performs well, suchan advantage will be reduced with averaging andis therefore misleading.Baseline 2 performed as expected, i.e.
higherthan baseline 1 for recall.
Looking only at F-measures for evaluating performance in this caseis misleading.
This is particularly the case becauseprecision is more important than recall ?
we wantaccuracy.
Coreference-chains achieved a precisionof over 87.2 compared to the 71.2 of baseline 2.The combined methods also showed promise.In particular, the intersection method had veryhigh precision (91.2 and 95.7), and marginallymanaged to extract more sentences than base-line 1.
The union method has more conservativescores.We also understood from our corpus that onlyabout half of c-sites were represented by c-site an-chor sentences.
The largest c-site in the corpuswas 6 sentences, and the average 1.8.
This meansusing the c-site anchor sentence alone excludes onaverage about half of the valuable data.These results are promising, but a larger corpusis necessary to validate the results presented here.5 Conclusions and Future WorkThe results demonstrate that a coreference-chain-based approach may be useful to the c-site ex-traction task.
We can also see that there is stillmuch work to be done.
The scores for the hy-brid methods also indicate potential for a methodthat more tightly couples these two tasks, suchas Rhetorical Structure Theory (RST) (Thompsonand Mann, 1987; Marcu, 2000).
Though it hasdemonstrated superior performance, coreferenceresolution is not a light-weight task; this makesreal-time application more difficult than with cue-phrase-based approaches.Our plans for future work include the construc-tion of a larger corpus of c-sites, investigation ofother features for improving our coreference re-solver, and applying RST to c-site extraction.AcknowledgmentsThe authors would like to express appreciation toMicrosoft for their contribution to this research byselecting it as a recipient of the 2008 WEBSCALEGrant (Web-Scale NLP 2008, 2008).ReferencesShannon Bradshaw.
2003.
Reference directed index-ing: Redeeming relevance for subject search in cita-tion indexes.
In Proceedings of the 7th ECDL, pages499?510.Eugene Garfield, Irving H. Sher, and Richard J. Torpie.1964.
The use of citation data in writing the his-tory of science.
Institute for Scientific Information,Philadelphia, Pennsylvania.Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In BernhardScho?lkopf, Christopher J. C. Burges, and Alexan-der J. Smola, editors, Advances in kernel methods:support vector learning, pages 169?184.
MIT Press,Cambridge, MA, USA.Dain Kaplan and Takenobu Tokunaga.
2008.
Sightingcitation sites: A collective-intelligence approach forautomatic summarization of research papers usingc-sites.
In ASWC 2008 Workshops Proceedings.Andrew Kehler.
2004.
The (non)utility of predicate-argument frequencies for pronoun interpretation.
InIn: Proceedings of 2004 North American chapter ofthe Association for Computational Linguistics an-nual meeting, pages 289?296.M.
M. Kessler.
1963.
Bibliographic coupling be-tween scientific papers.
American Documentation,14(1):10?25.LDC2001T02.
2001.
Message understanding confer-ence (MUC) 7.Daniel Marcu.
2000.
The rhetorical parsing of unre-stricted texts: A surface-based approach.
Computa-tional Linguistics, 26(3):395?448.Hidetsugu Nanba, Noriko Kando, and Manabu Oku-mura.
2000.
Classification of research papers usingcitation links and citation types: Towards automaticreview article generation.
In Proceedings of 11thSIG/CR Workshop, pages 117?134.Hidetsugu Nanba, Takeshi Abekawa, Manabu Oku-mura, and Suguru Saito.
2004.
Bilingual presri inte-gration of multiple research paper databases.
In Pro-ceedings of RIAO 2004, pages 195?211, Avignon,France.94Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting on Asso-ciation for Computational Linguistics, pages 104?111.J.
Nie.
2002.
Towards a unified approach to clir andmultilingual ir.
In In: Workshop on Cross LanguageInformation Retrieval: A Research Roadmap in the25th Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, pages 8?14.Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,Ryu Iida, Mamoru Komachi, and Kentaro Inui.2008.
Multiple purpose annotation using SLAT ?Segment and link-based annotation tool ?.
In Pro-ceedings of 2nd Linguistic Annotation Workshop,pages 61?64, May.John O?Connor.
1982.
Citing statements: Computerrecognition and use to improve retrieval.
Informa-tion Processing & Management., 18(3):125?131.Vahed Qazvinian and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation summarynetworks.Anna Ritchie, Simone Teufel, and Stephen Robertson.2006.
How to find better index terms through cita-tions.
In Proceedings of the Workshop on How CanComputational Linguistics Improve Information Re-trieval?, pages 25?32, Sydney, Australia, July.
As-sociation for Computational Linguistics.Anna Ritchie, Stephen Robertson, and Simone Teufel.2008.
Comparing citation contexts for informa-tion retrieval.
In CIKM ?08: Proceedings of the17th ACM conference on Information and knowl-edge management, pages 213?222, New York, NY,USA.
ACM.Serge Sharoff.
2006.
Creating general-purpose cor-pora using automated search engine queries.
InWaCky!
Working papers on the Web as Corpus.Gedit.H.
Small.
1973.
Co-citation in the scientific literature:A newmeasure of the relationship between two doc-uments.
JASIS, 24:265?269.Wee Meng Soon, Daniel Chung, Daniel Chung YongLim, Yong Lim, and Hwee Tou Ng.
2001.
Amachine learning approach to coreference resolu-tion of noun phrases.
Computational Linguistics,27(4):521?544.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.In In Proceedings of EMNLP-06.Sandra A. Thompson and William C. Mann.
1987.Rhetorical structure theory: A framework for theanalysis of texts.
Pragmatics, 1(1):79?105.Vladimir N. Vapnik.
1998.
Statistical Learning The-ory.
Adaptive and Learning Systems for Signal Pro-cessing Communications, and control.
JohnWiley &Sons.Web-Scale NLP 2008.
2008. http://research.microsoft.com/ur/asia/research/NLP.aspx.M.
Weinstock.
1971.
Citation indexes.
Encyclopediaof Library and Information Science, 5:16?41.Ying Zhang, Fei Huang, and Stephan Vogel.
2005.Mining translations of oov terms from the webthrough.
In International Conference on NaturalLanguage Processing and Knowledge Engineering(NLP-KE ?03), pages 669?670.95
