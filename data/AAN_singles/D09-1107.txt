Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1027?1036,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPSinuhe ?
Statistical Machine Translation using a Globally TrainedConditional Exponential Family Translation ModelMatti K?a?ari?ainenDepartment of Computer ScienceFI-00014 University of Helsinki, Finlandmatti.kaariainen@cs.helsinki.fiAbstractWe present a new phrase-based con-ditional exponential family translationmodel for statistical machine translation.The model operates on a feature repre-sentation in which sentence level transla-tions are represented by enumerating allthe known phrase level translations thatoccur inside them.
This makes the modela good match with the commonly usedphrase extraction heuristics.
The model?spredictions are properly normalized prob-abilities.
In addition, the model automati-cally takes into account information pro-vided by phrase overlaps, and does notsuffer from reference translation reacha-bility problems.We have implemented an open sourcetranslation system Sinuhe based on theproposed translation model.
Our experi-ments on Europarl and GigaFrEn corporademonstrate that finding the unique MAPparameters for the model on large scaledata is feasible with simple stochastic gra-dient methods.
Sinuhe is fast and mem-ory efficient, and the BLEU scores ob-tained by it are only slightly inferior tothose of Moses.1 IntroductionIn current phrase-based statistical machine transla-tion systems such as Moses1(Koehn et al, 2007),the translation model is defined in terms of phrasepairs (biphrases) extracted from a bilingual cor-pus as follows.
The corpus is first word-alignedusing a word alignment heuristic (Och and Ney,1Throughout this paper, we refer to Moses for concrete-ness, but most of the discussion applies to other standardphrase-based statistical machine translation systems as well.2003).
The phrase extraction heuristic then ex-tracts all the biphrases that are compatible withthe word alignment (Och et al, 1999).
This way,each sentence pair may generate any number ofpotentially overlapping biphrases.
However, whendefining the phrase-based sentence level transla-tion model, phrase overlaps are explicitly disal-lowed: The source sentence is segmented into dis-joint phrases, which are translated independentlyusing conditional phrase-level translation modelsthat have been estimated from extracted biphrasecounts.The disparity between the phrase extractionheuristic and the use of the extracted biphrasescan be addressed in at least three ways.
Oneapproach is to simply ignore the disparity as isdone, e.g., in Moses.
While empirically succes-ful, this approach is hard to justify theoretically,and begs the question of whether more principledmethods might lead to better translation results.The other extensively studied approach is to re-place the phrase extraction heuristic with a methodthat better matches the use of the extracted phrases(see, e.g., (Marcu and Wong, 2002; DeNero et al,2008) and the references therein).
While theo-retically sound, this approach is computationallychallenging both in practice (DeNero et al, 2008)and in theory (DeNero and Klein, 2008), may suf-fer from reference reachability problems (DeNeroet al, 2006), and in the end may lead to inferiortranslation quality (Koehn et al, 2003).In this paper, we study a third alternative.
Wepropose a new translation model that is compati-ble with the phrase extraction heuristic.
The pro-posed machine learning inspired translation modeltakes the form of a conditional exponential familyprobability distribution over a feature representa-tion for word-aligned sentence pairs.
The featurerepresentation represents a word-aligned sentencepair by essentially enumerating the (multi)set ofbiphrases that would have been extracted from it,1027together with the source positions at which thebiphrases occur.
The model?s predictions are con-ditional probabilities for such sets of biphrasesgiven the source sentence.The chosen feature representation has many ad-vantages.
Since all word-aligned sentence pairscan be represented, reference reachability prob-lems are automatically circumvented.
For exam-ple, if the translation of a sentence consisted solelyof words that do not occur in the phrase table, thenthe feature vector for the translation would be theall zero vector.
As the training data receives non-zero probability, maximum likelihood or maxi-mum a posteriori (MAP) parameters for the modelcan be estimated in a principled way without re-sorting to pseudo-references.
The fact that themodel is not restricted to using disjoint biphrasesmeans that the information in biphrase overlaps isautomatically taken into account.
This may helpin smoothing the model?s predictions on long andrare phrases, and in enhancing fluency at placesthat otherwise would be phrase boundaries.
Also,the model can be extended in a principled way byintroducing additional features (e.g., translationsfrom a dictionary, biphrases with gaps, biphrasesover POS tags,.
.
.
).The proposed model has one parameter perbiphrase feature, so the total number of parame-ters is easily millions or more.
Still, the modelstructure is designed so that feature expectationsand related quantities can be computed efficientlyby dynamic programming.
It is thus feasible tocompute the gradient of the MAP objective, andsimple gradient ascent can be used to efficientlyfind the globally optimal model parameters (withrespect to a suitably scaled Gaussian prior used forregularization).
Exact inference is also possible bydynamic programming when translations are pre-dicted by the translation model alone.
When otherfeatures like a language model are included, oneneeds to resort to beam search type approximatedynamic programming for decoding.We have implemented a translation systemcalled Sinuhe based on the proposed translationmodel.
The system has been released under theGPLv3 open source license (K?a?ari?ainen, 2009).Our experiments on Europarl and GigaFrEn cor-pora demonstrate that the proposed translationmodel scales well to large data, and offers trans-lation quality that is only slightly worse than thatof the baseline system Moses.
In terms of trans-lation speed, Sinuhe is already clearly better.The rest of this paper is organized as follows.After briefly reviewing related work in Section 2,we describe the proposed translation model in Sec-tion 3.
Finally, experimental results are presentedin Section 4, and conclusions in Section 5.2 Related workThe proposed translation model is strongly influ-enced by machine learning techniques for solv-ing sequence prediction tasks, most notably thework on conditional random fields (Lafferty et al,2001).
The modelling task in machine translationis, however, more complicated than sequence la-belling (not one-to-one, reorderings), so the stan-dard methods cannot be directly applied here.The model we propose is also related to standardphrase-based translation models through the use ofthe same phrase-level translation features.
How-ever, the way we use the features is quite different.There exists a number of discriminative ap-proaches whose model structure, training crite-ria, or both, are similar to ours.
However, to ourknowledge, none of the other systems operatesdirectly on biphrase features, scales up to bilin-gual corpora with millions of sentence pairs, andachieves translation quality comparable to fullytuned standard phrase-based systems.
The ap-proach most closely resembling ours is the in-dependently developed global discriminative log-linear model based on synchronous context-freegrammars (Blunsom and Osborne, 2008; Blun-som et al, 2008).
The version presented in (Blun-som and Osborne, 2008) operates on millions ofrule count features analogous to our biphrase fea-tures, and integrates a language model into train-ing and decoding.
The system can be trained ontens of thousands of short sentences yielding bet-ter translations than a baseline system Hiero onthis data.
The version presented in (Blunsom etal., 2008) scales to more than a hundred thousandshort training sentences, but does not integrate alanguage model and thus has performance that im-proves upon Hiero without a language modelonly.
Both versions deal with derivational ambi-guity by treating derivations as a latent variablesthat are integrated out to get conditional proba-bilities for translations2.
The downside of this2In our translation model, coping with multiple deriva-tions is not needed as there is just one derivation for eachword-aligned sentence pair.
However, dealing with alterna-tive word alignments might be beneficial, though as argued1028is that approximations are needed in computingthe maximum probability translation in decoding,and also in computing model expectations in train-ing when a language model is used.
In addi-tion, since the models operate directly on transla-tions, using probabilistic training criteria for learn-ing the model parameters is possible only if allreference translations in the training data can begenerated by the model.
In practice, this problemcan be circumvented by discarding the trainingsentence pairs with unreachable reference transla-tions, but this may mean a significant reduction inthe amount of training data (24% in (Blunsom etal., 2008)).Another closely related approach is the in-dependently developed discriminative block bi-gram prediction model presented in (Tillmannand Zhang, 2007).
This work proposes a globalphrase-based translation model very similar toours, but due to computational reasons, resorts to alocalized approximation thereof, and is restrictedto biphrases of length at most two.
In (Liang etal., 2006) a standard phrase-based model is aug-mented with more than a million features whoseweights are trained discriminatively by a variantof the perceptron algorithm.
Reference reachabil-ity is again a problem, and the method has not beenscaled up to use biphrase features directly.3 The proposed translation model3.1 Biphrase extractionThe biphrases used in Sinuhe are extracted fromthe training data with the Moses phrase extractionheuristics.
The sentence-aligned training corpus Sis first word-aligned by running Giza++ in bothdirections and then symmetrizing the alignments.This maps the original aligned sentence pairs(x, y) into word-aligned sentence pairs (x, a, y),where a is a many-to-many alignment between thewords in x and y.
Second, using a heuristic pro-posed in (Och et al, 1999), all the aligned phrasepairs (x?, a?, y?)
satisfying the following criteriaare extracted: (1) x?and y?consist of consecutivewords of x and y, and both have length at most k,(2) a?is the alignment between words of x?and y?induced by a, (3) a?contains at least one link, and(4) there are no links in a that have just one end inx?or y?.
Each aligned training sentence (x, a, y)thus generates a number of potentially overlappingin (DeNero et al, 2006), the ambiguity in word alignment isless prevalent than in phrase segmentation.aligned biphrase features (x?, a?, y?).
In our exper-iments, we chose k = 7 which is the default inMoses.
Unlike in Moses, we do not map thealigned biphrases (x?, a?, y?)
back to non-alignedbiphrases (x?, y?
).To reduce the number of extracted biphrases, foreach source phrase x?, only biphrases (x?, a?, y?
)whose occurrence count is among the top K = 20in the training data are retained (rank ties bro-ken by including all biphrases with rank equal tothe limit K).
For technical reasons related to ourdynamic programming algorithms, we also dropbiphrases whose source phrase begins or ends withunlinked words.
Finally, we drop all biphrasesthat occur only once in the training data.
This canbe motivated by a leave-one-out argument (cf thederivation of Good-Turing estimates): Droppingthe biphrases that occur only once in the train-ing data means that the feature representation fora training sentence pair (see Section 3.2) containsonly biphrases that occur also in other training ex-amples.
Without the leave-one-out pruning, thefeature vectors for training sentence pairs wouldbe maximally dense, whereas such feature densitycannot be expected on test data.
Our system canalso be used without the leave-one-out pruning,but according to our preliminary experiments thishas little effect on translation quality.
An excep-tion seems to be morphologically rich languageswith scarce training data on which pruning seemsto reduce translation quality.All the pruning steps combined reduce thephrase table size considerably, but in our experi-ments, millions of biphrases per language pair stillremain (2-4 million for Europarl data and over 95million for GigaFrEn data).3.2 FeaturesOur primary feature representation is a binary fea-ture vector that indicates which aligned biphrasesin the phrase table occur in an aligned sentencepair and where.
More specifically, a source sen-tence x aligned to a target sentence y by an align-ment a is represented by a binary feature vector?
(x, a, y) whose component ?
(x, a, y)(x?,a?,y?
),iis1 iff the aligned biphrase (x?, a?, y?)
occurs atsource position i in (x, a, y), and 0 otherwise.Here, (x?, a?, y?)
occurs in (x, a, y) at source posi-tion i iff the phrase extraction process described inSection 3.1 would have extracted it from (x, a, y)at source position i.1029The weights for the aligned biphrases are tiedtogether by mapping the binary feature vector ?
(indexed by pairs of an aligned biphrase and asource position) to an integral feature vector??
(in-dexed by aligned biphrases only) using the for-mula??(x?,a?,y?)=?i?(x?,a?,y?),i.
The ?real?
fea-tures that drive the translation process are thus thelowest level binary features ?, whereas the higherlevel representation??
is convenient in defining theconditional probabilities given by the translationmodel.3.3 The modelInstead of modelling the conditional distributionP (y|x) directly, we model the conditional dis-tribution P (?
(x, a, y)|x) by the following condi-tional exponential model:P (?
(x, a, y)|x) =exp(w ???
(x, a, y))???
?xexp(w ???
).Here, w is a parameter vector with one componentfor each aligned biphrase feature in the phrase ta-ble.
The set ?xdefines the set of possible predic-tions given x, and includes all feature vectors ?satisfying the following criteria:1.
There exists a translation y?and an alignmenta?such that all active features in ?
occur in(x, a?, y?)2.
Features corresponding to aligned biphrasesthat occur inside aligned biphrases whosefeatures are active in ?
are also active in ?.Thus, the set ?xhas a feature representation forall possible aligned sentence pairs (x, a?, y?)
thathave x as the source side, so all reference trans-lations y?word-aligned to x in any way a?arerepresentable by features in ?x.
By condition1, the predictions given by the model never con-tain conflicting biphrases, so given any predictionof the model, there always exists a translation y?where all the predicted biphrases do occur.
How-ever, since our dynamic programming algorithmscan only force active super-phrases implying ac-tive sub-phrases (condition 2) but not active sub-phrases implying active super-phrases, the set ?xalso contains some feature vectors in which the lat-ter type of implications are not enforced.
Havingsuch redundant representations for some transla-tions is a waste of probability mass, but we hope ithas little effect in practice.The choice of modelling P (?
(x, a, y)|x) in-stead of modelling P (y|x) directly is crucial, bothfrom a modelling and from a computational per-spective.
From the modelling perspective, the cru-cial point is that in our approach, any aligned sen-tence pair (x, a, y) has an associated feature vec-tor ?
(x, a, y) ?
?xthat is reachable (i.e., re-ceives non-zero probability) by the model.
Thismeans it is straightforward to use probabilistic cri-teria in learning the model parameters.
In con-trast, systems modelling P (y|x) directly are oftenplagued by the reference reachability problem.
Touse probabilistic training criteria for such systemsone needs to circumvent the reference reachabilityproblem, e.g., by using pseudo-references or bydropping out the non-reachable portion of trainingdata.Working with the feature vectors ?
(x, a, y) in-stead of working with a and y directly means thatwe model the ordering and choice of words in yonly partially.
This way, when computing the nor-malizing constants and feature expectations, wecan partition the unbounded set of potential trans-lations y and alignments a into a smaller set ofequivalence classes given by ?
(x, a, y).
Thoughthe number of feature vectors ?
?
?xmay belarge (exponential in length of x), all the necessarycomputations can be done exactly and efficientlyby dynamic programming.
For more details, seeSection 3.4.3.3.4 Learning the model parameters3.4.1 The objectiveWe use maximum a posteriori (MAP) estimationto estimate the model parameters w. To con-trol overfitting, we regularize the parameters by asuitably scaled Gaussian prior.
This can be alsoviewed as L2 regularization.
The prior guaranteesthat the MAP parameters are unique, and mod-els our belief that the observed feature occurrencecounts randomly deviate from their ?true?
valuesroughly proportionally to the standard deviationsof the occurrence count distributions.
The priorvariance ?2(x?,a?,y?
)for feature (x?, a?, y?)
is givenby the formula ?2(x?,a?,y?
)= ?/?(x?,a?,y?
), where?
> 0 is a free regularization parameter, and?2(x?,a?,y?
)is an empirical estimate of the varianceof the occurrence count of (x?, a?, y?)
in the train-ing data.
This is similar to (Chen and Rosenfeld,2000), except that we use standard deviations inplace of variances.
As the estimate for the vari-1030ance of a feature we use the occurrence count ofthe corresponding biphrase in the training data.This could be justified by assuming that the oc-currence counts follow a Poisson distribution.
Wehave also run preliminary experiments with otherforms of regularization (different ways of comput-ing ?(x?,a?,y?
), exponential priors corresponding toL1 regularization, no regularization), and it lookslike the system is not very sensitive to the chosenprior.Combining the prior with the model, we see thatthe negative log-posteriorL(w) is given by the for-mula?(x?,a?,y?)w2(x?,a?,y?)2?2(x?,a?,y?)??
(x,a,y)?SlogP (?
(x, a, y)|x) + C,where the sum over (x?, a?, y?)
is understood togo over all aligned biphrase features in the model.This is our criterion for learning w.3.4.2 OptimizationWe solve the optimization problem related tolearning w by first order gradient ascent methods.The gradient ?L(w) of L(w) with respect to wcan be written as?(x?,a?,y?)w(x?,a?,y?)?2(x?,a?,y?)??(x,a,y)?S[??
(x, a, y)?Ew[?
?|x]],where Ew[?
?|x] denotes the conditional expecta-tion of the aligned biphrase occurrence count fea-tures given x with respect to model parameters w.Feature expectations can be computed by combin-ing the results of a left-to-right and right-to-leftdynamic programming sweep over the source sen-tence.
For more details, see Section 3.4.3.Inspired by the empirical results in (Vish-wanathan et al, 2006), we use classic stochas-tic gradient ascent to solve the optimization prob-lem.
At each step t, we sample with replacementa batch Stof b examples from S. We start fromw0= 0, and use the update rulewt+1= wt?
?t?Lt(wt), (1)where ?t> 0 is the learning rate, and ?Lt(w)is the stochastic gradient of the negative log-posteriorLt(w) =|St||S|?iw2i2?2i??
(x,a,y)?StlogP (?
(x, a, y)|x)restricted to batch St.
The second term of thestochastic gradient involves only biphrases whosesource sides match the source sentences in thebatch.
Though the gradient of the regularizer isnon-zero for all non-zero biphrase features, theupdates of features that are not active in the sec-ond term of the gradient can be postponed untilthey become active again.
Due to feature sparsity,the number of features that are active in a smallbatch is small, and thus also the updates are sparse.Hence, it is possible to handle even feature vectorsthat do not fit into memory.Another advantage of the stochastic gradientmethod is that many processes can apply updates(1) to a weight vector asynchronously in parallel.We have implemented two strategies for dealingwith this.
The simpler one is to store the weightvector in a database that takes care of the neces-sary concurrency control.
This way, no processneeds to store the entire weight vector in memory.The downside is that all training processes mustbe able to mmap() to the common file-systemdue to limitations in the underlying Berkeley DBdatabase system.
We have also implemented aclient-server architecture in which a server processstores w in memory and manages read and updaterequests to its components that come from train-ing clients.
In this approach, the degree of paral-lelism is limited only by the number of availablemachines and server capacity.
The server couldbe further distributed for managing models that donot fit into the memory of a single server.3.4.3 Computing gradients etcThe computationally most challenging part inlearning the model parameters is computing?
logP (?
(x, a, y)|x), i.e., the vector of differ-ences between the observed occurrence counts ofbiphrase features in (x, a, y) and their conditionalexpectations under the current model parameters.The conditional feature expectations can becomputed by a dynamic programming proceduresimilar to the one used in training conditional ran-dom fields.
We combine the results of a left-to-right and right-to-left dynamic programming1031sweep over x.
In the left-to-right sweep, wehave for each biphrase feature (x?, a?, y?
), i a states(x?,a?,y?
),ifor translations starting from the be-ginning of x and ending in an occurrence of thebiphrase (x?, a?, y?)
at source position i.
This staterecords the contribution of all partial translationswhose right-most active biphrase feature on thesource side is (x?, a?, y?
), i to the conditional ex-pectation of feature (x?, a?, y?
), i (in log scale).The score for sempty,0= 0, and s(x?,a?,y?
),iis ob-tained from the recurrencescore(s(x?,a?,y?),i)=?(x??,a??,y??),i???A((x?,a?,y?),i)[score(s(x??,a??,y??),i??)+?(x???,a???,y???),i????Bw(x???,a???,y???),i???
]Here, A((x?, a?, y?
), i) is the set of prede-cessor states of s(x?,a?,y?
),iand includes allstates s(x??,a??,y??),i??
such that a proper suffix of(x?
?, a?
?, y??
), i??
(i.e., a biphrase whose source andtarget are proper suffices of x?
?and y?
?respec-tively) is equal to a prefix of (x?, a?, y?
), i. Asa special case, A includes all states s(x??,a??,y??),i?
?for which (x?
?, a?
?, y??
), i?
?ends before or at posi-tion i.
This takes care of translation paths thatleave some words in x untranslated.
Since thestarting position of a proper suffix of a biphraseis always after the biphrase?s original starting po-sition, going through the states in order of in-creasing i guarantees that the scores for biphrasesin A((x?, a?, y?
), i) are available when computingscore(s(x?,a?,y?
),i).The set B that depends on ((x?, a?, y?
), i) and((x?
?, a?
?, y??
), i??)
is defined by the formulaB = sub((x?, a?, y?
), i)\ sub((x?
?, a?
?, y??
), i??
),where sub ((x?, a?, y?
), i) denotes the set of sub-biphrases of (x?, a?, y?
), i (including the biphrase(x?, a?, y?
), i itself).
Thus, summing over theweights of biphrases in B adds the contribution offeatures introduced by extending translation pathsending in (x?
?, a?
?, y??
), i?
?by (x?, a?, y?
), i?.From the right-to-left dynamic programming,we get analogously the contribution of right-to-left partial translations whose left-most activebiphrase is (x?, a?, y?
), i.
The partition functionused for normalizing the expectations can be ob-tained as a side product of either of the sweeps.In conditional random fields, the (unnormal-ized) expectations for the feature can be ob-tained by multiplying the scores of the statescorresponding to the same feature in the left-to-right and right-to-left dynamic programmingmemories.
In our case, combining the two val-ues stored in the states for a feature (x?, a?, y?
), ionly gives the contribution of the translation pathswhere (x?, a?, y?
), i is active but not covered byany longer biphrase that extends (x?, a?, y?
), i bothleft and right.
To include the contribution ofthe remaining translation paths, we need to gothrough states corresponding to super-biphrases of(x?, a?, y?
), i.
Special care has to be taken in orderto include the contribution of all feature vectorsin which such super-biphrases are active exactlyonce.
An efficient way to do this is to process thestates for super-biphrases in topological order withrespect to biphrase inclusion, and to include onlythe contributions of states for super-biphrases thatextend the previously included states both left andright.Another complication in the dynamic program-ming is that a biphrase can extend the source sideof another overlapping biphrase to the right, butthe target side to the left, or visa versa.
Suchoverlaps are not directly covered by our dynamicprogramming.
To deal with them, we constructnew virtual biphrases that correspond to the re-sults of such overlaps in a pre-processing step.
Thenumber of such virtual combinations can in theorygrow exponentially, but in practice only a smallnumber of virtual biphrases seems to suffice.3.5 Prediction with translation model alonePrediction is done in two phases.
First, we find (bya dynamic programming procedure similar to theone outlined in Section 3.4.3) the highest proba-bility feature vector??
(x) defined by??
(x) = argmax??
?x:x covered by biphrases in ?P (?|x).Note that we restrict the search to feature vec-tors that cover the whole of x, i.e., to feature vec-tors ?
(x, a, y) in which each word in x is coveredby at least one active aligned biphrase (x?, a?, y?
).This forces the system to translate all words in thesource sentence even if the translation model pre-dicts that none of the translations are very likely.To translate words that are not covered by anyaligned biphrase feature in the model, we use thefollowing strategy: If the word is found from an1032optional out-of-vocabulary dictionary, we use thetranslation from the dictionary, and otherwise re-sort to an implicit zero weight aligned biphrasethat copies the input word to the output as is.In our experiments, the out-of-vocabulary dictio-nary is constructed from the word translations thatoccur once in the training data, so the out-of-vocabulary dictionary only compensates for theword translations lost in phrase table pruning.
Ifavailable, a real dictionary could be used as well.The second step in predicting a translation issolving the pre-image problem, i.e., constructinga translation y from the predicted feature vector??(x).
Since??
(x) ?
?x, there always exists analignment a and a translation y such that all thealigned biphrases in??
(x) occur in ?
(x, a, y), butthe a and y may not be unique.
We choose the aand y given by concatenating the target sides of thebiphrases active in??
(x) in the order induced bytheir positions in the source sentence.
Thus, thereis no phrase-level reordering, and the fluency ofthe target language output is induced by the phraseoverlaps only.3.6 Predicting with an integrated LMThe prediction strategy outlined in the previoussection is simple and conceptually clean.
How-ever, biphrase overlaps alone may not be enough toenforce fluent output, especially given that bilin-gual data is typically more scarce than monolin-gual data.
Also, the lack of a reverse transla-tion model means the system is unable to iden-tify phrase extraction errors in which rarely seensource phrases are translated to common targetphrases by chance.To address these shortcomings, we augment thetranslation model with the following additionalfeatures that have been observed to enhance trans-lation quality in other SMT systems.1.
Language model: logP (y), where P (y) isgiven by a smoothed n-gram language model2.
Lexical translation model (reverse direc-tion): logP (x|y, a) given by a word-level re-verse translation model3.
Translation length: number of words in y4.
Distortion: number of source words inphrases with swapped translationsThe final score driving the translation processis given by a linear combination of the trans-lation model score logP (?
(x, a, y)|x) and thesefeatures.
Besides the translation model, the lan-guage model feature is clearly the most influential,while the lexical translation feature has only a mi-nor positive effect on translation quality.We use an approximate dynamic program-ming variant of the commonly used beam searchprocedure to find the highest scoring candidatetranslation.
We compute the translation modellog-probability logP (?
(x, a, y)|x) incrementallywhile building up the corresponding candidatetranslation y and word alignment a from left toright.
We allow phrase-level distortions givenby swapping the order of translations of consec-utive non-overlapping source phrases.
Unlike inMoses, our beam search is structured around statetransitions, not around states.
This means that weapply each biphrase (state transition) simultane-ously to all applicable partial translations (states).This strategy is in our experience more efficient,does not rely on future score estimates, and is im-plementationally very similar to the dynamic pro-gramming procedures that we use in training themodel parameters and in prediction with a lan-guage model alone.The weights of the features are tuned by opti-mizing the BLEU score of development set trans-lations with amoeba search.
This simplistic strat-egy is feasible given our system?s fast translationspeed, and extends easily to cover non-linear fea-ture combinations.
The reason for using amoeba isthat it is simpler to implement?we do not believeamoeba yields any better values for the parametersin the end.4 Experiments4.1 Experimental setupOur experiments are on the Europarl translationtasks following the setup used in the shared trans-lation task of the ACL 2008 Third Workshop onStatistical Machine Translation (Callison-Burch etal., 2008), and on the French-to-English transla-tion task of the EACL 2009 Fourth Workshop onStatistical Machine Translation (Callison-Burch etal., 2009).
The size of the Europarl training cor-pora is about 1M sentence pairs per language pair,while the larger GigaFrEn corpus contains about22M sentence pairs.
The corpora were used forbiphrase extraction and translation model training.Decoder feature weights were tuned on the pro-vided development sets.
In case of Europarl, lan-guage models were trained on the target sides of1033es-en en-es fr-en en-fr de-en en-de timeSinuhe 31.38 30.94 31.50 28.91 25.03 19.26 338.0Moses 32.18 31.88 32.63 29.92 27.30 20.57 3729.5Sinuhetrans29.14 27.12 28.74 26.06 22.38 17.14 44.2Mosestrans24.32 22.75 23.84 21.22 19.62 13.59 1321.5Table 1: Left: The translation quality of the SMT systems as measured by the BLEU score.
Translationswere detokenized but not recased before evaluating their quality against lowercased reference translationsby the mteval-v11b.pl script.
Right: Average total translation time in seconds.the bilingual corpora.
In the GigaFrEn experi-ments we used the provided monolingual news do-main data.
All data was tokenized and lowercasedusing the tools in the Moses distribution.We experimented with four translation sys-tems: Sinuhetrans, Sinuhe, Mosestrans, andMoses.
Sinuhetransuses only the translationmodel in producing translations (see Section 3.5),while the full system Sinuhe uses also a lan-guage model and some additional features (seeSection 3.6).
As a baseline, we used the Mosestranslation system, which is known to be verycompetitive on the Europarl translation tasks asevidenced by the University of Edinburgh entriesin the translation challenge (Callison-Burch et al,2008).
The other comparison point Mosestranswas obtained from Moses by disabling distortionsand setting setting the weights of all features ex-cept the forward translation model to 0.
By com-paring Sinuhetransand Mosestrans, we hope toindirectly compare the performance of the under-lying translation models.
A more direct compar-ison was not possible as it is not feasible to nor-malize the ?probabilities?
predicted by the Mosestranslation model.4.1.1 Training the modelsWe trained Moses exactly as suggestedin (Callison-Burch et al, 2008), except thatwe used the -unk option for SRILM in trainingthe language models (both for Sinuhe andMoses).
The translation model for Sinuhe(and Sinuhetrans) was built from the phrasesextracted by Moses as described in Section 3.1.We chose ?
= 1.0 and set batch size to 1.
Thelearning rate was initially set to 0.1, and decayedproportional to 1/t after 2M or 100M iterationsof training for Europarl and GigaFrEn tasks,respectively.
These choices may not be optimalas we did not experiment with other choicesyet.
In case of Europarl, training was run for70-100M iterations using the Berkeley DB baseddistribution strategy (4 CPU cores per languagepair).
This took 10 days.
For GigaFrEn, weused the client-server architecture, and trained themodel for 620M stochastic gradient iterations onabout 200 CPUs.
This took 2 days, which is a lotless than the time needed to run (parallel) Gizaon this data.
The number of biphrase features inSinuhe?s model was 2-4 million on the Europarltasks, and about 95 million on the GigaFrEn task.The decoder parameters for Sinuhe weretuned on the development sets by amoeba, and forMoses by MERT.
As both amoeba and MERT tryto solve the same optimization problem, we be-lieve the difference in optimization methods haslittle influence on the results.4.1.2 Translation resultsEuroparl tasks The systems were tested onthe 2000 sentence Europarl domain develop-ment test sets provided for the shared translationtask (Callison-Burch et al, 2008).
The resultingBLEU scores and total translation times averagedover the datasets are reported in Table 1.
WhileMoses has the highest BLEU score for all thelanguage pairs, the BLEU score for Sinuhe isworse by only at most 1.31 BLEU points excepton the de-en task, where the difference is 2.27.Sinuhetransis clearly inferior to Sinuhe butequally clearly superior to Mosestrans.It takes less than a minute to translate thedevelopment test set by the fastest systemSinuhetrans.
The slowest system Moses needsaround an hour for the same task.
Memory usagefollows a similar pattern.
For example, Sinuherequires roughly one tenth of the memory usedby Moses.
Thus, in terms of resource usage,Sinuhetransand Sinuhe seem clearly supe-rior to Moses.
The quantitative results wouldchange if the systems?
parameters were optimizedfor speed rather than quality, but the differences1034are so clear that the general pattern would proba-bly remain the same.
For example, Moses withno distortion is still clearly slower than Sinuhe.GigaFrEn task The fr-en model was tested onthe 2525 sentence news domain test data used inthe preliminary evaluation of the translation chal-lenge results.
The BLEU scores for Sinuhe andMoses were 26.32 and 26.98, respectively.
Thetotal translation time was 13m 50s with Sinuhe,whereas Moses needed 82m 28s.
Thus, the pat-tern that was observed on Europarl tasks is re-peated here: The translation quality of Moses isslightly better, but Sinuhe is significantly faster.Surprisingly, both Sinuhe and Moses fare wellin comparison to the participants of the actualchallenge: According to the preliminary resultson the same test data we used (Koehn, 2009),the Moses baseline would have been beaten onlyby Google, and Sinuhe would have been sixthamong the 23 participating systems with a differ-ence of only 0.57 BLEU points to the second bestentry.
A partial explanation for the good relativeperformance could be that the challenge partici-pants had only a week to train their models onthe full version of GigaFrEn data, so they may nothave had time to take full advantage of it.
On theother hand, many of the top ranked systems reliedon external resources that were not available forus.Based on an informal human evaluation of theoutputs of Sinuhe and Moses, it looks like thetranslations of Sinuhe are slightly more accu-rate in conveying the meaning of the original sen-tences, but especially the translations of long rareexpressions (e.g., multi-word names of institu-tions) are less fluent.
This hints that the parame-ters for (rare) biphrases may have been regularizedtoo heavily ?
it looks like Sinuhe is underfittingrather than overfitting.
We will conduct more ex-periments to see how much the translation qualitycan be improved by a better choice of ?
or by us-ing a different prior for regularization.
Of course,there is room for tuning elsewhere, too.
For exam-ple, it would be a surprise if the phrase extractionpipeline that has been optimized for Moseswouldbe optimal for Sinuhe.5 ConclusionsIn this paper, we have shown that phrase-basedSMT can be viewed as an instance of structuralprediction.
The word alignment and phrase ex-traction heuristics serve as a strategy for featureextraction, and the translation task can be mod-elled as a structural prediction problems over thesefeatures.
Our methods scale to large corpora andare fast at predicting translations.
While speed isnot the primary goal, the faster translation timesmay be a key to success in applications where theamount of text that needs to be translated is large.In terms of BLEU scores, the results do not im-prove the state-of-the-art so far.
However, fine-tuning the standard phrase-based approach overthe years has increased its performance signifi-cantly, and we see no reason why the same wouldnot happen with the proposed approach, especiallyif the model is augmented with additional featureslike gapped biphrases and biphrases over POStags.The fact that our translation model is a prop-erly normalized conditional probability distribu-tion opens up many new possibilities.
For in-stance, instead of predicting translations, it is pos-sible to efficiently compute the expected num-ber of times each word would appear in them.Such output might be useful, e.g., if the transla-tions are to be post-processed by models relyingon bag-of-words representation.
Another researchdirection we are currently looking into is train-ing the proposed translation model in the reversedirection, and then predicting translations usingthe noisy channel approach, i.e., by maximizingP (x|y)P (y).
The key difference to previous workhere is that since P (x|y) is properly normalized,the noisy channel approach would not in our casesuffer from the potentially negative effects causedby ignoring the normalizer that depends on y. Be-sides being a viable (though computationally de-manding) alternative criterion for predicting trans-lations, the noisy channel approach could easily beused for, e.g., reranking n-best lists and for systemcombination.AcknowledgmentsThis work has been partially funded by theSMART EU project.
We wish to thank the anony-mous reviewers for their constructive commentsand especially for pointing out the related pa-per (Blunsom and Osborne, 2008) that we hadmissed.
Special thanks to Vladimir Poroshin forall the bugs he found in beta-testing and to EstherGalbrun for her comments on a draft version ofthis paper.1035ReferencesPhil Blunsom and Miles Osborne.
2008.
Probabilisticinference for machine translation.
In EMNLP.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In ACL.Chris Callison-Burch, Philipp Koehn, Christof Monz,Josh Schroeder, and Cameron Shaw Fordyce.2008.
ACL 2008 third workshop on statistical ma-chine translation.
http://www.statmt.org/wmt08.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
EACL 2009 fourthworkshop on statistical machine translation.
http://www.statmt.org/wmt09.Stanley Chen and Ronald Rosenfeld.
2000.
Asurvey of smoothing techniques for ME models.IEEE Transactions Speech and Audio Processing,8(1):37?50.John DeNero and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In ACL.John DeNero, Dan Gillick, James Zhang, and DanKlein.
2006.
Why generative phrase models un-derperform surface heuristics.
In Workshop on SMTat NAACL.John DeNero, Alex Bouchard, and Dan Klein.
2008.Sampling alignment structure under a bayesiantranslation model.
In EMNLP.Matti K?a?ari?ainen.
2009.
Sinuhe source code dis-tribution (v1.2).
Website.
http://www.cs.helsinki.fi/u/mtkaaria/sinuhe.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, andChris Callison-Burch et al 2007.
Moses: Opensource toolkit for statistical machine translation.
InACL.Philipp Koehn.
2009.
BLEU/NIST scores for sub-missions.
http://groups.google.com/group/WMT09/browse thread/thread/bfbce7b219648a4c.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In ICML.Percy Liang, Alexandre Bouchard-Cote, Dan Klein,and Ben Taskar.
2006.
An end-to-end discrimina-tive approach to machine translation.
In ACL.Daniel Marcu and William Wong.
2002.
A phrase-based, joint probability model for statistical machinetranslation.
In EMNLP.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29:2003.Franz Josef Och, Christoph Tillmann, and HermannNey.
1999.
Improved alignment models for statisti-cal machine translation.
In EMNLP, pages 20?28.Cristoph Tillmann and Tong Zhang.
2007.
A block bi-gram prediction model for statistical machine trans-lation.
ACM Transacions on Speech and LanguageProcessing, 4(3).S.
V. N. Vishwanathan, Nicol N. Schraudolph, MarkW.Schmidt, and Kevin P. Murphy.
2006.
Acceleratedtraining of conditional random fields with stochasticgradient methods.
In ICML.1036
