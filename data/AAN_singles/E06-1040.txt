Comparing Automatic and Human Evaluation of NLG SystemsAnja BelzNatural Language Technology GroupCMIS, University of BrightonUKA.S.Belz@brighton.ac.ukEhud ReiterDept of Computing ScienceUniversity of AberdeenUKereiter@csd.abdn.ac.ukAbstractWe consider the evaluation problem inNatural Language Generation (NLG) andpresent results for evaluating several NLGsystems with similar functionality, includ-ing a knowledge-based generator and sev-eral statistical systems.
We compare eval-uation results for these systems by humandomain experts, human non-experts, andseveral automatic evaluation metrics, in-cluding NIST, BLEU, and ROUGE.
Wefind that NIST scores correlate best (>0.8) with human judgments, but that allautomatic metrics we examined are bi-ased in favour of generators that select onthe basis of frequency alone.
We con-clude that automatic evaluation of NLGsystems has considerable potential, in par-ticular where high-quality reference textsand only a small number of human evalua-tors are available.
However, in general it isprobably best for automatic evaluations tobe supported by human-based evaluations,or at least by studies that demonstrate thata particular metric correlates well with hu-man judgments in a given domain.1 IntroductionEvaluation is becoming an increasingly importanttopic in Natural Language Generation (NLG), asin other fields of computational linguistics.
SomeNLG researchers are impressed by the success ofthe BLEU evaluation metric (Papineni et al, 2002)in Machine Translation (MT), which has trans-formed the MT field by allowing researchers toquickly and cheaply evaluate the impact of newideas, algorithms, and data sets.
BLEU and re-lated metrics work by comparing the output of anMT system to a set of reference (?gold standard?
)translations, and in principle this kind of evalua-tion could be done with NLG systems as well.
In-deed NLG researchers are already starting to useBLEU (Habash, 2004; Belz, 2005) in their evalua-tions, as this is much cheaper and easier to organ-ise than the human evaluations that have tradition-ally been used to evaluate NLG systems.However, the use of such corpus-based evalua-tion metrics is only sensible if they are known tobe correlated with the results of human-based eval-uations.
While studies have shown that ratings ofMT systems by BLEU and similar metrics corre-late well with human judgments (Papineni et al,2002; Doddington, 2002), we are not aware of anystudies that have shown that corpus-based evalu-ation metrics of NLG systems are correlated withhuman judgments; correlation studies have beenmade of individual components (Bangalore et al,2000), but not of systems.In this paper we present an empirical studyof how well various corpus-based metrics agreewith human judgments, when evaluating severalNLG systems that generate sentences which de-scribe changes in the wind (for weather forecasts).These systems do not perform content determina-tion (they are limited to microplanning and realisa-tion), so our study does not address corpus-basedevaluation of content determination.2 Background2.1 Evaluation of NLG systemsNLG systems have traditionally been evaluatedusing human subjects (Mellish and Dale, 1998).NLG evaluations have tended to be of the intrinsictype (Sparck Jones and Galliers, 1996), involvingsubjects reading and rating texts; usually subjects313are shown both NLG and human-written texts, andthe NLG system is evaluated by comparing the rat-ings of its texts and human texts.
In some cases,subjects are shown texts generated by several NLGsystems, including a baseline system which servesas another point of comparison.
This methodologywas first used in NLG in the mid-1990s by Coch(1996) and Lester and Porter (1997), and contin-ues to be popular today.Other, extrinsic, types of human evaluationsof NLG systems include measuring the impactof different generated texts on task performance(Young, 1999), measuring how much experts post-edit generated texts (Sripada et al, 2005), andmeasuring how quickly people read generatedtexts (Williams and Reiter, 2005).In recent years there has been growing interestin evaluating NLG texts by comparing them to acorpus of human-written texts.
As in other ar-eas of NLP, the advantages of automatic corpus-based evaluation are that it is potentially muchcheaper and quicker than human-based evaluation,and also that it is repeatable.
Corpus-based evalu-ation was first used in NLG by Langkilde (1998),who parsed texts from a corpus, fed the output ofher parser to her NLG system, and then comparedthe generated texts to the original corpus texts.Similar evaluations have been used e.g.
by Banga-lore et al (2000) andMarciniak and Strube (2004).Such corpus-based evaluations have sometimesbeen criticised in the NLG community, for exampleby Reiter and Sripada (2002).
Grounds for crit-icism include the fact that regenerating a parsedtext is not a realistic NLG task; that texts can bevery different from a corpus text but still effec-tively meet the system?s communicative goal; andthat corpus texts are often not of high enough qual-ity to form a realistic test.2.2 Automatic evaluation of generated textsin MT and SummarisationThe MT and document summarisation communi-ties have developed evaluation metrics based oncomparing output texts to a corpus of human texts,and have shown that some of these metrics arehighly correlated with human judgments.The BLEU metric (Papineni et al, 2002) in MThas been particularly successful; for example MT-05, the 2005 NIST MT evaluation exercise, usedBLEU-4 as the only method of evaluation.
BLEUis a precision metric that assesses the quality of atranslation in terms of the proportion of its word n-grams (n = 4 has become standard) that it shareswith one or more high-quality reference transla-tions.
BLEU scores range from 0 to 1, 1 being thehighest which can only be achieved by a transla-tion if all its substrings can be found in one of thereference texts (hence a reference text will alwaysscore 1).
BLEU should be calculated on a largetest set with several reference translations (four ap-pears to be standard in MT).
Properly calculatedBLEU scores have been shown to correlate reliablywith human judgments (Papineni et al, 2002).The NIST MT evaluation metric (Doddington,2002) is an adaptation of BLEU, but where BLEUgives equal weight to all n-grams, NIST gives moreimportance to less frequent (hence more infor-mative) n-grams.
BLEU?s ability to detect subtlebut important differences in translation quality hasbeen questioned, some research showing NIST tobe more sensitive (Doddington, 2002; Riezler andMaxwell III, 2005).The ROUGE metric (Lin and Hovy, 2003) wasconceived as document summarisation?s answer toBLEU, but it does not appear to have met with thesame degree of enthusiasm.
There are several dif-ferent ROUGE metrics.
The simplest is ROUGE-N,which computes the highest proportion in any ref-erence summary of n-grams that are matched bythe system-generated summary.
A procedure isapplied that averages the score across leave-one-out subsets of the set of reference texts.
ROUGE-N is an almost straightforward n-gram recall met-ric between two texts, and has several counter-intuitive properties, including that even a text com-posed entirely of sentences from reference textscannot score 1 (unless there is only one refer-ence text).
There are several other variants of theROUGE metric, and ROUGE-2, along with ROUGE-SU (based on skip bigrams and unigrams), wereamong the official scores for the DUC 2005 sum-marisation task.2.3 SUMTIMEThe SUMTIME project (Reiter et al, 2005) de-veloped an NLG system which generated textualweather forecasts from numerical forecast data.The SUMTIME system generates specialist fore-casts for offshore oil rigs.
It has two modules:a content-determination module that determinesthe content of the weather forecast by analysingthe numerical data using linear segmentation and314other data analysis techniques; and a microplan-ning and realisation module which generates textsbased on this content by choosing appropriatewords, deciding on aggregation, enforcing thesublanguage grammar, and so forth.
SUMTIMEgenerates very high-quality texts, in some casesforecast users believe SUMTIME texts are betterthan human-written texts (Reiter et al, 2005).SUMTIME is a knowledge-based NLG system.While its design was informed by corpus analysis(Reiter et al, 2003), the system is based on manu-ally authored rules and code.As part of the project, the SUMTIME team cre-ated a corpus of 1045 forecasts from the commer-cial output of five different forecasters and the in-put data (numerical predictions of wind, tempera-ture, etc) that the forecasters examined when theywrote the forecasts (Sripada et al, 2003).
In otherwords, the SUMTIME corpus contains both the in-puts (numerical weather predictions) and the out-puts (forecast texts) of the forecast-generation pro-cess.
The SUMTIME team also derived a con-tent representation (called ?tuples?)
from the cor-pus texts similar to that produced by SUMTIME?scontent-determination module.
The SUMTIMEmicroplanner/realiser can be driven by these tu-ples; this mode (combining human content deter-mination with SUMTIME microplanning and real-isation) is called SUMTIME-Hybrid.
Table 1 in-cludes an example of the tuples extracted from thecorpus text (row 1), and a SUMTIME-Hybrid textproduced from the tuples (row 5).2.4 pCRU language generationStatistical NLG has focused on generate-and-selectmodels: a set of alternatives is generated and oneis selected with a language model.
This techniqueis computationally very expensive.
Moreover, theonly type of language model used in NLG are n-gram models which have the additional disadvan-tage of a general preference for shorter realisa-tions, which can be harmful in NLG (Belz, 2005).pCRU1 language generation (Belz, 2006) is alanguage generation framework that was designedto facilitate statistical generation techniques thatare more efficient and less biased.
In pCRU gen-eration, a base generator is encoded as a set ofgeneration rules made up of relations with zeroor more atomic arguments.
The base generator1Probabilistic Context-free Representational Underspeci-fication.is then trained on raw text corpora to provide aprobability distribution over generation rules.
Theresulting PCRU generator can be run in severalmodes, including the following:Random: ignoring pCRU probabilities, randomlyselect generation rules.N-gram: ignoring pCRU probabilities, generateset of alternatives and select the most likely ac-cording to a given n-gram language model.Greedy: select the most likely among each set ofcandidate generation rules.Greedy roulette: select rules with likelihood pro-portional to their pCRU probability.The greedy modes are deterministic and there-fore considerably cheaper in computational termsthan the equivalent n-gram method (Belz, 2005).3 Experimental ProcedureThe main goal of our experiments was to deter-mine how well a variety of automatic evaluationmetrics correlated with human judgments of textquality in NLG.
A secondary goal was to deter-mine if there were types of NLG systems for whichthe correlation of automatic and human evaluationwas particularly good or bad.Data: We extracted from each forecast in theSUMTIME corpus the first description of wind (at10m height) from every morning forecast (the textshown in Table 1 is a typical example), which re-sulted in a set of about 500 wind forecasts.
Weexcluded several forecasts for which we had no in-put data (numerical weather predictions) or an in-complete set of system outputs; this left 465 texts,which we used in our evaluation.The inputs to the generators were tuples com-posed of an index, timestamp, wind direction,wind speed range, and gust speed range (see ex-amples at top of Table 1).We randomly selected a subset of 21 forecastdates for use in human evaluations.
For these 21forecast dates, we also asked two meteorologistswho had not contributed to the original SUMTIMEcorpus to write new forecasts texts; we used theseas reference texts for the automatic metrics.
Theforecasters created these texts by rewriting the cor-pus texts, as this was a more natural task for themthan writing texts based on tuples.500 wind descriptions may seem like a smallcorpus, but in fact provides very good coverage as315Input [[0,0600,SSW,16,20,-,-],[1,NOTIME,SSE,-,-,-,-],[2,0000,VAR,04,08,-,-]]Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENINGHuman1 SSW?LY 16-20 GRADUALLY BACKING SSE?LY THEN DECREASING VARIABLE 4-8 BY LATE EVENINGHuman2 SSW 16-20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE 4-8 BY LATE EVENINGSumTime SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHTpCRU-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATER-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8Table 1: Input tuples with corresponding forecasts in corpus, written by two experts and generated by allsystems (for 5 Oct 2000).the domain language is extremely simple, involv-ing only about 90 word forms (not counting num-bers and wind directions) and a small handful ofdifferent syntactic structures.Systems and texts evaluated: We evaluatedfour pCRU generators and the SUMTIME system,operating in Hybrid mode (Section 2.3) for bettercomparability because the pCRU generators do notperform content determination.A base pCRU generator was created semi-automatically by running a chunker over the cor-pus, extracting generation rules and adding somehigher-level rules taking care of aggregation, eli-sion etc.
This base generator was then trained on9/10 of the corpus (the training data).
5 differentrandom divisions of the corpus into training andtesting data were used (i.e.
all results were val-idated by 5-fold hold-out cross-validation).
Ad-ditionally, a back-off 2-gram model with Good-Turing discounting and no lexical classes was builtfrom the same training data, using the SRILMtoolkit (Stolcke, 2002).
Forecasts were then gen-erated for all corpus inputs, in all four generationmodes (Section 2.4).Table 1 shows an example of an input to the sys-tems, along with the three human texts (Corpus,Human1, Human2) and the texts produced by allfive NLG systems from this data.Automatic evaluations: We used NIST2,BLEU3, and ROUGE4 to automatically evaluate theabove systems and texts.
We computed BLEU-Nfor N = 1..4 (using BLEU-4 as our main BLEUscore).
We also computed NIST-5 and ROUGE-4.As a baseline we used string-edit (SE) distance2http://cio.nist.gov/esd/emaildir/lists/mt list/bin00000.bin3ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl4http://www.isi.edu/?cyl/ROUGE/latest.htmlwith substitution at cost 2, and deletion andinsertion at cost 1, and normalised to range 0 to1 (perfect match).
When multiple reference textsare used, the SE score for a generator forecastis the average of its scores against the referencetexts; the SE score for a set of generator forecastsis the average of scores for individual forecasts.Human evaluations: We recruited 9 experts(people with experience reading forecasts for off-shore oil rigs) and 21 non-experts (people with nosuch experience).
Subjects did not have a back-ground in NLP, and were native speakers of En-glish.
They were shown forecast texts from all thegenerators and from the corpus, and asked to scorethem on a scale of 0 to 5, for readability, clarityand general appropriateness.
Experts were addi-tionally shown the numerical weather data that theforecast text was based on.
At the start, subjectswere shown two practice examples.
The exper-iments were carried out over the web.
Subjectscompleted the experiment unsupervised, at a timeand place of their choosing.Expert subjects were shown a randomly se-lected forecast for 18 of the dates.
The non-expertswere shown 21 forecast texts, in a repeated Latinsquares (non-repeating column and row entries)experimental design where each combination ofdate and system is assigned one evaluation.4 ResultsTable 2 shows evaluation scores for the five NLGsystems and the corpus texts as assessed by ex-perts, non-experts, NIST-5, BLEU-4, ROUGE-4 andSE.
Scores are averaged over the 18 forecasts thatwere used in the expert experiments (for which wehad scores by all metrics and humans) in orderto make results as directly comparable as possi-316System Experts Non-experts NIST-5 BLEU-4 ROUGE-4 SESUMTIME-Hybrid 0.762 (1) 0.77 (1) 5.985 (2) 0.552 (2) 0.192 (3) 0.582 (3)pCRU-greedy 0.716 (2) 0.68 (3) 6.549 (1) 0.613 (1) 0.315 (1) 0.673 (1)SUMTIME-Corpus 0.644 (-) 0.736 (-) 8.262 (-) 0.877 (-) 0.569 (-) 0.835 (-)pCRU-roulette 0.622 (3) 0.714 (2) 5.833 (3) 0.478 (4) 0.156 (4) 0.571 (4)pCRU-2gram 0.536 (4) 0.65 (4) 5.592 (4) 0.519 (3) 0.223 (2) 0.626 (2)pCRU-random 0.484 (5) 0.496 (5) 4.287 (5) 0.296 (5) 0.075 (5) 0.464 (5)Table 2: Evaluation scores against 2 reference texts, for set of 18 forecasts used in expert evaluation.Experts Non-experts NIST-5 BLEU-4 ROUGE-4 SEExperts 1 (0.799) 0.845 (0.510) 0.825 0.791 0.606 0.576Non-experts 0.845 (0.496) 1 (0.609) 0.836 0.812 0.534 0.627NIST-5 0.825 (0.822) 0.836 (0.83) 1 (0.991) 0.973 0.884 0.911BLEU-4 0.791 (0.790) 0.812 (0.808) 0.973 1 (0.995) 0.925 0.949ROUGE-4 0.606 (0.604) 0.534 (0.534) 0.884 0.925 1 (0.995) 0.974SE 0.576 (0.568) 0.627 (0.614) 0.911 0.949 0.974 1 (0.984)Table 3: Pearson correlation coefficients between all scores for systems in Table 2.ble.
Human scores are normalised to range 0 to 1.Systems are ranked in order of the scores given tothem by experts.
All ranks are shown in bracketsbehind the absolute scores.Both experts and non-experts score SUMTIME-Hybrid the highest, and pCRU-2gram and pCRU-random the lowest.
The experts have pCRU-greedy in second place, where the non-expertshave pCRU-roulette.
The experts rank the corpusforecasts fourth, the non-experts second.We used approximate randomisation (AR) asour significance test, as recommended by Riezlerand Maxwell III (2005).
Pair-wise tests betweenresults in Table 2 showed all but three differencesto be significant with the likelihood of incorrectlyrejecting the null hypothesis p < 0.05 (the stan-dard threshold in NLP).
The exceptions were thedifferences in NIST and SE scores for SUMTIME-Hybrid/pCRU-roulette, and the difference in BLEUscores for SUMTIME-Hybrid/pCRU-2gram.Table 3 shows Pearson correlation coefficients(PCC) for the metrics and humans in Table 2.The strongest correlation with experts and non-experts is achieved by NIST-5 (0.82 and 0.83),with ROUGE-4 and SE showing especially poorcorrelation.
BLEU-4 correlates fairly well with thenon-experts but less with the experts.We computed another correlation statistic(shown in brackets in Table 3) which measureshow well scores by an arbitrary single human orrun of a metric correlate with the average scores bya set of humans or runs of a metric.
This is com-puted as the average PCC between the scores as-signed by individual humans/runs of a metric (in-dexing the rows in Table 3) and the average scoresassigned by a set of humans/runs of a metric (in-dexing the columns in Table 3).
For example, thePCC for non-experts and experts is 0.845, but theaverage PCC between individual non-experts andaverage expert judgment is only 0.496, implyingthat an arbitrary non-expert is not very likely tocorrelate well with average expert judgments.
Ex-perts are better predictors for each other?s judg-ments (0.799) than non-experts (0.609).
Interest-ingly, it turns out that an arbitrary NIST-5 run is abetter predictor (0.822) of average expert opinionthan an arbitrary single expert (0.799).The number of forecasts we were able to usein our human experiments was small, and to backup the results presented in Table 2 we reportNIST-5, BLEU-4, ROUGE-4 and SE scores aver-aged across the five test sets from the pCRU val-idation runs, in Table 4.
The picture is similarto results for the smaller data set: the rankingsassigned by all metrics are the same, except thatNIST-5 and SE have swapped the ranks of SUM-TIME-Hybrid and pCRU-roulette.
Pair-wise ARtests showed all differences to be significant withp < 0.05, except for the differences in BLEU, NISTand ROUGE scores for SUMTIME-Hybrid/pCRU-roulette, and the difference in BLEU scores forSUMTIME-Hybrid/pCRU-2gram.In both Tables 2 and 4, there are two majordifferences between the rankings assigned by hu-317System Experts NIST-5 BLEU-4 ROUGE-4 SESUMTIME-Hybrid 1 6.076 (3) 0.527 (2) 0.278 (3) 0.607 (4)pCRU-greedy 2 6.925 (1) 0.641 (1) 0.425 (1) 0.758 (1)SUMTIME-Corpus - 9.317 (-) 1 (-) 1 (-) 1 (-)pCRU-roulette 3 6.175 (2) 0.497 (4) 0.242 (4) 0.679 (3)pCRU-2gram 4 5.685 (4) 0.519 (3) 0.315 (2) 0.712 (2)pCRU-random 5 4.515 (5) 0.313 (5) 0.098 (5) 0.551 (5)Table 4: Evaluation scores against the SUMTIME corpus, on 5 test sets from pCRU validation.man and automatic evaluation: (i) Human evalua-tors prefer SUMTIME-Hybrid over pCRU-greedy,whereas all the automatic metrics have it theother way around; and (ii) human evaluators scorepCRU-roulette highly (second and third respec-tively), whereas the automatic metrics score it verylow, second worst to random generation (exceptfor NIST which puts it second).There are two clear tendencies in scores goingfrom left (humans) to right (SE) across Tables 2and 4: SUMTIME-Hybrid goes down in rank, andpCRU-2gram comes up.In addition to the BLEU-4 scores shown in thetables, we also calculated BLEU-1, BLEU-2, BLEU-3 scores.
These give similar results, except thatBLEU-1 and BLEU-2 rank pCRU-roulette as highlyas the human judges.It is striking how low the experts rank the cor-pus texts, and to what extent they disagree on theirquality.
This appears to indicate that corpus qual-ity is not ideal.
If an imperfect corpus is usedas the gold standard for the automatic metrics,then high correlation with human judgments is lesslikely, and this may explain the difference in hu-man and automatic scores for SUMTIME-Hybrid.5 DiscussionIf we assume that the human evaluation scores arethe most valid, then the automatic metrics do notdo a good job of comparing the knowledge-basedSUMTIME system to the statistical systems.One reason for this could be that there are caseswhere SUMTIME deliberately does not choose themost common option in the corpus, because itsdevelopers believed that it was not the best forreaders.
For example, in Table 1, the humanforecasters and pCRU-greedy use the phrase bylate evening to refer to 0000, pCRU-2gram usesthe phrase later, while SUMTIME-Hybrid uses thephrase by midnight.
The pCRU choices reflect fre-quency in the SUMTIME corpus: later (837 in-stances) and by late evening (327 instances) aremore common than by midnight (184 instances).However, forecast readers dislike this use of later(because later is used to mean something else ina different type of forecast), and also dislike vari-ants of by evening, because they are unsure howto interpret them (Reiter et al, 2005); this is whySUMTIME uses by midnight.The SUMTIME system builders believe deviat-ing from corpus frequency in such cases makesSUMTIME texts better from the reader?s perspec-tive, and it does appear to increase human ratingsof the system; but deviating from the corpus insuch a way decreases the system?s score undercorpus-similarity metrics.
In other words, judg-ing the output of an NLG system by comparing itto corpus texts by a method that rewards corpussimilarity will penalise systems which do not basechoice on highest frequency of occurrence in thecorpus, even if this is motivated by careful studiesof what is best for text readers.The MT community recognises that BLEU is noteffective at evaluating texts which are as good as(or better than) the reference texts.
This is nota problem for MT, because the output of current(wide-coverage) MT systems is generally worsethan human translations.
But it is an issue for NLG,where systems are domain-specific and can gen-erate texts that are judged better by humans thanhuman-written texts (as seen in Tables 4 and 2).Although the automatic evaluation metrics gen-erally replicated human judgments fairly wellwhen comparing different statistical NLG systems,there was a discrepancy in the ranking of pCRU-roulette (ranked high by humans, low by several ofthe automatic metrics).
pCRU-roulette differs fromthe other statistical generators because it does notalways try to make the most common choice (max-imise the likelihood of the corpus), instead it triesto vary choices.
In particular, if there are severalcompeting words and phrases with similar prob-318abilities, pCRU-roulette will tend to use differentwords and phrases in different texts, whereas theother statistical generators will stick to those withthe highest frequency.
This behaviour is penalisedby the automatic evaluation metrics, but the hu-man evaluators do not seem to mind it.One of the classic rules of writing is to vary lex-ical and syntactic choices, in order to keep text in-teresting.
However, this behaviour (variation forvariation?s sake) will always reduce a system?sscore under corpus-similarity metrics, even if itenhances text quality from the perspective of read-ers.
Foster and Oberlander (2006), in their study offacial gestures, have also noted that humans do notmind and indeed in some cases prefer variation,whereas corpus-based evaluations give higher rat-ings to systems which follow corpus frequency.Using more reference texts does counteract thistendency, but only up to a point: no matter howmany reference texts are used, there will still beone, or a small number of, most frequent variants,and using anything else will still worsen corpus-similarity scores.Canvassing expert opinion of text quality andaveraging the results is also in a sense frequency-based, as results reflect what the majority of ex-perts consider good variants.
Expert opinions canvary considerably, as shown by the low correla-tion among experts in our study (and as seen incorpus studies, e.g.
Reiter et al, 2005), and eval-uations by a small number of experts may also beproblematic, unless we have good reason to be-lieve that expert opinions are highly correlated inthe domain (which was certainly not the case inour weather forecast domain).
Ultimately, suchdisagreement between experts suggests that (in-trinsic) judgments of the text quality ?
whetherby human or metric ?
really should be be backedup by (extrinsic) judgments of the effectiveness ofa text in helping real users perform tasks or other-wise achieving its communicative goal.6 Future WorkWe plan to further investigate the performance ofautomatic evaluation measures in NLG in the fu-ture: (i) performing similar experiments to theone described here in other domains, and withmore subjects and larger test sets; (ii) investigatingwhether automatic corpus-based techniques canevaluate content determination; (iii) investigatinghow well both human ratings and corpus-basedmeasures correlate with extrinsic evaluations ofthe effectiveness of generated texts.
Ultimately,we would like to move beyond critiques of exist-ing corpus-based metrics to proposing (and vali-dating) new metrics which work well for NLG.7 ConclusionsCorpus quality plays a significant role in auto-matic evaluation of NLG texts.
Automatic metricscan be expected to correlate very highly with hu-man judgments only if the reference texts used areof high quality, or rather, can be expected to bejudged high quality by the human evaluators.
Thisis especially important when the generated textsare of similar quality to human-written texts.In MT, high-quality texts vary less than gener-ally in NLG, so BLEU scores against 4 referencetranslations from reputable sources (as in MT ?05)are a feasible evaluation regime.
It seems likelythat for automatic evaluation in NLG, a larger num-ber of reference texts than four are needed.In our experiments, we have found NIST a morereliable evaluation metric than BLEU and in par-ticular ROUGE which did not seem to offer any ad-vantage over simple string-edit distance.
We alsofound individual experts?
judgments are not likelyto correlate highly with average expert opinion, infact less likely than NIST scores.
This seems toimply that if expert evaluation can only be donewith one or two experts, but a high-quality refer-ence corpus is available, then a NIST-based eval-uation may produce more accurate results than anexpert-based evaluation.It seems clear that for automatic corpus-basedevaluation to work well, we need high-qualityreference texts written by many different authorsand large enough to give reasonable coverage ofphenomena such as variation for variation?s sake.Metrics that do not exclusively reward similaritywith reference texts (such as NIST) are more likelyto correlate well with human judges, but all of theexisting metrics that we looked at still penalisedgenerators that do not always choose the most fre-quent variant.The results we have reported here are for arelatively simple sublanguage and domain, andmore empirical research needs to be done on howwell different evaluation metrics and methodolo-gies (including different types of human evalua-tions) correlate with each other.
In order to es-tablish reliable and trusted automatic cross-system319evaluation methodologies, it seems likely that theNLG community will need to establish how to col-lect large amounts of high-quality reference textsand develop new evaluation metrics specificallyfor NLG that correlate more reliably with humanjudgments of text quality and appropriateness.
Ul-timately, research should also look at developingnew evaluation techniques that correlate reliablywith the real world usefulness of generated texts.In the shorter term, we recommend that automaticevaluations of NLG systems be supported by con-ventional large-scale human-based evaluations.AcknowledgmentsAnja Belz?s part of the research reported in thispaper was supported under UK EPSRC GrantGR/S24480/01.
Many thanks to John Carroll,Roger Evans and the anonymous reviewers forvery helpful comments.ReferencesS.
Bangalore, O. Rambow, and S. Whittaker.
2000.Evaluation metrics for generation.
In Proc.
1st In-ternational Conference on Natural Language Gen-eration, pages 1?8.A.
Belz.
2005.
Statistical generation: Three meth-ods compared and evaluated.
In Proc.
10th Euro-pean Workshop on Natural Language Generation(ENLG?05), pages 15?23.A.
Belz.
2006. pCRU: Probabilistic generation usingrepresentational underspecification.
Technical Re-port ITRI-06-01, ITRI, University of Brighton.J.
Coch.
1996.
Evaluating and comparing threetext production techniques.
In Proc.
16th Inter-national Conference on Computational Linguistics(COLING-1996).G.
Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proc.
ARPA Workshop onHuman Language Technology.M.
E. Foster and J. Oberlander.
2006.
Data-driven gen-eration of emphatic facial displays.
In Proceedingsof EACL-2006.N.
Habash.
2004.
The use of a structural n-gram lan-guage model in generation-heavy hybrid machinetranslation.
In Proc.
3rd International Conferenceon Natural Language Generation (INLG ?04), vol-ume 3123 of LNAI, pages 61?69.
Springer.I.
Langkilde.
1998.
An empirical verification of cover-age and correctness for a general-purpose sentencegenerator.
In Proc.
2nd International Natural Lan-guage Generation Conference (INLG ?02).J.
Lester and B. Porter.
1997.
Developing and empir-ically evaluating robust explanation generators: TheKNIGHT experiments.
Computational Linguistics,23(1):65?101.C.-Y.
Lin and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProc.
HLT-NAACL 2003, pages 71?78.T.
Marciniak and M. Strube.
2004.
Classification-based generation using TAG.
In Natural LanguageGeneration: Proceedings of INLG-2994, pages 100?109.
Springer.C.
Mellish and R. Dale.
1998.
Evaluation in thecontext of natural language generation.
ComputerSpeech and Language, 12:349?373.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: A method for automatic evaluation of machinetranslation.
In Proc.
ACL-2002, pages 311?318.E.
Reiter and S. Sripada.
2002.
Should corpora texts begold standards for NLG?
In Proc.
2nd InternationalConference on Natural Language Generation, pages97?104.E.
Reiter, S. Sripada, and R. Robertson.
2003.
Ac-quiring correct knowledge for natural language gen-eration.
Journal of Artificial Intelligence Research,18:491?516.E.
Reiter, S. Sripada, J.
Hunter, and J. Yu.
2005.Choosing words in computer-generated weatherforecasts.
Artificial Intelligence, 167:137?169.S.
Riezler and J. T. Maxwell III.
2005.
On some pit-falls in automatic evaluation and significance testingfor MT.
In Proc.
ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/or Sum-marization, pages 57?64.K.
Sparck Jones and J. R. Galliers.
1996.
EvaluatingNatural Language Processing Systems: An Analysisand Review.
Springer Verlag.S.
Sripada, E. Reiter, J.
Hunter, and J. Yu.
2003.
Ex-ploiting a parallel TEXT-DATA corpus.
In Proc.Corpus Linguistics 2003, pages 734?743.S.
Sripada, E. Reiter, and L. Hawizy.
2005.
Evalua-tion of an NLG system used post-edit data: Lessonslearned.
In Proc.
ENLG-2005, pages 133?139.A.
Stolcke.
2002.
SRILM: An extensible languagemodeling toolkit.
In Proc.
7th International Confer-ence on Spoken Language Processing (ICSLP ?02),pages 901?904,.S.
Williams and E. Reiter.
2005.
Generating read-able texts for readers with low basic skills.
In Proc.ENLG-2005, pages 140?147.M.
Young.
1999.
Using Grice?s maxim of quantityto select the content of plan descriptions.
ArtificialIntelligence, 115:215?256.320
