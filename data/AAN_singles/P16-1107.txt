Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1127?1137,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsContext-aware Argumentative Relation MiningHuy V. NguyenComputer Science DepartmentUniversity of PittsburghPittsburgh, PA 15260hvn3@pitt.eduDiane J. LitmanComputer Science Department andLearning Research and Development CenterUniversity of PittsburghPittsburgh, PA 15260dlitman@pitt.eduAbstractContext is crucial for identifying argumen-tative relations in text, but many argumentmining methods make little use of contex-tual features.
This paper presents context-aware argumentative relation mining thatuses features extracted from writing top-ics as well as from windows of contextsentences.
Experiments on student essaysdemonstrate that the proposed features im-prove predictive performance in two argu-mentative relation classification tasks.1 IntroductionBy supporting tasks such as automatically iden-tifying argument components1(e.g., premises,claims) in text, and the argumentative relations(e.g., support, attack) between components, ar-gument (argumentation) mining has been studiedfor applications in different research fields suchas document summarization (Teufel and Moens,2002), opinion mining (Boltu?zi?c and?Snajder,2014), automated essay evaluation (Burstein etal., 2003), legal information systems (Palau andMoens, 2009), and policy modeling platforms(Florou et al, 2013).Given a pair of argument components with onecomponent as the source and the other as the tar-get, argumentative relation mining involves deter-mining whether a relation holds from the source tothe target, and classifying the argumentative func-tion of the relation (e.g., support vs. attack).
Ar-1There is no consensus yet on an annotation scheme forargument components, or on the minimal textual units to beannotated.
We follow Peldszus and Stede (2013) and con-sider ?argument mining as the automatic discovery of an ar-gumentative text portion, and the identification of the relevantcomponents of the argument presented there.?
We also bor-row their term ?argumentative discourse unit?
to refer to thetextual units (e.g., text segment, sentence, clause) which areconsidered as argument components.Essay 73.
Topic: Is image more powerfulthan the written word?...
(1)Hence, I agree only to certain de-gree that in today?s world, image servesas a more effective means of communica-tion[MajorClaim].
(2)Firstly, pictures can influence the waypeople think[Claim].
(3)For example, nowa-days horrendous images are displayed onthe cigarette boxes to illustrate the con-sequences of smoking[Premise].
(4)As aresult, statistics show a slight reductionin the number of smokers, indicating thatthey realize the effects of the negativehabit[Premise]...Figure 1: Excerpt from a student persuasive essay(Stab and Gurevych, 2014a).
Sentences are num-bered and argument components are tagged.gumentative relation mining - beyond argumentcomponent mining - is perceived as an essentialstep towards more fully identifying the argumenta-tive structure of a text (Peldszus and Stede, 2013;Sergeant, 2013; Stab et al, 2014).
Consider thesecond paragraph shown in Figure 1.
Only detect-ing the argument components (a claim in sentence2 and two premises in sentences 3 and 4) does notgive a complete picture of the argumentation.
Bylooking for relations between these components,one can also see that the two premises together jus-tify the claim.
The argumentation structure of thetext in Figure 1 is illustrated in Figure 2.Our current study proposes a novel approachfor argumentative relation mining that makes useof contextual features extracted from surround-ing sentences of source and target components aswell as from topic information of the writings.1127Prior argumentative relation mining studies haveoften used features extracted from argument com-ponents to model different aspects of the relationsbetween the components, e.g., relative distance,word pairs, semantic similarity, textual entailment(Cabrio and Villata, 2012; Stab and Gurevych,2014b; Boltu?zi?c and?Snajder, 2014; Peldszus andStede, 2015b).
Features extracted from the textsurrounding the components have been less ex-plored, e.g., using words and their part-of-speechfrom adjacent sentences (Peldszus, 2014).
Thefirst hypothesis investigated in this paper is that thediscourse relations of argument components withadjacent sentences (called context windows in thisstudy, a formal definition is given in ?5.3) can helpcharacterize the argumentative relations that con-nect pairs of argument components.
Reconsider-ing the example in Figure 1, without knowing thecontent ?horrendous images are displayed on thecigarette boxes?
in sentence 3, one cannot easilytell that ?reduction in the number of smokers?
insentence 4 supports the ?pictures can influence?claim in sentence 2.
We expect that such contentrelatedness can be revealed from a discourse anal-ysis, e.g., the appearance of a discourse connective?As a result?.While topic information in many writing gen-res (e.g., scientific publications, Wikipedia arti-cles, student essays) has been used to create fea-tures for argument component mining (Teufel andMoens, 2002; Levy et al, 2014; Nguyen and Lit-man, 2015), topic-based features have been lessexplored for argumentative relation mining.
Thesecond hypothesis investigated in this paper is thatfeatures based on topic context also provide usefulinformation for improving argumentative relationmining.
In the excerpt below, knowing that ?onlinegame?
and ?computer?
are topically related mighthelp a model decide that the claim in sentence 1supports the claim in sentence 2:(1)People who are addicted to games,especially online games, can eventuallybear dangerous consequences[Claim].
(2)Although it is undeniable that computer isa crucial part of human life[Premise], it stillhas its bad side[MajorClaim].2Motivated by the discussion above, we proposecontext-aware argumentative relation mining ?
anovel approach that makes use of contextual fea-2In this excerpt, the Premise was annotated as an attack tothe MajorClaim in sentence 2.MajorClaim(1)Claim(2)Premise(4)Premise(3)Premise(6)SupportSupport AttackSupportSupport SupportFigure 2: Structure of the argumentation in the ex-cerpt.
Relations are illustrated accordingly to theannotation provided in the corpus.
Premises 3 and4 were annotated for separate relations to Claim 2.Our visualization should not mislead that the twopremises are linked or convergent.tures that are extracted by exploiting context sen-tence windows and writing topic to improve rela-tion prediction.
In particular, we derive featuresusing discourse relations between argument com-ponents and windows of their surrounding sen-tences.
We also derive features using an argumentand domain word lexicon automatically created bypost-processing an essay?s topic model.
Experi-mental results show that our proposed contextualfeatures help significantly improve performance intwo argumentative relation classification tasks.2 Related WorkUnlike argument component identification wheretextual inputs are typically sentences or clauses(Moens et al, 2007; Stab and Gurevych, 2014b;Levy et al, 2014; Lippi and Torroni, 2015), tex-tual inputs of argumentative relation mining varyfrom clauses (Stab and Gurevych, 2014b; Peld-szus, 2014) to multiple-sentences (Biran and Ram-bow, 2011; Cabrio and Villata, 2012; Boltu?zi?c and?Snajder, 2014).
Studying claim justification be-tween user comments, Biran and Rambow (2011)proposed that the argumentation in justification ofa claim can be characterized with discourse struc-ture in the justification.
They however only con-sidered discourse markers but not discourse re-lations.
Cabrio et al (2013) conducted a cor-pus analysis and found certain similarity betweenPenn Discourse TreeBank relations (Prasad et al,2008) and argumentation schemes (Walton et al,2008).
However they did not discuss how suchsimilarity could be applied to argument mining.Motivated by these findings, we propose touse features extracted from discourse relations be-1128tween sentences for argumentative relation min-ing.
Moreover, to enable discourse relationfeatures when the textual inputs are only sen-tences/clauses, we group the inputs with their con-text sentences.
Qazvinian and Radev (2010) usedthe term ?context sentence?
to refer to sentencessurrounding a citation that contained informationabout the cited source but did not explicitly cite it.In our study, we only require that the context sen-tences of an argument component must be in thesame paragraph and adjacent to the component.Prior work in argumentative relation mining hasused argument component labels to provide con-straints during relation identification.
For exam-ple, when an annotation scheme (e.g., (Peldszusand Stede, 2013; Stab and Gurevych, 2014a)) doesnot allow relations from claim to premise, no rela-tions are inferred during relation mining for anyargument component pair where the source is aclaim and the target is a premise.
In our work,we follow Stab and Gurevych (2014b) and use thepredicted labels of argument components as fea-tures during argumentative relation mining.
We,however, take advantage of an enhanced argumentcomponent model (Nguyen and Litman, 2016) toobtain more reliable argument component labelsthan in (Stab and Gurevych, 2014b).Argument mining research has studied differ-ent data-driven approaches for separating orga-nizational content (shell) from topical contentto improve argument component identification,e.g., supervised sequence model (Madnani et al,2012), unsupervised probabilistic topic models(S?eaghdha and Teufel, 2014; Du et al, 2014).Nguyen and Litman (2015) post-processed LDA(Blei et al, 2003) output to extract a lexicon of ar-gument and domain words from development data.Their semi-supervised approach exploits the topiccontext through essay titles to guide the extraction.Finally, prior research has explored predict-ing different argumentative relationship labels be-tween pairs of argument components, e.g., attach-ment (Peldszus and Stede, 2015a), support vs.non-support (Biran and Rambow, 2011; Cabrioand Villata, 2012; Stab and Gurevych, 2014b),{implicit, explicit}?
{support, attack} (Boltu?zi?cand?Snajder, 2014), verifiability of support (Parkand Cardie, 2014).
Our experiments use two suchargumentative relation classification tasks (Sup-port vs. Non-support, Support vs.
Attack) to eval-uate the effectiveness of our proposed features.3 Persuasive Essay CorpusStab and Gurevych (2014a) compiled the Persua-sive Essay Corpus consisting of 90 student argu-mentative essays and made it publicly available.3Because the corpus has been utilized for differ-ent argument mining tasks (Stab and Gurevych,2014b; Nguyen and Litman, 2015; Nguyen andLitman, 2016), we use this corpus to demonstrateour context-aware argumentative relation miningapproach, and adapt the model developed by Staband Gurevych (2014b) to serve as the baseline forevaluating our proposed approach.Three experts identified possible argumentcomponents of three types within each sentence inthe corpus (MajorClaim - writer?s stance towardthe writing topic, Claim - controversial statementsthat support or attack MajorClaim, and Premise -evidence used to underpin the validity of Claim),and also connected the argument components us-ing two argumentative relations (Support and At-tack).
According to the annotation manual, eachessay has exactly one MajorClaim.
A sentencecan have one or more argument components (Ar-gumentative sentences).
Sentences that do notcontain any argument component are labeled Non-argumentative.
Figure 1 shows an example essaywith components annotated, and Figure 2 illus-trates relations between those components.
Argu-mentative relations are directed and can hold be-tween a Premise and another Premise, a Premiseand a (Major-) Claim, or a Claim and a Major-Claim.
Except for the relation from Claim toMajorClaim, an argumentative relation does notcross paragraph boundaries.
The three expertsachieved inter-rater accuracy 0.88 for componentlabels and Krippendorff?s ?U0.72 for componentboundaries.
Given the annotated argument com-ponents, the three experts obtained Krippendorff?s?
0.81 for relation labels.
The number of relationsare shown in Table 1.4 Argumentative Relation Tasks4.1 Task 1: Support vs. Non-supportOur first task follows (Stab and Gurevych, 2014b):given a pair of source and target argument com-ponents, identify whether the source argumenta-tively supports the target or not.
Note that whena support relation does not hold, the source mayattack or has no relation with the target compo-3www.ukp.tu-darmstadt.de/data/argumentation-mining1129Label #instancesWithin-paragraph constraintSupport 989Attack 103No paragraph constraintSupport 1312Attack 161Table 1: Data statistics of the corpus.nent.
For each of two argument components inthe same paragraph4, we form two pairs (i.e., re-versing source and target).
In total we obtain 6330pairs, in which 989 (15.6%) have Support relation.Among 5341 Non-support pairs, 103 have Attackrelation and 5238 are no-relation pairs.Stab and Gurevych (2014b) split the corpus intoan 80% training set and a 20% test set which havesimilar label distributions.
We use this split to trainand test our proposed models, and directly com-pare our models?
performance to the reported per-formance in (Stab and Gurevych, 2014b).4.2 Task 2: Support vs. AttackTo further evaluate the effectiveness of our ap-proach, we conduct an additional task that clas-sifies an argumentative relation as Support or At-tack.
For this task, we assume that the relation(i.e., attachment (Peldszus, 2014)) between twocomponents is given, and aim at identifying theargumentative function of the relation.
Becausewe remove the paragraph constraint in this task,we obtain more Support relations than in Task 1.As shown in Table 1, of the total 1473 relations,we have 1312 (89%) Support and 161 (11%) At-tack relations.
Because this task was not studiedin (Stab and Gurevych, 2014b), we adapt Stab andGurevych?s model to use as the baseline.5 Argumentative Relation Models5.1 BaselineWe adapt (Stab and Gurevych, 2014b) to use asa baseline for evaluating our approach.
Given apair of argument components, we follow (Stab andGurevych, 2014b) by first extracting 3 feature sets:structural (e.g., word counts, sentence position),lexical (e.g., word pairs, first words), and gram-matical production rules (e.g., S?NP,VP).4Allowing cross-paragraph relations exponentially in-creases the number of no-relation pairs, which makes the pre-diction data extremely skewed (Stab and Gurevych, 2014b).Because a sentence may have more than one ar-gument component, the relative component posi-tions might provide useful information (Peldszus,2014).
Thus, we also include 8 new componentposition features: whether the source and targetcomponents are the whole sentences or the be-ginning/end components of the sentences; if thesource is before or after the target component; andthe absolute difference of their positions.Stab and Gurevych (2014b) used a 55-discoursemarker set to extract indicator features.
We ex-pand their discourse maker set by combining themwith a 298-discourse marker set developed in (Bi-ran and Rambow, 2011).
We expect the expandedset of discourse markers will represent better pos-sible discourse relations in the texts.Stab and Gurevych (2014b) used predicted labelof argument components as features for both train-ing and testing their argumentation structure iden-tification model.5As their predicted labels are notavailable to us, we adapt this feature set by usingthe argument component model in (Nguyen andLitman, 2016) which was shown to outperform thecorresponding model of Stab and Gurevych.For later presentation purposes, we name theset of all features from this section except wordpairs and production rules as the common fea-tures.
While word pairs and grammatical pro-duction rules were the most predictive features in(Stab and Gurevych, 2014b), we hypothesize thatthis large and sparse feature space may have nega-tive impact on model robustness (Nguyen and Lit-man, 2015).
Most of our proposed models re-place word pairs and production rules with differ-ent combinations of new contextual features.5.2 Topic-context ModelOur first proposed model (TOPIC) makes use ofTopic-context features derived from a lexicon ofargument and domain words for persuasive essays(Nguyen and Litman, 2015).
Argument words(e.g., ?believe?, ?opinion?)
signal the argumenta-tive content and are commonly used across differ-ent topics.
In contrast, domain words are specificterminologies commonly used within the topic(e.g., ?art?, ?education?).
The authors first use5Stab and Gurevych (2014b) reported that including gold-standard labels of argument component in both training andtesting phases yielded results close to human performance.Our preliminary experiment showed that including gold-standard argument component labels in training did not helpwhen predicted labels were used in the test set.1130topic prompts in development data of unannotatedpersuasive essays to semi-automatically collect ar-gument and domain seed words.
In particular, theyused 10 argument seed words: agree, disagree,reason, support, advantage, disadvantage, think,conclusion, result, opinion.
Domain seed wordsare those in the topic prompts but not argumentseed words or stop words.
The seeds words arethen used to supervise an automated extraction ofargument and domain words from output of LDAtopic model (Blei et al, 2003) on the develop-ment data.
The extracted lexicon consists of 263(stemmed) argument words and 1806 (stemmed)domain words mapped to 36 LDA topics.6All ar-gument words are from a single LDA topic while adomain word can map to multiple LDA topics (ex-cept the topic of argument words).
Using the lex-icon, we extract the following Topic-context fea-tures:Argument word: from all word pairs extractedfrom the source and target components, we re-move those that have at least one word not in theargument word list.
Each argument word pair de-fines a boolean feature indicating its presence inthe argument component pair.
We also includeeach argument word of the source and target com-ponents as a boolean feature which is true if theword is present in the corresponding component.We count number of common argument words, theabsolute difference in number of argument wordsbetween source and target components.Domain word count: to measure the topic sim-ilarity between the source and target components,we calculate number of common domain words,number of pairs of two domain words that sharean LDA topic, number of pairs that share no LDAtopic, and the absolute difference in number of do-main words between the two components.Non-domain MainVerb-Subject dependency:we extract MainVerb-Subject dependency triples,e.g., nsubj(belive, I), from the source and targetcomponents, and filter out triples that involve do-main words.
We model each extracted triple as aboolean feature which is true if the correspondingargument component has the triple.Finally, we include the common feature set.To illustrate the Topic-context features, con-sider the following source and target components.Argument words are in boldface, and domain6An LDA topic is simply represented by a number, andshould not be misunderstood with essay topics.words are in italic.Essay 54.
Topic: museum and art gallerywill disappear soon?Source: more and more people can watchexhibitions through television or internet athome due to modern technology[Premise]Target: some people think museums andart galleries will disappear soon[Claim]An argument word pair is people-think.
Thereare 35 pairs of domain words.
A pair of two do-main words that share an LDA topic is exhibitions-art.
A pair of two domain words that do not shareany LDA topic is internet-galleries.5.3 Window-context ModelOur second proposed model (WINDOW) extractsfeatures from discourse relations and commonwords between context sentences in the contextwindows of the source and target components.Definition.
Context window of an argument com-ponent is a text segment formed by neighboringsentences and the covering sentence of the com-ponent.
The neighboring sentences are called con-text sentences, and must be in the same paragraphwith the component.In this study, context windows are determinedusing window-size heuristics.7Given a window-size n, we form a context window by grouping thecovering sentence with at most n adjacently pre-ceding and n adjacently following sentences thatmust be in the same paragraph.To minimize noise in feature space, we re-quire that context windows of the source and tar-get components must be mutually exclusive.
Bi-ran and Rambow (2011) observed that the re-lation between a source argument and a targetargument is usually instantiated by some elabo-ration/justification provided in a support of thesource argument.
Therefore we prioritize the con-text window of source component when it overlapswith the target context window.
Particularly, wekeep overlapping context sentences in the sourcewindow, and remove them from the target win-dow.
For example, with window-size 1, contextwindows of the Claim in sentence 2 in Figure 1and the Premise in sentence 4 overlap at sentence3.
When the Claim is set as source component, its7Due to the paragraph constraint and window overlap-ping, window-size does not indicate the actual context win-dow size.
However, window-size tells what the maximumsize a window can have.1131BASELINECommon featuresWord pairs + Production rulesTOPICCommon featuresTopic context features +Window context featuresWINDOWCommon featuresWindow context featuresCOMBINEDCommon featuresTopic context features +Window context features +Word pairs + Production rulesFULLCommon featuresTopic context featuresFigure 3: Features used in different models.
Feature change across models are denoted by connectors.context window includes sentences {2, 3}, and thePremise as a target has context window with onlysentence 4.
We extract three Window-contextfeature sets from the context windows:Common word: as common word counts be-tween adjacent sentences were shown useful forargument mining (Nguyen and Litman, 2016), wecount common words between the covering sen-tence with preceding context sentences, and withfollowing context sentences, for source and targetcomponents.Discourse relation: for both source and tar-get components, we extract discourse relationsbetween context sentences, and within the cov-ering sentence.
We also extract discourse rela-tions between each pair of source context sen-tence and target context sentence.
Each relationdefines a boolean feature.
We extract both PennDiscourse Treebank (PDTB) relations (Prasad etal., 2008) and Rhetorical Structure Theory Dis-course Treebank (RST-DTB) relations (Carlsonet al, 2001) using publicly available discourseparsers (Ji and Eisenstein, 2014; Wang and Lan,2015).
Each PDTB relation has sense label de-fined in a 3-layered (class, type, subtype), e.g.,CONTINGENCY.Cause.result.
While there areonly four semantic class labels at the class-levelwhich may not cover well different aspects of ar-gumentative relation, subtype-level output is notavailable given the discourse parser we use.
Thus,we use relations at type-level as features.
For RST-DTB relations, we use only relation labels, but ig-nore the nucleus and satellite labels of componentsas they do not provide more information giventhe component order in the pair.
Because tempo-ral relations were shown not helpful for argumentmining tasks (Biran and Rambow, 2011; Stab andGurevych, 2014b), we exclude them here.Discourse marker: while the baseline modelonly considers discourse markers within the ar-gument components, we define a boolean featurefor each discourse marker classifying whether themarker is present before the covering sentence ofthe source and target components or not.
This im-plementation aims to characterize the discourse ofthe preceding and following text segments of eachargument component separately.Finally, we include the common feature set.5.4 Combined ModelWhile Window-context features are extractedfrom surrounding text of the argument com-ponents, which exploits the local context, theTopic-context features are an abstraction of topic-dependent information, e.g., domain words are de-fined within the context of topic domain (Nguyenand Litman, 2015), and thus make use of theglobal context of the topic domain.
We believethat local and global context information repre-sent complementary aspects of the relation be-tween argument components.
Thus, we expectto achieve the best performance by combiningWindow-context and Topic-context models.5.5 Full ModelFinally, the FULL model includes all features inBASELINE and COMBINED models.
That is, theFULL model is the COMBINED model plus wordpairs and production rules.
A summary of all mod-els is shown in Figure 3.6 Experiments6.1 Task 1: Support vs. Non-supportTuning Window-size ParameterBecause our WINDOW model uses a window-size parameter to form context windows of thesource and target argument components, we in-vestigate how the window-size of the context win-dow impacts the prediction performance of theWindow-context features.
We set up a model withonly Window-context features and determine the1132REPORTED BASELINE TOPIC WINDOW COMBINED FULLAccuracy 0.863 0.869 0.857 0.857 0.870 0.877Kappa ?
0.445 0.407 0.449 0.507* 0.481Macro F1 0.722 0.722 0.703 0.724 0.753* 0.739Macro Precision 0.739 0.758 0.728 0.729 0.754 0.777Macro Recall 0.705 0.699 0.685 0.720 0.752* 0.715F1:Support 0.519 0.519 0.488 0.533 0.583* 0.550F1:Non-support 0.920 0.925 0.917 0.916* 0.923 0.929Table 2: Support vs. Non-support classification performances on test set.
Best values are in bold.
Valuessmaller than baseline are underlined.
* indicates significantly different from the baseline (p < 0.05).0.620.630.640.650.660.670.680.690.70 1 2 3 4 5 6 7 8Window-sizeF1 score of Window-context FeaturesFigure 4: Performance of Window-context featureset by window-size.window-size in range [0, 8]8that yields the bestF1 score in 10-fold cross validation.
We use thetraining set as determined in (Stab and Gurevych,2014b) to train/test9the models using LibLINEARalgorithm (Fan et al, 2008) without parameter orfeature optimization.
Cross-validations are con-ducted using Weka (Hall et al, 2009).
We useStanford parser (Klein and Manning, 2003) to per-form text processing.
As shown in Figure 4, whileincreasing the window-size from 2 to 3 improvesperformance (significantly), using window-sizesgreater than 3 does not gain further improvement.We hypothesize that after a certain limit, largercontext windows will produce more noise thanhelpful information for the prediction.
Therefore,we set the window-size to 3 in all of our experi-ments involving Window-context model (all witha separate test set).8Windows-size 0 means covering sentence is the onlycontext sentence.
We experimented with not using contextsentence at all and obtained worse performance.
Our datadoes not have context window with window-size 9 or larger.9Note that via cross validation, in each fold some of ourtraining set serves as a development set.Performance on Test SetWe train all models using the training set and re-port their performances on the test set in Table 2.We also compare our baseline to the reported per-formance (REPORT) for Support vs. Non-supportclassification in (Stab and Gurevych, 2014b).
Thelearning algorithm with parameters are kept thesame as in the window-size tuning experiment.Given the skewed class distribution of this data,Accuracy and F1 of Non-support (the major class)are less important than Kappa, F1, and F1 of Sup-port (the minor class).
To conduct T-tests for per-formance significance, we split the test data intosubsets by essays?
ID, and record prediction per-formance for individual essays.We first notice that the performances of ourbaseline model are better than (or equal to) RE-PORTED, except the Macro Recall.
We reason thatthese performance disparities may be due to thedifferences in feature extractions between our im-plementation and Stab and Gurevych?s, and alsodue to the minor set of new features (e.g., newpredicted labels, expanded marker set, componentposition) that we added in our baseline.Comparing proposed models with BASELINE,we see that WINDOW, COMBINED, and FULLmodels outperform BASELINE in important met-rics: Kappa, F1, Recall, but TOPIC yields worseperformances than BASELINE.
However, the factthat COMBINED outperforms BASELINE, espe-cially with significantly higher Kappa, F1, Recall,and F1:Support, has shown the value of Topic-context features.
While Topic-context featuresalone are not effective, they help improve WIN-DOW model which supports our hypothesis thatTopic-context and Window-context features arecomplementary aspects of context, and they to-gether obtain better performance.Comparing our proposed TOPIC, WINDOW,1133BASELINE TOPIC WINDOW COMBINED FULLAccuracy 0.885 0.886 0.872 0.885 0.887Kappa 0.245 0.305* 0.306* 0.342* 0.274*Macro F1 0.618 0.651* 0.652* 0.670* 0.634*Macro Precision 0.680 0.692 0.663 0.697 0.693Macro Recall 0.595 0.628* 0.644* 0.652* 0.609*F1:Support 0.937 0.937 0.928* 0.936 0.938F1:Attack 0.300 0.365* 0.376* 0.404* 0.330*Table 3: 5?10-fold cross validation performance of Support vs.
Attack classification.
* indicates signif-icantly different from the baseline (p < 0.01).COMBINED models with each other shows thatCOMBINED obtains the best performance whileTOPIC performs the worst, which reveals thatTopic-context feature set is less effective thanWindow-context set.
While FULL model achievesthe best Accuracy, Precision, and F1:Non-support,it has lower performance than COMBINED modelin important metrics: Kappa, F1, F1:Support.
Wereason that the noise caused by word pairs andproduction rules even dominate the effectivenessof Topic-context and Window-context features,which degrades the overall performance.Overall, by combining TOPIC and WINDOWmodels, we obtain the best performance.
Mostnotably, we obtain the highest improvement inF1:Support, and have the best balance betweenPrecision and Recall values among all models.These reveal that our contextual features not onlydominate generic features like word pairs and pro-duction rules, but also are effective to predict mi-nor positive class (i.e., Support).6.2 Task 2: Support vs. AttackTo evaluate the robustness of our proposed mod-els, we conduct an argumentative relation classifi-cation experiment that classifies a relation as Sup-port or Attack.
Because this task was not stud-ied in (Stab and Gurevych, 2014b) and the train-ing/test split for Support vs. Not task is not ap-plicable here, we conduct 5?10-fold cross valida-tion.
We do not optimize the window-size param-eter of the WINDOW model, and use the value 3 asset up before.
Average prediction performance ofall models are reported in Table 3.Comparing our proposed models with the base-line shows that all of our proposed models sig-nificantly outperform the baseline in importantmetrics: Kappa, F1, F1:Attack.
More notablythan in the Support vs. Non-support classifica-tion, all of our proposed models predict the minorclass (Attack) significantly more effectively thanthe baseline.
The baseline achieves significantlyhigher F1:Support than WINDOW model.
How-ever, F1:Support of the baseline is in a tie withTOPIC, COMBINED, and FULL.Comparing our proposed models, we see thatTOPIC and WINDOW models reveal different be-haviors.
TOPIC model has significantly higherPrecision and F1:Support, and significantly lowerRecall and F1:Attack than WINDOW.
Moreover,WINDOW model has slightly higher Kappa, F1,but significantly lower Accuracy.
These com-parisons indicate that Topic-context and Window-context features are equally effective but impactdifferently to the prediction.
The different naturebetween these two feature sets is clearer than inthe prior experiment, as now the classification in-volves classes that are more semantically differ-ent, i.e., Support vs.
Attack.
We recall that TOPICmodel performs worse than WINDOW model inSupport vs. Non-support task.Our FULL model performs significantly worsethan all of TOPIC, WINDOW, and COMBINED inKappa, F1, Recall, and F1:Attack.
Along with re-sults from Support vs. Non-support task, this fur-ther suggests that word pairs and production rulesare less effective and cannot be combined wellwith our contextual features.Despite the fact that the Support vs.
Attack task(Task 2) has smaller and more imbalanced datathan the Support vs. Non-support (Task 1), ourproposed contextual features seem to add evenmore value in Task 2 compared to Task 1.
Us-ing Kappa to roughly compare prediction perfor-mance across the two tasks, we observe a greaterperformance improvement from Baseline to Com-bined model in Task 2 than in Task 1.
This is anevidence that our proposed context-aware features1134work well even in a more imbalanced with smallerdata classification task.
The lower performancevalues of all models in Support vs.
Attack thanin Support vs. Non-support indirectly suggest thatSupport vs.
Attack classification is a more difficulttask.
We hypothesize that the difference betweensupport and attack exposes a deeper semantic re-lation than that between support and no-relation.We plan to extract textual text similarity and tex-tual entailment features (Cabrio and Villata, 2012;Boltu?zi?c and?Snajder, 2014) to investigate this hy-pothesis in our future work.7 Conclusions and Future WorkIn this paper, we have presented context-awareargumentative relation mining that makes use ofcontextual features by exploiting information fromtopic context and context sentences.
We haveexplored different ways to incorporate our pro-posed features with baseline features used in aprior study, and obtained insightful results aboutfeature effectiveness.
Experimental results showthat Topic-context and Window-context featuresare both effective but impact predictive perfor-mance measures differently.
In addition, predict-ing an argumentative relation will benefit mostfrom combining these two set of features as theycapture complementary aspects of context to bet-ter characterize the argumentation in justification.The results obtained in this preliminary studyare promising and encourage us to explore moredirections to enable contextual features.
Our nextstep will investigate uses of topic segmentationto identify context sentences and compare thislinguistically-motivated approach to our currentwindow-size heuristic.
We plan to follow priorresearch on graph optimization to refine the argu-mentation structure and improve argumentative re-lation prediction.
Also, we will apply our context-aware argumentative relation mining to differentargument mining corpora to further evaluate itsgenerality.AcknowledgmentsThis research is supported by NSF Grant 1122504.We thank the reviewers for their helpful feedback.We also thank Christian Stab for providing us thedata split for the first experiment.ReferencesOr Biran and Owen Rambow.
2011.
Identifying Jus-tifications in Written Dialogs by Classifying Text asArgumentative.
International Journal of SemanticComputing, 5(4):363?381.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet alocation.
The Journal ofMachine Learning Research, 3:993?1022.Filip Boltu?zi?c and Jan?Snajder.
2014.
Back upyour Stance: Recognizing Arguments in Online Dis-cussions.
In Proceedings of the First Workshopon Argumentation Mining, pages 49?58, Baltimore,Maryland, June.
Association for Computational Lin-guistics.Jill Burstein, Daniel Marcu, and Kevin Knight.
2003.Finding the WRITE Stuff: Automatic Identificationof Discourse Structure in Student Essays.
IEEE In-telligent Systems, 18(1):32?39, January.Elena Cabrio and Serena Villata.
2012.
CombiningTextual Entailment and Argumentation Theory forSupporting Online Debates Interactions.
In Pro-ceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Short Papers- Volume 2, ACL ?12, pages 208?212, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Elena Cabrio, Sara Tonelli, and Serena Villata.2013.
From Discourse Analysis to ArgumentationSchemes and Back: Relations and Differences.
InComputational Logic in Multi-Agent Systems, vol-ume 8143 of Lecture Notes in Computer Science,pages 1?17.
Springer Berlin Heidelberg.Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2001.
Building a Discourse-taggedCorpus in the Framework of Rhetorical StructureTheory.
In Proceedings of the Second SIGdial Work-shop on Discourse and Dialogue - Volume 16, SIG-DIAL ?01, pages 1?10, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Jianguang Du, Jing Jiang, Liu Yang, Dandan Song, andLejian Liao.
2014.
Shell Miner: Mining Organiza-tional Phrases in Argumentative Texts in Social Me-dia.
In Proceedings of the 2014 IEEE InternationalConference on Data Mining, ICDM ?14, pages 797?802, Washington, DC, USA.
IEEE Computer Soci-ety.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A Library for Large Linear Classification.
The Jour-nal of Machine Learning Research, 9:1871?1874,June.Eirini Florou, Stasinos Konstantopoulos, AntonisKoukourikos, and Pythagoras Karampiperis.
2013.Argument extraction for supporting public policyformulation.
In Proceedings of the 7th Workshop onLanguage Technology for Cultural Heritage, Social1135Sciences, and Humanities, pages 49?54, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: AnUpdate.
ACM SIGKDD Explorations Newsletter,11(1):10?18, November.Yangfeng Ji and Jacob Eisenstein.
2014.
Representa-tion Learning for Text-level Discourse Parsing.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 13?24, Baltimore, Maryland,June.
Association for Computational Linguistics.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Ran Levy, Yonatan Bilu, Daniel Hershcovich, EhudAharoni, and Noam Slonim.
2014.
Context Depen-dent Claim Detection.
In Proceedings of COLING2014, the 25th International Conference on Compu-tational Linguistics: Technical Papers, pages 1489?1500, Dublin, Ireland, August.Marco Lippi and Paolo Torroni.
2015.
Context-independent Claim Detection for Argument Mining.In Proceedings of the 24th International Conferenceon Artificial Intelligence, IJCAI?15, pages 185?191,Buenos Aires, Argentina.
AAAI Press.Nitin Madnani, Michael Heilman, Joel Tetreault, andMartin Chodorow.
2012.
Identifying High-Level Organizational Elements in ArgumentativeDiscourse.
In Proceedings of the 2012 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 20?28, Montr`eal, Canada.
As-sociation for Computational Linguistics.Marie-Francine Moens, Erik Boiy, Raquel MochalesPalau, and Chris Reed.
2007.
Automatic Detec-tion of Arguments in Legal Texts.
In Proceedings ofthe 11th International Conference on Artificial Intel-ligence and Law, ICAIL ?07, pages 225?230, NewYork, NY, USA.
ACM.Huy Nguyen and Diane Litman.
2015.
ExtractingArgument and Domain Words for Identifying Ar-gument Components in Texts.
In Proceedings ofthe 2nd Workshop on Argumentation Mining, pages22?28, Denver, CO, June.
Association for Computa-tional Linguistics.Huy Nguyen and Diane Litman.
2016.
Improvingargument mining in student essays by learning andexploiting argument indicators versus essay topics.In Proceedings 29th International FLAIRS Confer-ence, Key Largo, FL, May.Raquel Mochales Palau and Marie-Francine Moens.2009.
Argumentation Mining: The Detection, Clas-sification and Structure of Arguments in Text.
InProceedings of the 12th International Conference onArtificial Intelligence and Law, ICAIL ?09, pages98?107, New York, NY, USA.
ACM.Joonsuk Park and Claire Cardie.
2014.
Identifying Ap-propriate Support for Propositions in Online UserComments.
In Proceedings of the First Workshopon Argumentation Mining, pages 29?38, Baltimore,Maryland, June.
Association for Computational Lin-guistics.Andreas Peldszus and Manfred Stede.
2013.
FromArgument Diagrams to Argumentation Mining inTexts: A Survey.
International Journal of Cogni-tive Informatics and Natural Intelligence (IJCINI),7(1):1?31, January.Andreas Peldszus and Manfred Stede.
2015a.
Jointprediction in MST-style discourse parsing for ar-gumentation mining.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 938?948, Lis-bon, Portugal, September.
Association for Compu-tational Linguistics.Andreas Peldszus and Manfred Stede.
2015b.
To-wards Detecting Counter-considerations in Text.
InProceedings of the 2nd Workshop on ArgumentationMining, pages 104?109, Denver, CO, June.
Associ-ation for Computational Linguistics.Andreas Peldszus.
2014.
Towards segment-basedrecognition of argumentation structure in short texts.In Proceedings of the First Workshop on Argumen-tation Mining, pages 88?97, Baltimore, Maryland,June.
Association for Computational Linguistics.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse TreeBank 2.0.In Proceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC-08),Marrakech, Morocco, May.
European Language Re-sources Association (ELRA).
ACL Anthology Iden-tifier: L08-1093.Vahed Qazvinian and Dragomir R. Radev.
2010.
Iden-tifying Non-explicit Citing Sentences for Citation-based Summarization.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 555?564,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Diarmuid?O S?eaghdha and Simone Teufel.
2014.
Un-supervised learning of rhetorical structure with un-topic models.
In Proceedings of the 25th Inter-national Conference on Computational Linguistics(COLING-14), Dublin, Ireland.Alan Sergeant.
2013.
Automatic argumentation ex-traction.
In The 10th European Semantic Web1136Conference, pages 656?660, Montpellier, France.Springer.Christian Stab and Iryna Gurevych.
2014a.
Anno-tating Argument Components and Relations in Per-suasive Essays.
In Proceedings of COLING 2014,the 25th International Conference on ComputationalLinguistics: Technical Papers, pages 1501?1510,Dublin, Ireland.
Dublin City University and Asso-ciation for Computational Linguistics.Christian Stab and Iryna Gurevych.
2014b.
IdentifyingArgumentative Discourse Structures in PersuasiveEssays.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 46?56, Doha, Qatar.
Associationfor Computational Linguistics.Christian Stab, Christian Kirschner, Judith Eckle-Kohler, and Iryna Gurevych.
2014.
ArgumentationMining in Persuasive Essays and Scientific Articlesfrom the Discourse Structure Perspective.
In ElenaCabrio, Serena Villata, and Adam Wyner, editors,Proceedings of the Workshop on Frontiers and Con-nections between Argumentation Theory and Natu-ral Language Processing, pages 40?49, Bertinoro,Italy, July.
CEUR-WS.Simone Teufel and Marc Moens.
2002.
Summariz-ing Scientific Articles: Experiments with Relevanceand Rhetorical Status.
Computational Linguistics,28(4), December.Douglas Walton, Christopher Reed, and FabrizioMacagno.
2008.
Argumentation Schemes.
Cam-bridge University Press.Jianxiang Wang and Man Lan.
2015.
A RefinedEnd-to-End Discourse Parser.
In Proceedings ofthe Nineteenth Conference on Computational Natu-ral Language Learning - Shared Task, pages 17?24,Beijing, China, July.
Association for ComputationalLinguistics.1137
