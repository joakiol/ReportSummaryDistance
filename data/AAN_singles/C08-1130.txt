Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1033?1040Manchester, August 2008Chinese Term Extraction Using Minimal ResourcesYuhaSchool of CScience and Technology,Harbin InTechnHarbin 151983yang@gmail.comQin LDepartment of Computing,The HonPolytechnHong Kocsluqin@comp.polyu.edu.TiejuSchool of CScience and Technology,Harbin InTechnHarbin 1tjzhao@mtlab.hit.edu.ctThis papterm extraction nimal resources.A term candidate extraction algorithm isproposed to i tures of the1Ter stfun  domain.
Termng Yangomputerstitute ofology,0001, Chinaug Kongic University,ng, Chinahkn Zhaoomputerstitute ofology,50001, ChinacnAbstraer presents a new approach forusing midentify fearelatively stable and domain independentterm delimiters rather than that of theterms.
For term verification, a linkanalysis based method is proposed tocalculate the relevance between termcandidates and the sentences in thedomain specific corpus from which thecandidates are extracted.
The proposedapproach requires no prior domainknowledge, no general corpora, no fullsegmentation and minimal adaptation fornew domains.
Consequently, the methodcan be used in any domain corpus and itis especially useful for resource-limiteddomains.
Evaluations conducted on twodifferent domains for Chinese termextraction show quite significantimprovements over existing techniquesand also verify the efficiency and relativedomain independent nature of theapproach.
Experiments on new termextraction also indicate that the approachis quite effective for identifying newterms in a domain making it useful fordomain knowledge update.Introductionms are the lexical units to represent the modamental knowledge of a?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.extraction isknowledge acqan essential task in domainuisition whichlexicon update, domain ontoetc.
Term extraction involves twtes by unithood calculations a valid term.
The secondendent features of domainters.Otcan be used forlogy construction,o steps.
The firststep extracts candidato qualify a string astep verifies them through termhood measures(Kageura and Umino, 1996) to validate theirdomain specificity.Existing techniques extract term candidatesmainly by two kinds of statistic based measuresincluding internal association (e.g.
Schone andJurafsky, 2001) and context dependency (e.g.Sornlertlamvanich et al, 2000).
These techniquesare also used in Chinese term candidateextraction (e.g.
Luo and Sun, 2003; Ji and Lu,2007).
Domain depms are used in a weighted manner to identifyterm boundaries.
However, these algorithmsalways face the dilemma that fewer features arenot enough to identify terms from non-termswhereas more features lead to more conflictsamong selected features in a specific instance.Most term verification techniques use featureson the difference in distribution of a termoccurred within a domain and across domains,such as TF-IDF (Salton and McGill, 1983; Frank,1999) and Inter-Domain Entropy (Chang, 2005).Limited distribution information on termcandidates in different documents are far fromenough to distinguish terms from non-termher researches attempted to use more directinformation.
The therm verification algorithm,TV_ConSem, proposed in (Ji and Lu, 2007) forChinese calculate the percentage of contextwords in a domain lexicon using both frequencyinformation and semantic information.
However,this technique requires a large domain lexiconand relies heavily on both the size and the qualityof the lexicon.
Some supervised learning1033approaches have been applied to protein/genename recognition (Zhou et al, 2005) and Chinesenew word identification (Li et al, 2004) usingSVM classifiers (Vapnik, 1995) which alsorequire large domain corpora and annotations,and intensive training is needed for a new domain.Current term extraction techniques (e.g.
Franket al, 1999; Chang, 2005; Ji and Lu, 2007) sufferfrom three major problems.
The first problem isthat these algorithms cannot identify certainkinds of terms such as the ones that have lessstatistical significance.
The second problem istheir dependency on full segmentation forChinese text which is particularly vulnerable tohaidates and the sentences in domainsps (terms for short) are more likely tobe domain substantives.
Words immediate befores, called predecessors andesconne .
These predecessors andChndle domain specific data (Huang et al, 2007).The third problem is their dependency on some apriori domain knowledge such as a domainlexicon making it difficult to be applied to a newdomain.In this work, the proposed algorithm extractscandidates by identifying the relatively stable anddomain independent term boundary markersinstead of looking for features associated with theterm candidate themselves.
Furthermore, a novelalgorithm for term verification is proposed usinglink analysis to calculate the relevance betweenterm candecific corpus to validate their domainspecificity.The rest of the paper is organized as follows.Section 2 describes the proposed algorithms.Section 3 explains the experiments and theperformance evaluation.
Section 4 is theconclusion.2 Methodology2.1 Delimiters Based Term CandidateExtractionGenerally speaking, sentences are constituted bysubstantives and functional words.
Domainspecific termand after these termsucc sors of the terms, are likely to be eitherfunctional words or other general substantivescting termssuccessors can be considered as markers of terms,and are referred to as term delimiters in thispaper.
In contrast to terms, delimiters arerelatively stable and domain independent.
Thus,they can be extracted more easily.
Instead oflooking for features associated with terms as inother works, this paper looks for featuresassociated with term delimiters.
That is, termdelimiters are identified first.
Words betweendelimiters are then taken as term candidates.The proposed delimiter identification basedalgorithm, referred to as TCE_DI (TermCandidate Extraction ?
Delimiter Identification),extracts term candidates from a domain corpusby using a delimiter list, referred to as the DList.Given a DList, the algorithm TCE_DI itself isstraight forward.
For a given character string CS(CS = C1C2?Cn) shown in Figure 1, where Ci is ainese character.
Suppose there are twodelimiters D1 = Ci1?Cil and D2 = Cj1?Cjm in CSwhere D1 ?
DList and D2 ?
DList.
The string CSis then segmented to five substrings: C1?Cib,Ci1?Cil, Cia?Cjb, Cj1?Cjm, and Cja?Cn.
SinceCi1?Cil and Cj1?Cjm are delimiters, C1?Cib,Cia?Cjb, and Cja?Cn are regarded as termcandidates as labeled by TC1, TC2 and TC3 inFigure 1, respectively.
If there is no delimitercontained in CS, the whole string C1C2?Cn isregarded as one term candidate.Figure 1.
Paradigm of Term Candidate ExtractionDList can be obtained either from a delimitertraining corpus or from a given  o l stop w rd ist.Given a delimiter training corpus, CorpusTraining,normally a domain specific corpus, and a domainlexicon Lexicon, DList can be obtained based onthe following algorithm, referred to as DList_ExtStwithoua stop  experts or from age(DelimiterList Extraction Algorithm).ep 1: For each term Ti in Lexicon, mark Ti inCorpusTraining as a non-divisible lexical unit.Step 2: Segment remaining text in CorpusTraining.Step 3: Extracts predecessors and successors ofall Ti as delimiter candidates.Step 4: Remove delimiter candidates that arecontained in a Ti in Lexicon.Step 5: Rank delimiter candidates by frequencyand the top NDI number of items areconsidered delimiters.The DList_Ext algorithm basically use knownterms in a domain specific Lexicon to find thedelimiters.
It can be shown in the experimentslater that Lexicon does not need to becomprehensive.
Even if a small training corpus,CorpusTraining, is not available in a languaget sufficient domain specific NLP resources,-word list produced byneral corpus can serve as DList directlywithout using the DList_Ext algorithm.10342.2 Link Analysis Based Term VerificationIn a domain corpus, some sentences are domainrelevant sentences which contain more domainspecific information whereas others are generalsentences which contain less domain information.A domain specific term is more likely to becontained in domain relevant sentences, whichmeans that domain relevant sentences andai, w(pn)l numbdom n specific terms have a mutuallyreinforcing relationship.
A novel algorithm,referred to as TV_LinkA (Term Verification ?Link Analysis) based the Hyperlink-InducedTopic Search (HITS) algorithm (Kleinberg, 1997)originally proposed for information retrieval, isproposed using link analysis to calculate therelevance between term candidates and thesentences in domain specific corpora for termverification.In TV_LinkA, a node p can either be a sentenceor a term candidate.
If a term candidate TermC iscontained in a sentence Sen of the corpusCorpusExtract where the candidates were extracted,there is a directional link from Sen to TermC.
Thisway, a graph for the candidates and the sentencesin CorpusExtract can be constructed and the linksbetween them indicate their relationships.
A goodhub  Corpu in sExtract is a sentence that containsmany good authorities; a good authority is a termcandidate that is contained in many good hubs.Each node p is associated with a non-negativeauthority weight Apw )(  and a non-negative hubweight Hpw )( .
Link analysis in TV_LinkAmakes use of the relationship between hubs andauthorities via an iterative process to maintainand update authority/hub weights for each nodeof the graph.Let VA denote the authority vector (w(p1)A,w(p2)A,?, w(pn)A)  and VH denote the hub vector(w(p1)H, w(p2)H,?
H), where n is the sum ofthe tota er of sentences and the totalnumber of term candidates.
Given weights VA andVH with a directional link p?q, the I operation(anin-pointer to a node) and the O operation(an out-pointer to a node) update w(q)A and w(p)H asfollows.I operation: ??
?=EqpHA w(p)w(q)          (1)O operation: ??
?e calculated as follows.For iApply the I operation to ( ),o .factor=EqpAH w(q)w(p)         (2)Let k be the iteration termination parameter and zbe the vector (1, 1, 1,?, 1) , and VA and VH areinitialized to AV0  =HV0  = z.
Hubs and authoritiescan then b= 1, 2,?, kAiV 1- ,HiV 1-btaining new AiV 'Apply the O operation to ( AiV ' ,HiV 1- ),obtaining new HiV ' .Normalize iV '  by dividing theAnormalization  ?
2)'( A(p)w  to'  by dividing theobtain AiV .Normalize V Hinormalization factor ?
2)'( H(p)w  toEndRIn sExt , term candidates with highauthority indom terms whhigh uments are mlikelon this observation, the termhood of eachcandidate term TermC, denoted as TermhoodC, iscalculated according to formula (3) definedbeobtain iV .eturn ( AkV ,HkV )Corpu ractHa few documents are likely to beain specific ereas candidates withauthority in many doc orey to be commonly used general words.
Basedlow.
)log()(CjAjC DFDw(C)Termhood ?=      (3)where Ajw(C)  is the authority of TermC in adocument Dj of CorpusExtract, |D| is the totalnumber of documents in CorpusExtract and DFC isthe total number of documents in which TermCoccurs.
Term stermhood valC  are then ranked according to theirues TermhoodC, and the top rankedNTCList candidates are considered terms.
NTan algorithm parameter to be determinedentallye two sets of non-overlappingacademic papers in the IT domain andCorpusIT_Small is identical to the corpus used inTV_ConSem(Ji and Lu, 2007).
CorpusLegal_Small isa complete set of official Chinese criminal lawarticles.
CorpusLegal_Large includes the complete setCList isexperim .3 Performance Evaluation3.1 Data PreparationTo evaluate the performance of the proposedalgorithms for Chinese, experiments areconducted on four corpora of two differentdomains as listed in Table 1.
CorpusIT_Small andCorpusIT_Large ar1035of ficial Chinese constitutional lof aw articles anddEconomics/Finance law articles (http://www.law-lib.com/).
Three domain lexicons used in theexperiments are detailed in Table 2.
LexiconIT isobtained according to the term extractionalgorithm (Ji and Lu, 2007) with manualverification.
LexiconLegal is extracted fromCorpusLegal_Small by manual verification too.Because legal text covers a lot of different areassuch finance, science, advertisement, etc., theactually legal specific terms are relatively smallin size.
LexiconPKU contains a total of 144Kmanually verified IT terms supplied by theInstitute of Computational Linguistics, PekingUniversity.
LexiconPKU, is used as the standardterm set for evaluation on the IT domain.CorpusIT_Small and LexiconIT are used to obtain thedelimiter list of IT domain, DListIT.CorpusLegal_Small and LexiconLegal are used toobtain the elimiter list of legal domain,DListLegal.
CorpusIT_Large and CorpusLegal_Large areused as open test data to evaluate the proposedalgorithms in IT domain and legal domain,respectively.Corpus Domain Size Text typeCorpusIT_Small IT 77K AcademicpapersCorpusIT_Large IT 6.64M AcademicpapersCorpusLegal_SmallLegal 344K LawarticleCorpusLegal_LargeLegal 1.04M LawarticleTable 1.
Different Corpora Used for ExperimentsLexicon Domain Size SourceLexiconIT IT 3,337 CorpusIT_SmallLex 3iconLegal Legal 94 CorpusLegal_SmallLexiconPKU IT 144K PKUTable 2.
D rent Le sexperimerify that the approach works with aop word list without delimiter extraction,rd list, , is d a encethe 494 general purpose stop wordsweb www n) thou.in the ITalua efollow formula:iffeExicons Untsd forTo vesimple sta stop wo DListSW also use s referby takingdownloaded from a Chinese NLP resourcesite (ion.nlp.org.c wi t anymodificatThe performance of the algorithmdomain is ev ted by precision according to thTCListNewLexicon N+TE NNrecis =where tes interm candidate xtracted by aneve verification of all the new termsisarked them as correct terms.
Asthere is no reasonably large standard llist available, the evaluation of the legp ion            (4)NTCList is the number of term candidalist TCList ealuated algorithm, NLexicon denotes the numberof term candidates in TCList contained inLexiconPKU, NNew denotes the number of extractedterm candidates that are not in LexiconPKU, yetare considered correct.
Thus, NNew is the numberof newly discovered terms with respect toLexiconPKU.
Thcarried out manually by two expertsindependently.
A new term is considered correctif both experts megal termal domainin terms of precision is conducted manually.
Noevaluation on new term extraction is conducted.To evaluate the ability of the algorithms inidentify new terms in the IT domain, anothermeasurement is applied to the IT corpus againstLexiconPKU based on the following formula:TCListNewNTE NNR =                          (5)where TCList and NNew are the same as given informula (4).
A higher RNTE indicates that moreextracted terms are outside of LexiconPKU and arethus considered new terms.
This is similar to themeasurements of out of vocabulary (OOV) inChinese segmentation.
A higher RNTE indicatesthe algorithm can be useful for domainknowledge update including lexicon expansion.3.2 Evaluation on Term ExtractionFor comparison, a statistical based termcandidate extraction algorithm, TCE_SEF&CVwith the best performance inusing both internal association and externale; one is a(Ji and Lu, 2007)strength, is used as the reference algorithm forthe evaluation of TCE_DI.
A statistics based termverification algorithm, TV_ConSem (Ji and Lu,2007) using semantic information within acontext window is used for the evaluation ofTV_LinkA.
LexiconPKU is also used inTV_ConSem.
Two popular methods integratedwithout division of candidate extraction andverification steps are used for comparison.
Thfirst one is based on TF-IDF (Salton and McGill,1983 Frank et al, 1999).
The secondsupervised learning approach based on a SVMclassifier, SVMlight (Joachims, 1999).
Thefeatures used by SVMlight are shown in Table 3.Two training sets are constructed for the SVMclassifier.
The first one includes 3,337 positiveexamples (LexiconIT) and 5,950 negativeexamples extracted from CorpusIT_Small.
Thesecond one includes 394 positive examples1036(LexiconLegal) and 28,051 negative examplesextracted from CorpusLegal_Small.No.
Feature Explanation1 Percentage of the Chinese charactersoccurred in LexiconDomain2 Frequency in the domain corpus3 Frequency in the general corpus4 Part of speech5 The length of Chinese characters inthe candidate6 The length of non-Chinesecharacters in the candidate7 Contextual evidenceTable 3.
Features Used in the SVM ClassifierperforFigure 2 shows the mance of theproposed TCE_ for termex tio sfor IT do ITTCE_DIl tractedde iter  NDI =50 esp SWword listDI and TV_LinkAtrac n compared to the reference algorithmmain using CorpusIT_Large.
TCE_DI  andegal indicate TCE_DI using exlim  lists DListIT and DListLegal withI  simply uses the stop 0, r ectively.
TCE_DDListSW.0 1000 2000 3000 4000 5000404550556065707580859095100PrecisionExtracted Terms (NTCList)TCE_DIIT+TV_LinkATCE_DILegal+TV_LinkATCE_DI +TV_LSWinkATCE_SEF&CV+TV_LinkATCE_DIIT+TV_ConSemTCE_SEF&CV+TV_ConSemTF-IDFSVMFigure 2 Performance of Different Algorithms onIT DomainAs shown in Figure 2, term extraction basedon TCE_DIIT combined with TV_LinkA gives thebest performance.
It achieves 75.4% precisionwhen the number of extracted terms NTCListreaches 5,000.
The performance is 9.6% and29.4% higher in precision compared to TF-IDFand TCE_SEF&CV combined with TV_ConSem,respectively.
These translate to improvements ofver 14.8% and 63.9%, respectively.When applying the same TV_LinkA algorithmfor term verification, TCE_DI using differentdelimiter lists provide 24% better performance onaverage compared to the TCE_SEF&CValgorithm which translates to improvement ofover 47%.
The result from using delimiters oflegal domain (DListLegal) to data in IT domain (asshown in TCE_DIlegal) is better on average thanusing a simple general stop word list.
It should bever, that TCE_DISW still performsmuch better than the reference algorithms, whichmeans that delimiter based term candidateextraction algorithm can improve performanceeven without any domain specific training.
Whenapplying the same TCE_DIIT algorithm in termcandidate extraction, TV_LinkA provides 10%higher performance compared to the TV_ConSemTV_LinkA using o word list withoutanofprecision of onoted, howealgorithm which translates to improvement ofover 15.3%.
It is important to point out thatnly the stopy domain specific knowledge performs betterthan TV_ConSem using a large domain lexicon.In other words, delimiter based extraction withlink analysis use much less resources and stillimprove performance of TV_ConSem.The performance of TCE_DIIT orTCE_SEF&CV combined with TV_ConSem havean upward trend when more terms are extractedwhich seems to be against intuition.
The principlethe TV_ConSem algorithm is that a candidateis considered a valid term if a majority of itscontext words already appear in the domainlexicon.
General words are more likely to beranked on top because they are commonly usedwhich explains the low performance ofTV_ConSem in the lower range of NTCList.
WhenNTCList increases, more domain terms are included.Thus, there is an upward trend in precision.
But,the upward trend reverts at around 4,500 becausethe measurement in percentage is too low todistinguish valid terms from non-term candidates.It is also interesting to point out that the simpleTF-IDF algorithm which was rarely used inChinese term extraction performs as well as theSVM classifier.
The main reason is that the testcorpus consists of academic papers.
So, manyterms are consistent and repeated a lot of times indifferent documents which accords with the ideaof TF-IDF.
Thus, TF-IDF performs relativelywell because of the high-quality domain corpus.However, TF-IDF, as a statistics based algorithmsuffers from similar problem as others based on1037statistics.
Thus it does not perform as well as theproposed TCE_DI and TV_LinkA algorithms.acFigure 3 shows that the proposed algorithmsachieve similar performance on the legal domain.TCE_DILegal combined with TV_LinkA performthe best.
The result from using IT domaindelimiters (DListIT) in legal domain as shown inTCE_DIIT is better on average than using thegeneral purpose stop list.
This further proves thatextracted delimiter list even from a differentdomain can be more effective than a general stopword list.
When applying the same TV_LinkAalgorithm for term verification, TCE_DI usingdifferent delimiter lists are better than all thereference algorithms.
Without large lexicon inChinese legal domain, the TV_ConSem algorithmdoes not even work.
TV_LinkA using no priordomain knowledge for term verification stillhieves similar improvement compared to thatof the IT domain where a comprehensive domainlexicon is available.7080901000 1000 2000 3000 4000 5000405060Extracted Terms (NTCList)PrecisionTCE_DIIT+TV_LinkATCE_DILegal+TV_LinkATCE_DISW+TV_LinkATCE_SEF&CV+TV_LinkATF-IDFSVMFigure 3.
Performance of Different Algorithmson Legal DomainThere are three main reasons for theperformance improvements of the proposedTCE_DI and TV_LinkA algorithms.
Firstly, thedelimiters which are mainly functional words (e.g.
???
(at/in), ???
(or)) and general substantive(e.g.
???
(be), ????
(adopt)) can be extractedeasily and are effective term boundary markerssince they are quite domain independent andstable.
Secondly, the granularity of domainspecific terms extracted the proposed algorithm ismuch larger than words obtained by wordsegmentation.
This keeps many noisy strings outof the term candidate set.
Thus, the proposeddelimiter based algorithm performs much betterover segmentation based statistical methods.Thirdly, the proposed approach is not as sensitiveto term frequency as other statistical basedapproaches because term candidates areidentified without regards to the frequencies ofthe candidates.
In the TV_LinkA algorithm, termsare verified by calculating the relevance betweencandidates and the sentences instead of thedistributions of terms in different types ofdocuments.
Terms having low frequencies can beidentified as long as they are in domain relevantsentences whereas in the previous approachesincluding TF-IDF, terms with less statisticalsignificance are weeded out.
For example, a longIT term ?????????
(Hierarchical storagesystem) with a low frequency of 6 is extractedusing the proposed approach.
It cannot beiinformation is is term cannotbe extractedentified by TF-IDF since the statisticalnot significant.
Thd by the segmentation basedalgorithms either because general segmentor splitlong terms into pieces making them difficult tobe reunited using term extraction techniques.It is interesting to know that the proposedapproach not only achieves the best performancefor both domains, it also achieves second bestwhen using extracted delimiters from a differentdomain.
The results confirm that delimiters arequite stable across domains and the relevancebetween candidates and sentences are efficientfor distinguishing terms from non-terms indifferent domains.
In fact, the proposed approachcan be applied to different domains with minimaltraining or no training if resources are limited.3.3 Evaluation on New Term ExtractionAs LexiconPKU is the only ready-to-use domainlexicon, the evaluation on new term extraction isconducted on CorpusIT_Large only.
Figure 4 showsthe evaluation of the proposed algorithmscompared to the reference algorithms in terms ofRNTE, the ratio of new terms among all identifiedterms.It can be seen that the proposed algorithmsTCE_DIIT combined with TV_LinkA is basicallythe top performer throughout the range.
It canidentify 4% (with respect to TCE_SEF&CV+TV_ConSem) to 27% (with respect to TF-IDF)more new terms when NTCList reaches 5,000 whichtranslate to improvements of over 9% to 170%,respectively.
The second best performer isTCE_DIlegal combined with TV_LinkA usingdelimiters of legal domain.
In fact, it onlyunderperforms in the lower range of NTCList1038compared to TCE_DIIT.
When NTCList reaches5,000, their performance is basically the same.However, the TCE_DISW algorithm using scontext words occur in the domain lexicon thanthat of other terms.
Thus, new terms are actuallyranked higher than other terms in TV_ConSemwhich explains its higher ability to identify newterms in the low range of NTCList.
However, itsperformance drops in the high range of NTCListbecause the influence of context wordsdiminishes in terms of percentage in the domainlexicon to distinguish terms from non-terms.Figure 4 also shows that TF-IDF and SVMperform the worst in new term extractioncompared to other algorithms.
TF-IDF hasrelatively low ability to identify new terms sincenew terms are not widely used and they do notrepeat a lot of times in many documents.
AsSVM  is sensitive to training data, it is naturallynot adaptive to new terms.All current Chinese term extraction algorithmsrely on segmentation with comprehensive lexicalknowledge and yaproblem.
T xtractionpatopwoinmi ionards performs much worse than using extracteddelimiter lists as shown for TCE_DIIT andTCE_DIlegal.
In the TCE_DI algorithm, characterstrings are split by delimiters and the remainedparts are taken as term candidates.
Generallyspeaking, if a new term contains a delimiter or astop word as its component, it cannot beidentified correctly.
Consequently, if a new termcontains a stop word as its component, it cannotbe extracted correctly using TCE_DISW.However, new terms are less likely to contadeli ters because the delimiter extractalgorithm DList_Ext would not considercomponent as a delimiter if it is contained in aterm in LexiconDomain.
Consequently, TCE_DISWis less adaptive to domain specific data comparedto TCE_DIIT and TCE_DIlegal.
That is also whyTCE_DISW picks up new terms much more slowly.0 1000 2000 3000 4000 500001020304050Percentageof NewTermsExtracted Terms (NTCList)TCE_DI +TV_LinkAITTCE_DILegal+TV_LinkATCE_DISW+TV_LinkATCE_SEF&CV+TV_LinkATCE_DIIT+TV_ConSemTCE_SEF&CV+TV_ConSemTF-IDFSVMFigure 4.
Performance of Different Algorithmsfor New Term ExtractionIt is interesting to know that TCE_DIITcombined with TV_ConSem identifies more newterms in the low range of NTCList.
In theTV_ConSem algorithm, the major informationused for term verification is the percentage of thecontext words appear in the domain lexicon.
Asdiscussed earlier in Section 3.2, TV_ConSemranks commonly used general words higher thanothers which leads to the low precision ofTV_ConSem for term extraction.
A new termfaces a similar scenario because more of itset Chinese segmentationlgorithms have the OOV (out of vocabulary)his makes Chinese term erticularly vulnerable to new term extraction.The proposed approach, on the other hand, isbased on delimiters which is more stable, domainindependent, and OOV independent.
Figure 4shows that TCE_DI and TV_LinkA using minimaltraining from different domains can extract muchmore new terms than previous techniques.
In fact,the proposed approach can serve as a much bettertool to identify new domain terms and can bequite effective for domain lexicon expansion.4 ConclusionIn conclusion, this paper presents a robust termextraction approach using minimal resources.
Itincludes a delimiter based algorithm for termcandidate extraction and a link analysis basedalgorithm for term verification.
The proposedapproach is not sensitive to term frequency as theprevious works.
It requires no prior domainknowledge, no general corpora, no fullsegmentation, and minimal adaptation for newdomains.Experiments for term extraction are conductedon IT domain and legal domain, respectively.Evaluations indicate that the proposed approachhas a number of advantages.
Firstly, the proposedapproach can improve precision of termextraction quite significantly.
Secondly, the factthat the proposed approach achieves the bestperformance on two different domains verifies itsdomain independent nature.
The proposedapproach using delimiters extracted from a1039different domain also achieves the second bestperformance which indicates that the delimitersare quite stable and domain independent.
Theproposed approach still performs much betterthan the reference algorithms when using ageneral purpose stop word list, which means thatthe proposed approach can improve performancewell even as a completely unsupervised approachwithout any training.
Consequently, the resultsdemonstrate that the proposed approach can beapplied to different domains easily even withouthe proposed approach isRAugust 2002.a.
2002.
A measure of termed on the number of co-inese WordSegmentation: Tokenization, Charactern, or Wordbreak Identification.
InKa K., and B. Umino.
1996.
Methods ofKlonment.
In Proceedings of the 9thJi2 ?
74.LuMeasures.
InMe Identification and SemanticNaSc rafsky D. 2001.
Is Knowledge-freeSoordVlZhtraining.
Thirdly, tparticularly good for identifying new terms sothat it can serve as an effective tool for domainlexicon expansion.AcknowledgementsThis work was done while the first author wasworking at the Hong Kong PolytechnicUniversity supported by CERG Grant B-Q941and Central Research Grant: G-U297.eferencesChang Jing-Shin.
2005.
Domain Specific WordExtraction from Hierarchical Web Documents: AFirst Step toward Building Lexicon Trees fromWeb Corpora.
In Proceedings of the FourthSIGHAN Workshop on Chinese Language Learning:64-71.Chien LF.
1999.
Pat-tree-based adaptive keyphraseextraction for intelligent Chinese informationretrieval.
Information Processing and Management,vol.35: 501-521.Eibe Frank, Gordon.
W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific Keyphrase Extraction.
InProceedings of 16th International Joint Conferenceon Artificial Intelligence IJCAI-99: 668-673.Feng Haodi, Kang Chen, Xiaotie Deng , and WeiminZheng, 2004.
Accessor variety criteria for Chineseword extraction.
Computational Linguistics,30(1):75-93.Hiroshi Nakagawa, and Tatsunori Mori.
2002.
Asimple but powerful automatic term extractionmethod.
In COMPUTERM-2002 Proceedings ofthe 2nd International Workshop on ComputationalTerm: 29-35.
Taiwan,Hisamitsu T., and Y. Niwrepresentativeness basoccurring salient words.
In Proceedings of the 19thCOLING, 2002.Huang Chu-Ren, Petr ?Simon, Shu-Kai Hsieh, andLaurent Pr?evot.
2007.
Rethinking ChClassificatioProceedings of the ACL 2007 Demo and PosterSessions: 69?72.
Joachims T. 2000.
Estimating theGeneralization Performance of a SVM Efficiently.In Proceedings of the International Conference onMachine Learning, Morgan Kaufman, 2000.geuraautomatic term recognition: a review.
Term3(2):259-289.einberg J.
1997.
Authoritative sources in ahyperlinked envirACM-SIAM Symposium on Discrete Algorithms:668-677.
New Orleans, America, January 1997.Luning, and Qin Lu.
2007.
Chinese Term ExtractionUsing Window-Based Contextual Information.
InProceedings of CICLing 2007, LNCS 4394: 6Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, andXiaozhong Fan.
The Use of SVM for Chinese NewWord Identification.
In Proceedings of the 1stInternational Joint Conference on NaturalLanguage Processing ( IJCNL P2004): 723-732.Hainan Island, China, March 2004.o Shengfen, and Maosong Sun.
2003.
Two-Character Chinese Word Extraction Based onHybrid of Internal and ContextualProceedings of the Second SIGHAN Workshop onChinese Language Processing: 24-30.cDonald, David D. 1993.
Internal and ExternalEvidence in thCategorization of Proper Names.
In Proceedings ofthe Workshop on Acquisition of LexicalKnowledge from Text, pages 32--43, Columbus,OH, June.
Special Interest Group on the Lexicon ofthe Association for Computational Linguistics.sreen AbdulJaleel and Yan Qu.
2005.
DomainTerm Extraction and Structuring via Link Analysis.In Proceedings of the AAAI '05 Workshop on LinkAnalysis: 39-46.Salton, G., and McGill, M.J. (1983).
Introduction toModern Information Retrieval.
McGraw-Hill.hone, P. and JuInduction of Multiword Unit Dictionary Headwordsa solved problem?
In Proceedings of EMNLP2001.rnlertlamvanich V., Potipiti T., and Charoenporn T.2000.
Automatic Corpus-based Thai WExtraction with the C4.5 Learning Algorithm.
InProceedings of COLING 2000.adimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, 1995.ou GD, Shen D, Zhang J, Su J, and Tan SH.
2005.Recognition of Protein/Gene Names from Textusing an Ensemble of Classifiers.
BMCBioinformatics 2005, 6(Suppl 1):S7.1040
