Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224?229,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Bilingual Word Representations by Marginalizing AlignmentsTom?a?s Ko?cisk?y Karl Moritz HermannDepartment of Computer ScienceUniversity of OxfordOxford, OX1 3QD, UK{tomas.kocisky,karl.moritz.hermann,phil.blunsom}@cs.ox.ac.ukPhil BlunsomAbstractWe present a probabilistic model that si-multaneously learns alignments and dis-tributed representations for bilingual data.By marginalizing over word alignmentsthe model captures a larger semantic con-text than prior work relying on hard align-ments.
The advantage of this approach isdemonstrated in a cross-lingual classifica-tion task, where we outperform the priorpublished state of the art.1 IntroductionDistributed representations have become an in-creasingly important tool in machine learning.Such representations?typically continuous vec-tors learned in an unsupervised setting?can fre-quently be used in place of hand-crafted, and thusexpensive, features.
By providing a richer rep-resentation than what can be encoded in discretesettings, distributed representations have been suc-cessfully used in many areas.
This includes AI andreinforcement learning (Mnih et al, 2013), imageretrieval (Kiros et al, 2013), language modelling(Bengio et al, 2003), sentiment analysis (Socheret al, 2011; Hermann and Blunsom, 2013), frame-semantic parsing (Hermann et al, 2014), and doc-ument classification (Klementiev et al, 2012).In Natural Language Processing (NLP), the useof distributed representations is motivated by theidea that they could capture semantics and/or syn-tax, as well as encoding a continuous notion ofsimilarity, thereby enabling information sharingbetween similar words and other units.
The suc-cess of distributed approaches to a number oftasks, such as listed above, supports this notionand its implied benefits (see also Turian et al(2010) and Collobert and Weston (2008)).While most work employing distributed repre-sentations has focused on monolingual tasks, mul-tilingual representations would also be useful forseveral NLP-related tasks.
Such problems includedocument classification, machine translation, andcross-lingual information retrieval, where multi-lingual data is frequently the norm.
Furthermore,learning multilingual representations can also beuseful for cross-lingual information transfer, thatis exploiting resource-fortunate languages to gen-erate supervised data in resource-poor ones.We propose a probabilistic model that simulta-neously learns word alignments and bilingual dis-tributed word representations.
As opposed to pre-vious work in this field, which has relied on hardalignments or bilingual lexica (Klementiev et al,2012; Mikolov et al, 2013), we marginalize outthe alignments, thus capturing more bilingual se-mantic context.
Further, this results in our dis-tributed word alignment (DWA) model being thefirst probabilistic account of bilingual word repre-sentations.
This is desirable as it allows better rea-soning about the derived representations and fur-thermore, makes the model suitable for inclusionin higher-level tasks such as machine translation.The contributions of this paper are as follows.We present a new probabilistic similarity measurewhich is based on an alignment model and priorlanguage modeling work which learns and relatesword representations across languages.
Subse-quently, we apply these embeddings to a standarddocument classification task and show that theyoutperform the current published state of the art(Hermann and Blunsom, 2014b).
As a by-productwe develop a distributed version of FASTALIGN(Dyer et al, 2013), which performs on par withthe original model, thereby demonstrating the ef-ficacy of the learned bilingual representations.2 BackgroundThe IBM alignment models, introduced by Brownet al (1993), form the basis of most statistical ma-chine translation systems.
In this paper we baseour alignment model on FASTALIGN (FA), a vari-224ation of IBM model 2 introduced by Dyer et al(2013).
This model is both fast and producesalignments on par with the state of the art.
Further,to induce the distributed representations we incor-porate ideas from the log-bilinear language modelpresented by Mnih and Hinton (2007).2.1 IBM Model 2Given a parallel corpus with aligned sentences, analignment model can be used to discover matchingwords and phrases across languages.
Such mod-els are an integral part of most machine translationpipelines.
An alignment model learns p(f ,a|e) (orp(e,a?|f)) for the source and target sentences eand f (sequences of words).
a represents the wordalignment across these two sentences from sourceto target.
IBM model 2 (Brown et al, 1993) learnsalignment and translation probabilities in a gener-ative style as follows:p(f ,a|e) = p(J |I)J?j=1p(aj|j, I, J) p(fj|eaj),where p(J |I) captures the two sentence lengths;p(aj|j, I, J) the alignment and p(fj|eaj)thetranslation probability.
Sentence likelihood isgiven by marginalizing out the alignments, whichresults in the following equation:p(f |e) = p(J |I)J?j=1I?i=0p(i|j, I, J) p(fj|ei) .We use FASTALIGN (FA) (Dyer et al, 2013), alog-linear reparametrization of IBM model 2.
Thismodel uses an alignment distribution defined bya single parameter that measures how close thealignment is to the diagonal.
This replaces theoriginal multinomial alignment distribution whichoften suffered from sparse counts.
This improvedmodel was shown to run an order of magnitudefaster than IBM model 4 and yet still outperformedit in terms of the BLEU score and, on Chinese-English data, in alignment error rate (AER).2.2 Log-Bilinear Language ModelLanguage models assign a probability measureto sequences of words.
We use the log-bilinearlanguage model proposed by Mnih and Hinton(2007).
It is an n-gram based model defined interms of an energy function E(wn;w1:n?1).
Theprobability for predicting the next word wngivenits preceding context of n ?
1 words is expressedusing the energy functionE(wn;w1:n?1)=?
(n?1?i=1rTwiCi)rwn?bTrrwn?bwnas p(wn|w1:n?1) =1Zcexp (?E(wn;w1:n?1))where Zc=?wnexp (?E(wn;w1:n?1)) is thenormalizer, rwi?
Rdare word representations,Ci?
Rd?dare context transformation matrices,and br?
Rd, bwn?
R are representation and wordbiases respectively.
Here, the sum of the trans-formed context-word vectors endeavors to be closeto the word we want to predict, since the likelihoodin the model is maximized when the energy of theobserved data is minimized.This model can be considered a variant of alog-linear language model in which, instead ofdefining binary n-gram features, the model learnsthe features of the input and output words, anda transformation between them.
This provides avastly more compact parameterization of a lan-guage model as n-gram features are not stored.2.3 Multilingual Representation LearningThere is some recent prior work on multilin-gual distributed representation learning.
Simi-lar to the model presented here, Klementiev etal.
(2012) and Zou et al (2013) learn bilingualembeddings using word alignments.
These twomodels are non-probabilistic and conditioned onthe output of a separate alignment model, un-like our model, which defines a probability dis-tribution over translations and marginalizes overall alignments.
These models are also highly re-lated to prior work on bilingual lexicon induc-tion (Haghighi et al, 2008).
Other recent ap-proaches include Sarath Chandar et al (2013),Lauly et al (2013) and Hermann and Blunsom(2014a, 2014b).
These models avoid word align-ment by transferring information across languagesusing a composed sentence-level representation.While all of these approaches are related to themodel proposed in this paper, it is important tonote that our approach is novel by providing aprobabilistic account of these word embeddings.Further, we learn word alignments and simultane-ously use these alignments to guide the represen-tation learning, which could be advantageous par-ticularly for rare tokens, where a sentence basedapproach might fail to transfer information.Related work also includes Mikolov et al(2013), who learn a transformation matrix to225reconcile monolingual embedding spaces, in anl2norm sense, using dictionary entries instead ofalignments, as well as Schwenk et al (2007) andSchwenk (2012), who also use distributed repre-sentations for estimating translation probabilities.Faruqui and Dyer (2014) use a technique based onCCA and alignments to project monolingual wordrepresentations to a common vector space.3 ModelHere we describe our distributed word alignment(DWA) model.
The DWA model can be viewedas a distributed extension of the FA model in thatit uses a similarity measure over distributed wordrepresentations instead of the standard multino-mial translation probability employed by FA.
Wedo this using a modified version of the log-bilinearlanguage model in place of the translation proba-bilities p(fj|ei) at the heart of the FA model.
Thisallows us to learn word representations for bothlanguages, a translation matrix relating these vec-tor spaces, as well as alignments at the same time.Our modifications to the log-bilinear model areas follows.
Where the original log-bilinear lan-guage model uses context words to predict the nextword?this is simply the distributed extension ofan n-gram language model?we use a word fromthe source language in a parallel sentence to pre-dict a target word.
An additional aspect of ourmodel, which demonstrates its flexibility, is that itis simple to include further context from the sourcesentence, such as words around the aligned wordor syntactic and semantic annotations.
In this pa-per we experiment with a transformed sum overk context words to each side of the aligned sourceword.
We evaluate different context sizes and re-port the results in Section 5.
We define the energyfunction for the translation probabilities to beE(f, ei) = ?
(k?s=?krTei+sTs)rf?bTrrf?bf(1)where rei, rf?
Rdare vector representations forsource and target words ei+s?
VE, f ?
VFintheir respective vocabularies, Ts?
Rd?dis thetransformation matrix for each surrounding con-text position, br?
Rdare the representation bi-ases, and bf?
R is a bias for each word f ?
VF.The translation probability is given byp(f |ei) =1Zeiexp (?E(f, ei)) , whereZei=?fexp (?E(f, ei)) is the normalizer.In addition to these translation probabilities, wehave parameterized the translation probabilitiesfor the null word using a softmax over an addi-tional weight vector.3.1 Class FactorizationWe improve training performance using a classfactorization strategy (Morin and Bengio, 2005)as follows.
We augment the translation probabil-ity to be p(f |e) = p(cf|e) p(f |cf, e) where cfis a unique predetermined class of f ; the classprobability is modeled using a similar log-bilinearmodel as above, but instead of predicting a wordrepresentation rfwe predict the class representa-tion rcf(which is learned with the model) and weadd respective new context matrices and biases.Note that the probability of the word f dependson both the class and the given context words: it isnormalized only over words in the class cf.In our training we create classes based on wordfrequencies in the corpus as follows.
Consideringwords in the order of their decreasing frequency,we add word types into a class until the total fre-quency of the word types in the currently consid-ered class is less thantotal tokens?|VF|and the class size isless than?|VF|.
We have found that the maximalclass size affects the speed the most.4 LearningThe original FA model optimizes the likelihoodusing the expectation maximization (EM) algo-rithm where, in the M-step, the parameter updateis analytically solvable, except for the ?
parameter(the diagonal tension), which is optimized usinggradient descent (Dyer et al, 2013).
We modifiedthe implementations provided with CDEC (Dyer etal., 2010), retaining its default parameters.In our model, DWA, we optimize the likelihoodusing the EM as well.
However, while training wefix the counts of the E-step to those computed byFA, trained for the default 5 iterations, to aid theconvergence rate, and optimize the M-step only.Let ?
be the parameters for our model.
Then thegradient for each sentence is given by??
?log p(f |e) =J?k=1I?l=0[p(l|k, I, J) p(fk|el)?Ii=0p(i|k, I, J) p(fk|ei)???
?log(p(l|k, I, J) p(fk|el))]226where the first part are the counts from the FAmodel and second part comes from our model.We compute the gradient for the alignmentprobabilities in the same way as in the FA model,and the gradient for the translation probabilitiesusing back-propagation (Rumelhart et al, 1986).For parameter update, we use ADAGRAD as thegradient descent algorithm (Duchi et al, 2011).5 ExperimentsWe first evaluate the alignment error rate of ourapproach, which establishes the model?s ability toboth learn alignments as well as word representa-tions that explain these alignments.
Next, we usea cross-lingual document classification task to ver-ify that the representations are semantically useful.We also inspect the embedding space qualitativelyto get some insight into the learned structure.5.1 Alignment EvaluationWe compare the alignments learned here withthose of the FASTALIGN model which producesvery good alignments and translation BLEUscores.
We use the same language pairs anddatasets as in Dyer et al (2013), that is the FBISChinese-English corpus, and the French-Englishsection of the Europarl corpus (Koehn, 2005).
Weused the preprocessing tools from CDEC and fur-ther replaced all unique tokens with UNK.
Wetrained our models with 100 dimensional repre-sentations for up to 40 iterations, and the FAmodel for 5 iterations as is the default.Table 1 shows that our model learns alignmentson part with those of the FA model.
This is in linewith expectation as our model was trained usingthe FA expectations.
However, it confirms thatthe learned word representations are able to ex-plain translation probabilities.
Surprisingly, con-text seems to have little impact on the alignmenterror, suggesting that the model receives sufficientinformation from the aligned words themselves.5.2 Document ClassificationA standard task for evaluating cross-lingual wordrepresentations is document classification wheretraining is performed in one and evaluation in an-other language.
This tasks require semanticallyplausible embeddings (for classification) whichare valid across two languages (for the semantictransfer).
Hence this task requires more of theword embeddings than the previous task.Languages ModelFA DWA DWAk = 0 k = 3ZH|EN 49.4 48.4 48.7EN|ZH 44.9 45.3 45.9FR|EN 17.1 17.2 17.0EN|FR 16.6 16.3 16.1Table 1: Alignment error rate (AER) compar-ison, in both directions, between the FASTAL-IGN (FA) alignment model and our model (DWA)with k context words (see Equation 1).
Lowernumbers indicate better performance.We mainly follow the setup of Klementiev et al(2012) and use the German-English parallel cor-pus of the European Parliament proceedings totrain the word representations.
We perform theclassification task on the Reuters RCV1/2 corpus.Unlike Klementiev et al (2012), we do not use thatcorpus during the representation learning phase.We remove all words occurring less than five timesin the data and learn 40 dimensional word embed-dings in line with prior work.To train a classifier on English data and test iton German documents we first project word rep-resentations from English into German: we selectthe most probable German word according to thelearned translation probabilities, and then computedocument representations by averaging the wordrepresentations in each document.
We use theseprojected representations for training and subse-quently test using the original German data andrepresentations.
We use an averaged perceptronclassifier as in prior work, with the number ofepochs (3) tuned on a subset of the training set.Table 2 shows baselines from previous workand classification accuracies.
Our model outper-forms the model by Klementiev et al (2012), andit also outperforms the most comparable modelsby Hermann and Blunsom (2014b) when trainingon German data and performs on par with it whentraining on English data.1It seems that our modellearns more informative representations towardsdocument classification, even without additionalmonolingual language models or context informa-tion.
Again the impact of context is inconclusive.1From Hermann and Blunsom (2014a, 2014b) we onlycompare with models equivalent with respect to embeddingdimensionality and training data.
They still achieve the stateof the art when using additional training data.227Model en?
de de?
enMajority class 46.8 46.8Glossed 65.1 68.6MT 68.1 67.4Klementiev et al 77.6 71.1BiCVM ADD 83.7 71.4BiCVM BI 83.4 69.2DWA (k = 0) 82.8 76.0DWA (k = 3) 83.1 75.4Table 2: Document classification accuracy whentrained on 1,000 training examples of the RCV1/2corpus (train?test).
Baselines are the majorityclass, glossed, and MT (Klementiev et al, 2012).Further, we are comparing to Klementiev et al(2012), BiCVM ADD (Hermann and Blunsom,2014a), and BiCVM BI (Hermann and Blunsom,2014b).
k is the context size, see Equation 1.5.3 Representation VisualizationFollowing the document classification task wewant to gain further insight into the types of fea-tures our embeddings learn.
For this we visu-alize word representations using t-SNE projec-tions (van der Maaten and Hinton, 2008).
Fig-ure 1 shows an extract from our projection of the2,000 most frequent German words, together withan expected representation of a translated Englishword given translation probabilities.
Here, it isinteresting to see that the model is able to learnrelated representations for words chair and rat-spr?asidentschaft (presidency) even though thesewords were not aligned by our model.
Figure 2shows an extract from the visualization of the10,000 most frequent English words trained on an-other corpus.
Here again, it is evident that the em-beddings are semantically plausible with similarwords being closely aligned.6 ConclusionWe presented a new probabilistic model for learn-ing bilingual word representations.
This dis-tributed word alignment model (DWA) learns bothrepresentations and alignments at the same time.We have shown that the DWA model is ableto learn alignments on par with the FASTALIGNalignment model which produces very good align-ments, thereby determining the efficacy of thelearned representations which are used to calculateFigure 1: A visualization of the expected represen-tation of the translated English word chair amongthe nearest German words: words never aligned(green), and those seen aligned (blue) with it.Figure 2: A cluster of English words from the10,000 most frequent English words visualized us-ing t-SNE.
Word representations were optimizedfor p(zh|en) (k = 0).word translation probabilities for the alignmenttask.
Subsequently, we have demonstrated thatour model can effectively be used to project doc-uments from one language to another.
The wordrepresentations our model learns as part of thealignment process are semantically plausible anduseful.
We highlighted this by applying these em-beddings to a cross-lingual document classifica-tion task where we outperform prior work, achieveresults on par with the current state of the art andprovide new state-of-the-art results on one of thetasks.
Having provided a probabilistic account ofword representations across multiple languages,future work will focus on applying this model tomachine translation and related tasks, for whichprevious approaches of learning such embeddingsare less suited.
Another avenue for further studyis to combine this method with monolingual lan-guage models, particularly in the context of se-mantic transfer into resource-poor languages.AcknowledgementsThis work was supported by a Xerox FoundationAward and EPSRC grant number EP/K036580/1.We acknowledge the use of the Oxford ARC.228ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155, February.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:parameter estimation.
Computational Linguistics,19(2):263?311, June.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: deepneural networks with multitask learning.
In Pro-ceedings of ICML.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121?2159, July.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of ACL System Demonstrations.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM model 2.
In Proceedings of NAACL-HLT.Manaal Faruqui and Chris Dyer.
2014.
Improving Vec-tor Space Word Representations Using MultilingualCorrelation.
In Proceedings of EACL.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL-HLT.Karl Moritz Hermann and Phil Blunsom.
2013.
TheRole of Syntax in Vector Space Models of Compo-sitional Semantics.
In Proceedings of ACL.Karl Moritz Hermann and Phil Blunsom.
2014a.
Mul-tilingual Distributed Representations without WordAlignment.
In Proceedings of ICLR.Karl Moritz Hermann and Phil Blunsom.
2014b.
Mul-tilingual Models for Compositional DistributionalSemantics.
In Proceedings of ACL.Karl Moritz Hermann, Dipanjan Das, Jason Weston,and Kuzman Ganchev.
2014.
Semantic Frame Iden-tification with Distributed Word Representations.
InProceedings of ACL.Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdi-nov. 2013.
Multimodal neural language models.
InNIPS Deep Learning Workshop.Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.
In Proceedings of COLING.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofthe 10th Machine Translation Summit.Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.2013.
Learning multilingual word representationsusing a bag-of-words autoencoder.
In NIPS DeepLearning Workshop.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013.Exploiting similarities among languages for ma-chine translation.
CoRR, abs/1309.4168.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of ICML.Volodymyr Mnih, Koray Kavukcuoglu, David Silver,Alex Graves, Ioannis Antonoglou, Daan Wierstra,and Martin Riedmiller.
2013.
Playing atari withdeep reinforcement learning.
In NIPS Deep Learn-ing Workshop.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InRobert G. Cowell and Zoubin Ghahramani, editors,Proceedings of the Tenth International Workshop onArtificial Intelligence and Statistics, pages 246?252.D.
E. Rumelhart, G. E. Hinton, and R. J. Williams.1986.
Learning representations by back-propagating errors.
Nature, 323:533?536, October.A P Sarath Chandar, M Khapra Mitesh, B Ravindran,Vikas Raykar, and Amrita Saha.
2013.
Multilingualdeep learning.
In Deep Learning Workshop at NIPS.Holger Schwenk, Marta R. Costa-jussa, and Jose A.R.
Fonollosa.
2007.
Smooth bilingual n-gram trans-lation.
In Proceedings of EMNLP-CoNLL.Holger Schwenk.
2012.
Continuous space translationmodels for phrase-based statistical machine transla-tion.
In Proceedings of COLING: Posters.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings ofEMNLP.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of ACL.L.J.P.
van der Maaten and G.E.
Hinton.
2008.
Visual-izing high-dimensional data using t-sne.
Journal ofMachine Learning Research, 9:2579?2605.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual Word Embed-dings for Phrase-Based Machine Translation.
InProceedings of EMNLP.229
