Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 9?16,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe Role of Lexical Resources in CJK Natural Language ProcessingJack Halpern?????
?The CJK Dictionary Institute (CJKI) (????????
)34-14, 2-chome, Tohoku, Niiza-shi, Saitama 352-0001, Japanjack@cjk.orgAbstractThe role of lexical resources is often un-derstated in NLP research.
The complex-ity of Chinese, Japanese and Korean(CJK) poses special challenges to devel-opers of NLP tools, especially in the areaof word segmentation (WS), informationretrieval (IR), named entity extraction(NER), and machine translation (MT).These difficulties are exacerbated by thelack of comprehensive lexical resources,especially for proper nouns, and the lackof a standardized orthography, especiallyin Japanese.
This paper summarizes someof the major linguistic issues in the de-velopment NLP applications that are de-pendent on lexical resources, and dis-cusses the central role such resourcesshould play in enhancing the accuracy ofNLP tools.1 IntroductionDevelopers of CJK NLP tools face various chal-lenges, some of the major ones being:1.
Identifying and processing the large number oforthographic variants in Japanese, and alternatecharacter forms in CJK languages.2.
The lack of easily available comprehensivelexical resources, especially lexical databases,comparable to the major European languages.3.
The accurate conversion between Simplifiedand Traditional Chinese (Halpern and Kerman1999).4.
The morphological complexity of Japanese andKorean.5.
Accurate word segmentation (Emerson 2000and Yu et al 2000) and disambiguating am-biguous segmentations strings (ASS) (Zhouand Yu 1994).6.
The difficulty of lexeme-based retrieval andCJK CLIR (Goto et al 2001).7.
Chinese and Japanese proper nouns, which arevery numerous, are difficult to detect without alexicon.8.
Automatic recognition of terms and their vari-ants (Jacquemin 2001).The various attempts to tackle these tasks bystatistical and algorithmic methods (Kwok 1997)have had only limited success.
An important mo-tivation for such methodology has been the pooravailability and high cost of acquiring and main-taining large-scale lexical databases.This paper discusses how a lexicon-driven ap-proach exploiting large-scale lexical databasescan offer reliable solutions to some of the princi-pal issues, based on over a decade of experiencein building such databases for NLP applications.2 Named Entity ExtractionNamed Entity Recognition (NER) is useful inNLP applications such as question answering,machine translation and information extraction.A major difficulty in NER, and a strong motiva-tion for using tools based on probabilistic meth-ods, is that the compilation and maintenance oflarge entity databases is time consuming and ex-pensive.
The number of personal names and theirvariants (e.g.
over a hundred ways to spell Mo-hammed) is probably in the billions.
The numberof place names is also large, though they are rela-tively stable compared with the names of organi-zations and products, which change frequently.A small number of organizations, includingThe CJK Dictionary Institute (CJKI), maintaindatabases of millions of proper nouns, but evensuch comprehensive databases cannot be keptfully up-to-date as countless new names are cre-ated daily.
Various techniques have been used toautomatically detect entities, one being the use ofkeywords or syntactic structures that co-occurwith proper nouns, which we refer to as namedentity contextual clues (NECC).9Table 1.
Named Entity Contextual CluesHeadword Reading Example????
????
???????????
???
???????
??
?????
?????
???????
?Table 1 shows NECCs for Japanese propernouns, which when used in conjunction with en-tity lexicons like the one shown in Table 2 belowachieve high precision in entity recognition.
Ofcourse for NER there is no need for such lexi-cons to be multilingual, though it is obviouslyessential for MT.Table 2.
Multilingual Database of Place NamesEnglish Japanese SimplifiedChineseLO TraditionalChineseKoreanAzerbaijan ????????
????
L ????
?????
?Caracas ????
????
L ????
???
?Cairo ???
??
O ??
??
?Chad ???
??
L ??
?
?New Zealand ????????
???
L ???
???
?Seoul ???
??
O ??
?
?Seoul ???
??
O ??
?
?Yemen ????
??
L ??
?
?Note how the lexemic pairs (?L?
in the LOcolumn) in Table 2 above are not merely simpli-fied and traditional orthographic (?O?)
versionsof each other, but independent lexemes equiva-lent to American truck and British lorry.NER, especially of personal names and placenames, is an area in which lexicon-driven meth-ods have a clear advantage over probabilisticmethods and in which the role of lexical re-sources should be a central one.3 Linguistic Issues in Chinese3.1 Processing Multiword UnitsA major issue for Chinese segmentors is how totreat compound words and multiword lexicalunits (MWU), which are often decomposed intotheir components rather than treated as singleunits.
For example, ???
l?xi?ngd?i 'videocassette' and ????
j?qif?ny?
'machine trans-lation' are not tagged as segments in ChineseGigaword, the largest tagged Chinese corpus inexistence, processed by the CKIP morphologicalanalyzer (Ma 2003).
Possible reasons for thisinclude:1.
The lexicons used by Chinese segmentors aresmall-scale or incomplete.
Our testing of vari-ous Chinese segmentors has shown that cover-age of MWUs is often limited.2.
Chinese linguists disagree on the concept ofwordhood in Chinese.
Various theories such asthe Lexical Integrity Hypothesis (Huang 1984)have been proposed.
Packard?s outstandingbook (Packard 98) on the subject clears upmuch of the confusion.3.
The "correct?
segmentation can depend on theapplication, and there are various segmenta-tion standards.
For example, a search engineuser looking for ???
is not normally inter-ested in ??
'to videotape' and ?
'belt' per se,unless they are part of ??
?.This last point is important enough to meritelaboration.
A user searching for ?
?
?zh?nggu?r?n 'Chinese (person)' is not interestedin ??
'China', and vice-versa.
A search for ??
should not retrieve ???
as an instance of??.
Exactly the same logic should apply to ???
?, so that a search for that keyword shouldonly retrieve documents containing that string inits entirety.
Yet performing a Google search on????
in normal mode gave some 2.3 mil-lion hits, hundreds of thousands of which hadzero occurrences of ????
but numerous10occurrences of unrelated words like ???
'ro-bot', which the user is not interested in.This is equivalent to saying that headwaitershould not be considered an instance of waiter,which is indeed how Google behaves.
More tothe point, English space-delimited lexemes likehigh school are not instances of the adjectivehigh.
As shown in Halpern (2000b), "the degreeof solidity often has nothing to do with the statusof a string as a lexeme.
School bus is just as le-gitimate a lexeme as is headwaiter or word-processor.
The presence or absence of spaces orhyphens, that is, the orthography, does not de-termine the lexemic status of a string.
"In a similar manner, it is perfectly legitimateto consider Chinese MWUs like those shownbelow as indivisible units for most applications,especially information retrieval and machinetranslation.????
s?ch?uzh?l?
silk road????
j?qif?ny?
machine translation????
?igu?zh?y?
patriotism???
l?xi?ngd?i video cassette???
X?nx?l?n New Zealand????
l?nzh?nm?qi?ngstart to prepare at the last momentOne could argue that ????
is composi-tional and therefore should be considered "twowords."
Whether we count it as one or two"words" is not really relevant ?
what matters isthat it is one lexeme (smallest distinctive unitsassociating meaning with form).
On the otherextreme, it is clear that idiomatic expressionslike ???
?, literally "sharpen one's spear be-fore going to battle," meaning 'start to prepare atthe last moment,?
are indivisible units.Predicting compositionality is not trivial andoften impossible.
For many purposes, the onlypractical solution is to consider all lexemes asindivisible.
Nonetheless, currently even the mostadvanced segmentors fail to identify such lex-emes and missegment them into their constitu-ents, no doubt because they are not registered inthe lexicon.
This is an area in which expandedlexical resources can significantly improve seg-mentation accuracy.In conclusion, lexical items like ????
'machine translation' represent stand-alone, well-defined concepts and should be treated as singleunits.
The fact that in English machineless isspelled solid and machine translation is not is anhistorical accident of orthography unrelated tothe fundamental fact that both are full-fledgedlexemes each of which represents an indivisible,independent concept.
The same logic applies to???
?,which is a full-fledged lexeme thatshould not be decomposed.3.2 Multilevel SegmentationChinese MWUs can consist of nested compo-nents that can be segmented in different waysfor different levels to satisfy the requirements ofdifferent segmentation standards.
The examplebelow shows how ???????
B?ij?ngR?b?nr?n Xu?xi?o 'Beijing School for Japanese(nationals)' can be segmented on five differentlevels.1.
???????
multiword lexemic2.
??+???+??
lexemic3.
??+??+?+??
sublexemic4.
??
+ [??
+ ?]
[?+?]
morphemic5.
[?+?]
[?+?+?]
[?+?]
submorphemicFor some applications, such as MT and NER,the multiword lexemic level is most appropriate(the level most commonly used in CJKI?s dic-tionaries).
For others, such as embedded speechtechnology where dictionary size matters, thelexemic level is best.
A more advanced and ex-pensive solution is to store presegmentedMWUs in the lexicon, or even to store nestingdelimiters as shown above, making it possible toselect the desired segmentation level.The problem of incorrect segmentation is es-pecially obvious in the case of neologisms.
Ofcourse no lexical database can expect to keep upwith the latest neologisms, and even the firstedition of Chinese Gigaword does not yet have??
b?k?
'blog'.
Here are some examples ofMWU neologisms, some of which are not (atleast bilingually), compositional but fully qual-ify as lexemes.???
di?nn?om?
cyberphile????
di?nz?sh?ngw?
e-commerce???
zhu?ch?z?
auto fan3.3 Chinese-to-Chinese Conversion (C2C)Numerous Chinese characters underwent drasticsimplifications in the postwar period.
Chinesewritten in these simplified forms is called Sim-plified Chinese (SC).
Taiwan, Hong Kong, andmost overseas Chinese continue to use the old,complex forms, referred to as Traditional Chi-nese (TC).
Contrary to popular perception, the11process of accurately converting SC to/from TCis full of complexities and pitfalls.
The linguisticissues are discussed in Halpern and Kerman(1999), while technical issues are described inLunde (1999).
The conversion can be imple-mented on three levels in increasing order ofsophistication:1.
Code Conversion.
The easiest, but most un-reliable, way to perform C2C is to transcode byusing a one-to-one mapping table.
Because ofthe numerous one-to-many ambiguities, asshown below, the rate of conversion failure isunacceptably high.Table 3.
Code ConversionSC TC1 TC2 TC3 TC4 Remarks?
?
one-to-one?
?
one-to-one?
?
?
one-to-many?
?
?
one-to-many?
?
?
?
?
one-to-many2.
Orthographic Conversion.
The next level ofsophistication is to convert orthographic units,rather than codepoints.
That is,  meaningful lin-guistic units, equivalent to lexemes, with theimportant difference that the TC is the tradi-tional version of the SC on a character formlevel.
While code conversion is ambiguous, or-thographic conversion gives much better resultsbecause the orthographic mapping tables enableconversion on the lexeme level, as shown below.Table 4.
Orthographic ConversionEnglish SC TC1 TC2 IncorrectTelephone ??
?
?Dry ??
??
??
??
????
??
?
?As can be seen, the ambiguities inherent incode conversion are resolved by using ortho-graphic mapping tables, which avoids false con-versions such as shown in the Incorrect column.Because of segmentation ambiguities, such con-version must be done with a segmentor that canbreak the text stream into meaningful units (Em-erson 2000).An extra complication, among various others,is that some lexemes have one-to-many ortho-graphic mappings, all of which are correct.
Forexample, SC ??
correctly maps to both TC ??
'dry in the shade' and TC ??
'the five evennumbers'.
Well designed orthographic mappingtables must take such anomalies into account.3.
Lexemic Conversion.
The most sophisti-cated form of C2C conversion is called lexemicconversion, which maps SC and TC lexemesthat are semantically, not orthographically,equivalent.
For example, SC ??
x?nx?
'infor-mation' is converted into the semanticallyequivalent TC ??
z?x?n.
This is similar to thedifference between British pavement andAmerican sidewalk.
Tsou (2000) has demon-strated that there are numerous lexemic differ-ences between SC and TC, especially in techni-cal terms and proper nouns, e.g.
there are morethan 10 variants for Osama bin Laden.Table 5.
Lexemic ConversionEnglish SC Taiwan TC HK TC IncorrectTCSoftware ??
??
??
?
?Taxi ????
???
??
????OsamaBinLaden???????????????????????
?Oahu ???
???
??
?3.4 Traditional Chinese VariantsTraditional Chinese has numerous variant char-acter forms, leading to much confusion.
Disam-biguating these variants can be done by usingmapping tables such as the one shown below.
Ifsuch a table is carefully constructed by limitingit to cases of 100% semantic interchangeabilityfor polysemes, it is easy to normalize a TC textby trivially replacing variants by their standard-ized forms.
For this to work, all relevant compo-nents, such as MT dictionaries, search engineindexes and the related documents should benormalized.
An extra complication is that Tai-wanese and Hong Kong variants are sometimesdifferent (Tsou 2000).Table 6.
TC VariantsVar.
1Var.
2 English Comment?
?
Inside 100% interchangeable?
?
Particle variant 2 not in Big5?
?
sink; surname partially interchangeable124 Orthographic Variation in Japanese4.1 Highly Irregular OrthographyThe Japanese orthography is highly irregular,significantly more so than any other major lan-guage, including Chinese.
A major factor is thecomplex interaction of the four scripts used towrite Japanese, e.g.
kanji, hiragana, katakana,and the Latin alphabet, resulting in countlesswords that can be written in a variety of oftenunpredictable ways, and the lack of a standard-ized orthography.
For example, toriatsukai 'han-dling' can be written in six ways: ???
?, ??
?, ?
?, ???
?, ?????
?, ?????
?.An example of how difficult Japanese IR canbe is the proverbial 'A hen that lays golden eggs.
'The "standard" orthography would be ???????
Kin no tamago o umu niwatori.
In real-ity, tamago 'egg' has four variants (?, ?
?, ??
?, ???
), niwatori 'chicken' three (?, ???
?, ????)
and umu 'to lay' two (??,??
), which expands to 24 permutations like ?????????
?, ????????
etc.As can be easily verified by searching the web,these variants occur frequently.Linguistic tools that perform segmentation,MT, entity extraction and the like must identifyand/or normalize such variants to perform dic-tionary lookup.
Below is a brief discussion ofwhat kind of variation occurs and how suchnormalization can be achieved.4.2 Okurigana VariantsOne of the most common types of orthographicvariation in Japanese occurs in kana endings,called okurigana, that are attached to a kanjistem.
For example, okonau 'perform' can bewritten ??
or ??
?, whereas toriatsukai canbe written in the six ways shown above.
Okuri-gana variants are numerous and unpredictable.Identifying them must play a major role in Japa-nese orthographic normalization.
Although it ispossible to create a dictionary of okurigana vari-ants algorithmically, the resulting lexicon wouldbe huge and may create numerous false positivesnot semantically interchangeable.
The most ef-fective solution is to use a lexicon of okuriganavariants, such as the one shown below:Table 7.
Okurigana VariantsHEADWORD READING NORMALIZED????
??????
?????????
??????
???????
??????
????????
??????
???
?Since Japanese is highly agglutinative andverbs can have numerous inflected forms, a lexi-con such as the above must be used in conjunc-tion with a morphological analyzer that can doaccurate stemming, i.e.
be capable of recogniz-ing that ??????????
is the politeform of the canonical form ???
?.4.3 Cross-Script Orthographic VariationVariation across the four scripts in Japanese iscommon and unpredictable, so that the sameword can be written in any of several scripts, oreven as a hybrid of multiple scripts, as shownbelow:Table 8.
Cross-Script VariationKanji Hiragana katakana Latin Hybrid Gloss??
????
????
carrot????
OPEN  open??
???
sulfur?????
Y???
shirt??
??
??
skinCross-script variation can have major conse-quences for recall, as can be seen from the tablebelow.Table 9: Hit Distribution for ??
'carrot' ninjinID Keyword Normal-izedGoogleHitsA  ??
??
67,500B  ????
??
66,200C  ????
??
58,000Using the ID above to represent the number ofGoogle hits, this gives a total of A?B?C?
?123= 191,700.  ?
is a coincidental occurrence factor,such as in  '100??
?, in which '??'
is unre-lated to the 'carrot' sense.
The formulae for cal-culating the above are as follows.13Unnormalized recall:123?+++ CBAC?58?000191?700 (?30%)Normalized recall:123?+++++CBACBA?191?700191?700 (?100?
?Unnormalized precision:3?+CC?58?00058?000 (?100?
?Normalized precision:123?+++ CBAC?191?700191?700 (?100????
'carrot' illustrates how serious a problemcross-orthographic variants can be.
If ortho-graphic normalization is not implemented to en-sure that all variants are indexed on a standard-ized form like ?
?, recall is only 30%; if it is,there is a dramatic improvement and recall goesup to nearly 100%, without any loss in precision,which hovers at 100%.4.4  Kana VariantsA sharp increase in the use of katakana in re-cent years is a major annoyance to NLP applica-tions because katakana orthography is often ir-regular; it is quite common for the same word tobe written in multiple, unpredictable ways.
Al-though hiragana orthography is generally regular,a small number of irregularities persist.
Some ofthe major types of kana variation are shown inthe table below.Table 10.
Kana VariantsType English Standard VariantsMacron computer ??????
??????
?Long vowels maid ???
??
?Multiple kana team ???
???
?Traditional big ????
?????
vs. ?
continue ???
??
?The above is only a brief introduction to themost important types of kana variation.
Thoughattempts at algorithmic solutions have beenmade by some NLP research laboratories (Brill2001), the most practical solution is to use a ka-takana normalization table, such as the oneshown below, as is being done by Yahoo!
Japanand other major portals.Table 11.
Kana VariantsHEADWORD NORMALIZED English???????
????????
Architecture????????
????????
Architecture????????
????????
Architecture4.5 Miscellaneous VariantsThere are various other types of orthographicvariants in Japanese, described in Halpern(2000a).
To mention some, kanji even in con-temporary Japanese sometimes have variants,such as ?
for ?
and ?
for ?, and traditionalforms such as ?
for ?.
In addition, many kunhomophones and their variable orthography areoften close or even identical in meaning, i.e.,noboru means 'go up' when written ??
but'climb' when written ??
, so that great caremust be taken in the normalization process so asto assure semantic interchangeability for allsenses of polysemes; that is, to ensure that suchforms are excluded from the normalization table.4.6  Lexicon-driven NormalizationLeaving statistical methods aside, lexicon-driven normalization of Japanese orthographicvariants can be achieved by using an ortho-graphic mapping table such as the one shownbelow, using various techniques such as:1.
Convert variants to a standardized form forindexing.2.
Normalize queries for dictionary lookup.3.
Normalize all source documents.4.
Identify forms as members of a variant group.Table 12.
Orthographic Normalization TableHEADWORD READING NORMALIZED???
????
?????
????
??????
????
??????
????
??????
????
???????
????
???????
????
??????
????
?????
????
??????
????
?????
????
??
?14Other possibilities for normalization includeadvanced applications such as domain-specificsynonym expansion, requiring Japanese thesauribased on domain ontologies, as is done by a se-lect number of companies like Wand and Con-vera who build sophisticated Japanese IR sys-tems.5 Orthographic Variation in KoreanModern Korean has is a significant amount oforthographic variation, though far less than inJapanese.
Combined with the morphologicalcomplexity of the language, this poses variouschallenges to developers of NLP tools.
The is-sues are similar to Japanese in principle but dif-fer in detail.Briefly, Korean has variant hangul spellingsin the writing of loanwords, such as ??
?keikeu and ??
keik for 'cake', and in the writ-ing of non-Korean personal names, such as???
keulrinteon and ???
keulrinton for'Clinton'.
In addition, similar to Japanese but ona smaller scale, Korean is written in a mixture ofhangul, Chinese characters and the Latin alpha-bet.
For example, 'shirt' can be written ???
?wai-syeacheu or Y??
wai-syeacheu, whereas'one o'clock' hanzi can written as ?
?, 1?
or??.
Another issue is the differences betweenSouth and North Korea spellings, such as N.K.???
osakka vs. S.K.
???
osaka for'Osaka', and the old (pre-1988) orthography ver-sus the new, i.e.
modern ??
'worker' (ilgun)used to be written ??
(ilkkun).Lexical databases, such as normalization ta-bles similar to the ones shown above for Japa-nese, are the only practical solution to identify-ing such variants, as they are in principle unpre-dictable.6 The Role of Lexical DatabasesBecause of the irregular orthography of CJKlanguages, procedures such as orthographicnormalization cannot be based on statistical andprobabilistic methods (e.g.
bigramming) alone,not to speak of pure algorithmic methods.
Manyattempts have been made along these lines, asfor example Brill (2001) and Goto et al (2001),with some claiming performance equivalent tolexicon-driven methods, while Kwok (1997)reports good results with only a small lexiconand simple segmentor.Emerson (2000) and others have reported thata robust morphological analyzer capable ofprocessing lexemes, rather than bigrams or n-grams, must be supported by a large-scale com-putational lexicon.
This experience is shared bymany of the world's major portals and MT de-velopers, who make extensive use of lexical da-tabases.Unlike in the past, disk storage is no longer amajor issue.
Many researchers and developers,such as Prof. Franz Guenthner of the Universityof Munich, have come to realize that ?languageis in the data,?
and ?the data is in the diction-ary,?
even to the point of compiling full-formdictionaries with millions of entries rather thanrely on statistical methods, such as MeaningfulMachines who use a full form dictionary con-taining millions of entries in developing a hu-man quality Spanish-to-English MT system.CJKI, which specializes in CJK and Arabiccomputational lexicography, is engaged in anongoing research and development effort tocompile CJK and Arabic lexical databases (cur-rently about seven million entries), with specialemphasis on proper nouns, orthographic nor-malization, and C2C.
These resources are beingsubjected to heavy industrial use under real-world conditions, and the feedback thereof isbeing used to further expand these databases andto enhance the effectiveness of the NLP toolsbased on them.7 ConclusionsPerforming such tasks as orthographic normali-zation and named entity extraction accurately isbeyond the ability of statistical methods alone,not to speak of C2C conversion and morpho-logical analysis.
However, the small-scale lexi-cal resources currently used by many NLP toolsare inadequate to these tasks.
Because of the ir-regular orthography of the CJK writing systems,lexical databases fine-tuned to the needs of NLPapplications are required.
The building oflarge-scale lexicons based on corpora consistingof even billions of words has come of age.
Sincelexicon-driven techniques have proven their ef-fectiveness, there is no need to overly rely onprobabilistic methods.
Comprehensive, up-to-date lexical resources are the key to achievingmajor enhancements in NLP technology.15ReferencesBrill, E. and Kacmarick, G. and Brocket, C. (2001)Automatically Harvesting Katakana-English TermPairs from Search Engine Query Logs.
MicrosoftResearch, Proc.
of the Sixth Natural LanguageProcessing Pacific Rim Symposium, Tokyo, Japan.Packard, L. Jerome (1998) ?New Approaches toChinese Word Formation?, Mouton Degruyter,Berlin and New York.Emerson, T. (2000) Segmenting Chinese in Unicode.Proc.
of the 16th International Unicode Confer-ence, AmsterdamGoto, I., Uratani, N. and Ehara T. (2001) Cross-Language Information Retrieval of Proper Nounsusing Context Information.
NHK Science andTechnical Research Laboratories.
Proc.
of theSixth Natural Language Processing Pacific RimSymposium, Tokyo, JapanHuang, James C. (1984) Phrase Structure, LexicalIntegrity, and Chinese Compounds, Journal of theChinese Teachers Language Association, 19.2: 53-78Jacquemin, C. (2001) Spotting and DiscoveringTerms through Natural Language Processing.
TheMIT Press, Cambridge, MAHalpern, J. and Kerman J.
(1999) The Pitfalls andComplexities of Chinese to Chinese Conversion.Proc.
of the Fourteenth International Unicode Con-ference in Cambridge, MA.Halpern, J.
(2000a) The Challenges of IntelligentJapanese Searching.
Working paper(www.cjk.org/cjk/joa/joapaper.htm), The CJKDictionary Institute, Saitama, Japan.Halpern, J.
(2000b) Is English Segmentation Trivial?.Working paper,(www.cjk.org/cjk/reference/engmorph.htm) TheCJK Dictionary Institute, Saitama, Japan.Kwok, K.L.
(1997) Lexicon Effects on Chinese In-formation Retrieval.
Proc.
of 2nd Conf.
on Em-pirical Methods in NLP.
ACL.
pp.141-8.Lunde, Ken (1999) CJKV Information Processing.O?Reilly & Associates, Sebastopol, CA.Yu, Shiwen, Zhu, Xue-feng and Wang, Hui (2000)New Progress of the Grammatical Knowledge-base of Contemporary Chinese.
Journal of ChineseInformation Processing, Institute of ComputationalLinguistics, Peking University, Vol.15 No.1.Ma, Wei-yun and Chen, Keh-Jiann (2003) Introduc-tion to CKIP Chinese Word Segmentation Systemfor the First International Chinese Word Segmen-tation Bakeoff, Proceedings of the SecondSIGHAN Workshop on Chinese Language Proc-essingpp.
168-171 Sapporo, JapanYu, Shiwen, Zhu, Xue-feng and Wang, Hui (2000)New Progress of the Grammatical Knowledge-base of Contemporary Chinese.
Journal of ChineseInformation Processing, Institute of ComputationalLinguistics, Peking University, Vol.15 No.1.Tsou, B.K., Tsoi, W.F., Lai, T.B.Y.
Hu, J., and ChanS.W.K.
(2000) LIVAC, a Chinese synchronouscorpus, and some applications.
In "2000 Interna-tional Conference on Chinese Language Comput-ingICCLC2000", Chicago.Zhou, Qiang.
and  Yu, Shiwen (1994) Blending Seg-mentation with Tagging in Chinese LanguageCorpus Processing, 15th International Conferenceon Computational Linguistics (COLING 1994)16
