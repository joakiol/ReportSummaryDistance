Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 232?241,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsReducing Approximation and Estimation Errors for Chinese LexicalProcessing with Heterogeneous AnnotationsWeiwei Sun?
and Xiaojun Wan?
??
?Institute of Computer Science and Technology, Peking University?Saarbru?cken Graduate School of Computer Science?Department of Computational Linguistics, Saarland University?Language Technology Lab, DFKI GmbH{ws,wanxiaojun}@pku.edu.cnAbstractWe address the issue of consuming heteroge-neous annotation data for Chinese word seg-mentation and part-of-speech tagging.
We em-pirically analyze the diversity between tworepresentative corpora, i.e.
Penn ChineseTreebank (CTB) and PKU?s People?s Daily(PPD), on manually mapped data, and showthat their linguistic annotations are systemat-ically different and highly compatible.
Theanalysis is further exploited to improve pro-cessing accuracy by (1) integrating systemsthat are respectively trained on heterogeneousannotations to reduce the approximation error,and (2) re-training models with high qualityautomatically converted data to reduce the es-timation error.
Evaluation on the CTB andPPD data shows that our novel model achievesa relative error reduction of 11% over the bestreported result in the literature.1 IntroductionA majority of data-driven NLP systems rely onlarge-scale, manually annotated corpora that are im-portant to train statistical models but very expensiveto build.
Nowadays, for many tasks, multiple het-erogeneous annotated corpora have been built andpublicly available.
For example, the Penn Treebankis popular to train PCFG-based parsers, while theRedwoods Treebank is well known for HPSG re-search; the Propbank is favored to build general se-mantic role labeling systems, while the FrameNet isattractive for predicate-specific labeling.
The anno-?This work is mainly finished when the first author wasin Saarland University and DFKI.
Both authors are the corre-sponding authors.tation schemes in different projects are usually dif-ferent, since the underlying linguistic theories varyand have different ways to explain the same lan-guage phenomena.
Though statistical NLP systemsusually are not bound to specific annotation stan-dards, almost all of them assume homogeneous an-notation in the training corpus.
The co-existence ofheterogeneous annotation data therefore presents anew challenge to the consumers of such resources.There are two essential characteristics of hetero-geneous annotations that can be utilized to reducetwo main types of errors in statistical NLP, i.e.
theapproximation error that is due to the intrinsic sub-optimality of a model and the estimation error that isdue to having only finite training data.
First, hetero-geneous annotations are (similar but) different as aresult of different annotation schemata.
Systems re-spectively trained on heterogeneous annotation datacan produce different but relevant linguistic analy-sis.
This suggests that complementary features fromheterogeneous analysis can be derived for disam-biguation, and therefore the approximation error canbe reduced.
Second, heterogeneous annotations are(different but) similar because their linguistic analy-sis is highly correlated.
This implies that appropriateconversions between heterogeneous corpora couldbe reasonably accurate, and therefore the estimationerror can be reduced by reason of the increase of re-liable training data.This paper explores heterogeneous annotationsto reduce both approximation and estimation errorsfor Chinese word segmentation and part-of-speech(POS) tagging, which are fundamental steps formore advanced Chinese language processing tasks.We empirically analyze the diversity between tworepresentative popular heterogeneous corpora, i.e.232Penn Chinese Treebank (CTB) and PKU?s People?sDaily (PPD).
To that end, we manually label 200sentences from CTB with PPD-style annotations.1Our analysis confirms the aforementioned two prop-erties of heterogeneous annotations.
Inspired bythe sub-word tagging method introduced in (Sun,2011), we propose a structure-based stacking modelto fully utilize heterogeneous word structures to re-duce the approximation error.
In particular, jointword segmentation and POS tagging is addressedas a two step process.
First, character-based tag-gers are respectively trained on heterogeneous an-notations to produce multiple analysis.
The outputsof these taggers are then merged into sub-word se-quences, which are further re-segmented and taggedby a sub-word tagger.
The sub-word tagger is de-signed to refine the tagging result with the help ofheterogeneous annotations.
To reduce the estima-tion error, we employ a learning-based approach toconvert complementary heterogeneous data to in-crease labeled training data for the target task.
Boththe character-based tagger and the sub-word taggercan be refined by re-training with automatically con-verted data.We conduct experiments on the CTB and PPDdata, and compare our system with state-of-the-art systems.
Our structure-based stacking modelachieves an f-score of 94.36, which is superior toa feature-based stacking model introduced in (Jianget al, 2009).
The converted data can also enhancethe baseline model.
A simple character-based modelcan be improved from 93.41 to 94.11.
Since thetwo treatments are concerned with reducing differ-ent types of errors and thus not fully overlapping, thecombination of them gives a further improvement.Our final system achieves an f-score of 94.68, whichyields a relative error reduction of 11% over the bestpublished result (94.02).2 Joint Chinese Word Segmentation andPOS TaggingDifferent from English and other Western languages,Chinese is written without explicit word delimiterssuch as space characters.
To find and classify the1The first 200 sentences of the development data for experi-ments are selected.
This data set is submitted as a supplementalmaterial for research purposes.basic language units, i.e.
words, word segmentationand POS tagging are important initial steps for Chi-nese language processing.
Supervised learning withspecifically defined training data has become a dom-inant paradigm.
Joint approaches that resolve thetwo tasks simultaneously have received much atten-tion in recent research.
Previous work has shownthat joint solutions led to accuracy improvementsover pipelined systems by avoiding segmentation er-ror propagation and exploiting POS information tohelp segmentation (Ng and Low, 2004; Jiang et al,2008a; Zhang and Clark, 2008; Sun, 2011).Two kinds of approaches are popular for jointword segmentation and POS tagging.
The first is the?character-based?
approach, where basic processingunits are characters which compose words (Jiang etal., 2008a).
In this kind of approach, the task is for-mulated as the classification of characters into POStags with boundary information.
For example, thelabel B-NN indicates that a character is located at thebegging of a noun.
Using this method, POS infor-mation is allowed to interact with segmentation.
Thesecond kind of solution is the ?word-based?
method,also known as semi-Markov tagging (Zhang andClark, 2008; Zhang and Clark, 2010), where the ba-sic predicting units are words themselves.
This kindof solver sequentially decides whether the local se-quence of characters makes up a word as well as itspossible POS tag.
Solvers may use previously pre-dicted words and their POS information as clues toprocess a new word.In addition, we proposed an effective and efficientstacked sub-word tagging model, which combinesstrengths of both character-based and word-basedapproaches (Sun, 2011).
First, different character-based and word-based models are trained to producemultiple segmentation and tagging results.
Sec-ond, the outputs of these coarse-grained models aremerged into sub-word sequences, which are fur-ther bracketed and labeled with POS tags by a fine-grained sub-word tagger.
Their solution can beviewed as utilizing stacked learning to integrate het-erogeneous models.Supervised segmentation and tagging can be im-proved by exploiting rich linguistic resources.
Jianget al (2009) presented a preliminary study for an-notation ensemble, which motivates our research aswell as similar investigations for other NLP tasks,233e.g.
parsing (Niu et al, 2009; Sun et al, 2010).
Intheir solution, heterogeneous data is used to train anauxiliary segmentation and tagging system to pro-duce informative features for target prediction.
Ourprevious work (Sun and Xu, 2011) and Wang et al(2011) explored unlabeled data to enhance strongsupervised segmenters and taggers.
Both of theirwork fall into the category of feature induction basedsemi-supervised learning.
In brief, their methodsharvest useful string knowledge from unlabeled orautomatically analyzed data, and apply the knowl-edge to design new features for discriminative learn-ing.3 About Heterogeneous AnnotationsFor Chinese word segmentation and POS tag-ging, supervised learning has become a dominantparadigm.
Much of the progress is due to the devel-opment of both corpora and machine learning tech-niques.
Although several institutions to date havereleased their segmented and POS tagged data, ac-quiring sufficient quantities of high quality trainingexamples is still a major bottleneck.
The annotationschemes of existing lexical resources are different,since the underlying linguistic theories vary.
Despitethe existence of multiple resources, such data cannotbe simply put together for training systems, becausealmost all of statistical NLP systems assume homo-geneous annotation.
Therefore, it is not only inter-esting but also important to study how to fully utilizeheterogeneous resources to improve Chinese lexicalprocessing.There are two main types of errors in statisticalNLP: (1) the approximation error that is due to theintrinsic suboptimality of a model and (2) the esti-mation error that is due to having only finite train-ing data.
Take Chinese word segmentation for ex-ample.
Our previous analysis (Sun, 2010) showsthat one main intrinsic disadvantage of character-based model is the difficulty in incorporating thewhole word information, while one main disadvan-tage of word-based model is the weak ability to ex-press word formation.
In both models, the signifi-cant decrease of the prediction accuracy of out-of-vocabulary (OOV) words indicates the impact of theestimation error.
The two essential characteristicsabout systematic diversity of heterogeneous annota-tions can be utilized to reduce both approximationand estimation errors.3.1 Analysis of the CTB and PPD StandardsThis paper focuses on two representative popularcorpora for Chinese lexical processing: (1) the PennChinese Treebank (CTB) and (2) the PKU?s Peo-ple?s Daily data (PPD).
To analyze the diversity be-tween their annotation standards, we pick up 200sentences from CTB and manually label them ac-cording to the PPD standard.
Specially, we employ aPPD-style segmentation and tagging system to auto-matically label these 200 sentences.
A linguistic ex-pert who deeply understands the PPD standard thenmanually checks the automatic analysis and correctsits errors.These 200 sentences are segmented as 3886 and3882 words respectively according to the CTB andPPD standards.
The average lengths of word tokensare almost the same.
However, the word bound-aries or the definitions of words are different.
3561word tokens are consistently segmented by bothstandards.
In other words, 91.7% CTB word tokensshare the same word boundaries with 91.6% PPDword tokens.
Among these 3561 words, there are552 punctuations that are simply consistently seg-mented.
If punctuations are filtered out to avoidoverestimation of consistency, 90.4% CTB wordshave same boundaries with 90.3% PPD words.
Theboundaries of words that are differently segmentedare compatible.
Among all annotations, only onecross-bracketing occurs.
The statistics indicates thatthe two heterogenous segmented corpora are sys-tematically different, and confirms the aforemen-tioned two properties of heterogeneous annotations.Table 1 is the mapping between CTB-style tagsand PPD-style tags.
For the definition and illus-tration of these tags, please refers to the annotationguidelines2.
The statistics after colons are how manytimes this POS tag pair appears among the 3561words that are consistently segmented.
From this ta-ble, we can see that (1) there is no one-to-one map-ping between their heterogeneous word classifica-tion but (2) the mapping between heterogeneous tagsis not very uncertain.
This simple analysis indicates2Available at http://www.cis.upenn.edu/?chinese/posguide.3rd.ch.pdf and http://www.icl.pku.edu.cn/icl_groups/corpus/spec.htm.234that the two POS tagged corpora also hold the twoproperties of heterogeneous annotations.
The dif-ferences between the POS annotation standards aresystematic.
The annotations in CTB are treebank-driven, and thus consider more functional (dynamic)information of basic lexical categories.
The annota-tions in PPD are lexicon-driven, and thus focus onmore static properties of words.
Limited to the doc-ument length, we only illustrate the annotation ofverbs and nouns for better understanding of the dif-ferences.?
The CTB tag VV indicates common verbs thatare mainly labeled as verbs (v) too accordingto the PPD standard.
However, these words canbe also tagged as nominal categories (a, vn, n).The main reason is that there are a large num-ber of Chinese adjectives and nouns that can berealized as predicates without linking verbs.?
The tag NN indicates common nouns in CTB.Some of them are labeled as verbal categories(vn, v).
The main reason is that a majority ofChinese verbs could be realized as subjects andobjects without form changes.4 Structure-based Stacking4.1 Reducing the Approximation Error viaStackingEach annotation data set alne can yield a predictorthat can be taken as a mechanism to produce struc-tured texts.
With different training data, we can con-struct multiple heterogeneous systems.
These sys-tems produce similar linguistic analysis which holdsthe same high level linguistic principles but differ indetails.
A very simple idea to take advantage of het-erogeneous structures is to design a predictor whichcan predict a more accurate target structure basedon the input, the less accurate target structure andcomplementary structures.
This idea is very closeto stacked learning (Wolpert, 1992), which is welldeveloped for ensemble learning, and successfullyapplied to some NLP tasks, e.g.
dependency parsing(Nivre and McDonald, 2008; Torres Martins et al,2008).Formally speaking, our idea is to include two?levels?
of processing.
The first level includes oneAS?
u:44; CD?
m:134;DEC?
u:83; DEV?
u:7;DEG?
u:123; ETC?
u:9;LB?
p:1; NT?
t:98;OD?
m:41; PU?
w:552;SP?
u:1; VC?
v:32;VE?
v:13; BA?
p:2; d:1;CS?
c:3; d:1; DT?
r:15; b:1;MSP?
c:2; u:1; PN?
r:53; n:2;CC?
c:73; p:5; v:2; M?
q:101; n:11; v:1;LC?
f:51; Ng:3; v:1; u:1; P?
p:133; v:4; c:2; Vg:1;VA ?
a:57; i:4; z:2; ad:1;b:1;NR ?
ns:170; nr:65; j:23;nt:21; nz:7; n:2; s:1;VV ?
v:382; i:5; a:3; Vg:2;vn:2; n:2; p:2; w:1;JJ ?
a:43; b:13; n:3; vn:3;d:2; j:2; f:2; t:2; z:1;AD?
d:149; c:11; ad:6; z:4;a:3; v:2; n:1; r:1; m:1; f:1;t:1;NN ?
n:738; vn:135; v:26;j:19; Ng:5; an:5; a:3; r:3; s:3;Ag:2; nt:2; f:2; q:2; i:1; t:1;nz:1; b:1;Table 1: Mapping between CTB and PPD POS Tags.or more base predictors f1, ..., fK that are indepen-dently built on different training data.
The secondlevel processing consists of an inference function hthat takes as input ?x, f1(x), ..., fK(x)?3 and out-puts a final prediction h(x, f1(x), ..., fK(x)).
Theonly difference between model ensemble and anno-tation ensemble is that the output spaces of modelensemble are the same while the output spaces of an-notation ensemble are different.
This framework isgeneral and flexible, in the sense that it assumes al-most nothing about the individual systems and takethem as black boxes.4.2 A Character-based TaggerWith IOB2 representation (Ramshaw and Marcus,1995), the problem of joint segmentation and tag-ging can be regarded as a character classificationtask.
Previous work shows that the character-basedapproach is an effective method for Chinese lexicalprocessing.
Both of our feature- and structure-basedstacking models employ base character-based tag-gers to generate multiple segmentation and taggingresults.
Our base tagger use a discriminative sequen-tial classifier to predict the POS tag with positionalinformation for each character.
Each character canbe assigned one of two possible boundary tags: ?B?for a character that begins a word and ?I?
for a char-acter that occurs in the middle of a word.
We denote3x is a given Chinese sentence.235a candidate character token ci with a fixed windowci?2ci?1cici+1ci+2.
The following features are usedfor classification:?
Character unigrams: ck (i?
l ?
k ?
i+ l)?
Character bigrams: ckck+1 (i?
l ?
k < i+ l)4.3 Feature-based StackingJiang et al (2009) introduced a feature-based stack-ing solution for annotation ensemble.
In their so-lution, an auxiliary tagger CTagppd is trained on acomplementary corpus, i.e.
PPD, to assist the tar-get CTB-style tagging.
To refine the character-basedtagger CTagctb, PPD-style character labels are di-rectly incorporated as new features.
The stackingmodel relies on the ability of discriminative learningmethod to explore informative features, which playcentral role to boost the tagging performance.
Tocompare their feature-based stacking model and ourstructure-based model, we implement a similar sys-tem CTagppd?ctb.
Apart from character uni/bigramfeatures, the PPD-style character labels are used toderive the following features to enhance our CTB-style tagger:?
Character label unigrams: cppdk (i?lppd ?
k ?i+ lppd)?
Character label bigrams: cppdk cppdk+1 (i?
lppd ?k < i+ lppd)In the above descriptions, l and lppd are the win-dow sizes of features, which can be tuned on devel-opment data.4.4 Structure-based StackingWe propose a novel structured-based stacking modelfor the task, in which heterogeneous word struc-tures are used not only to generate features but alsoto derive a sub-word structure.
Our work is in-spired by the stacked sub-word tagging model in-troduced in (Sun, 2011).
Their work is motivatedby the diversity of heterogeneous models, whileour work is motivated by the diversity of heteroge-neous annotations.
The workflow of our new sys-tem is shown in Figure 1.
In the first phase, onecharacter-based CTB-style tagger (CTagctb) andone character-based PPD-style tagger (CTagppd)are respectively trained to produce heterogenousRaw sentencesCTB-style charactertagger CTagctbPPD-style charactertagger CTagppdSegmented andtagged sentencesSegmented andtagged sentencesMergingSub-wordsequencesCTB-stylesub-word tag-ger STagctbFigure 1: Sub-word tagging based on heterogeneous tag-gers.word boundaries.
In the second phase, this systemfirst combines the two segmentation and tagging re-sults to get sub-words which maximize the agree-ment about word boundaries.
Finally, a fine-grainedsub-word tagger (STagctb) is applied to bracket sub-words into words and also to label their POS tags.We can also apply a PPD-style sub-word tagger.
Tocompare with previous work, we specially concen-trate on the PPD-to-CTB adaptation.Following (Sun, 2011), the intermediate sub-wordstructures is defined to maximize the agreement ofCTagctb and CTagppd.
In other words, the goal isto make merged sub-words as large as possible butnot overlap with any predicted word produced bythe two taggers.
If the position between two con-tinuous characters is predicted as a word boundaryby any segmenter, this position is taken as a separa-tion position of the sub-word sequence.
This strat-egy makes sure that it is still possible to correctlyre-segment the strings of which the boundaries aredisagreed with by the heterogeneous segmenters inthe sub-word tagging stage.To train the sub-word tagger STagctb, featuresare formed making use of both CTB-style and PPD-style POS tags provided by the character-based tag-gers.
In the following description, ?C?
refers to thecontent of a sub-word; ?Tctb?
and ?Tppd?
refers tothe positional POS tags generated from CTagctb andCTagppd; lC , lctbT and lppdT are the window sizes.For convenience, we denote a sub-word with its con-236text ...si?1sisi+1..., where si is the current token.The following features are applied:?
Unigram features: C(sk) (i ?
lC ?
k ?
+lC),Tctb(sk) (i ?
lctbT ?
k ?
i + lctbT ), Tppd(sk)(i?
lppdT ?
k ?
i+ lppdT )?
Bigram features: C(sk)C(sk+1) (i ?
lC ?
k <i + lC), Tctb(sk)Tctb(sk+1) (i ?
lctbT ?
k <i+ lctbT ), Tppd(sk)Tppd(sk+1) (i?
lppdT ?
k <i+ lppdT )?
C(si?1)C(si+1) (if lC ?
1),Tctb(si?1)Tctb(si+1) (if lctbT ?
1),Tppd(si?1)Tppd(si+1) (if lppdT ?
1)?
Word formation features: character n-gramprefixes and suffixes for n up to 3.Cross-validation CTagctb and CTagppd are di-rectly trained on the original training data, i.e.
theCTB and PPD data.
Cross-validation technique hasbeen proved necessary to generate the training datafor sub-word tagging, since it deals with the train-ing/test mismatch problem (Sun, 2011).
To con-struct training data for the new heterogeneous sub-word tagger, a 10-fold cross-validation on the origi-nal CTB data is performed too.5 Data-driven Annotation ConversionIt is possible to acquire high quality labeled datafor a specific annotation standard by exploring ex-isting heterogeneous corpora, since the annotationsare normally highly compatible.
Moreover, the ex-ploitation of additional (pseudo) labeled data aims toreduce the estimation error and enhances a NLP sys-tem in a different way from stacking.
We thereforeexpect the improvements are not much overlappingand the combination of them can give a further im-provement.The stacking models can be viewed as annota-tion converters: They take as input complementarystructures and produce as output target structures.In other words, the stacking models actually learnstatistical models to transform the lexical represen-tations.
We can acquire informative extra samplesby processing the PPD data with our stacking mod-els.
Though the converted annotations are imperfect,they are still helpful to reduce the estimation error.Character-based Conversion The feature-basedstacking model CTagppd?ctb maps the input char-acter sequence c and its PPD-style character labelsequence to the corresponding CTB-style characterlabel sequence.
This model by itself can be taken asa corpus conversion model to transform a PPD-styleanalysis to a CTB-style analysis.
By processing theauxiliary corpus Dppd with CTagppd?ctb, we ac-quire a new labeled data set D?ctb = DCTagppd?ctbppd?ctb .We can re-train the CTagctb model with both origi-nal and converted data Dctb ?D?ctb.Sub-word-based Conversion Similarly, thestructure-based stacking model can be also takenas a corpus conversion model.
By processing theauxiliary corpus Dppd with STagctb, we acquirea new labeled data set D?
?ctb = DSTagctbppd?ctb.
We canre-train the STagctb model with Dctb ?
D??ctb.
Ifwe use the gold PPD-style labels of D?
?ctb to extractsub-words, the new model will overfit to the goldPPD-style labels, which are unavailable at test time.To avoid this training/test mismatch problem, wealso employ a 10-fold cross validation procedure toadd noise.It is not a new topic to convert corpus from oneformalism to another.
A well known work is trans-forming Penn Treebank into resources for variousdeep linguistic processing, including LTAG (Xia,1999), CCG (Hockenmaier and Steedman, 2007),HPSG (Miyao et al, 2004) and LFG (Cahill et al,2002).
Such work for corpus conversion mainlyleverages rich sets of hand-crafted rules to convertcorpora.
The construction of linguistic rules is usu-ally time-consuming and the rules are not full cover-age.
Compared to rule-based conversion, our statis-tical converters are much easier to built and empiri-cally perform well.6 Experiments6.1 SettingPrevious studies on joint Chinese word segmenta-tion and POS tagging have used the CTB in experi-ments.
We follow this setting in this paper.
We useCTB 5.0 as our main corpus and define the train-ing, development and test sets according to (Jianget al, 2008a; Jiang et al, 2008b; Kruengkrai et al,2009; Zhang and Clark, 2010; Sun, 2011).
Jiang et237al.
(2009) present a preliminary study for the annota-tion adaptation topic, and conduct experiments withthe extra PPD data4.
In other words, the CTB-sytleannotation is the target analysis while the PPD-styleannotation is the complementary/auxiliary analysis.Our experiments for annotation ensemble followstheir setting to lead to a fair comparison of our sys-tem and theirs.
A CRF learning toolkit, wapiti5(Lavergne et al, 2010), is used to resolve sequencelabeling problems.
Among several parameter esti-mation methods provided by wapiti, our auxiliaryexperiments indicate that the ?rprop-?
method worksbest.
Three metrics are used for evaluation: preci-sion (P), recall (R) and balanced f-score (F) definedby 2PR/(P+R).
Precision is the relative amount ofcorrect words in the system output.
Recall is the rel-ative amount of correct words compared to the goldstandard annotations.
A token is considered to becorrect if its boundaries match the boundaries of aword in the gold standard and their POS tags areidentical.6.2 Results of StackingTable 2 summarizes the segmentation and taggingperformance of the baseline and different stackingmodels.
The baseline of the character-based jointsolver (CTagctb) is competitive, and achieves anf-score of 92.93.
By using the character labelsfrom a heterogeneous solver (CTagppd), which istrained on the PPD data set, the performance of thischaracter-based system (CTagppd?ctb) is improvedto 93.67.
This result confirms the importance of aheterogeneous structure.
Our structure-based stack-ing solution is effective and outperforms the feature-based stacking.
By better exploiting the heteroge-neous word boundary structures, our sub-word tag-ging model achieves an f-score of 94.03 (lctbT andlppdT are tuned on the development data and both setto 1).The contribution of the auxiliary tagger is two-fold.
On one hand, the heterogeneous solver pro-vides structural information, which is the basis toconstruct the sub-word sequence.
On the otherhand, this tagger provides additional POS informa-tion, which is helpful for disambiguation.
To eval-4http://icl.pku.edu.cn/icl_res/5http://wapiti.limsi.fr/Devel.
P R FCTagctb 93.28% 92.58% 92.93CTagppd?ctb 93.89% 93.46% 93.67STagctb 94.07% 93.99% 94.03Table 2: Performance of different stacking models on thedevelopment data.uate these two contributions, we do another experi-ment by just using the heterogeneous word boundarystructures without the POS information.
The f-scoreof this type of sub-word tagging is 93.73.
This re-sult indicates that both the word boundary and POSinformation are helpful.6.3 Learning CurvesWe do additional experiments to evaluate the effectof heterogeneous features as the amount of PPD datais varied.
Table 3 summarizes the f-score change.The feature-based model works well only when aconsiderable amount of heterogeneous data is avail-able.
When a small set is added, the performance iseven lower than the baseline (92.93).
The structure-based stacking model is more robust and obtainsconsistent gains regardless of the size of the com-plementary data.PPD?
CTB#CTB #PPD CTag STag18104 7381 92.21 93.2618104 14545 93.22 93.8218104 21745 93.58 93.9618104 28767 93.55 93.8718104 35996 93.67 94.039052 9052 92.10 92.40Table 3: F-scores relative to sizes of training data.
Sizes(shown in column #CTB and #PPD) are numbers of sen-tences in each training corpus.6.4 Results of Annotation ConversionThe stacking models can be viewed as data-drivenannotation converting models.
However they are nottrained on ?real?
labeled samples.
Although the tar-get representation (CTB-style analysis in our case)is gold standard, the input representation (PPD-styleanalysis in our case) is labeled by a automatic tag-ger CTagppd.
To make clear whether these stacking238models trained with noisy inputs can tolerate per-fect inputs, we evaluate the two stacking models onour manually converted data.
The accuracies pre-sented in Table 4 indicate that though the conver-sion models are learned by applying noisy data, theycan refine target tagging with gold auxiliary tagging.Another interesting thing is that the gold PPD-styleanalysis does not help the sub-word tagging modelas much as the character tagging model.Auto PPD Gold PPDCTagppd?ctb 93.69 95.19STagctb 94.14 94.70Table 4: F-scores with gold PPD-style tagging on themanually converted data.6.5 Results of Re-trainingTable 5 shows accuracies of re-trained models.
Notethat a sub-word tagger is built on character taggers,so when we re-train a sub-word system, we shouldconsider whether or not re-training base charactertaggers.
The error rates decrease as automaticallyconverted data is added to the training pool, espe-cially for the character-based tagger CTagctb.
Whenthe base CTB-style tagging is improved, the finaltagging is improved in the end.
The re-training doesnot help the sub-word tagging much; the improve-ment is very modest.CTagctb STagctb P(%) R(%) FDctb ?D?ctb - - 94.46 94.06 94.26Dctb ?D?ctb Dctb 94.61 94.43 94.52Dctb Dctb ?D?
?ctb 94.05 94.08 94.06Dctb ?D?ctb Dctb ?D?
?ctb 94.71 94.53 94.62Table 5: Performance of re-trained models on the devel-opment data.6.6 Comparison to the State-of-the-ArtTable 6 summarizes the tagging performance ofdifferent systems.
The baseline of the character-based tagger is competitive, and achieve an f-scoreof 93.41.
By better using the heterogeneous wordboundary structures, our sub-word tagging modelachieves an f-score of 94.36.
Both character andsub-word tagging model can be enhanced with auto-matically converted corpus.
With the pseudo labeleddata, the performance goes up to 94.11 and 94.68.These results are also better than the best publishedresult on the same data set that is reported in (Jianget al, 2009).Test P R F(Sun, 2011) - - - - 94.02(Jiang et al, 2009) - - - - 94.02(Wang et al, 2011) - - - - 94.186Character model 93.31% 93.51% 93.41+Re-training 93.93% 94.29% 94.11Sub-word model 94.10% 94.62% 94.36+Re-training 94.42% 94.93% 94.68Table 6: Performance of different systems on the testdata.7 ConclusionOur theoretical and empirical analysis of two rep-resentative popular corpora highlights two essentialcharacteristics of heterogeneous annotations whichare explored to reduce approximation and estima-tion errors for Chinese word segmentation and POStagging.
We employ stacking models to incorporatefeatures derived from heterogeneous analysis andapply them to convert heterogeneous labeled data forre-training.
The appropriate application of hetero-geneous annotations leads to a significant improve-ment (a relative error reduction of 11%) over the bestperformance for this task.
Although our discussionis for a specific task, the key idea to leverage het-erogeneous annotations to reduce the approximationerror with stacking models and the estimation errorwith automatically converted corpora is very generaland applicable to other NLP tasks.AcknowledgementThis work is mainly finished when the first authorwas in Saarland University and DFKI.
At that time,this author was funded by DFKI and German Aca-demic Exchange Service (DAAD).
While workingin Peking University, both author are supported byNSFC (61170166) and National High-Tech R&DProgram (2012AA011101).6This result is achieved with much unlabeled data, which isdifferent from our setting.239ReferencesAoife Cahill, Mairead Mccarthy, Josef Van Genabith, andAndy Way.
2002.
Automatic annotation of the penntreebank with lfg f-structure information.
In Proceed-ings of the LREC Workshop on Linguistic KnowledgeAcquisition and Representation: Bootstrapping Anno-tated Language Data, Las Palmas, Canary Islands,pages 8?15.Julia Hockenmaier and Mark Steedman.
2007.
Ccgbank:A corpus of ccg derivations and dependency structuresextracted from the penn treebank.
Computational Lin-guistics, 33(3):355?396.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.2008a.
A cascaded linear model for joint Chineseword segmentation and part-of-speech tagging.
InProceedings of ACL-08: HLT, pages 897?904, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Wenbin Jiang, Haitao Mi, and Qun Liu.
2008b.
Wordlattice reranking for Chinese word segmentation andpart-of-speech tagging.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(Coling 2008), pages 385?392, Manchester, UK, Au-gust.
Coling 2008 Organizing Committee.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and pos tagging ?
a case study.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 522?530, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hybridmodel for joint Chinese word segmentation and postagging.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 513?521, Suntec, Singapore,August.
Association for Computational Linguistics.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
pages 504?513, July.Yusuke Miyao, Takashi Ninomiya, and Jun ichi Tsujii.2004.
Corpus-oriented grammar development for ac-quiring a head-driven phrase structure grammar fromthe penn treebank.
In IJCNLP, pages 684?693.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based?
In Dekang Lin and DekaiWu, editors, Proceedings of EMNLP 2004, pages 277?284, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Zheng-Yu Niu, Haifeng Wang, and Hua Wu.
2009.
Ex-ploiting heterogeneous treebanks for parsing.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 46?54, Suntec, Singapore, August.
As-sociation for Computational Linguistics.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proceedings of ACL-08: HLT, pages 950?958,Columbus, Ohio, June.
Association for ComputationalLinguistics.Lance Ramshaw and Mitch Marcus.
1995.
Text chunk-ing using transformation-based learning.
In DavidYarowsky and Kenneth Church, editors, Proceedingsof the Third Workshop on Very Large Corpora, pages82?94, Somerset, New Jersey.
Association for Compu-tational Linguistics.Weiwei Sun and Jia Xu.
2011.
Enhancing Chinese wordsegmentation using unlabeled data.
In Proceedings ofthe 2011 Conference on Empirical Methods in Natu-ral Language Processing, pages 970?979, Edinburgh,Scotland, UK., July.
Association for ComputationalLinguistics.Weiwei Sun, Rui Wang, and Yi Zhang.
2010.
Dis-criminative parse reranking for Chinese with homoge-neous and heterogeneous annotations.
In Proceedingsof Joint Conference on Chinese Language Processing(CIPS-SIGHAN), Beijing, China, August.Weiwei Sun.
2010.
Word-based and character-basedword segmentation models: Comparison and combi-nation.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (Coling 2010),pages 1211?1219, Beijing, China, August.
Coling2010 Organizing Committee.Weiwei Sun.
2011.
A stacked sub-word model for jointChinese word segmentation and part-of-speech tag-ging.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1385?1394, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Andre?
Filipe Torres Martins, Dipanjan Das, Noah A.Smith, and Eric P. Xing.
2008.
Stacking dependencyparsers.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 157?166, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.2011.
Improving chinese word segmentation andpos tagging with semi-supervised methods using large240auto-analyzed data.
In Proceedings of 5th Interna-tional Joint Conference on Natural Language Process-ing, pages 309?317, Chiang Mai, Thailand, Novem-ber.
Asian Federation of Natural Language Processing.David H. Wolpert.
1992.
Original contribution: Stackedgeneralization.
Neural Netw., 5:241?259, February.Fei Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proceedings of Natural Lan-guage Processing Pacific Rim Symposium, pages 398?403.Yue Zhang and Stephen Clark.
2008.
Joint word segmen-tation and POS tagging using a single perceptron.
InProceedings of ACL-08: HLT, pages 888?896, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Yue Zhang and Stephen Clark.
2010.
A fast decoder forjoint word segmentation and POS-tagging using a sin-gle discriminative model.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 843?852, Cambridge, MA,October.
Association for Computational Linguistics.241
