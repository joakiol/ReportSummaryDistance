Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103?114,Vancouver, October 2005. c?2005 Association for Computational LinguisticsEfficacy of Beam Thresholding, Unification Filtering and HybridParsing in Probabilistic HPSG ParsingTakashi NinomiyaCREST, JSTandDepartment of Computer ScienceThe University of Tokyoninomi@is.s.u-tokyo.ac.jpYoshimasa TsuruokaCREST, JSTandDepartment of Computer ScienceThe University of Tokyotsuruoka@is.s.u-tokyo.ac.jpYusuke MiyaoDepartment of Computer ScienceThe University of Tokyoyusuke@is.s.u-tokyo.ac.jpJun?ichi TsujiiDepartment of Computer ScienceThe University of TokyoandSchool of InformaticsUniversity of ManchesterandCREST, JSTtsujii@is.s.u-tokyo.ac.jpAbstractWe investigated the performance efficacyof beam search parsing and deep parsingtechniques in probabilistic HPSG parsingusing the Penn treebank.
We first testedthe beam thresholding and iterative pars-ing developed for PCFG parsing with anHPSG.
Next, we tested three techniquesoriginally developed for deep parsing: quickcheck, large constituent inhibition, and hy-brid parsing with a CFG chunk parser.
Thecontributions of the large constituent inhi-bition and global thresholding were not sig-nificant, while the quick check and chunkparser greatly contributed to total parsingperformance.
The precision, recall and av-erage parsing time for the Penn treebank(Section 23) were 87.85%, 86.85%, and 360ms, respectively.1 IntroductionWe investigated the performance efficacy of beamsearch parsing and deep parsing techniques inprobabilistic head-driven phrase structure grammar(HPSG) parsing for the Penn treebank.
We firstapplied beam thresholding techniques developed forCFG parsing to HPSG parsing, including localthresholding, global thresholding (Goodman, 1997),and iterative parsing (Tsuruoka and Tsujii, 2005b).Next, we applied parsing techniques developed fordeep parsing, including quick check (Malouf et al,2000), large constituent inhibition (Kaplan et al,2004) and hybrid parsing with a CFG chunk parser(Daum et al, 2003; Frank et al, 2003; Frank, 2004).The experiments showed how each technique con-tributes to the final output of parsing in terms ofprecision, recall, and speed for the Penn treebank.Unification-based grammars have been extensivelystudied in terms of linguistic formulation and com-putation efficiency.
Although they provide preciselinguistic structures of sentences, their processing isconsidered expensive because of the detailed descrip-tions.
Since efficiency is of particular concern in prac-tical applications, a number of studies have focusedon improving the parsing efficiency of unification-based grammars (Oepen et al, 2002).
Although sig-nificant improvements in efficiency have been made,parsing speed is still not high enough for practicalapplications.The recent introduction of probabilistic models ofwide-coverage unification-based grammars (Maloufand van Noord, 2004; Kaplan et al, 2004; Miyaoand Tsujii, 2005) has opened up the novel possibil-ity of increasing parsing speed by guiding the searchpath using probabilities.
That is, since we often re-quire only the most probable parse result, we cancompute partial parse results that are likely to con-tribute to the final parse result.
This approach hasbeen extensively studied in the field of probabilistic103CFG (PCFG) parsing, such as Viterbi parsing andbeam thresholding.While many methods of probabilistic parsing forunification-based grammars have been developed,their strategy is to first perform exhaustive pars-ing without using probabilities and then select thehighest probability parse.
The behavior of their al-gorithms is like that of the Viterbi algorithm forPCFG parsing, so the correct parse with the high-est probability is guaranteed.
The interesting pointof this approach is that, once the exhaustive pars-ing is completed, the probabilities of non-local de-pendencies, which cannot be computed during pars-ing, are computed after making a packed parse for-est.
Probabilistic models where probabilities are as-signed to the CFG backbone of the unification-basedgrammar have been developed (Kasper et al, 1996;Briscoe and Carroll, 1993; Kiefer et al, 2002), andthe most probable parse is found by PCFG parsing.This model is based on PCFG and not probabilis-tic unification-based grammar parsing.
Geman andJohnson (Geman and Johnson, 2002) proposed a dy-namic programming algorithm for finding the mostprobable parse in a packed parse forest generated byunification-based grammars without expanding theforest.
However, the efficiency of this algorithm isinherently limited by the inefficiency of exhaustiveparsing.In this paper we describe the performance of beamthresholding, including iterative parsing, in proba-bilistic HPSG parsing for a large-scale corpora, thePenn treebank.
We show how techniques developedfor efficient deep parsing can improve the efficiencyof probabilistic parsing.
These techniques were eval-uated in experiments on the Penn Treebank (Marcuset al, 1994) with the wide-coverage HPSG parser de-veloped by Miyao et al (Miyao et al, 2005; Miyaoand Tsujii, 2005).2 HPSG and probabilistic modelsHPSG (Pollard and Sag, 1994) is a syntactic theorybased on lexicalized grammar formalism.
In HPSG,a small number of schemata describe general con-struction rules, and a large number of lexical en-tries express word-specific characteristics.
The struc-tures of sentences are explained using combinationsof schemata and lexical entries.
Both schemata andlexical entries are represented by typed feature struc-tures, and constraints represented by feature struc-tures are checked with unification.Figure 1 shows an example of HPSG parsing ofthe sentence ?Spring has come.?
First, each of thelexical entries for ?has?
and ?come?
is unified with adaughter feature structure of the Head-ComplementSpringHEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1hasHEAD  verbSUBJ  <    >COMPS  < >1come2head-compHEAD  verbSUBJ  < >COMPS  < >HEAD  nounSUBJ  < >COMPS  < >1=?SpringHEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1hasHEAD  verbSUBJ  <    >COMPS  < >1come2HEAD  verbSUBJ  <    >COMPS  < >1HEAD  verbSUBJ  < >COMPS  < >1subject-headhead-compFigure 1: HPSG parsingSchema.
Unification provides the phrasal sign ofthe mother.
The sign of the larger constituent isobtained by repeatedly applying schemata to lexi-cal/phrasal signs.
Finally, the parse result is outputas a phrasal sign that dominates the sentence.Given set W of words and set F of feature struc-tures, an HPSG is formulated as a tuple, G = ?L,R?,whereL = {l = ?w,F ?|w ?
W, F ?
F} is a set of lexicalentries, andR is a set of schemata, i.e., r ?
R is a partialfunction: F ?
F ?
F .Given a sentence, an HPSG computes a set of phrasalsigns, i.e., feature structures, as a result of parsing.Previous studies (Abney, 1997; Johnson et al,1999; Riezler et al, 2000; Miyao et al, 2003; Mal-ouf and van Noord, 2004; Kaplan et al, 2004; Miyaoand Tsujii, 2005) defined a probabilistic model ofunification-based grammars as a log-linear model ormaximum entropy model (Berger et al, 1996).
Theprobability of parse result T assigned to given sen-tence w = ?w1, .
.
.
, wn?
isp(T |w) = 1Zwexp(?i?ifi(T ))Zw =?T ?exp(?i?ifi(T ?
)),where ?i is a model parameter, and fi is a featurefunction that represents a characteristic of parse treeT .
Intuitively, the probability is defined as the nor-malized product of the weights exp(?i) when a char-acteristic corresponding to fi appears in parse resultT .
Model parameters ?i are estimated using numer-104ical optimization methods (Malouf, 2002) so as tomaximize the log-likelihood of the training data.However, the above model cannot be easily esti-mated because the estimation requires the computa-tion of p(T |w) for all parse candidates assigned tosentence w. Because the number of parse candidatesis exponentially related to the length of the sentence,the estimation is intractable for long sentences.To make the model estimation tractable, Ge-man and Johnson (Geman and Johnson, 2002) andMiyao and Tsujii (Miyao and Tsujii, 2002) proposeda dynamic programming algorithm for estimatingp(T |w).
They assumed that features are functionson nodes in a packed parse forest.
That is, parse treeT is represented by a set of nodes, i.e., T = {c}, andthe parse forest is represented by an and/or graphof the nodes.
From this assumption, we can redefinethe probability asp(T |w) = 1Zwexp(?c?T?i?ifi(c))Zw =?T ?exp(?c?T ?
?i?ifi(c)).A packed parse forest has a structure similar to achart of CFG parsing, and c corresponds to an edgein the chart.
This assumption corresponds to theindependence assumption in PCFG; that is, onlya nonterminal symbol of a mother is considered infurther processing by ignoring the structure of itsdaughters.
With this assumption, we can computethe figures of merit (FOMs) of partial parse results.This assumption restricts the possibility of featurefunctions that represent non-local dependencies ex-pressed in a parse result.
Since unification-basedgrammars can express semantic relations, such aspredicate-argument relations, in their structure, theassumption unjustifiably restricts the flexibility ofprobabilistic modeling.
However, previous research(Miyao et al, 2003; Clark and Curran, 2004; Kaplanet al, 2004) showed that predicate-argument rela-tions can be represented under the assumption offeature locality.
We thus assumed the locality of fea-ture functions and exploited it for the efficient searchof probable parse results.3 Techniques for efficient deepparsingMany of the techniques for improving the parsingefficiency of deep linguistic analysis have been de-veloped in the framework of lexicalized grammarssuch as lexical functional grammar (LFG) (Bresnan,1982), lexicalized tree adjoining grammar (LTAG)(Shabes et al, 1988), HPSG (Pollard and Sag, 1994)or combinatory categorial grammar (CCG) (Steed-man, 2000).
Most of them were developed for ex-haustive parsing, i.e., producing all parse results thatare given by the grammar (Matsumoto et al, 1983;Maxwell and Kaplan, 1993; van Noord, 1997; Kieferet al, 1999; Malouf et al, 2000; Torisawa et al, 2000;Oepen et al, 2002; Penn and Munteanu, 2003).
Thestrategy of exhaustive parsing has been widely usedin grammar development and in parameter trainingfor probabilistic models.We tested three of these techniques.Quick check Quick check filters out non-unifiablefeature structures (Malouf et al, 2000).
Sup-pose we have two non-unifiable feature struc-tures.
They are destructively unified by travers-ing and modifying them, and then finally theyare found to be not unifiable in the middle of theunification process.
Quick check quickly judgestheir unifiability by peeping the values of thegiven paths.
If one of the path values is notunifiable, the two feature structures cannot beunified because of the necessary condition of uni-fication.
In our implementation of quick check,each edge had two types of arrays.
One con-tained the path values of the edge?s sign; wecall this the sign array.
The other contained thepath values of the right daughter of a schemasuch that its left daughter is unified with theedge?s sign; we call this a schema array.
Whenwe apply a schema to two edges, e1 and e2, theschema array of e1 and the sign array of e2 arequickly checked.
If it fails, then quick check re-turns a unification failure.
If it succeeds, thesigns are unified with the schemata, and the re-sult of unification is returned.Large constituent inhibition (Kaplan et al,2004) It is unlikely for a large medial edge tocontribute to the final parsing result if it spansmore than 20 words and is not adjacent to thebeginning or ending of the sentence.
Largeconstituent inhibition prevents the parser fromgenerating medial edges that span more thansome word length.HPSG parsing with a CFG chunk parser Ahybrid of deep parsing and shallow parsingwas recently found to improve the efficiencyof deep parsing (Daum et al, 2003; Frank etal., 2003; Frank, 2004).
As a preprocessor, theshallow parsing must be very fast and achievehigh precision but not high recall so that the105procedure Viterbi(?w1, .
.
.
, wn?, ?L?, R?, ?, ?, ?
)for i = 1 to nforeach Fu ?
{F |?wi, F ?
?
L}?
=?i ?ifi(Fu)pi[i?
1, i]?
pi[i?
1, i] ?
{Fu}if (?
> ?[i?
1, i, Fu]) then?[i?
1, i, Fu]?
?for d = 1 to nfor i = 0 to n?
dj = i + dfor k = i + 1 to j ?
1foreach Fs ?
pi[i, k], Ft ?
pi[k, j], r ?
Rif F = r(Fs, Ft) has succeeded?
= ?
[i, k, Fs] + ?
[k, j, Ft] +?i ?ifi(F )pi[i, j]?
pi[i, j] ?
{F}if (?
> ?
[i, j, F ]) then?
[i, j, F ]?
?Figure 2: Pseudo-code of Viterbi algorithms for probabilistic HPSG parsingtotal parsing performance in terms of precision,recall and speed is not degraded.
Because thereis trade-off between speed and accuracy inthis approach, the total parsing performancefor large-scale corpora like the Penn treebankshould be measured.
We introduce a CFGchunk parser (Tsuruoka and Tsujii, 2005a) as apreprocessor of HPSG parsing.
Chunk parsersmeet the requirements for preprocessors; theyare very fast and have high precision.
Thegrammar for the chunk parser is automaticallyextracted from the CFG treebank translatedfrom the HPSG treebank, which is generatedduring grammar extraction from the Penntreebank.
The principal idea of using the chunkparser is to use the bracket information, i.e.,parse trees without non-terminal symbols, andprevent the HPSG parser from generating edgesthat cross brackets.4 Beam thresholding for HPSGparsing4.1 Simple beam thresholdingMany algorithms for improving the efficiency ofPCFG parsing have been extensively investigated.They include grammar compilation (Tomita, 1986;Nederhof, 2000), the Viterbi algorithm, controllingsearch strategies without FOM such as left-cornerparsing (Rosenkrantz and Lewis II, 1970) or head-corner parsing (Kay, 1989; van Noord, 1997), andwith FOM such as the beam search, the best-firstsearch or A* search (Chitrao and Grishman, 1990;Caraballo and Charniak, 1998; Collins, 1999; Rat-naparkhi, 1999; Charniak, 2000; Roark, 2001; Kleinand Manning, 2003).
The beam search and best-first search algorithms significantly reduce the timerequired for finding the best parse at the cost of los-ing the guarantee of finding the correct parse.The CYK algorithm, which is essentially a bottom-up parser, is a natural choice for non-probabilisticHPSG parsers.
Many of the constraints are ex-pressed as lexical entries in HPSG, and bottom-upparsers can use those constraints to reduce the searchspace in the early stages of parsing.For PCFG, extending the CYK algorithm to out-put the Viterbi parse is straightforward (Ney, 1991;Jurafsky and Martin, 2000).
The parser can effi-ciently calculate the Viterbi parse by taking the max-imum of the probabilities of the same nonterminalsymbol in each cell.
With the probabilistic modeldefined in Section 2, we can also define the Viterbisearch for unification-based grammars (Geman andJohnson, 2002).
Figure 2 shows the pseudo-code ofViterbi algorithm.
The pi[i, j] represents the set ofpartial parse results that cover words wi+1, .
.
.
, wj ,and ?
[i, j, F ] stores the maximum FOM of partialparse result F at cell (i, j).
Feature functions aredefined over lexical entries and results of rule appli-cations, which correspond to conjunctive nodes in afeature forest.
The FOM of a newly created partialparse, F , is computed by summing the values of ?
ofthe daughters and an additional FOM of F .The Viterbi algorithm enables various pruningtechniques to be used for efficient parsing.
Beamthresholding (Goodman, 1997) is a simple and effec-tive technique for pruning edges during parsing.
Ineach cell of the chart, the method keeps only a por-tion of the edges which have higher FOMs comparedto the other edges in the same cell.106procedure BeamThresholding(?w1, .
.
.
, wn?, ?L?, R?, ?, ?, ?
)for i = 1 to nforeach Fu ?
{F |?wi, F ?
?
L}?
=?i ?ifi(Fu)pi[i?
1, i]?
pi[i?
1, i] ?
{Fu}if (?
> ?[i?
1, i, Fu]) then?[i?
1, i, Fu]?
?for d = 1 to nfor i = 0 to n?
dj = i + dfor k = i + 1 to j ?
1foreach Fs ?
pi[i, k], Ft ?
pi[k, j], r ?
Rif F = r(Fs, Ft) has succeeded?
= ?
[i, k, Fs] + ?
[k, j, Ft] +?i ?ifi(F )pi[i, j]?
pi[i, j] ?
{F}if (?
> ?
[i, j, F ]) then?
[i, j, F ]?
?LocalThresholding(?, ?
)GlobalThresholding(n, ?
)procedure LocalThresholding(?, ?
)sort pi[i, j] according to ?
[i, j, F ]pi[i, j]?
{pi[i, j]1, .
.
.
, pi[i, j]?
}?max = maxF ?
[i, j, F ]foreach F ?
pi[i, j]if ?
[i, j, F ] < ?max ?
?pi[i, j]?
pi[i, j]\{F}procedure GlobalThresholding(n, ?
)f [0..n]?
{0,??
?
?, .
.
.
,??}b[0..n]?
{??,?
?, .
.
.
,?
?, 0}#forwardfor i = 0 to n?
1for j = i + 1 to nforeach F ?
pi[i, j]f [j]?
max(f [j], f [i] + ?
[i, j, F ])#backwardfor i = n?
1 to 0for j = i + 1 to nforeach F ?
pi[i, j]b[i]?
max(b[i], b[j] + ?
[i, j, F ])#global thresholding?max = f [n]for i = 0 to n?
1for j = i + 1 to nforeach F ?
pi[i, j]if f [i] + ?
[i, j, F ] + b[j] < ?max ?
?
thenpi[i, j]?
pi[i, j]\{F}Figure 3: Pseudo-code of local beam search and global beam search algorithms for probabilistic HPSGparsing107procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ?
?, ?
?, ?
?, ?last, ?last, ?last)??
?0; ?
?
?0; ?
?
?0loop while ?
?
?last and ?
?
?last and ?
?
?lastcall BeamThresholding(w, G, ?, ?, ?
)if pi[1, n] 6= ?
then exit??
?
+ ??
; ?
?
?
+ ??
; ?
?
?
+ ?
?Figure 4: Pseudo-code of iterative beam thresholdingWe tested three selection schemes for decidingwhich edges to keep in each cell.Local thresholding by number of edges Eachcell keeps the top ?
edges based on their FOMs.Local thresholding by beam width Each cellkeeps the edges whose FOM is greater than?max ?
?, where ?max is the highest FOMamong the edges in the cell.Global thresholding by beam width Each cellkeeps the edges whose global FOM is greaterthan ?max?
?, where ?max is the highest globalFOM in the chart.Figure 3 shows the pseudo-code of local beamsearch, and global beam search algorithms for prob-abilistic HPSG parsing.
The code for local thresh-olding is inserted at the end of the computation foreach cell.
In Figure 3, pi[i, j]k denotes the k-th ele-ment in sorted set pi[i, j].
We first take the first ?elements that have higher FOMs and then removethe elements with FOMs lower than ?max ?
?.Global thresholding is also used for pruning edges,and was originally proposed for CFG parsing (Good-man, 1997).
It prunes edges based on their globalFOM and the best global FOM in the chart.
Theglobal FOM of an edge is defined as its FOM plus itsforward and backward FOMs, where the forward andbackward FOMs are rough estimations of the outsideFOM of the edge.
The global thresholding is per-formed immediately after each line of the CYK chartis completed.
The forward FOM is calculated first,and then the backward FOM is calculated.
Finally,all edges with a global FOM lower than ?max ?
?are pruned.
Figure 3 gives further details of the al-gorithm.4.2 Iterative beam thresholdingWe tested the iterative beam thresholding proposedby Tsuruoka and Tsujii (2005b).
We started theparsing with a narrow beam.
If the parser outputresults, they were taken as the final parse results.
Ifthe parser did not output any results, we widened theTable 1: Abbreviations used in experimental resultsnum local beam thresholding by numberwidth local beam thresholding by widthglobal global beam thresholding by widthiterative iterative parsing with local beamthresholding by number and widthchp parsing with CFG chunk parserbeam, and reran the parsing.
We continued widen-ing the beam until the parser output results or thebeam width reached some limit.The pseudo-code is presented in Figure 4.
It callsthe beam thresholding procedure shown in Figure 3and increases parameters ?
and ?
until the parseroutputs results, i.e., pi[1, n] 6= ?.Preserved iterative parsing Our implementedCFG parser with iterative parsing cleared thechart and edges at every iteration although theparser regenerated the same edges using thosegenerated in the previous iteration.
This isbecause the computational cost of regeneratingedges is smaller than that of reusing edges towhich the rules have already been applied.
ForHPSG parsing, the regenerating cost is evengreater than that for CFG parsing.
In ourimplementation of HPSG parsing, the chartand edges were not cleared during the iterativeparsing.
Instead, the pruned edges were markedas thresholded ones.
The parser counted thenumber of iterations, and when edges weregenerated, they were marked with the iterationnumber, which we call the generation.
Ifedges were thresholded out, the generation wasreplaced with the current iteration number plus1.
Suppose we have two edges, e1 and e2.
Thegrammar rules are applied iff both e1 and e2 arenot thresholded out, and the generation of e1or e2 is equal to the current iteration number.Figure 5 shows the pseudo-code of preservediterative parsing.108procedure BeamThresholding(?w1, .
.
.
, wn?, ?L?, R?, ?, ?, ?, iternum)for i = 1 to nforeach Fu ?
{F |?wi, F ?
?
L}?
=?i ?ifi(Fu)pi[i?
1, i]?
pi[i?
1, i] ?
{Fu}if (?
> ?[i?
1, i, Fu]) then?[i?
1, i, Fu]?
?for d = 1 to nfor i = 0 to n?
dj = i + dfor k = i + 1 to j ?
1foreach Fs ?
?
[i, k], Ft ?
?
[k, j], r ?
Rif gen[i, k, Fs] = iternum ?
gen[k, j, Ft] = iternumif F = r(Fs, Ft) has succeededgen[i, j, F ]?
iternum?
= ?
[i, k, Fs] + ?
[k, j, Ft] +?i ?ifi(F )pi[i, j]?
pi[i, j] ?
{F}if (?
> ?
[i, j, F ]) then?
[i, j, F ]?
?LocalThresholding(?, ?, iternum)GlobalThresholding(n, ?, iternum)procedure LocalThresholding(?, ?, iternum)sort pi[i, j] according to ?
[i, j, F ]?
[i, j]?
{pi[i, j]1, .
.
.
, pi[i, j]?
}?max = maxF ?
[i, j, F ]foreach F ?
?
[i, j]if ?
[i, j, F ] < ?max ?
??
[i, j]?
?
[i, j]\{F}foreach F ?
(pi[i, j]?
?
[i, j])gen[i, j, F ]?
iternum + 1procedure GlobalThresholding(n, ?, iternum)f [0..n]?
{0,??
?
?, .
.
.
,??}b[0..n]?
{??,?
?, .
.
.
,?
?, 0}#forwardfor i = 0 to n?
1for j = i + 1 to nforeach F ?
pi[i, j]f [j]?
max(f [j], f [i] + ?
[i, j, F ])#backwardfor i = n?
1 to 0for j = i + 1 to nforeach F ?
pi[i, j]b[i]?
max(b[i], b[j] + ?
[i, j, F ])#global thresholding?max = f [n]for i = 0 to n?
1for j = i + 1 to nforeach F ?
?
[i, j]if f [i] + ?
[i, j, F ] + b[j] < ?max ?
?
then?
[i, j]?
?
[i, j]\{F}foreach F ?
(pi[i, j]?
?
[i, j])gen[i, j, F ]?
iternum + 1procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ?
?, ?
?, ?
?, ?last, ?last, ?last)??
?0; ?
?
?0; ?
?
?0; iternum = 0loop while ?
?
?last and ?
?
?last and ?
?
?lastcall BeamThresholding(w, G, ?, ?, ?, iternum)if pi[1, n] 6= ?
then exit??
?
+ ??
; ?
?
?
+ ??
; ?
?
?
+ ??
; iternum?
iternum + 1Figure 5: Pseudo-code of preserved iterative parsing for HPSG109Table 2: Experimental results for development set (section 22) and test set (section 23)Precision Recall F-score Avg.
Time (ms) No.
of failed sentencesdevelopment set 88.21% 87.32% 87.76% 360 12test set 87.85% 86.85% 87.35% 360 15          	   	         	          	   	         	Figure 7: Parsing time for the sentences in Section 24 of less than 15 words of Viterbi parsing (none) (Left)and iterative parsing (iterative) (Right)                                  	Figure 6: Parsing time versus sentence length for thesentences in Section 23 of less than 40 words5 EvaluationWe evaluated the efficiency of the parsing techniquesby using the HPSG for English developed by Miyaoet al (2005).
The lexicon of the grammar was ex-tracted from Sections 02-21 of the Penn Treebank(Marcus et al, 1994) (39,832 sentences).
The gram-mar consisted of 2,284 lexical entry templates for10,536 words1.
The probabilistic disambiguationmodel of the grammar was trained using the sameportion of the treebank (Miyao and Tsujii, 2005).1Lexical entry templates for POS are also developed.They are assigned to unknown words.The model included 529,856 features.
The param-eters for beam searching were determined manuallyby trial and error using Section 22; ?0 = 12,??
=6, ?last = 30, ?0 = 6.0,??
= 3.0, ?last = 15.1,?0 = 8.0,??
= 4.0, and ?last = 20.1.
We usedthe chunk parser developed by Tsuruoka and Tsu-jii (2005a).
Table 1 shows the abbreviations used inpresenting the results.We measured the accuracy of the predicate-argument relations output by the parser.
Apredicate-argument relation is defined as a tuple?
?,wh, a, wa?, where ?
is the predicate type (e.g., ad-jective, intransitive verb), wh is the head word of thepredicate, a is the argument label (MODARG, ARG1,..., ARG4), and wa is the head word of the argu-ment.
Precision/recall is the ratio of tuples correctlyidentified by the parser.
This evaluation scheme wasthe same as used in previous evaluations of lexical-ized grammars (Hockenmaier, 2003; Clark and Cur-ran, 2004; Miyao and Tsujii, 2005).
The experimentswere conducted on an AMD Opteron server with a2.4-GHz CPU.
Section 22 of the Treebank was usedas the development set, and performance was evalu-ated using sentences of less than 40 words in Section23 (2,164 sentences, 20.3 words/sentence).
The per-formance of each parsing technique was analyzed us-ing the sentences in Section 24 of less than 15 words(305 sentences) and less than 40 words (1145 sen-tences).Table 2 shows the parsing performance using all110            	  	 	  	   	    	              !""" !     " !Figure 8: F-score versus average parsing time            	  	 	  	   	             !
"#!!"#!
         !
"#!   !"#!
Figure 9: F-score versus average parsing time with/without chunk parser111Table 3: Viterbi parsing versus beam thresholding versus iterative parsingPrecision Recall F-score Avg.
Time (ms) No.
of failed sentencesviterbi parsing (none) 88.22% 87.94% 88.08% 103923 2beam search parsing (num+width) 88.96% 82.38% 85.54% 88 26iterative parsing (iterative) 87.61% 87.24% 87.42% 99 2Table 4: Contribution to performance of each implementationPrecision Recall F-score Avg.
Time (ms) diff(*) No.
of failed sentencesfull 85.49% 84.21% 84.84% 407 0 13full?piter 85.74% 84.70% 85.22% 631 224 10full?qc 85.49% 84.21% 84.84% 562 155 13full?chp 85.77% 84.76% 85.26% 510 103 10full?global 85.23% 84.32% 84.78% 434 27 9full?lci 85.68% 84.40% 85.03% 424 17 13full?piter?qc?chp?global?lci 85.33% 84.71% 85.02% 1033 626 6full ... iterative + global + chppiter ... preserved iterative parsingqc ... quick checklci ... large constituent inhibitiondiff(*) ... (Avg.
Time of full) - (Avg.
Time)thresholding techniques and implementations de-scribed in Section 4 for the sentences in the devel-opment set (Section 22) and the test set (Section 23)of less than 40 words.
In the table, precision, recall,average parsing time per sentence, and the number ofsentences that the parser failed to parse are detailed.Figure 6 shows the distribution of parsing time forthe sentence length.Table 3 shows the performance of the Viterbi pars-ing, beam search parsing, and iterative parsing forthe sentences in Section 24 of less than 15 words2.
The parsing without beam searching took morethan 1,000 times longer than with beam searching.However, the beam searching reduced the recall from87.9% to 82.4%.
The main reason for this reduc-tion was parsing failure.
That is, the parser couldnot output any results when the beam was too nar-row instead of producing incorrect parse results.
Al-though iterative parsing was originally developed forefficiency, the results revealed that it also increasesthe recall.
This is because the parser continues try-ing until some results are output.
Figure 7 shows thelogarithmic graph of parsing time for the sentencelength.
The left side of the figure shows the parsingtime of the Viterbi parsing and the right side showsthe parsing time of the iterative parsing.Figure 8 shows the performance of the parsingtechniques for different parameters for the sentencesin Section 24 of less than 40 words.
The combina-tions of thresholding techniques achieved better re-2The sentence length was limited to 15 words becauseof inefficiency of Viterbi parsingsults than the single techniques.
Local thresholdingusing the width (width) performed better than thatusing the number (num).
The combination of us-ing width and number (num+width) performed bet-ter than single local and single global thresholding.The superiority of iterative parsing (iterative) wasagain demonstrated in this experiment.
Although wedid not observe significant improvement with globalthresholding, the global plus iterative combinationslightly improved performance.Figure 9 shows the performance with and with-out the chunk parser.
The lines with white symbolsrepresent parsing without the chunk parser, and thelines with black symbols represent parsing with thechunk parser.
The chunk parser improved the to-tal parsing performance significantly.
The improve-ments with global thresholding were less with thechunk parser.Finally, Table 4 shows the contribution to perfor-mance of each implementation for the sentences inSection 24 of less than 40 words.
The ?full?
meansthe parser including all thresholding techniques andimplementations described in Section 4.
The ?full?
x?
means the full minus x.
The preserved itera-tive parsing, the quick check, and the chunk parsergreatly contributed to the final parsing speed, whilethe global thresholding and large constituent inhibi-tion did not.6 ConclusionWe have described the results of experiments with anumber of existing techniques in head-driven phrase112structure grammar (HPSG) parsing.
Simple beamthresholding, similar to that for probabilistic CFG(PCFG) parsing, significantly increased the parsingspeed over Viterbi algorithm, but reduced the re-call because of parsing failure.
Iterative parsing sig-nificantly increased the parsing speed without de-grading precision or recall.
We tested three tech-niques originally developed for deep parsing: quickcheck, large constituent inhibition, and HPSG pars-ing with a CFG chunk parser.
The contributionsof the large constituent inhibition and global thresh-olding were not significant, while the quick check andchunk parser greatly contributed to total parsing per-formance.
The precision, recall and average parsingtime for the Penn treebank (Section 23) were 87.85%,86.85%, and 360 ms, respectively.ReferencesSteven P. Abney.
1997.
Stochastic attribute-valuegrammars.
Computational Linguistics, 23(4):597?618.Adam Berger, Stephen Della Pietra, and Vin-cent Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Com-putational Linguistics, 22(1):39?71.Joan Bresnan.
1982.
The Mental Representation ofGrammatical Relations.
MIT Press, Cambridge,MA.Ted Briscoe and John Carroll.
1993.
Generalizedprobabilistic LR-parsing of natural language (cor-pora) with unification-based grammars.
Compu-tational Linguistics, 19(1):25?59.Sharon A. Caraballo and Eugene Charniak.
1998.New figures of merit for best-first probabilis-tic chart parsing.
Computational Linguistics,24(2):275?298.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proc.
of NAACL-2000, pages132?139.Mahesh V. Chitrao and Ralph Grishman.
1990.Edge-based best-first chart parsing.
In Proc.
ofthe DARPA Speech and Natural Language Work-shop, pages 263?266.Stephen Clark and James R. Curran.
2004.
Parsingthe WSJ using CCG and log-linear models.
InProc.
of ACL?04, pages 104?111.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,Univ.
of Pennsylvania.Michael Daum, Kilian A. Foth, and Wolfgang Men-zel.
2003.
Constraint based integration of deepand shallow parsing techniques.
In Proc.
of EACL-2003, pages 99?106.Anette Frank, Markus Becker, Berthold Crysmann,Bernd Kiefer, and Ulrich Schaefer.
2003.
In-tegrated shallow and deep parsing: TopP meetsHPSG.
In Proc.
of ACL?03, pages 104?111.Anette Frank.
2004.
Constraint-based RMRS con-struction from shallow grammars.
In Proc.
ofCOLING-2004, pages 1269?1272.Stuart Geman and Mark Johnson.
2002.
Dy-namic programming for parsing and estimation ofstochastic unification-based grammars.
In Proc.
ofACL?02, pages 279?286.Joshua Goodman.
1997.
Global thresholding andmultiple pass parsing.
In Proc.
of EMNLP-1997,pages 11?25.Julia Hockenmaier.
2003.
Parsing with generativemodels of predicate-argument structure.
In Proc.of ACL?03, pages 359?366.Mark Johnson, Stuart Geman, Stephen Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Proc.of ACL ?99, pages 535?541.Dainiel Jurafsky and James H. Martin.
2000.
Speechand Language Processing.
Prentice Hall.R.
M. Kaplan, S. Riezler, T. H. King, J. T. MaxwellIII, and A. Vasserman.
2004.
Speed and accuracyin shallow and deep stochastic parsing.
In Proc.of HLT/NAACL?04.Walter Kasper, Hans-Ulrich Krieger, Jo?rg Spilker,and Hans Weber.
1996.
From word hypothesesto logical form: An efficient interleaved approach.In Proceedings of Natural Language Processing andSpeech Technology.
Results of the 3rd KONVENSConference, pages 77?88.Martin Kay.
1989.
Head driven parsing.
In Proc.
ofIWPT?89, pages 52?62.Bernd Kiefer, Hans-Ulrich Krieger, John Carroll,and Robert Malouf.
1999.
A bag of useful tech-niques for efficient and robust parsing.
In Proc.
ofACL?99, pages 473?480, June.Bernd Kiefer, Hans-Ulrich Krieger, and DetlefPrescher.
2002.
A novel disambiguation methodfor unification-based grammars using probabilisticcontext-free approximations.
In Proc.
of COLING2002.Dan Klein and Christopher D. Manning.
2003.
A*parsing: Fast exact viterbi parse selection.
InProc.
of HLT-NAACL?03.113Robert Malouf and Gertjan van Noord.
2004.
Widecoverage parsing with stochastic attribute valuegrammars.
In Proc.
of IJCNLP-04 Workshop ?Be-yond Shallow Analyses?.Robert Malouf, John Carroll, and Ann Copestake.2000.
Efficient feature structure operations with-out compilation.
Journal of Natural Language En-gineering, 6(1):29?46.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
InProc.
of CoNLL-2002, pages 49?55.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1994.
Building a largeannotated corpus of English: The Penn Treebank.Computational Linguistics, 19(2):313?330.Yuji Matsumoto, Hozumi Tanaka, Hideki Hirakawa,Hideo Miyoshi, and Hideki Yasukawa.
1983.
BUP:A bottom up parser embedded in Prolog.
NewGeneration Computing, 1(2):145?158.John Maxwell and Ron Kaplan.
1993.
The interfacebetween phrasal and functional constraints.
Com-putational Linguistics, 19(4):571?589.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximumentropy estimation for feature forests.
In Proc.
ofHLT 2002, pages 292?297.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Proba-bilistic disambiguation models for wide-coverageHPSG parsing.
In Proc.
of ACL?05, pages 83?90.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-jii.
2003.
Probabilistic modeling of argumentstructures including non-local dependencies.
InProc.
of RANLP ?03, pages 285?291.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-jii, 2005.
Keh-Yih Su, Jun?ichi Tsujii, Jong-HyeokLee and Oi Yee Kwong (Eds.
), Natural LanguageProcessing - IJCNLP 2004 LNAI 3248, chapterCorpus-oriented Grammar Development for Ac-quiring a Head-driven Phrase Structure Grammarfrom the Penn Treebank, pages 684?693.
Springer-Verlag.Mark-Jan Nederhof.
2000.
Practical experimentswith regular approximation of context-free lan-guages.
Computational Linguistics, 26(1):17?44.H.
Ney.
1991.
Dynamic programming parsing forcontext-free grammars in continuous speech recog-nition.
IEEE Transactions on Signal Processing,39(2):336?340.Stephan Oepen, Dan Flickinger, Jun?ichi Tsujii, andHans Uszkoreit, editors.
2002.
Collaborative Lan-guage Engineering: A Case Study in EfficientGrammar-Based Processing.
CSLI Publications.Gerald Penn and Cosmin Munteanu.
2003.
Atabulation-based parsing method that reducescopying.
In Proc.
of ACL?03).Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress.Adwait Ratnaparkhi.
1999.
Learning to parse natu-ral language with maximum entropy models.
Ma-chine Learning, 34(1-3):151?175.Stefan Riezler, Detlef Prescher, Jonas Kuhn, andMark Johnson.
2000.
Lexicalized stochasticmodeling of constraint-based grammars using log-linear measures and EM training.
In Proc.
ofACL?00, pages 480?487.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tics, 27(2):249?276.Daniel J. Rosenkrantz and Philip M. Lewis II.
1970.Deterministic left corner parsing.
In IEEE Con-ference Record of the 11th Annual Symposium onSwitching and Automata Theory, pages 139?152.Yves Shabes, Anne Abeille`, and Aravind K. Joshi.1988.
Parsing strategies with ?lexicalized?
gram-mars: Application to tree adjoining grammars.
InProc.
of COLING?88, pages 578?583.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press.Masaru Tomita.
1986.
Efficient Parsing for NaturalLanguage.
Kluwer Academic Publishers.Kentaro Torisawa, Kenji Nishida, Yusuke Miyao, andJun?ichi Tsujii.
2000.
An HPSG parser with CFGfiltering.
Journal of Natural Language Engineer-ing, 6(1):63?80.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005a.Chunk parsing revisited.
In Proc.
of IWPT-2005.Yoshimasa Tsuruoka and Jun?ichi Tsujii, 2005b.Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok Lee andOi Yee Kwong (Eds.
), Natural Language Process-ing - IJCNLP 2004 LNAI 3248, chapter Itera-tive CKY Parsing for Probabilistic Context-FreeGrammars, pages 52?60.
Springer-Verlag.Gertjan van Noord.
1997.
An efficient implemen-tation of the head-corner parser.
ComputationalLinguistics, 23(3):425?456.114
