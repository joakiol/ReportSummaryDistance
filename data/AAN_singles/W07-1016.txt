BioNLP 2007: Biological, translational, and clinical language processing, pages 121?128,Prague, June 2007. c?2007 Association for Computational LinguisticsA Study of Structured Clinical Abstracts and the Semantic Classification ofSentencesGrace Y. Chung and Enrico CoieraCentre for Health InformaticsUniversity of New South WalesSydney NSW 2052 Australia{graceyc, e.coiera}@unsw.edu.auAbstractThis paper describes experiments in classi-fying sentences of medical abstracts into anumber of semantic classes given by sectionheadings in structured abstracts.
Using con-ditional random fields, we obtain F -scoresranging from 0.72 to 0.97.
By using a smallset of sentences that appear under the PAR-TICPANTS heading, we demonstrate that it ispossible to recognize sentences that describepopulation characteristics of a study.
Wepresent a detailed study of the structure ofabstracts of randomized clinical trials, andexamine how sentences labeled under PAR-TICIPANTS could be used to summarize thepopulation group.1 IntroductionMedical practitioners are increasingly apply-ing evidence-based medicine (EBM) to supportdecision-making in patient treatments.
The aimof EBM (Sackett, 1998) is to provide improvedcare leading to better outcomes through locatingevidence for a clinical problem, evaluating thequality of the evidence, and then applying to acurrent problem at hand.
However, the adoption ofEBM is hampered by the overwhelming amountof information available, and insufficient time andskills on the clinician?s part to locate and synthesizethe best evidence in the scientific literature.MEDLINE abstracts about randomized clinical tri-als (RCTs) play a critical role in providing the bestevidence for the latest interventions for any givenconditions.
The MEDLINE database now has 16 mil-lion bibliographic entries, many of them include theabstract and more than 3 million of these were pub-lished in the last 5 years (Hunter, 2006).To alleviate the information overload, someresources such as the Cochrane Collabo-ration (Cochrane, 2007), Evidence-BasedMedicine (EBM, 2007), the ACP JournalClub (ACP, 2007) and BMJ Clinical Evi-dence (BMJCE, 2007), employ human expertsto summarize knowledge within RCTs throughextensive searches and critical assessments.In (Sim, 2000), RCT information is entered intoelectronic knowledge bases or ?trial banks?, eas-ing the task for systematic reviewing and criticalappraisal.
This project requires manual entry ofdescriptions about the design and execution (sub-jects, recruitment, treatment assignment, follow-up),and hence, only small numbers of RCTs have beenarchived thus far.The goal of our research is to use natural languageprocessing to extract the most important pieces of in-formation from RCTs for the purpose of automaticsummarization, tailored towards the medical prac-titioner?s clinical question at hand.
Ultimately, itis our vision that data mined from full text articlesof RCTs not only aid clinicians?
assessments but re-searchers who are conducting meta-analyses.In this paper, we examine the use of section head-ings that are frequently given in abstracts of medicaljournal articles.
These section headings are topic-independent.
Effectively they define the discoursestructure for the abstract, and provide semantic la-bels to the sentences that fall under them.121Other researchers have recognized the potentialutility of these heading (McKnight, 2003; Xu, 2006;Lin, 2006).
It has also been recognized that scien-tific abstracts with such labels could be of impor-tance to text summarization, information retrievaland question answering (Lee, 2006; Zweigenbaum,2003).
We share similar goals to previous research;the section headings of these structured medical ab-stracts can be used as training data for building la-belers that can tag unstructured abstracts with dis-course structure.
But also, there is a large numberof heading names.
Sentences that occur under theseheading names form a labeled training set whichcould be used to build a classifier that recognizessimilar sentences.
Ultimately, we would like to buildfiner-grained classifiers that exploit these semanticlabels.In our work, we seek to demonstrate that infor-mation about patient characteristics can now be ex-tracted from structured and unstructured abstracts.We are motivated by the fact that patient character-istics is one of the fundamental factors most perti-nent to evaluation of relevance to a clinical question.The total number of subjects in a trial reflects on thequality of the RCT, and additional factors such asage, gender and other co-existing conditions, will becrucial for assessing whether an RCT is relevant tothe medical practitioner?s immediate patient.This paper is organized as follows.
In Section 1we will describe how the RCT abstracts were ob-tained, and we present a study of the discourse head-ings that occur in our document corpus.
Section 3will detail our sentence classification experiments.We first explore classification in which the abstractsare labeled under five subheadings, one of which de-scribes the patients or population group.
We alsoperform classification using a combined two-stagescheme, bootstrapping from partially labeled data.Finally in Section 4, we consider how well the PAR-1.
RESULTS 6.
METHODS / RESULTS2.
METHODS 7.
OBJECTIVE3.
CONCLUSION 8.
PATIENTS / METHODS4.
BACKGROUND 9.
PURPOSE5.
CONCLUSION 10.
DESIGNTable 1: The most common headings in RCT ab-stracts.TICIPANTS labeled sentences capture sentences con-taining the total number of participants in a trial.
InSection 5, we will give a detailed analysis of the la-beled sentences.2 The Data2.1 Corpus CreationThe current corpus is obtained by a MEDLINE searchfor RCTs.
We did not constrain publications bytheir date.
For the purpose of constraining the sizeof our corpus in these preliminary experiments, itwas our intention to use RCTs pertaining to a fixedset of clinical conditions.
Hence, we conducted aMEDLINE search for RCTs with the following key-words: asthma, diabetes, breast cancer, prostate can-cer, erectile dysfunction, heart failure, cardiovascu-lar, angina.
The resultant corpus contains 7535 ab-stracts of which 4268 are structured.2.2 Structure of Medical AbstractsStructured abstracts were introduced in 1987 (Ad-Hoc, 2005) to help clinical readers to quickly se-lect appropriate articles, and allow more precise in-formation retrieval.
However, currently, the major-ity of medical abstracts remain unstructured.
Previ-ous studies have concluded that while many scien-tific abstracts follow consistent patterns (e.g.
Intro-duction, Problem, Method, Evaluation, Conclusion)many still contain missing sections or have differ-ing structures (Orasan, 2001; Swales, 1990; Meyer,1990).
Journals vary widely in their requirementsfor abstract structures.We have conducted a study of the structured ab-stracts in our corpus.
Of 4268 structured abstracts,we have found a total of 238 unique section head-ings.
The most common ones are shown in Table 1.To investigate the numbers of variations in the ab-stract structure, we first manually map headings thatClass Example Heading NamesAim AIM, AIMS, AIM OF THE STUDY..Setting SETTING, SETTINGS, STUDY SETTING..Participants PARTICIPANTS, PATIENTS, SUBJECTS..Setting/ PARTICIPANTS AND SETTINGS,Subjects SETTING/PATIENTS..Table 2: Examples of manual mappings for headingnames into equivalence classes.122Structure of Abstracts % of CorpusBACKGROUND, METHOD, RESULT, CONCLUSION 16%AIM, METHOD, RESULT, CONCLUSION 14%AIM, PATIENT AND METHOD, RESULT, CONCLUSION 8.5%BACKGROUND, AIM, METHOD, RESULT, CONCLUSION 7.6%BACKGROUND, METHOD AND RESULTS, CONCLUSION 6.6%AIM, PARTICIPANTS, DESIGN, MEASUREMENTS, RESULT, CONCLUSION <1%CONTEXT, DESIGN, SETTING, PARTICIPANTS, OUTCOME MEASURES, RESULT, CONCLUSION <1%AIM, DESIGN AND SETTING, PARTICIPANTS, INTERVENTION <1%MEASUREMENTS AND MAIN RESULTS, CONCLUSIONTable 3: Examples of the patterns that occur in the section headings of structured RCT abstracts.are essentially semantically equivalent to the sameclasses, resulting in 106 classes.
Examples of thesemappings are shown in Table 2.
After the classmappings are applied, it turns out that there are still400 different patterns in the combinations of sectionheadings in these medical abstracts, with over 90%of these variations occurring less than 10 times.
Themost common section heading patterns are shown inTable 3.
Some of the less common ones are alsoshown.In studying the structure of these medical ab-stracts, we find that the variation in structural or-dering is large, and many of the heading names areunique, chosen at the discretion of the paper author.Some of the most frequent heading names are alsocompound headings such as: METHODS/RESULTS,RESULTS/CONCLUSION, PATIENTS/RESULTS, SUB-JECTS AND SETTINGS.3 Sentence Classification Experiments3.1 Extracting Participant SentencesIn this work, we seek to build a classifier using train-ing data from the semantic labels already providedby structured abstracts.
It is our intention ultimatelyto label both structured and unstructured abstractswith the semantic labels that are of interest for thepurposes of information extraction and answeringspecific questions regarding the trial.
In our ap-proach, we identify in our structured abstracts theones with section headings about patient character-istics.
These are collapsed under one semantic classand used as training data for a classifier.From our 4268 structured abstracts, all the head-ing names are examined and are re-mapped by handto one of five heading names: AIM, METHOD, PAR-TICIPANTS, RESULTS, CONCLUSION.
Most head-ing names can be mapped to these general headingsbut the subset containing compound headings suchas METHOD/RESULT are discarded.All the abstracts are segmented into sentences andtokenized via Metamap (Aronson, 2001).
Some ab-stracts are discarded due to sentence segmentationerrors.
The remainder (3657 abstracts) forms thecorpus that we will work with here.
These abstractsare randomly divided into a training set and an initialtest set, and for purposes of our experiments, theyare further subdivided into abstracts with the PAR-TICIPANTS label and those without.
The exact sizeof our data sets are given in Table 4.Although abstracts in Train Set A are generallystructured as (AIM, METHOD, RESULTS, CONCLU-SION), they contain sentences pertaining to patientor population group largely in the METHOD sec-tion.
In the following, we will explore three waysfor labeling sentences in the abstract including label-ing for sentences that describe the population group.The first employs a 5-class classifier, the second usesa two-stage approach and the third employs an ap-proach which uses partially labeled data.3.2 Using Labeled Data OnlyUsing only abstracts from Train Set B, all sen-tences are mapped into one of 5 classes: AIM, PAR-TICIPANTS, METHOD, RESULTS, CONCLUSION.Data Set Number of Number ofAbstracts SentencesTotal in Corpus 3657 45kTotal Train Set 3439 42kTrain Set A (no PARTICIPANTS) 2643 32kTrain Set B (w/ PARTICIPANTS) 796 10kTest Set (w/ PARTICIPANTS) 62 878Table 4: Sizes of data sets.123Recall Precision F -scoreCRF Accuracy = 84.4%Aim 0.98 0.91 0.95Method 0.52 0.73 0.61Participants 0.79 0.73 0.76Results 0.95 0.87 0.91Conclusion 0.91 0.97 0.94SVM Accuracy = 80.2%Aim 0.87 0.91 0.90Method 0.64 0.68 0.67Participants 0.73 0.70 0.72Results 0.89 0.84 0.86Conclusion 0.80 0.88 0.83Table 5: Classification of sentences in RCT abstractsinto 5 semantic classes using CRFs and SVMs.
Therecall, precision and F -score are reported on our un-seen test set.The PARTICIPANTS class subsume all headingsthat include mention of population characteristics.These include compound headings such as: SET-TING/POPULATION, PATIENTS/DESIGN.
Sentencesassociated with these compound headings often in-clude long sentences that describe the participantgroup as well as a second aspect of the study suchas setting or design.We build a 5-class classifier using linear-chainconditional random fields (CRFs).1 CRFs (Sutton,2006) are undirected graphical models that are dis-criminatively trained to maximize the conditionalprobability of a set of output variables given a set ofinput variables.
We simply use bag-of-words as fea-tures because past studies (McKnight, 2003), usingn-gram-based features did not improve accuracies.2As a baseline comparison, we have performedclassification using a Support Vector Machine(SVM) classifier (Burges, 1998; Witten, 2005), witha radial basis functions (RBF) kernel.
To help modelthe sequential ordering, a normalized integer for thesentence number in the abstract is included as a fea-ture.Experimental results are shown in Table 5.
CRFsclearly outperform SVMs in this classification task.This may in part be attributable to the explicitsequential modeling in the CRFs compared with1We used the SimpleTagger command line interface of theMallet software package (McCallum, 2002).2In other experiments, attempts to use stemming and re-moval of stop words also did not improve performance.SVMs.
While our training set (796 abstracts in Trainset B) is substantially smaller than that reported inprevious studies (McKnight, 2003; Lin, 2006; Xu,2006), the F -score for AIM, RESULTS, CONCLU-SION are comparable to previous results.
By far thelargest sources of classification error are the confu-sions between METHOD and PARTICIPANTS class.In training we have included into the PARTICIPANTSclass all sentences that come under compoundheadings, and therefore the PARTICIPANTS sectioncan often encompass several sentences that containdetailed information regarding the intervention,and the type of study, as exemplified below.Doppler echocardiography was performed in 21 GH de-ficient patients after 4 months placebo and 4 months GHtherapy, in a double blind cross-over study.
In an opendesign study, 13 patients were reinvestigated following16 months and 9 patients following 38 months of GHtherapy.
Twenty-one age and sex-matched normal con-trol subjects were also investigated.Nonetheless, information about the patient popula-tion is embedded within these sentences.3.3 Using a Two-Stage MethodAn alternative approach is to adopt a two-stage hi-erarchical strategy.
First we build a classifier whichperforms a 4-way classification based on the labelsAIM, METHOD, RESULTS, CONCLUSION, and a sec-ond stage binary classifier tags all the METHOD sen-tences into either METHOD or PARTICIPANTS.
Thereare two distinct advantages to this approach.
(1) Inour 5-class classifier, it is clear that METHOD andPARTICIPANTS are confusable and a dedicated clas-sifier to perform this subtask may be more effective.
(2) The corpus of abstracts with only the 4 classeslabeled is much larger (3439 abstracts), and hencethe resultant classifier is likely to be trained morerobustly.
Our first stage classifier is a CRF tagger.It is trained on the combined training sets A and B,whereby all sentences in the structured abstracts aremapped to the 4-class labels.
The second stage bi-nary classifier is an SVM classifier.
The SVM clas-sifier has been augmented with additional features ofthe semantic labels tagged via Metamap tagger.
It istrained on the subset of Train Set A (3499 sentences)that is labeled as either METHOD or PARTICIPANTS.Classification results for the unseen test set are re-ported in Table 6.
The 4-class classifier yields F -scores between 0.92 and 0.96.
We report results for124(1) 4-class Accuracy = 92.7%Recall Precision F -scoreAim 0.98 0.94 0.96Method 0.89 0.95 0.92Results 0.95 0.89 0.92Conclusion 0.91 0.97 0.94(2) 2-class Accuracy = 80.1%Method 0.73 0.83 0.78Participants 0.87 0.78 0.81(3) 5-class Accuracy = 86.0%Aim 0.96 0.92 0.96Method 0.66 0.79 0.71Participants 0.77 0.72 0.75Results 0.94 0.89 0.92Conclusion 0.91 0.97 0.94Table 6: (1) Classification using CRFs into 4 ma-jor semantic classes with combined Train Set A andB as training data.
(2) Binary SVM classificationof a subset of test set sentences.
(3) Classificationinto 5 classes as described in Section 3.3.
All results(recall, precision and F -score) are reported on theunseen test set.the binary SVM classifier on the subset of test setsentences (253 sentences) that are either METHODor PARTICIPANTS in Table 6.The two stage method here has yielded somegains in performance for each class except for PAR-TICIPANTS.
The gains are likely to have been due toincreased training data particularly for the classes,AIM, RESULTS and CONCLUSION.3.4 Augmenting with Partially Labeled DataWe investigate a second method for leveraging thedata available in Train Set A.
We hypothesize thatmany sentences within the METHOD section of TrainSet A do in fact describe patient information andcould be used as training data.
We propose a boot-strapping method whereby some of the sentences inTrain Set A are tagged by a binary SVM classifierand used as training data in the 5-class CRF classi-fier.
The following describes each step:1.
A binary SVM classifier is trained on the sub-set of sentences in Train Set B labeled withMETHOD and PARTICIPANTS.2.
The trained SVM classifier is used to label allthe sentences in Train Set A that are originallylabeled with the METHOD class.Recall Precision F -score5-class Accuracy = 87.6%Aim 0.99 0.95 0.97Method 0.67 0.77 0.72Participants 0.90 0.77 0.83Results 0.91 0.92 0.92Conclusion 0.90 0.97 0.93Table 7: Classification into 5 classes as describedin Section 3.4.
All results (recall, precision and F -score) are reported on the unseen test set.3.
All the sentences in Train Set A are now labeledin terms of the 5 classes, and a score is avail-able from the SVM output is associated withthose sentences labeled as either METHOD orPARTICIPANTS.
The abstracts that contain sen-tences scoring above a pre-determined thresh-old score are then pooled with sentences inTrain Set B into a single training corpus.
Wetuned the threshold value by testing on a de-velopment set held out from Train Set B.
As aresult, 1217 sentences from Train Set A is com-bined with Train Set B.4.
The final training corpus is used to train a CRFtagger to label sentences into one of 5 classes.The results of classification on the unseen test setare reported in Table 7.
Overall accuracy for classifi-cation improves to 87.6% primarily because there isa marked improvement is observed for the F -scoresof the PARTICIPANTS class.
Our best results hereare comparable to those previously reported on sim-ilar tasks on the class, AIM, RESULTS and CON-CLUSION (Xu, 2006; Lin, 2006).
The F -score forMETHOD is lower because introducing a PARTICI-PANTS label has increased confusability.4 Extraction of Number of PatientsWe have demonstrated that for a structured abstractit is possible to predict sentences that are associ-ated with population characteristics.
However, ourultimate objective is to extract these kinds of sen-tences from unstructured abstracts, and even to ex-tract more fine-grained information.
In this section,we will examine whether labeling sentences into oneof 5 classes can aid us in the extraction of the totalnumber of patients from an RCT.125Abstracts w/ % tagged asTotal Subjects PARTICIPANTSStructured 46 87%Unstructured 103 72%Table 8: Extraction of the total number of subjectsin a trial in a human annotated test set, as describedin Section 4.24.1 AnnotationIn a concurrent annotation effort to label RCT ab-stracts, human annotators manually tagged a sepa-rate test set of 204 abstracts with the total numberof participants in each study.
Of the 204 abstracts,148 are unstructured and 56 are structured.
None ofthese 204 abstracts are part of the training set, de-scribed in this paper.4.2 ExperimentsThe abstracts from this annotated test set are pro-cessed by the classifier described in Section 3.4.
Forall the abstracts which mention the total number ofparticipants in the RCT, we compute the frequencyfor which this is included in the sentences labeled asPARTICIPANTS.
Results are depicted in Table 8.Upon subsequent examination of the test set, it isfound that only 82% (46/56) of the structured ab-stracts and 70% (103/148) of unstructured abstractscontain information about total number of partici-pants in the trial.
As seen in Table 8, in 87% of the46 structured abstracts, and in 72% of the 103 un-structured abstracts, the total number of participantsare mentioned in the labeled PARTICIPANTS sen-tences.
The extraction of the total number of partici-pants is significantly worse in unstructured abstractswhich do not adhere to the strict discourse structuresgiven by the headings of structured abstracts.
In13% (13/103) of the unstructured abstracts, the totalnumber of participants appears in the first sentence,which is usually tagged as the AIM.
It is evident thatin the absence of structure, patient information canoccur in any sentence in the abstract, or for that mat-ter, it may appear only in the body of the paper.
Ourmethod of training first on structured abstracts maybe a strong limitation to extraction of informationfrom unstructured abstracts.Even for the structured abstracts in the test set, 9%(4/46) of the set of abstracts containing populationnumber actually mention the number in the AIM orRESULTS section, rather than the METHOD or PAR-TICIPANTS.
Only 12 abstracts contain explicit head-ings referring to participants, where the total numberof subjects in the trial is mentioned under the corre-sponding heading.In this task, we only consider that total number ofsubjects enrolled in a study, and have yet to accountfor additional population numbers such as the dropout rate, the follow-up rate, or the number of sub-jects in each arm of a study.
These are often reportedin an abstract without mentioning the total numberof patients to begin with.
The classifier will tag sen-tences that describe these as PARTICIPANT sentencesnonetheless.5 Analysis and DiscussionWe will further analyze the potential for using sen-tences tagged as PARTICIPANTS as summaries ofpopulation characteristics for a trial.
Table 9 givessome examples of sentences tagged by the classifier.Sentences that appear under PARTICIPANTS instructured abstracts are often concise descriptions ofthe population group with details about age, gender,and conditions, as seen in Example 1.
Otherwise,they can also be extensive descriptions, providingselection criteria and some detail about method, asin Example 2.Examples 3 and 4 show sentences from the test setof Section 4.
Example 3 has been labeled as a PAR-TICIPANTS sentence by the classifier.
It describespatient characteristics, giving the population num-ber for each arm of the trial but does not reveal thetotal number of subjects.
Example 3 appears underthe heading METHODS AND RESULTS in the originalabstract.
Example 4 is from an unstructured abstract,where information about the intervention and popu-lation and study design are interleaved in the samesentences but tagged by the classifier as PARTICI-PANTS.
Many sentences tagged as PARTICIPANTSalso do not give explicit information about popula-tion numbers but only provide descriptors for patientcharacteristics.It is also plausible that our task has been mademore challenging compared with previous reportedstudies because our corpus has not been filtered forpublication date.
Hence, the numbers of publica-1261.
Male smokers aged 50?69 years who had angina pectoris in the Rose chest pain questionnaire at baseline (n = 1795).PMID: 96591912.
The study included 809 patients under 70 years of age with stable angina pectoris.
The mean age of the patients was 59 +/-7 years and 31% were women.
Exclusion criteria were myocardial infarction within the previous 3 years and contraindicationsto beta-blockers and calcium antagonists.
The patients were followed between 6 and 75 months (median 3.4 years and a totalof 2887 patient years).
PMID: 86821343.
Subjects with Canadian Cardiovascular Society (CCS) class 3/4 angina and reversible perfusion defects were randomizedto SCS (34) or PMR (34).
PMID: 165543134.
Sixty healthy women, half of whom had been using OCs for at least the previous 6 months, participated in the study.
Approx-imately two thirds were smokers and were randomized to be tested after either a 12 hr nicotine deprivation or administrationof nicotine gum.
One third were nonsmokers.
PMID: 11495215Table 9: Examples of sentences labeled under PARTICIPANTS class, forming summaries of the populationcharacteristics of a trial.
Examples 1 and 2 are typical sentences under the PARTICIPANTS heading in thetrain set.
Examples 3 and 4 are from the annotated test set.
See Section 5 for more detailed explanation.tions and structural characteristics of our abstractsmay be broader than previous reports which filter forabstracts to a narrow time frame (Xu, 2006).6 Related WorkIn recent years, there has been a growth in researchin information extraction and NLP in the medicaldomain particularly in the RCT literature.
This isdue in part to the emergence of lexical and seman-tic resources such as the Unified Medical LanguageSystem (UMLS) (Lindberg, 1993), and softwaresuch as MetaMap (Aronson, 2001), which trans-forms text into UMLS concepts, and SemRep (Rind-flesch, 2003), which identifies semantic proposi-tions.There are a number of previous attempts to per-form text categorization on sentences in MEDLINEabstracts into generic discourse level section head-ings.
They all share the goal of assigning structureto unstructured abstracts for the purpose of sum-marization or question answering.
All previous at-tempts have mapped the given headings to four orfive generic classes, and performed text categoriza-tion on large sets of RCTs without any disease orcondition-specific filtering.
Studies have shown thatresults deteriorate when classifying sentences in un-structured abstracts (McKnight, 2003; Lin, 2006).In (McKnight, 2003), McKnight and Srinivisan usedan SVM for tagging sentences into 4 classes.
Usinga corpus of 7k abstracts, they obtain F -scores from0.82 to 0.89.
Later papers in (Xu, 2006; Lin, 2006)have found that Hidden Markov Models (HMMs)based approaches more effectively model the se-quential ordering of sentences in abstracts.
In (Xu,2006), several machine learning methods, decisiontree, maximum entropy and naive Bayes, are evalu-ated with an HMM-based algorithm.
3.8k abstractsfrom 2004 and 2005 were used as training data, andexperiments yielded average precision of 0.94 andrecall of 0.93.One driving model for information extraction inRCTs is the PICO framework (Richardson, 1995).This is a task-based model for EBM formulated toassist EBM practitioners to articulate well-formedquestions in order to find useful answers in clinicalscenarios.
PICO elements are Patient/Population,Intervention, Comparison and Outcome.
This modelhas been adopted by researchers (Demner-Fushman,2005; Niu, 2004) as a guideline for elements that canbe automatically extracted from RCTs and patientrecords.
However, doubts have been raised aboutthe utility of PICO as a generic knowledge repre-sentation for computational approaches to answer-ing clinical questions (Huang, 2006).In experiments reported in (Demner-Fushman,2005), the PICO framework was used as a basisfor extracting population, problem, intervention andcomparison for the purpose of evaluating relevanceof an abstract to a particular clinical question.
In thiswork, the population statements were located via aset of hand-written rules that were based on extract-ing an actual numeric value for the population.7 ConclusionsIn this study, we investigated the use of conditionalrandom fields for classifying sentences in medicalabstracts.
Our results particularly in terms of F -scores for generic section headings such as AIM, RE-127SULTS and CONCLUSION were comparable to previ-ous studies, even with smaller training sets.
We in-vestigated the use of text classification by leveragingthe subset of abstracts with explicitly labeled PAR-TICIPANTS sentences combining the use of CRFsand SVMs, and exploiting partially labeled data.One main objective here is to label sentences thatdescribe population characteristics in structured andunstructured abstracts.
We found that unstructuredabstracts differ substantially from structured ones,and alternative approaches will be necessary forextracting information from unstructured abstracts.Furthermore, critical details that are needed by aphysician when evaluating a study such as exclusioncriteria, drop out rate, follow up rate, etc, may onlybe listed in the full text of the study.
Future workwill address extracting information beyond the ab-stract.8 AcknowledgmentThe authors would like to acknowledge the anony-mous reviewers and the executive committee fortheir comments and suggestions, and MarianneByrne, Brenda Anyango Omune and Wei Shin Yufor annotation of the abstracts.
This project isfunded by the Australian Research Council, grantnumber DP0666600.ReferencesACP Journal Club.
Available from: http://www.acpjp.orgAd Hoc working group for Critical Appraisal of the MedicalLiterature 1987.
A proposal for more informative abstractsof clinical articles.
Annals of Int.
Medicine 106:595?604.A.
R. Aronson.
2001.
Effective mapping of biomedical textto the UMLS Metathesaurus: The MetaMap program.
Ann.Symp.
of AMIA pp 17?21.Clinical Evidence.
BMJ Publishing Group.
Available from:http://www.clinicalevidence.comC.
Burges.
1998.
A Tutorial on Support Vector Machines forPattern Recognition Journal Data Mining and KnowledgeDiscovery, 2(2), June.The Cochrane Collaboration.
Available from:http://www.cochrane.orgD.
Demner-Fushman and J. Lin.
2005.
Knowledge extractionfor clinical question answering: Preliminary results.
AAAIWorkshop on Question Answering in Restricted Domains.Evidence Based Medicine.
Available from:http://ebm.bmjjournals.comX.
Huang et al 2006.
Evaluation of PICO as a KnowledgeRepresentation for Clinical Questions.
Ann.
Symp.
of AMIApp359?363.L.
Hunter and K. Bretonnel Cohen.
2006.
Biomedical lan-guage processing: what?s beyond PubMed?
Molecular Cell,21:589-594.J.
Lin et al 2006.
Generative Content Models for StructuralAnalysis of Medical Abstracts.
Workshop on BiomedicalNatural Language Processing BioNLP New York.D.
A. Lindberg et al 1993.
The Unified Medical LanguageSystem.
Methods of Information in Medicine, 32(4):281?291.A.
McCallum.
2002.
MALLET: A Machine Learning for Lan-guage Toolkit.
http://mallet.cs.umass.edu.L.
McKnight and P. Srinivasan 2003.
Categorization of Sen-tence Types in Medical Abstracts.
Ann.
Symp.
of AMIApp440?444.M.
Lee et al 2006.
Beyond Information Retrieval?MedicalQuestion Answering.
Ann.
Symp.
of AMIA.Y.
Niu and G. Hirst.
2005.
Analysis of semantic classes inmedical text for question answering.
Workshop on QuestionAnswering in Restricted Domains, Barcelona.C.
Orasan.
2001.
Patterns in Scientific Abstracts.
2001 CorpusLinguistics Conference.W.
S. Richardson et al 1995.
The well-built clinical ques-tion: a key to evidence-based decisions.
ACP J Club, Nov-Dec;123(3):A12-3.T.
Rindflesch and M. Fiszman.
2003.
The interaction of domainknowledge and linguistic structure in natural language pro-cessing: Interpreting hypernymic propositions in biomedicaltext.
J. of Biomedical Informatics, 36(6):462?477, Dec.D.
L. Sackett et al.
1998.
Evidence Based Medicine: How toPractice and Teach EBM.
Churchill Livingstone, Edinburgh.F.
Salager-Meyer.
1990.
Discourse Movements in Medical En-glish Abstracts and their linguistic exponents: A genre anal-ysis study.
INTERFACE: J. of Applied Linguistics, 4(2):107?124.I.
Sim et al 2000.
Electronic Trial Banks: A ComplementaryMethod for Reporting Randomized Trials.
Med Decis Mak-ing, Oct-Dec;20(4):440-50.C.
Sutton and A McCallum.
2006.
An introduction to condi-tional random fields for relational learning.
In Lise Getoorand Ben Taskar, editors, Introduction to Statistical Rela-tional Learning.
MIT Press.
To appear.J.
Swales.
1990.
Genre Analysis: English in Academic andResearch Settings.
Cambridge University Press, CambridgeUniversity.I.
H. Witten and E. Frank.
2005.
Data Mining: Practical ma-chine learning tools and techniques, 2nd Ed, Morgan Kauf-mann, San Francisco.R.
Xu et al 2006.
Combining Text Classification and HiddenMarkov Modeling Techniques for Structuring RandomizedClinical Trial Abstracts.
Ann.
Symp.
of AMIA.P.
Zweigenbaum.
2003.
Question answering in biomedicine.Workshop on Natural Language Processing for Question An-swering, Budapest.128
