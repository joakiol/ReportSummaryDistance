Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 455?466,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsCorroborating Text Evaluation Results with Heterogeneous MeasuresEnrique Amigo?
?
Julio Gonzalo ?
Jesu?s Gime?nez ?
Felisa Verdejo??
UNED, Madrid{enrique,julio,felisa}@lsi.uned.es?
UPC, Barcelona{jgimenez}@lsi.upc.eduAbstractAutomatically produced texts (e.g.
transla-tions or summaries) are usually evaluated withn-gram based measures such as BLEU orROUGE, while the wide set of more sophisti-cated measures that have been proposed in thelast years remains largely ignored for practicalpurposes.
In this paper we first present an in-depth analysis of the state of the art in orderto clarify this issue.
After this, we formalizeand verify empirically a set of properties thatevery text evaluation measure based on simi-larity to human-produced references satisfies.These properties imply that corroborating sys-tem improvements with additional measuresalways increases the overall reliability of theevaluation process.
In addition, the greater theheterogeneity of the measures (which is mea-surable) the higher their combined reliability.These results support the use of heterogeneousmeasures in order to consolidate text evalua-tion results.1 IntroductionThe automatic evaluation of textual outputs is acore issue in many Natural Language Processing(NLP) tasks such as Natural Language Generation,Machine Translation (MT) and Automatic Sum-marization (AS).
State-of-the-art automatic evalu-ation methods all operate by rewarding similari-ties between automatically-produced candidate out-puts and manually-produced reference solutions, so-called human references or models.Over the last decade, a wide variety of measures,based on different quality assumptions, have beenproposed.
Recent work suggests exploiting exter-nal knowledge sources and/or deep linguistic an-notation, and measure combination (see Section 2).However, original measures based on lexical match-ing, such as BLEU (Papineni et al, 2001a) andROUGE (Lin, 2004) are still preferred as de factostandards in MT and AS, respectively.
There are,in our opinion, two main reasons behind this fact.First, the use of a common measure certainly allowsresearchers to carry out objective comparisons be-tween their work and other published results.
Sec-ond, the advantages of novel measures are not easyto demonstrate in terms of correlation with humanjudgements.Our goal is not to answer which is the most re-liable metric or to propose yet another novel mea-sure.
Rather than this, we first analyze in depth thestate of the art, concluding that it is not easy to de-termine the reliability of a measure.
In absence of aclear proof of the advantages of novel measures, sys-tem developers naturally tend to prefer well-knownstandard measures.
Second, we formalize and checkempirically two intrinsic properties that any evalua-tion measure based on similarity to human-producedreferences satisfies.
Assuming that a measure satis-fies a set of basic formal constraints, these propertiesimply that corroborating a system comparison withadditional measures always increases the overall re-liability of the evaluation process, even when theadded measures have a low correlation with humanjudgements.
In most papers, evaluation results arecorroborated with similar n-gram based measures(eg.
BLEU and ROUGE).
However, according toour second property, the greater the heterogeneity of455the measures (which is measurable) the higher theirreliability.
The practical implication is that, corrob-orating evaluation results with measures based onhigher linguistic levels increases the heterogeneity,and therefore, the reliability of evaluation results.2 State of the Art2.1 Individual measuresAmong NLP disciplines, MT probably has thewidest set of automatic evaluation measures.
Thedominant approach to automatic MT evaluation is,today, based on lexical metrics (also called n-grambased metrics).
These metrics work by rewardinglexical similarity between candidate translations anda set of manually-produced reference translations.Lexical metrics can be classified according to howthey compute similarity.
Some are based on edit dis-tance, e.g., WER (Nie?en et al, 2000), PER (Till-mann et al, 1997), and TER (Snover et al, 2006).Other metrics are based on computing lexical preci-sion, e.g., BLEU (Papineni et al, 2001b) and NIST(Doddington, 2002), lexical recall, e.g., ROUGE(Lin and Och, 2004a) and CDER (Leusch et al,2006), or a balance between the two, e.g., GTM(Melamed et al, 2003; Turian et al, 2003b), ME-TEOR (Banerjee and Lavie, 2005), BLANC (Lita etal., 2005), SIA (Liu and Gildea, 2006), MAXSIM(Chan and Ng, 2008), and Ol (Gime?nez, 2008).The lexical measure BLEU has been criticized inmany ways.
Some drawbacks of BLEU are the lackof interpretability (Turian et al, 2003a), the fact thatit is not necessary to increase BLEU to improve sys-tems (Callison-burch and Osborne, 2006), the over-scoring of statistical MT systems (Le and Przybocki,2005), the low reliability over rich morphology lan-guages (Homola et al, 2009), or even the fact that apoor system translation of a book can obtain higherBLEU results than a manually produced translation(Culy and Riehemann, 2003).The reaction to these criticisms has been focusedon the development of more sophisticated measuresin which candidate and reference translations areautomatically annotated and compared at differentlinguistic levels.
Some of the features employedinclude parts of speech (Popovic and Ney, 2007;Gime?nez and Ma`rquez, 2007), syntactic dependen-cies (Liu and Gildea, 2005; Gime?nez and Ma`rquez,2007; Owczarzak et al, 2007a; Owczarzak et al,2007b; Owczarzak et al, 2008; Chan and Ng,2008; Kahn et al, 2009), CCG parsing (Mehay andBrew, 2007), syntactic constituents (Liu and Gildea,2005; Gime?nez and Ma`rquez, 2007), named entities(Reeder et al, 2001; Gime?nez and Ma`rquez, 2007),semantic roles (Gime?nez and Ma`rquez, 2007), dis-course representations (Gime?nez, 2008), and textualentailment features (Pado?
et al, 2009).
In general,when a higher linguistic level is incorporated, lin-guistic features at lower levels are preserved.The proposals for summarization evaluation areless numerous.
Some proposals for AS tasks arebased on syntactic units (Tratz and Hovy, 2008), de-pendency triples (Owczarzak, 2009) or convolutionkernels (Hirao et al, 2005) which reported some re-liability improvement over ROUGE in terms of cor-relation with human judgements.In general, however, it is not easy to determineclearly the contribution of deeper linguistic knowl-edge in those proposals.
In the case of MT, im-provements versus BLEU have been reported (Liuand Gildea, 2005; Kahn et al, 2009), but not overa more elaborated metric such as METEOR (Mehayand Brew, 2007; Chan and Ng, 2008).
Besides, con-troversial results on their performance at sentence vssystem level have been reported in shared evaluationtasks (Callison-Burch et al, 2008; Callison-Burch etal., 2009; Callison-Burch et al, 2010).2.2 Combined measuresSeveral researchers have suggested integrating het-erogeneous measures.
Some of them optimize themeasure combination function according to the met-ric?s ability to emulate the behavior of human as-sessors (i.e., correlation with human assessments).For instance, using linear combinations (Pado?
et al,2009; Liu and Gildea, 2007; Gime?nez and Ma`rquez,2008), Decision Trees (Akiba et al, 2001; Quirk,2004), regression based algorithms (Paul et al,2007; Albrecht and Hwa, 2007a; Albrecht and Hwa,2007b) or a variety of supervised machine learn-ing algorithms(Quirk et al, 2005; Corston-Oliver etal., 2001; Kulesza and Shieber, 2004; Gamon et al,2005; Amigo?
et al, 2005).Some of these works report evidence on the con-tribution of combining heterogeneous measures.
Forinstance, Albrecht and Hwa included syntax-based456measures together with lexical measures, outper-forming other combination schemes (Albrecht andHwa, 2007a; Albrecht and Hwa, 2007b).
Liu andGildea, after examining the contribution of eachcomponent metric, found that ?metrics showing dif-ferent properties of a sentence are more likely tomake a good combined metric?
(Liu and Gildea,2007).
Akiba et al, which combined multiple edit-distance features based on lexical, morphosyntac-tic and lexical semantic information, observed thattheir approach improved single editing distance forseveral data sets (Akiba et al, 2001).
More evi-dence was provided by Corston and Oliver.
Theyshowed that results on the task of discriminating be-tween manual and automatic translations improvewhen combining linguistic and n-gram based fea-tures.
In addition, they showed that this mixed com-bination improved over the combination of linguisticor n-gram based measures alone (Corston-Oliver etal., 2001).
(Pado?
et al, 2009) reported a reliabilityimprovement by including measures based on tex-tual entailment in the set.
In (Gime?nez and Ma`rquez,2008), a simple arithmetic mean of scores for com-bining measures at different linguistic levels was ap-plied with remarkable results in recent shared evalu-ation tasks (Callison-Burch et al, 2010).2.3 Meta-evaluation criteriaMeta-evaluation methods have been gradually intro-duced together with evaluation measures.
For in-stance, Papineni et al (2001b) evaluated the reliabil-ity of the BLEU metric according to its ability to em-ulate human assessors, as measured in terms of Pear-son correlation with human assessments of adequacyand fluency at the document level.
The measureNIST (Doddington, 2002) was meta-evaluated alsoin terms of correlation with human assessments, butover different document sources and for a varyingnumber of references and segment sizes.
Melamedet al (2003) argued, at the time of introducing theGTM metric, that Pearson correlation coefficientscan be affected by scale properties.
They suggestedusing the non-parametric Spearman correlation co-efficients instead.
Lin and Och meta-evaluatedROUGE over both Pearson and Spearman correla-tion over a wide set of metrics, including NIST,WER, PER, and variants of ROUGE, BLEU andGTM.
They obtained similar results in both cases(Lin and Och, 2004a).
Banerjee and Lavie (2005)argued that the reliability of metrics at the documentlevel can be due to averaging effects but might notbe robust across sentence translations.
In order toaddress this issue, they computed the translation-by-translation correlation with human assessments (i.e.,correlation at the sentence level).However, correlation with human judgements isnot enough to determine the reliability of measures.First, correlation at sentence level (unlike correla-tion at system level) tends to be low and difficult tointerpret.
Second, correlation at system and segmentlevels can produce contradictory results.
In (Amigo?et al, 2009) it is observed that higher linguistic lev-els in measures increases the correlation with humanjudgements at the system level at the cost of corre-lation at the segment level.
As far as we know, aclear explanation for these phenomena has not beenprovided yet.Third, a high correlation at system level doesnot ensure a high reliability.
Culy and Riehemanobserved that, although BLEU can achieve a highcorrelation at system level in some test suites, itover-scores a poor automatic translation of ?TomSawyer?
against a human produced translation (Culyand Riehemann, 2003).
This meta-evaluation crite-rion based on the ability to discern between man-ual and automatic translations have been referred toas human likeness (Amigo?
et al, 2006), in contrastto correlation with human judgements which is re-ferred to as human acceptability.
Examples of meta-measures based on this criterion are ORANGE (Linand Och, 2004b) and KING (Amigo?
et al, 2005).In addition, many of the approaches to metric com-bination described in Section 2.2 take human like-ness as the optimization criterion (Corston-Oliveret al, 2001; Kulesza and Shieber, 2004; Gamon etal., 2005).
The main advantage of meta-evaluationbased on human likeness is that, since human as-sessments are not required, metrics can be evaluatedover larger test beds.
However, the meta-evaluationin terms of human likeness is difficult to interpret.2.4 The use of evaluation measuresIn general, the state of the art includes a wide setof results that show the drawbacks of n-gram basedmeasures as BLEU, and a wide set of proposals fornew single and combined measures which are meta-457evaluated in terms of human acceptability (i.e., theirability to emulate human judges, typically measuredin terms of correlation with human judgements) orhuman-likeness (i.e., their ability to discern betweenautomatic and human translations) (Amigo?
et al,2006).
However, the original measures BLEU andROUGE are still preferred.We believe that one of the reasons is the lack ofan in-depth study on to what extent providing ad-ditional evaluation results with other metrics con-tributes to the reliability of such results.
The state ofthe art suggests that the use of heterogeneous mea-sures can improve the evaluation reliability.
How-ever, as far as we know, there is no comprehen-sive analysis on the contribution of novel measureswhen corroborating evaluation results with addi-tional measures.3 Similarity Based Evaluation MeasuresIn general, automatic evaluation measures appliedin tasks like MT or AS are similarity measures be-tween system outputs and human references.
Thesemeasures are related with precision, recall or overlapover specific types of linguistic units.
For instance,ROUGE measures n-gram recall.
Other measuresthat work at higher linguistic levels apply precision,recall or overlap of linguistic components such asdependency relations, grammatical categories, se-mantic roles, etc.In order to delimit our hypothesis, let us first de-fine what is a similarity measure in this context.
Un-fortunately, as far as we know, there is no formalconcept covering the properties of current evaluationsimilarity measures.
A close concept is that of ?met-ric?
or ?distance function?.
But, actually, measuressuch as ROUGE or BLEU are not proper ?metrics?,because they do not satisfy the symmetry and the tri-angle inequality properties.
Therefore, we need anew definition.Being ?
the universe of system outputs s andgold-standards g, we assume that a similarity mea-sure, in our context, is a function x : ?2 ??
< suchthat there exists a decomposition function f : ?
??
{e1..en} (e.g., words or other linguistic units orrelationships) satisfying the following constraints:(i) maximum similarity is achieved only when thenthe decomposition of the system output resemblesexactly the gold-standard decomposition; and (ii)growing overlap or removing non overlapped ele-ments implies growing x.
Formally, if x ranges from0 to 1:f(s) = f(g)?
x(s, g) = 1(f(s) = f(s?)
?
{e ?
f(g) \ f(s?)})?
x(s, g) > x(s?, g)(f(s) = f(s?)?
{e ?
f(s?)
\ f(g)})?
x(s, g) > x(s?, g)For instance, a random function and the reversalof a similarity funtion (f ?
(s) = 1f(s) ) do not satisfythese constraints.
While the F measure over Pre-cision and Recall satisfies these constraints1, pre-cision and recall in isolation do not satisfy all ofthem: maximum recall can be achieved without re-sembling the goldstandard text decomposition; andmaximum precision can be achieved with only a fewoverlapped elements.BLEU (Papineni et al, 2001a) computes the n-gram precision while the metric ROUGE (Lin andOch, 2004a) computes the n-gram recall.
How-ever, in general, both metrics satisfy all the con-straints, given that BLEU includes a brevity penaltyand ROUGE penalizes or limits the system outputlength.
The measure METEOR creates an align-ment between the two strings (Banerjee and Lavie,2005).
This overlap-based measure satisfies also theprevious constraints.
Measures based on edit dis-tance over n-grams (Tillmann et al, 1997; Nie?enet al, 2000) or other linguistic units (Akiba et al,2001; Popovic and Ney, 2007) match also our def-inition of similarity measure.
The editing distanceis minimum when the two compared text are equal.The more the evaluated text contains elements fromthe gold-standard the more the editing distance is re-duced (higher similarity).
The word ordering can bealso expressed in terms of a decomposition function.A similar reasoning applies to every relevant mea-sure in the state-of-the art.4 Data Sets and Measures4.1 Data setsIn this paper, we provide empirical results forMT and AS.
For MT, we use the data sets fromthe Arabic-to-English (AE) and Chinese-to-English(CE) NIST MT Evaluation campaigns in 2004 and1There is an exception.
In an extreme case, when recall iszero, removing non overlapped elements does not modify the Fmeasure.458AE2004 CE2004 AE2005 CE2005#human-references 5 5 5 4#systems 5 10 7 10#system-outputs-assessed 5 10 6 5#system-outputs 1,353 1,788 1,056 1,082#outputs-assessed per-system 347 447 266 272Table 1: Description of the test beds from 2004 and 2005 NIST MT evaluation campaigns used in the experimentsthroughout the paper.DUC 2005 DUC 2006#human-references 3-4 3-4#systems 32 35#system-outputs-assessed 32 35#system-outputs 50 50#outputs-assessed per-system 50 50Table 2: Description of the test beds from 2005 and 2006 DUC evaluation campaigns used in the experiments through-out the paper.20052.
Both include two translations exercises: forthe 2005 campaign we contacted each participantindividually and asked for permission to use theirdata3.
In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a globalmeasure of quality (LDC, 2005).
Thus, human as-sessments are in a 2-10 scale.
For AS, we have usedthe AS test suites developed in the DUC 2005 andDUC 2006 evaluation campaigns4.
This AS taskwas to generate a question focused summary of 250words from a set of 25-50 documents to a complexquestion.
Summaries were evaluated according toseveral criteria.
Here, we will consider the respon-siveness judgements, in which the quality score wasan integer between 1 and 5.
See Tables 1 and 2 for abrief quantitative description of these test beds.2http://www.nist.gov/speech/tests/mt3We are grateful to a number of groups and companies whoresponded positively: University of Southern California Infor-mation Sciences Institute (ISI), University of Maryland (UMD),Johns Hopkins University & University of Cambridge (JHU-CU), IBM, University of Edinburgh, University of Aachen(RWTH), National Research Council of Canada (NRC), Chi-nese Academy of Sciences Institute of Computing Technology(ICT), Instituto Trentino di Cultura - Centro per la Ricerca Sci-entifica e Tecnologica(ITC-IRST), MITRE.4http://duc.nist.gov/4.2 MeasuresAs for evaluation measures, for MT we have used arich set of 64 measures provided within the ASIYAToolkit (Gime?nez and Ma`rquez, 2010)5.
This in-cludes measures operating at different linguistic lev-els: lexical, syntactic, and semantic.
At the lexicallevel this set includes variants of 8 measures em-ployed in the state of the art: BLEU, NIST, GTM,METEOR, ROUGE, WER, PER and TER.
In addi-tion, we have included a basic measure Ol that com-putes the lexical overlap without considering wordordering.
All these measures have similar granular-ity.
They use n-grams of a varying length as the ba-sic unit with additional information provided by lin-guistic tools.
The underlying similarity criteria in-clude precision, recall, overlap, or edit rate, and thedecomposition functions include words, dependencytree nodes (DP HWC, DP-Or, etc.
), constituencyparsing (CP-STM), discourse roles (DR-Or), seman-tic roles (SR-Or), named entities, etc.
Further detailson the measure set may be found in the ASIYA tech-nical manual (Gime?nez and Ma`rquez, 2010).According to our computations, our measurescover high and low correlations at both levels.
Cor-relation at system level spans between 0.63 and 0.95.Correlations at sentence level ranges from 0.18 up to0.54.
We will discriminate between two subsets of5http://www.lsi.upc.edu/?nlp/Asiya459measures.
The first one includes those that decom-pose the text into words, n-grams, stems or lexicalsemantic tags.
This set includes BLEU, ROUGE,NIST, GTM, PER and WER families.
We will re-fer to them as ?lexical?
measures.
The second setare those that consider deeper linguistic levels suchas parts of speech, syntactic dependencies, syntacticconstituents, etc.
We will refer to them as ?linguis-tic?
measures.In the case of automatic summarization (AS), wehave employed the standard variants of ROUGE(Lin, 2004).
These 7 measures are ROUGE-{1..4},ROUGE-SU, ROUGE-L and ROUGE-W.
In addi-tion we have included the reversed precision versionfor each variant and the F measure of both.
Noticethat the original ROUGE measures are oriented torecall.
In total, we have 21 measures for the sum-marization task.
All of them are based on n-gramoverlap.5 Additive reliabilityAs discussed in Section 2, a number of recent pub-lications address the problem of measure combi-nation with successful results, specially when het-erogeneous measures are combined.
The followingproperty clarifies this issue and justifies the use ofheterogeneous measures when corroborating evalu-ation results.
It asserts that the reliability of systemimprovements always increases when the evaluationresult is corroborated by an additional similaritymeasure, regardless of the correlation achieved bythe additional measure in isolation.For the sake of clarity, in the rest of the paper,we will denote the similarity x(s, g) between sys-tem output s and human reference g by x(s).
Thequality of a system output s will be referred to asQ(s).
Let us define the reliability R(X) of a mea-sure set as the probability of a real improvement (asmeasured by human judges) when a score improve-ment is observed simultaneously for all measures inthe set X. :R(X) ?
P (Q(s) ?
Q(s?
)|x(s) ?
x(s?)
?x ?
X)According to this definition, we may not be ableto predict the quality of any system output (i.e.
atranslation) with a highly reliable measure set, butwe can ensure a system improvement when all mea-sures corroborate the result.
Then the additive relia-bility property can be stated as:R(X ?
{x}) ?
R(X)We could think of violating this property byadding, for instance, a measure consisting of a ran-dom function (x?
(s) = rand(0..1)) or a reversal ofthe original measure (x?
(s) = 1/x(s)).
These kindof measures, however, would not satisfy the con-straints defined in Section 3.This property is based on the idea that similar-ity with human references according to any aspectshould not imply statistically a quality decrease.
Al-though our test suites includes measures with lowcorrelation at segment and system level, we can con-firm empirically that all of them satisfy this property.We have developed the following experiment:taking all possible measure pairs in the test suites,we have compared their reliability as a set versus themaximal reliability of any of them (by computingthe difference R(X)?max(R(x1), R(x2)).
Figure1 shows the obtained distribution of this differencefor our MT and AS test suites.
Remarkably, in al-most every case this difference is positive.This result has a key implication: Corroboratingevaluation results with a new measure, even whenit has lower correlation with human judgements, in-creases the reliability of results.
Therefore, if thecorrelation with judgements is not determinant, thequestion is now what factor determines the contri-bution of the new measures.
According to the fol-lowing property, this factor is the heterogeneity ofmeasures.6 HeterogeneityThis property states that the reliability of any mea-sure combination is lower bounded by the hetero-geneity of the measure set.
In other words, a singlemeasure can be more or less reliable, but a systemimprovement according to all measures in an het-erogeneous set is reliable.Let us define the heterogeneity H(X) of a set ofmeasures X as, given two system outputs s and s?such that g 6= s 6= s?
6= g (g is the referencetext), the probability that there exist two measuresthat contradict each other.
That is:H(X) ?
P (?x, x?
?
X.x(s) > x(s?)
?
x?
(s) < x?(s?
))460Figure 1: Additive reliability for metric pairs.Thus, given a set X of measures, the propertystates that there exists a strict growing function Fsuch that:R(X) ?
F (H(X)) and H(X) = 1?
R(X) = 1In other words, the more the similarity measurestend to contradict each other, the more a unanimousimprovement over all similarity measures is reliable.Clearly, the harder it is that measures agree, the moremeaningful it is when they do.The first part is derived from the Additive Re-liability property.
Intuitively, any individual mea-sure has zero heterogeneity.
Increasing the hetero-geneity implies joining measures or measure setsprogressively.
According to the Additive Reliabil-ity property, this joining implies a reliability in-crease.
Therefore, the higher the heterogeneity, thehigher the minimum Reliability achieved by the cor-responding measure sets.The second part is derived from the Heterogeneitydefinition.
If H(X) = 1 then, for any distinct pairof outputs that differ from the reference, there existat least two measures in the set contradicting eachother.
That is, H(X) = 1 implies that:?s 6= s?
6= g(?x, x?
?
X.x(s) > x(s?)
?
x?
(s) < x?(s?
))Therefore, if one output improves the other ac-cording to all measures, then the output must beequal than the reference.?
(?x, x?
?
X.x(s) > x(s?)
?
x?
(s) < x?(s?
))?Figure 2: Heterogeneity vs. reliability in MT test suites.?
(g 6= s 6= s?
6= g)?
g = s ?
g = s?According to the first constraint of similarity mea-sures, a text that is equal to the reference achievesthe maximum score:g = s?
f(g) = g(s)?
?x.x(s) ?
x(s?
)Finally, if we assume that the reference (human pro-duced texts) has a maximum quality, then it willhave equal or higher quality than the other output.g = s?
Q(s) ?
Q(s?
)Therefore, the reliability of the measure set is maxi-mal.
In summary, if H(X) = 1 then:R(X) = P (Q(s) ?
Q(s?
)|x(s) ?
x(s?)
?x ?
X) == P (Q(s) ?
Q(s?
)|s = g) = 1Figures 2 and 3 show the relationship between theheterogeneity of randomly selected measure sets andtheir reliability for the MT and summarization testsuites.
As the figures show, the higher the hetero-geneity, the higher the reliability of the measure set.The results in AS are less pronounced due to the re-dundancy in ROUGE measure.Notice that the heterogeneity property does notnecessarily imply a high correlation between reli-ability and heterogeneity.
For instance, an idealsingle measure would have zero heterogeneity and461Figure 3: Heterogeneity vs. reliability in summarizationtest suites.achieve maximum reliability, appearing in the topleft area.
The property rather brings us to the fol-lowing situation: let us suppose that we have a setof single measures available which achieve a certainrange of reliability.
We can improve our system ac-cording to any of these measures.
Without humanassessments, we do not know what is the most re-liable measure.
But if we combine them, increas-ing the heterogeneity, the minimal reliability of theselected measures will be higher.
This implies thatcombining heterogeneous measures (e.g.
at high lin-guistic levels) that do not achieve high correlationin isolation, is better than corroborating results withany individual measure alone, such as ROUGE andBLEU, which is the common practice in the state ofthe art.The main drawback of this property is that in-creasing the heterogeneity implies a sensitivity re-duction.
For instance, if H(X) = 0.9, then onlyfor 10% of output pairs in the corpus there existsan improvement according to all measures.
In otherwords, unanimous evaluation results from heteroge-neous measures are reliable but harder to achieve forthe system developer.
The next section investigateson this issue.Finally, Figure 4 shows that linguistic measuresincrease the heterogeneity of measure sets.
We havegenerated sets of metrics of size 1 to 10 made upby lexical or lexical and linguistic metrics.
As thefigure shows, in the second case, the measure setsachieve a higher heterogeneity.Figure 4: Heterogeneity of lexical measures vs. lexicaland linguistic measures.7 Score thresholds vs.
Additive ReliabilityAccording to the previous properties, corroboratingevaluation results with several measures increasesthe reliability of evaluation results at the cost of sen-sitivity.
On the other hand, increasing the scorethreshold of a single measure should have a similareffect.
Which is then the best methodology to im-prove reliability?
In this section we provide exper-imental evidence on the relationship between bothways of increasing reliability: we have found that,corroborating evaluation results over single textswith additional measures is more reliable than re-quiring higher score differences according to any in-dividual measure in the set.
More specifically, wehave found that the reliability of a measure set ishigher than the reliability of each of the individualmeasures at a similar level of sensitivity.Formally, we define the sensitivity S(X) of a met-ric set X as the probability of finding a score im-provement within text pairs with a real (i.e.
humanassessed) quality improvement:S(X) = P (x(s) ?
x(s?
)?x ?
X|Q(s) ?
Q(s?
))Being Rth(x) and Sth(x) the reliability and sen-sitivity of a single measure x for a certain increasescore threshold th:462Figure 5: Heterogeneity vs. reliability Gain for MT testsuites.Rth(x) = P (Q(s) ?
Q(s?)|x(s)?
x(s?)
?
th)Sth(x) = P (x(s)?
x(s?)
?
th|Q(s) ?
Q(s?
))The property that we want to check is that, at thesame sensitivity level, combining measures is morereliable than increasing the score threshold of singlemeasures:S(X) = Sth(x).x ?
X ??
R(X) ?
Rth(x)Note that if we had a perfect measure xp such thatR(xp) = S(xp) = 1, then combining this measurewith a low reliability measure xl would produce alower sensitivity, but the maximal reliability wouldbe preserved.In order to confirm empirically this property, wehave developed the following experiment: (i) Wecompute the reliability and sensitivity of randomlychosen measure sets over single text pairs.
We havegenerated sets of 2,3,5,10,20 and 40 measures.
Inthe case of summarization corpora we have com-bined up to 20 measures.
In addition, we com-pute also the heterogeneity H(X) of each measureset; (ii) Experimenting with different values for thethreshold th, we compute the reliability of singlemeasures for all potential sensitivity levels; (iii) Foreach measure set, we compare the reliability of themeasure set versus the reliability of single measuresat the same sensitivity level.
We will refer to this asthe Reliability Gain:Figure 6: Heterogeneity vs. reliability Gain for MT testsuites.Reliability Gain =R(X)?max{Rth(x)/x ?
X ?
Sth(x) = S(X)}If there are several reliability values with the samesensitivity for a given single measures, we choosethe highest reliability value for the single measure.Figures 5 and 6 illustrate the results for the MTand AS corpora.
The horizontal axis represents theHeterogeneity of measure sets, while the verticalaxis represents the reliability gain.
Remarkably, thereliability gain is positive for all cases in our testsuites.
The maximum reliability gain is 0.34 in thecase of MT and 0.08 for AS (note that summariza-tion measures are more redundant in our corpora).In both test suites, the largest information gains areobtained with highly heterogeneous measure sets.In summary, given comparable measures in termsof reliability, corroborating evaluation results withseveral measures is more effective than optimizingsystems according to the best measure in the set.This empirical property provides an additional ev-idence in favour of the use of heterogeneous mea-sures and, in particular, of the use of linguistic mea-sures in combination with standard lexical measures.8 ConclusionsIn this paper, we have analyzed the state of the art inorder to clarify why novel text evaluation measures463are not exploited by the community.
Our first con-clusion is that it is not easy to determine the reliabil-ity of measures, which is highly corpus-dependentand often contradictory when comparing correlationwith human judgements at segment vs. system lev-els.In order to tackle this issue, we have studied anumber of properties that suggest the convenience ofusing heterogeneous measures to corroborate eval-uation results.
According to these properties, wecan ensure that, even when if we can not determinethe reliability of individual measures, corroboratinga system improvement with additional measures al-ways increases the reliability of the results.
In ad-dition, the more heterogeneous the measures em-ployed (which is measurable), the higher the relia-bility of the results.
But perhaps the most impor-tant practical finding is that the reliability at similarsensitivity levels by corroborating evaluation resultswith several measures is always higher than improv-ing systems according to any of the combined mea-sures in isolation.These properties point to the practical advantagesof considering linguistic knowledge (beyond lexi-cal information) in measures, even if they do notachieve a high correlation with human judgements.Our experiments show that linguistic knowledge in-creases the heterogeneity of measure sets, whichin turn increases the reliability of evaluation resultswhen corroborating system comparisons with sev-eral measures.AcknowledgementsThis work has been partially funded by the SpanishGovernment (Holopedia, TIN2010-21128-C02 andOpenMT-2, TIN2009-14675-C03) and the Euro-pean Community?s Seventh Framework Programme(FP7/2007-2013) under grant agreement number247762 (FAUST project, FP7-ICT-2009-4-247762).ReferencesYasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.2001.
Using Multiple Edit Distances to AutomaticallyRank Machine Translation Output.
In Proceedings ofMachine Translation Summit VIII, pages 15?20.Joshua Albrecht and Rebecca Hwa.
2007a.
A Re-examination of Machine Learning Approaches forSentence-Level MT Evaluation.
In Proceedings of the45th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 880?887.Joshua Albrecht and Rebecca Hwa.
2007b.
Regressionfor Sentence-Level MT Evaluation with Pseudo Refer-ences.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 296?303.Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and Fe-lisa Verdejo.
2005.
QARLA: a Framework for theEvaluation of Automatic Summarization.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pages 280?289.Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??sMa`rquez.
2006.
MT Evaluation: Human-Like vs. Hu-man Acceptable.
In Proceedings of the Joint 21st In-ternational Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics (COLING-ACL), pages 17?24.Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Fe-lisa Verdejo.
2009.
The contribution of linguis-tic features to automatic machine translation evalua-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 1 - Volume 1, ACL ?09,pages 306?314, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization.Chris Callison-burch and Miles Osborne.
2006.
Re-evaluating the role of bleu in machine translation re-search.
In In EACL, pages 249?256.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
Furthermeta-evaluation of machine translation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 70?106.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009 work-shop on statistical machine translation.
In Proceedingsof the Fourth Workshop on Statistical Machine Trans-lation.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53.
Revised August 2010.464Yee Seng Chan and Hwee Tou Ng.
2008.
MAXSIM:A maximum similarity metric for machine translationevaluation.
In Proceedings of ACL-08: HLT, pages55?62.Simon Corston-Oliver, Michael Gamon, and ChrisBrockett.
2001.
A Machine Learning Approach to theAutomatic Evaluation of Machine Translation.
In Pro-ceedings of the 39th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 140?147.Christopher Culy and Susanne Z. Riehemann.
2003.
TheLimits of N-gram Translation Evaluation Metrics.
InProceedings of MT-SUMMIT IX, pages 1?8.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the 2nd Inter-national Conference on Human Language Technology,pages 138?145.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-Level MT evaluation without refer-ence translations: beyond language modeling.
In Pro-ceedings of EAMT, pages 103?111.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2007.
LinguisticFeatures for Automatic Evaluation of HeterogeneousMT Systems.
In Proceedings of the ACL Workshop onStatistical Machine Translation, pages 256?264.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008.
Hetero-geneous Automatic MT Evaluation Through Non-Parametric Metric Combinations.
In Proceedings ofthe Third International Joint Conference on NaturalLanguage Processing (IJCNLP), pages 319?326.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010.
Asiya:An Open Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathemat-ical Linguistics, 1(94):77?86.Jesu?s Gime?nez.
2008.
Empirical Machine Transla-tion and its Evaluation.
Ph.D. thesis, UniversitatPolite`cnica de Catalunya.Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.2005.
Kernel-based approach for automatic evaluationof natural language generation technologies: Applica-tion to automatic summarization.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 145?152, Vancouver, British Columbia,Canada, October.
Association for Computational Lin-guistics.Petr Homola, Vladislav Kubon?, and Pavel Pecina.
2009.A simple automatic mt evaluation metric.
In Proceed-ings of the Fourth Workshop on Statistical MachineTranslation, StatMT ?09, pages 33?36, Stroudsburg,PA, USA.
Association for Computational Linguistics.Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.2009.
Expected Dependency Pair Match: Predictingtranslation quality with expected syntactic structure.Machine Translation.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the 10th International Conferenceon Theoretical and Methodological Issues in MachineTranslation (TMI), pages 75?84.LDC.
2005.
Linguistic Data Annotation Spec-ification: Assessment of Adequacy and Flu-ency in Translations.
Revision 1.5.
Tech-nical report, Linguistic Data Consortium.http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf.Audrey Le and Mark Przybocki.
2005.
NIST 2005 ma-chine translation evaluation official results.
In Officialrelease of automatic evaluation scores for all submis-sions, August.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006.CDER: Efficient MT Evaluation Using Block Move-ments.
In Proceedings of 11th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics (EACL), pages 241?248.Chin-Yew Lin and Franz Josef Och.
2004a.
Auto-matic Evaluation of Machine Translation Quality Us-ing Longest Common Subsequence and Skip-BigramStatics.
In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics (ACL).Chin-Yew Lin and Franz Josef Och.
2004b.
ORANGE: aMethod for Evaluating Automatic Evaluation Metricsfor Machine Translation.
In Proceedings of the 20thInternational Conference on Computational Linguis-tics (COLING).Chin-Yew Lin.
2004.
Rouge: A Package for Auto-matic Evaluation of Summaries.
In Marie-FrancineMoens and Stan Szpakowicz, editors, Text Summariza-tion Branches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.Lucian Vlad Lita, Monica Rogati, and Alon Lavie.
2005.BLANC: Learning Evaluation Metrics for MT.
InProceedings of the Joint Conference on Human Lan-guage Technology and Empirical Methods in NaturalLanguage Processing (HLT-EMNLP), pages 740?747.Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Proceed-ings of ACL Workshop on Intrinsic and Extrinsic Eval-uation Measures for MT and/or Summarization, pages25?32.Ding Liu and Daniel Gildea.
2006.
Stochastic Iter-ative Alignment for Machine Translation Evaluation.In Proceedings of the Joint 21st International Confer-ence on Computational Linguistics and the 44th An-nual Meeting of the Association for ComputationalLinguistics (COLING-ACL), pages 539?546.465Ding Liu and Daniel Gildea.
2007.
Source-LanguageFeatures and Maximum Correlation Training for Ma-chine Translation Evaluation.
In Proceedings of the2007 Meeting of the North American Chapter of theAssociation for Computational Linguistics (NAACL),pages 41?48.Dennis Mehay and Chris Brew.
2007.
BLEUATRE:Flattening Syntactic Dependencies for MT Evaluation.In Proceedings of the 11th Conference on Theoreti-cal and Methodological Issues in Machine Translation(TMI).I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Translation.
InProceedings of the Joint Conference on Human Lan-guage Technology and the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL).Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-mann Ney.
2000.
An Evaluation Tool for MachineTranslation: Fast Evaluation for MT Research.
In Pro-ceedings of the 2nd International Conference on Lan-guage Resources and Evaluation (LREC).Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007a.
Dependency-Based Automatic Evalua-tion for Machine Translation.
In Proceedings of SSST,NAACL-HLT/AMTA Workshop on Syntax and Struc-ture in Statistical Translation, pages 80?87.Karolina Owczarzak, Josef van Genabith, and Andy Way.2007b.
Labelled Dependencies in Machine Transla-tion Evaluation.
In Proceedings of the ACL Workshopon Statistical Machine Translation, pages 104?111.Karolina Owczarzak, Josef van Genabith, and Andy Way.2008.
Evaluating machine translation with lfg depen-dencies.
Machine Translation, 21(2):95?119.Karolina Owczarzak.
2009.
Depeval(summ):dependency-based evaluation for automatic sum-maries.
In ACL-IJCNLP ?09: Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP: Volume 1, pages190?198, Morristown, NJ, USA.
Association for Com-putational Linguistics.Sebastian Pado?, Michael Galley, Dan Jurafsky, andChristopher D. Manning.
2009.
Robust machinetranslation evaluation with entailment features.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 297?305.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001a.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 311?318, Philadelphia, jul.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001b.
Bleu: a method for automatic evalu-ation of machine translation, RC22176.
Technical re-port, IBM T.J. Watson Research Center.Michael Paul, Andrew Finch, and Eiichiro Sumita.
2007.Reducing Human Assessments of Machine Transla-tion Quality to Binary Classifiers.
In Proceedings ofthe 11th Conference on Theoretical and Methodologi-cal Issues in Machine Translation (TMI).Maja Popovic and Hermann Ney.
2007.
Word ErrorRates: Decomposition over POS classes and Applica-tions for Error Analysis.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages48?55, Prague, Czech Republic, June.
Association forComputational Linguistics.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency Treelet Translation: Syntactically InformedPhrasal SMT.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 271?279.Chris Quirk.
2004.
Training a Sentence-Level MachineTranslation Confidence Metric.
In Proceedings of the4th International Conference on Language Resourcesand Evaluation (LREC), pages 825?828.Florence Reeder, Keith Miller, Jennifer Doyon, and JohnWhite.
2001.
The Naming of Things and the Confu-sion of Tongues: an MT Metric.
In Proceedings ofthe Workshop on MT Evaluation ?Who did what towhom??
at Machine Translation Summit VIII, pages55?59.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human Anno-tation.
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas(AMTA), pages 223?231.Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zu-biaga, and H. Sawaf.
1997.
Accelerated DP basedSearch for Statistical Translation.
In Proceedings ofEuropean Conference on Speech Communication andTechnology.Stephen Tratz and Eduard Hovy.
2008.
Summarizationevaluation using transformed basic elements.
In InProceedings of TAC-08.
Gaithersburg, Maryland.Joseph Turian, Luke Shen, and I. Dan Melamed.
2003a.Evaluation of machine translation and its evaluation.In In Proceedings of MT Summit IX, pages 386?393.Joseph P. Turian, Luke Shen, and I. Dan Melamed.2003b.
Evaluation of Machine Translation and itsEvaluation.
In Proceedings of MT SUMMIT IX.466
