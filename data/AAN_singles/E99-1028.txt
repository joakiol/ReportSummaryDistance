Proceedings of EACL '99Word Sense Disambiguation in Untagged Text based on TermWeight LearningFumiyo  Fukumoto  and  Yosh imi  Suzuk i tDepar tment  of  Computer  Science and  Med ia  Eng ineer ing ,Yamanash i  Un ivers i ty4-3-11 Takeda ,  Ko fu  400-8511 Japan{fukumoto@skye.esb, ysuzuki@windermere.alpsl.esit }.yamanashi.ac.jpAbst rac tThis paper describes unsupervised learn-ing algorithm for disambiguating verbalword senses using term weight learning.In our method, collocations which char-acterise every sense are extracted usingsimilarity-based estimation.
For the re-sults, term weight learning is performed.Parameters of term weighting are thenestimated so as to maximise the colloca-tions which characterise every sense andminimise the other collocations.
The re-suits of experiment demonstrate the ef-fectiveness of the method.1 In t roduct ionOne of the major approaches to disambiguateword senses is supervised learning (Gale et al,1992), (Yarowsky, 1992), (Bruce and Janyce,1994), (Miller et al, 1994), (Niwa and Nitta,1994), (Luk, 1995), (Ng and Lee, 1996), (Wilksand Stevenson, 1998).
However, a major obstacleimpedes the acquisition of lexical knowledge fromcorpora, i.e.
the difficulties of manually sense-tagging a training corpus, since this limits the ap-plicability of many approaches to domains wherethis hard to acquire knowledge is already avail-able.This paper describes unsupervised learning al-gorithm for disambiguating verbal word senses us-ing term weight learning.
In our approach, anoverlapping clustering algorithm based on Mutualinformation-based (Mu) term weight learning be-tween a verb and a noun is applied to a set ofverbs.
It is preferable that Mu is not low (Mu(x,y)_> 3) for a reliable statistical analysis (Church etal., 1991).
However, this suffers from the problemof data sparseness, i.e.
the co-occurrences whichare used to represent every distinct senses doesnot appear in the test data.
To attack this prob-lem, for a low Mu value, we distinguish betweenunobserved co-occurrences that are likely to oc-cur in a new corpus and those that are not, byusing similarity-based estimation between two co-occurrences of words.
For the results, term weightlearning is performed.
Parameters of term weight-ing are then estimated so as to maximise the col-locations which characterise every sense and min-imise the other collocations.In the following sections, we first define a pol-ysemy from the viewpoint of clustering, then de-scribe how to extract collocations using similarity-based estimation.
Next, we present a clusteringmethod and a method for verbal word sense dis-ambiguation using the result of clustering.
Fi-nally, we report on an experiment in order to showthe effect of the method.2 Po lysemy in  ContextMost previous corpus-based WSD algorithms arebased on the fact that semantically similar wordsappear in a similar context.
Semantically sim-ilar verbs, for example, co-occur with the samenouns.
The following sentences from the WallStreet Journal show polysemous usages of take.
(sl) Coke has typically taken a minoritystake in such ventures.
(sl ') Guber and pepers tried to buy a stakein mgm in 1988.
(s2) That process of sorting out specifies islikely to take time.
(s2') We spent a lot of time and money inbuilding our group of stations.Let us consider a two-dimensional Euclidean spacespanned by the two axes, each associated withstake and time, and in which take is assigned avector whose value of the i-th dimension is thevalue of Mu between the verb and the noun as-signed to the i-th axis.
Take co-occurs with thetwo nouns, while buy and spend co-occur onlywith one of the two nouns.
Therefore, the dis-tances between take and these two verbs are large209Proceedings of EACL '99and the synonymy of take with them disappears?stakeAL>buytakel ~ -  o ~  takependtimeFigure 1: The decomposition of the verb takeIn order to capture the synonymy of take withthe two verbs correctly, one has to decompose thevector assigned to take into two component vec-tors, take l  and take2, each of which correspondsto one of the two distinct usages of take (in Figure1).
(we call them hypothetical verbs in the follow-ing).
The decomposition of a vector into a set ofits component vectors requires a proper decom-position of the context in which the word occurs.Furthermore, in a general situation, a polysemousverb co-occurs with a large group of nouns andone has to divide the group of nouns into a set ofsubgroups, each of which correctly characterisesthe context for a specific sense of the polysemousword.
Therefore, the algorithm has to be able todetermine when the context of a word should bedivided and how.The approach proposed in this paper explic-itly introduces new entities, i.e.
hypothetical verbswhen an entity is judged polysemous and asso-ciates them with contexts which are sub-contextsof the context of the original entity?
Our algorithmhas two basic operations, splitting and lumping?Splitting means to divide a polysemous verb intotwo hypothetical verbs and lumping means to com-bine two hypothetical verbs to make one verb outof them (Fukumoto and Tsujii, 1994).3 Ext rac t ion  o f  Co l locat ionsGiven a set of verbs, vl, v2,--., v,~, the algorithmproduces a set of semantic lusters, which are or-dered in the ascending order of their semantic de-viation values?
Semantic deviation is a measureof the deviation of the set in an n-dimensionalEuclidean space, where n is the number of nounswhich co-occur with the verbs?In our algorithm, if vi is non-polysemous, it be-longs to at least one of the resultant semantic lus-ters.
If it is polysemous, the algorithm splits itinto several hypothetical verbs and each of thembelongs to at least one of the clusters?
Table 1summarises the sample result from the set {close,open, end}.Table 1: Distinct senses of the verb 'close'Vi n Mu(vi ,n)c losel(open)c lose2(end)accountbankingacquisitionbookbottleannouncementconnectionconversationperiodpractice2.1162.0261.0724.4273.6501.6922.7454.8901.8762.564In Table 1, subsets 'open' and 'end' correspond tothe distinct senses of 'c lose' .
Mu(vi,n) is the valueof mutual information between a verb and a noun.If a polysemous verb is followed by a noun whichbelongs to a set of the nouns, the meaning of theverb within the sentence can be determined ac-cordingly, because a set of the nouns characterisesone of the possible senses of the verb.The basic assumption of our approach is thata polysemous verb could not be recognised cor-rectly if collocations which represent every dis-tinct senses of a polysemous verb were notweighted correctly.
In particular, for a low Muvalue, we have to distinguish between those unob-served co-occurrences that are likely to occur in anew corpus and those that are not.
We extractedthese collocations which represent every distinctsenses of a polysemous verb using similarity-basedestimation.
Let (wv, nq) and (w~i , nq) be two dif-ferent co-occurrence pairs.
We say that wv andnq are semantically related if w~i and nq are se-mantically related and (wp, nq) and (w~i , nq) aresemantically similar (Dagan et al, 1993).
Us-ing the estimation, collocations are extracted andterm weight learning is performed.
Parametersof term weighting are then estimated so as tomaximise the collocations which characterise ev-ery sense and minimise the other collocations.Let v be two senses, wp and wl, but not bejudged correctly.
Let N_Setl be a set of nounswhich co-occur with both v and wp, but do not co-occur with wl.
Let alo N.Set2 be a set of nounswhich co-occur with both v and wl, but do notco-occur with wp, and N-Set3 be a set of nounswhich co-occur with v, wp and wl.
Extractionof collocations using similarity-based estimation210Proceedings of EACL '99begin(a) for all nq E N_Sett - N_Set3 such that Mu(wp,nq) < 3t Extract wpi (1 < i < s) such that Mu(w~i, nq) > 3.
Here, s is the number of verbs whichco-occur with nqfor all w;ii f  w~i exists such that Sim(wp,w'pi ) > 0(a-l) then  parameters of Mu of(wp,nq) and (v,rtq) are set to a (1 < a)(a-2) else parameters of Mu of (wp,nq) and (V,nq) are set to ~ (0 </3 < 1)end_ifend_forend_for(b) for all n, E g_Set3 such that Mu(wp,rt,) >_ 3 and Mu(wt,n,) > 3t Extract wp~ (1 < i < t) such that Mu(w~, ~)  > 3.
Here, t is the number of verbs whichco-occur with n,for all w~ii f  w;, exists such that Sirn(wp,w'pl ) > 0 and Sirn(wt,w;i ) > 0then  parameters of Mu of (v,n.
), (wp,n.)
and (wl,n.)
are set to/3 (0 < /3 < 1)end_ifend_forend_forendFigure 2: Extraction of collocationsis shown in Figure 2 tIn Figure 2, (a-l) is the procedure to extractcollocations which were not weighted correctlyand (a-2) and (b) are the procedures to extractother words which were not weighted correctly.Sim(vi, v~) in Figure 2 is the similarity value ofvland v~ which is measured by the inner product oftheir normalised vectors, and is shown in formula(1).v i ?
~)~vi = (v~: , .
.
- , v~)(1){ Mu(vi,nj) ifMu(vi,nj) >_ 3vii = 0 otherwise (2)In formula (1), k is the number of nouns whichco-occur with vi.
vii is the Mu value between vland nj.We recall that wp and nq are semantically re-lated if w~i and nq are semantically related and(wv,n q) and (w'pi,nq) are semantically similar.
(a)' and nq are se- in Figure 2, we represent wpimantically related when Mu(w~i,nq) >__ 3.
Also,(wv,nq) and (w'pi,nq) are semantically similar ift For wt, we can replace wp with wt, nq 6 N_Sett -N_Sets with nq E N_Set, - N.Sets, and Sim(wp, w'pl)> 0 with Sirn(wt, w'pi) > O.Sim(wp, w~i ) > 0.
In (a)of  Figure 2, for example,when (wp,nq) is judged to be a collocation whichrepresents every distinct senses, we set Mu valuesof (wp,nq) and (v,nq) to a x Mu(wp,nq) and a xMu(v,r%), 1 < a.
On the other hand, when nqis judged not to be a collocation which representsevery distinct senses, we set Mu values of theseco-occurrence pairs to fl x Mu(wp,nq) and /3 xMu(v,nq), 0 < j3 < 1 24 C lus ter ing  a Set  o f  VerbsGiven a set of verbs, VG = {vl, -- -, vm}, the algo-rithm produces a set of semantic lusters, whichare sorted in ascending order of their semantic de-viation.
The deviation value of VG, Dev(VG) isshown in formula (3).Dev(VG)1 E (vo  ~)2191(~*m+7)  ~=: j__:(3)/3 and 7 are ob-tained by least square estimation 3 .
vii is the1 m Mu value between v{ and n i.
~ = ~-~i=lv i jIn the experiment, we set increment value of aand decrease value of/3 to 0.001.3 Using Wall Street Journal, we obtained 13 = 0.964and 7 = -0.495.211Proceedings of EACL '99is the j-th value of the centre of gravity.
\[ 0 \[ =1 n m 2 ~i~j=l (~ i  vii) is the length of the centre ofgravity.
In formula (3), a set with a smaller valueis considered semantically ess deviant.Figure 3 shows the flow of the clustering algo-rithm.
As shown in '( '  in Figure 3, the func-tion Make- In l t ia l -C luster -Set  applies to VGand produces all possible pairs of verbs withtheir semantic deviation values.
The result is alist of pairs called the ICS (Initial Cluster Set).The CCS (Created Cluster Set) shows the clus-ters which have been created so far.
The func-tion Make-Temporary -C lus ter -Set  retrievesthe clusters from the CCS which contain one ofthe verbs of Seti.
The results (Set~3) are passed tothe function Reeogn i t ion-o f -Po lysemy,  whichdetermines whether or not a verb is polysemous.Let v be an element included in both Seti andSet 3.
To determine whether v has two senses wp,where wp is an element of Seti, and wl, where wlis an element of Set3, we make two clusters, asshown in (4) and their merged cluster, as shownin (5).
{vl, wp}, {v=, wl,---, (4){v, wp,---, (5)Here, v and wp are verbs and wl, ?
?
-, w,~ are verbsor hypothetical verbs, wl, "-', wp, -.-, w,~ in (5)satisfy Dev(v, wi) < Dev(v,wj) (1 < i _< j < n).vl and v2 in (4) are new hypothetical verbs whichcorrespond to two distinct senses of v.If v is a polysemy, but is not recognised cor-rectly, then Ext ract ion-o f -Co l locat ions  shownin Figure 2 is applied.
In Extract ion-of -Col locations, for (4) and (5), a and /3 are es-timated so as to satisfy (6) and (7).D,v(,.,,,~,,)_< O~v(,,~,,,-.-,~,,,,.-,,=n) (6)Dev(v2,w,,...,w,~) < Oev(v,w,,...,wp,..,,w,~) (7)The whole process is repeated until the newly ob-tained cluster, Setx, contains all the verbs in theinput or the ICS is exhausted.5 Word  Sense  D isambiguat ionWe used the result of our clustering analysis,which consists of pairs of collocations of a distinctsense of a polysemous verb and a noun.Let v has senses vl, v2, "--, v,~.
The senseof a polysemous verb v is vi (1 < i < m) ift ~- Ej Mu(vi,ni) is largest among Ej Mu(vl,nj),?
.. and Et~ Mu(v,~,nj).
Here, t is the number ofnouns which co-occur with v within the five-worddistance.6 Exper imentThis section describes an experiment conductedto evaluate the performance of our method.6.1 DataThe data we have used is 1989 Wall Street Jour-nal (WSJ) in ACL/DCI  CD-ROM which consistsof 2,878,688 occurrences of part-of-speech taggedwords (Brill, 1992).
The inflected forms of thesame nouns and verbs are treated as single units.For example, 'book' and 'books' are treated as sin-gle units.
We obtained 5,940,193 word pairs in awindow size of 5 words, 2,743,974 different wordpairs.
From these, we selected collocations of averb and a noun.As a test data, we used 40 sets of verbs.
Weselected at most four senses for each verb, the bestsense, from among the set of the Collins dictionaryand thesaurus (McLeod, 1987), is determined bya human judge.6.2 Resu l tsThe results of the experiment are shown in Table2, Table 3 and Table 4.In Table 2, 3 and 4, every polysemous verb hastwo, three and four senses, respectively.
Column1 in Table 2, 3 and 4 shows the test data.
Theverb v is a polysemous verb and the remains howthese senses.
For example, ' cause '  of (1) in Table2 has two senses, 'effect' and 'produce'.
'Sentence'shows the number of sentences of occurrences ofa polysemous verb, and column 4 shows their dis-tributions.
'v' shows the number of polysemousverbs in the data.
W in Table 2 shows the num-ber of nouns which co-occur with wp and wl.
vn W shows the number of nouns which co-occurwith both v and W. In a similar way, W in Table3 and 4 shows the number of nouns which co-occurwith wp ~ w2 and wp ~ w3, respectively.
'Correct'shows the performance of our method.
'Total' inthe bottom of Table 4 shows the performance of40 sets of verbs.Table 2 shows when polysemous verbs have twosenses, the percentage attained at 80.0%.
Whenpolysemous verbs have three and four senses, thepercentage was 77.7% and 76.4%, respectively.This shows that there is no striking differenceamong them.
Column 8 and 9 in Table 2, 3 and4 show the results of collocations which were ex-tracted by our method.212Proceedings of EACL '99beginICS := Make- In i t ia l -C lus ter -Set (VG)vo  = {v~ l i = 1 , .
.
.
,  m} I t s  = {sa l , - - - ,  Set.,,,,;-,, }where Setp = {vi, vj} and Setq = {vk,vt} E ICS (1 ~ p < q < m) satisfy Dev(vi, vj) < Dev(vk,vtfor i :=  1 to ~ doi f  CCS = ?then Set 7 := Set~ i.e.
Seti is stored in CCS as a newly obtained clusterelse i f  Set a E CCS exists such that SeQ C Seththen Seti is removed from ICS and Set 7 := ?else i ffor all Seth E CCS doif  Setl fq Set,, = ?then  Set 7 := Seti i.e.
Seti is stored in CCS as a newly obtained clusterend_ifend_forelse Setz := Make-Temporary-C luster -Set(  Set~,CCS)( Set~ := Seth E CCS such that Seti M Seta ~?
?Set 7 := Recognlt ion-of-Polysemy( Seti,Set~ )i f  Set 7 was not recognised correctlythen  for v, wp and wl, doExtract lon-of-  C oUo cations.end..fori :=1end_ifend_.ifend_ifend_ifi f  Set 7 = VGthen  exit from the for_loop ;end_ifend_.forendFigure 3: Flow of the algorithmMu < 3 shows the number of nouns which satisfyMu(wp,n) < 3 or Mu(wt,n) <3.
'Correct' showsthe total number of collocations which could beestimated correctly.
Table 2 ~ 4 show that thefrequency of v is proportional to that of v M W.As a result, the larger the number of v M W is,the higher the percentage of correctness of collo-cations is.7 Re la ted  WorkUnsupervised learning approaches, i.e.
to de-termine the class membership of each object tobe classified in a sample without using sense-tagged training examples of correct classifications,is considered to have an advantage over supervisedlearning algorithms, as it does not require costlyhand-tagged training data.Schiitze and Zernik's methods avoid taggingeach occurrence in the training corpus.
Theirmethods associate each sense of a polysemousword with a set of its co-occurring words (Schutze,1992), (Zernik, 1991).
I fa  word has several senses,then the word is associated with several differentsets of co-occurring words, each of which corre-sponds to one of the senses of the word.
Theweakness of Schiitze and Zernik's method, how-ever, is that it solely relies on human intuition foridentifying different senses of a word, i.e.
the hu-man editor has to determine, by her/his intuition,how many senses a word has, and then identifythe sets of co-occurring words that correspond tothe different senses.213Proceedings of EACL '99Table 2: The result of disambiguation experiment(two senses)(6)\ [ _ _122"-~cause~ e~'ect ~ require a-~"-Telose, open,~ rrect(~"-'(fall, decline, win} \] 278"-~feel, think, sense T T 280{hit, attack, strike} I 250{leave, remain, go} \[ 183gcty t ~Olaccomplish, operate'}-- 216--{occur, happen, ~--{order, request, ar range- '~"~ 240"-~ass, adopt, ~274-'~roduce, create, g ro '~~"- - " "2~--~ush, attack, pull~-~s~ve,223"-{ship, put, send}{stop, end, move}{add, append, total}{keep, maintain, protect}Total215(77.3181(72.4160(87.4349(92.3)~ - ~  Correct(%)\]83(77.0)113(86.2)I169(87.5) JYarowsky used an unsupervised learning pro-cedure to perform noun WSD (Yarowsky, 1995).This algorithm requires a small number of trainingexamples to serve as a seed.
The result shows thatthe average percentage attained was 96.1% for 12nouns when the training data was a 460 millionword corpus, although Yarowsky uses only nounsand does not discuss distinguishing more than twosenses of a word.A more recent unsupervised approach is de-scribed in (Pedersen and Bruce, 1997).
Theypresented three unsupervised learning algorithmsthat distinguish the sense of an ambiguous word inuntagged text, i.e.
McQuitty's similarity analysis,Ward's minimum-variance method and the EM al-gorithm.
These algorithms assign each instanceof an ambiguous word to a known sense definitionbased solely on the values of automatically iden-tifiable features in text.
Their methods are per-haps the most similar to our present work.
Theyreported that disambiguating nouns is more suc-cessful rather than adjectives or verbs and the bestresult of verbs was McQuitty's method (71.8%),although they only tested 13 ambiguous words(of these, there are only 4 verbs).
Furthermore,each has at most three senses.
In future, we willcompare our method with their methods using thedata we used in our experiment.8 Conc lus ionIn this study, we proposed a method for disam-biguating verbal word senses using term weightlearning based on similarity-based estimation.The results showed that when polysemous verbshave two, three and four senses, the average per-centage attained at 80.0%, 77.7% and 76.4%, re-spectively.
Our method assumes that nouns whichco-occur with a polysemous verb is disambiguatedin advance.
In future, we will extend our methodto cope with this problem and also apply our214Proceedings of EACL '99Nunl(21)(22)(23)(24)(2s)(26)(27)(28)(29)(30)Table 3: The result of disambiguation experiment(three senses){catch, acquire, grab, watch}{complete, end, develop, fill}{gain, win, get, increase}{grow, increase, develop become}{operate, run, act, control}{rise, increase, appear, grow}{see, look, know, feel}{want, desire, search, lack}{lead, cause, guide, precede}{carry, bring, capture, behave}Total (3 senses)Sentence w__w__w__w__w__w__~ v v N HI Correct(%) Mu < 3 Correct(%)240 120(50.0) 447 432 180(75.0) 124 99(79.9)21(9.0)199(41.0)365 107(29.3) 727 450 280(76.7) 240 193(80.4)242(66.3)16(4.4)334 47(14.0) 527 467 270(80.8) 187 152(81.4)228(68.2)59(17.8)310 68(21.9) 903 651 241(77.7) 372 305(82.0)132(42.5)11o(35.6)232 76(32.7) 812 651 187(80.6) 311 255(82.3)83(35.7)73(31.6)276 51(18.4) 711 414 198(71.7) 372 294(79.1)137(49.6)88(32.0)318 128(40.2) 1,785 934 263(82.7) 497 414(83.4)162(50.9)28(8.9~267 66(24.7) 590 470 208(77.9) 198 159(80.8)53t19.8)148(55.5)183 139(75.9) 548 456 138(75.4) 274 221(80.9)38(20.7)6(3.4)186 142(76.3) 474 440 142(76.3) 207 167(80.7)39(20.9)5(2.8)2,711 1,573(56.5) 2,107(77.7)method to not only a verb but also a noun andan adjective sense disambiguation to evaluate ourmethod.AcknowledgmentsThe authors would like to thank the reviewersfor their valuable comments.
This work was sup-ported by the Grant-in-aid for the Japan Societyfor the Promotion of Science(JSPS).Re ferencesE.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proc.
of the 3rd Conference on Ap-plied Natural Language Processing, pages 152-155.R.
Bruce and W. Janyce.
1994.
Word-sense dis-ambiguation using decomposable models.
InProc.
of the 32nd Annual Meeting, pages 139-145.K.
W. Church, W. Gale, P. Hanks, and D. Hindte.1991.
Using statistics in lexical analysis.
InLezical acquisition: Ezploiting on-line resourcesto build a lezicon, pages 115-164.
(Zernik Uri(ed.
)), London, Lawrence Erlbaum Associates.I.
Dagan, P. Fernando, and L. Lilian.
1993.
Con-textual word similarity and estimation fromsparse data.
In Proc.
of the 31th Annual Meet-ing of the ACL, pages 164-171.F.
Fukumoto and J. Tsujii.
1994.
Automaticrecognition of verbal polysemy.
In Proc.
of the15th COLING, Kyoto, Japan, pages 762-768.W.
K. Gale, K. W. Church, and D. Yarowsky.1992.
A method for disambiguating word sensesin a large corpus.
In Computers and the Hu-manities, volume 26, pages 415-439.A.
K. Luk.
1995.
Statistical sense disambiguationwith relatively small corpora using dictionarydefinitions.
In Proc.
of the 335t Annual Meetingof ACL, pages 181-188.W.
T. McLeod.
1987.
The new collins dictionaryand thesaurus in one volume.
London, Harper-Collins Publishers.G.
Miller, C. Martin, L. Shari, L. Claudia, andR.
G. Thomas.
1994.
Using a semantic oncor-dance for sense identification.
In Proc.
of theARPA Workshop on Human Language Technol-ogy, pages 240-243.H.
T. Ng and H. B. Lee.
1996.
Integrating mul-tiple knowledge sources to disambiguate word215Proceedings of EACL '99Table 4: The result of disambiguation experiment(four senses)Num {v, wp, wl, w~, wa}(31) {develop, create, grow, improve, 187expand}(32) {face, confront, cover, lie, turn} 222(33) {get, become, lose, understand, 302catch}(34) {go, come, become, run, fit}(35) {make, create, do, get, behave} 227(36) {show, appear, inform, prove, 227expi'ess}(37) {take, buy, obtain, spend, bring} 246Sentence wp(%) v v N W Correct(%) Mu < 3 Correct(%)w~(%)117(62.5) 922 597 155(82.8) 253 218(86.1)34118.1 )412.1)32(17.3)54(24.3) 859 567 184(82.8) 178 154(86.5)103(46.3)12(s.4)53(24.0}88(29.1) 762 513 229(75.8) 424 365(86.2)98(~2.4)34(11.2182(27.3)217 101(46.5) 732 435 145(66.8) 374 302(80.9)66(30.4)36(16.5)14(6.6)123(54.1) 783 555 178(78.4) 435 370(85.2)28(12.3)58(25.5)18(8.1)121(53.3) 996 560 181(79.7) 258 214(83.2)16(7.0)40(17.6)50(22.1)20(8.1) 2,742 1,244 i79(72.7) 829 677(81.6)123(5o.o)42(17.o}6i(24.9)7(4.81 727 459 111(76.5) 394 300(76.2)53(36.5)2(1.5)83(57.2)2(1.1) 746 491 151(74.0) 341 272(79.7)81(39.7}8614~.1 }35(17.1)78(48.1) 798 533 123(75.9) 143 119(83.2)13(8.o)43(26.5)~8(17.4)(as)(39)(40){hold, keep, carry, reserve, 145accept }{raise, lift, increase, create, 204Collect}{draw, attract, pull, close, 162write}Total (4 senses)I Tot al2,139 11636(76.4)\[ 9,706\[ \[ \[ 7,572(75.6) II I Isense: An examplar-based approach.
In Proc.of the 34th Annual Meeting of ACL, pages 40-47.Y.
Niwa and Y. Nitta.
1994.
Co-occurrence vec-tors from corpora vs. distance vectors from dic-tionaries.
In Proc.
of 15th COLING, Kyoto,Japan, pages 304-309.T.
Pedersen and R. Bruce.
1997.
Distinguishingword senses in untagged text.
In Proc.
of the2nd Conference on Empirical Methods in Natu-ral Language Processing, pages 197-207.H.
Schutze.
1992.
Dimensions of meaning.
InProc.
of Supercomputing, pages 787-796.Y.
Wilks and M. Stevenson.
1998.
Word sense dis-ambiguation using optimised combinations ofknowledge sources.
In Proe.
of the COLING-ACL'98, pages 1398-1402.D.
Yarowsky.
1992.
Word sense disambiguationusing statistical models of roget's categoriestrained on large corpora.
In Proc.
of the l$thCOLING, pages 454--460.D.
Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
InProc.
of the 33rd Annual Meeting of the ACL,pages 189-196.U.
Zernik.
1991.
Trainl vs. train2: Taggingword senses in corpus.
In Lexical acquisi-tion: Exploiting on-line resources to build a lex-icon, pages 91-112.
Uri Zernik(Ed.
), London,Lawrence Erlbaum Associates.216
