Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 591?601,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsWord Sense Induction for Novel Sense DetectionJey Han Lau,??
Paul Cook,?
Diana McCarthy, ?David Newman,?
and Timothy Baldwin???
NICTA Victoria Research Laboratory?
Dept of Computer Science and Software Engineering, University of Melbourne?
Dept of Computer Science, University of California Irvine?
Lexical Computingjhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,diana@dianamccarthy.co.uk, newman@uci.edu, tb@ldwin.netAbstractWe apply topic modelling to automaticallyinduce word senses of a target word, anddemonstrate that our word sense inductionmethod can be used to automatically de-tect words with emergent novel senses, aswell as token occurrences of those senses.We start by exploring the utility of stan-dard topic models for word sense induction(WSI), with a pre-determined number oftopics (=senses).
We next demonstrate thata non-parametric formulation that learns anappropriate number of senses per word ac-tually performs better at the WSI task.
Wego on to establish state-of-the-art resultsover two WSI datasets, and apply the pro-posed model to a novel sense detection task.1 IntroductionWord sense induction (WSI) is the task of auto-matically inducing the different senses of a givenword, generally in the form of an unsupervisedlearning task with senses represented as clustersof token instances.
It contrasts with word sensedisambiguation (WSD), where a fixed sense in-ventory is assumed to exist, and token instancesof a given word are disambiguated relative to thesense inventory.
While WSI is intuitively appeal-ing as a task, there have been no real examples ofWSI being successfully deployed in end-user ap-plications, other than work by Schutze (1998) andNavigli and Crisafulli (2010) in an information re-trieval context.
A key contribution of this paperis the successful application of WSI to the lexico-graphical task of novel sense detection, i.e.
identi-fying words which have taken on new senses overtime.One of the key challenges in WSI is learningthe appropriate sense granularity for a given word,i.e.
the number of senses that best captures thetoken occurrences of that word.
Building on thework of Brody and Lapata (2009) and others, weapproach WSI via topic modelling ?
using La-tent Dirichlet Allocation (LDA: Blei et al(2003))and derivative approaches ?
and use the topicmodel to determine the appropriate sense gran-ularity.
Topic modelling is an unsupervised ap-proach to jointly learn topics ?
in the form ofmultinomial probability distributions over words?
and per-document topic assignments ?
in theform of multinomial probability distributions overtopics.
LDA is appealing for WSI as it both as-signs senses to words (in the form of topic alloca-tion), and outputs a representation of each senseas a weighted list of words.
LDA offers a solu-tion to the question of sense granularity determi-nation via non-parametric formulations, such asa Hierarchical Dirichlet Process (HDP: Teh et al(2006), Yao and Durme (2011)).Our contributions in this paper are as follows.We first establish the effectiveness of HDP forWSI over both the SemEval-2007 and SemEval-2010WSI datasets (Agirre and Soroa, 2007; Man-andhar et al 2010), and show that the non-parametric formulation is superior to a standardLDA formulation with oracle determination ofsense granularity for a given word.
We nextdemonstrate that our interpretation of HDP-basedWSI is superior to other topic model-based ap-proaches to WSI, and indeed, better than the best-published results for both SemEval datasets.
Fi-nally, we apply our method to the novel sense de-tection task based on a dataset developed in thisresearch, and achieve highly encouraging results.2 MethodologyIn topic modelling, documents are assumed to ex-hibit multiple topics, with each document having591its own distribution over topics.
Words are gen-erated in each document by first sampling a topicfrom the document?s topic distribution, then sam-pling a word from that topic.
In this work weuse the topic models?s probabilistic assignment oftopics to words for the WSI task.2.1 Data Representation and Pre-processingIn the context of WSI, topics form our sense rep-resentation, and words in a sentence are gener-ated conditioned on a particular sense of the targetword.
The ?document?
in the WSI case is a sin-gle sentence or a short document fragment con-taining the target word, as we would not expectto be able to generate a full document from thesense of a single target word.1 In the case of theSemEval datasets, we use the word contexts pro-vided in the dataset, while in our novel sense de-tection experiments, we use a context window ofthree sentences, one sentence to either side of thetoken occurrence of the target word.As our baseline representation, we use a bag ofwords, where word frequency is kept but not wordorder.
All words are lemmatised, and stopwordsand low frequency terms are removed.We also experiment with the addition of po-sitional context word information, as commonlyused in WSI.
That is, we introduce an additionalword feature for each of the three words to the leftand right of the target word.Pado?
and Lapata (2007) demonstrated the im-portance of syntactic dependency relations in theconstruction of semantic space models, e.g.
forWSD.
Based on these findings, we include depen-dency relations as additional features in our topicmodels,2 but just for dependency relations that in-volve the target word.2.2 Topic ModellingTopic models learn a probability distribution overtopics for each document, by simply aggregatingthe distributions over topics for each word in thedocument.
In WSI terms, we take this distribu-tion over topics for each target word (?instance?in WSI parlance) as our distribution over sensesfor that word.1Notwithstanding the one sense per discourse heuristic(Gale et al 1992).2We use the Stanford Parser to do part of speech taggingand to extract the dependency relations (Klein and Manning,2003; De Marneffe et al 2006).In our initial experiments, we use LDA topicmodelling, which requires us to set T , the num-ber of topics to be learned by the model.
TheLDA generative process is: (1) draw a latenttopic z from a document-specific topic distribu-tion P (t = z|d) then; (2) draw a word w fromthe chosen topic P (w|t = z).
Thus, the probabil-ity of producing a single copy of word w given adocument d is given by:P (w|d) =T?z=1P (w|t = z)P (t = z|d).In standard LDA, the user needs to specify thenumber of topics T .
In non-parametric variants ofLDA, the model dynamically learns the number oftopics as part of the topic modelling.
The particu-lar implementation of non-parametric topic modelwe experiment with is Hierarchical Dirichlet Pro-cess (HDP: Teh et al(2006)),3 where, for eachdocument, a distribution of mixture componentsP (t|d) is sampled from a base distribution G0as follows: (1) choose a base distribution G0 ?DP (?,H); (2) for each document d, generate dis-tribution P (t|d) ?
DP (?0, G0); (3) draw a la-tent topic z from the document?s mixture compo-nent distribution P (t|d), in the same manner asfor LDA; and (4) draw a word w from the chosentopic P (w|t = z).4For both LDA and HDP, we individually topicmodel each target word, and determine the senseassignment z for a given instance by aggregatingover the topic assignments for each word in theinstance and selecting the sense with the highestaggregated probability, argmaxz P (t = z|d).3 SemEval ExperimentsTo facilitate comparison of our proposed methodfor WSI with previous approaches, we use thedataset from the SemEval-2007 and SemEval-2010 word sense induction tasks (Agirre and3We use the C++ implementation of HDP(http://www.cs.princeton.edu/?blei/topicmodeling.html) in our experiments.4The two HDP parameters ?
and ?0 control the variabil-ity of senses in the documents.
In particular, ?
controls thedegree of sharing of topics across documents ?
a high ?value leads to more topics, as topics for different documentsare more dissimilar.
?0, on the other hand, controls the de-gree of mixing of topics within a document?
a high ?0 gen-erates fewer topics, as topics are less homogeneous within adocument.592Soroa, 2007; Manandhar et al 2010).
We firstexperiment with the SemEval-2010 dataset, as itincludes explicit training and test data for eachtarget word and utilises a more robust evaluationmethodology.
We then return to experiment withthe SemEval-2007 dataset, for comparison pur-poses with other published results for topic mod-elling approaches to WSI.3.1 SemEval-20103.1.1 Dataset and MethodologyOur primary WSI evaluation is based onthe dataset provided by the SemEval-2010 WSIshared task (Manandhar et al 2010).
The datasetcontains 100 target words: 50 nouns and 50 verbs.For each target word, a fixed set of training andtest instances are supplied, typically 1 to 3 sen-tences in length, each containing the target word.The default approach to evaluation for theSemEval-2010 WSI task is in the form of WSDover the test data, based on the senses that havebeen automatically induced from the trainingdata.
Because the induced senses will likely varyin number and nature between systems, the WSDevaluation has to incorporate a sense alignmentstep, which it performs by splitting the test in-stances into two sets: a mapping set and an eval-uation set.
The optimal mapping from inducedsenses to gold-standard senses is learned from themapping set, and the resulting sense alignment isused to map the predictions of the WSI system topre-defined senses for the evaluation set.
The par-ticular split we use to calculate WSD effective-ness in this paper is 80%/20% (mapping/test), av-eraged across 5 random splits.5The SemEval-2010 training data consists of ap-proximately 163K training instances for the 100target words, all taken from the web.
The testdata is approximately 9K instances taken from avariety of news sources.
Following the standardapproach used by the participating systems in theSemEval-2010 task, we induce senses only fromthe training instances, and use the learned modelto assign senses to the test instances.5A 60%/40% split is also provided as part of the tasksetup, but the results are almost identical to those for the80%/20% split, and so are omitted from this paper.
The orig-inal task also made use of V-measure and Paired F-score toevaluate the induced word sense clusters, but have degen-erate behaviour in correlating strongly with the number ofsenses induced by the method (Manandhar et al 2010), andare hence omitted from this paper.In our original experiments with LDA, we setthe number of topics (T ) for each target word tothe number of senses represented in the test datafor that word (varying T for each target word).This is based on the unreasonable assumption thatwe will have access to gold-standard informationon sense granularity for each target word, and isdone to establish an upper bound score for LDA.We then relax the assumption, and use a fixed Tsetting for each of sets of nouns (T = 7) andverbs (T = 3), based on the average number ofsenses from the test data in each case.
Finally,we introduce positional context features for LDA,once again using the fixed T values for nouns andverbs.We next apply HDP to the WSI task, usingpositional features, but learning the number ofsenses automatically for each target word via themodel.
Finally, we experiment with adding de-pendency features to the model.To summarise, we provide results for the fol-lowing models:1.
LDA+Variable T : LDA with variable Tfor each target word based on the number ofgold-standard senses.2.
LDA+Fixed T : LDA with fixed T for eachof nouns and verbs.3.
LDA+Fixed T+Position: LDA with fixedT and extra positional word features.4.
HDP+Position: HDP (which automaticallylearns T ), with extra positional word fea-tures.5.
HDP+Position+Dependency: HDP withboth positional word and dependency fea-tures.We compare our models with two baselinesfrom the SemEval-2010 task: (1) Baseline Ran-dom ?
randomly assign each test instance to oneof four senses; (2) Baseline MFS ?
most fre-quent sense baseline, assigning all test instancesto one sense; and also a benchmark system(UoY), in the form of the University of York sys-tem (Korkontzelos and Manandhar, 2010), whichachieved the best overall WSD results in the orig-inal SemEval-2010 task.3.2 SemEval-2010 ResultsThe results of our experiments over the SemEval-2010 dataset are summarised in Table 1.593System WSD (80%/20%)All Verbs NounsBaselinesBaseline Random 0.57 0.66 0.51Baseline MFS 0.59 0.67 0.53LDAVariable T 0.64 0.69 0.60Fixed T 0.63 0.68 0.59Fixed T +Position 0.63 0.68 0.60HDP+Position 0.68 0.72 0.65+Position+Dependency 0.68 0.72 0.65BenchmarkUoY 0.62 0.67 0.59Table 1: WSD F-score over the SemEval-2010 datasetLooking first at the results for LDA, we seethat the first LDA approach (variable T ) is verycompetitive, outperforming the benchmark sys-tem.
In this approach, however, we assume per-fect knowledge of the number of gold senses ofeach target word, meaning that the method isn?ttruly unsupervised.
When we fixed T for eachof the nouns and verbs, we see a small drop inF-score, but encouragingly the method still per-forms above the benchmark.
Adding positionalword features improves the results very slightlyfor nouns.When we relax the assumption on the numberof word senses in moving to HDP, we observe amarked improvement in F-score over LDA.
Thisis highly encouraging and somewhat surprising,as in hiding information about sense granularityfrom the model, we have actually improved ourresults.
We return to discuss this effect below.For the final feature, we add dependency featuresto the HDP model (in addition to retaining thepositional word features), but see no movementin the results.6 While the dependency featuresdidn?t reduce F-score, their utility is questionableas the generation of the features from the Stanfordparser is computationally expensive.To better understand these results, we presentthe top-10 terms for each of the senses induced forthe word cheat in Table 2.
These senses are learntusing HDP with both positional word features(e.g.
husband #-1, indicating the lemma husbandto the immediate left of the target word) and de-pendency features (e.g.
cheat#prep on#wife).
Thefirst observation to make is that senses 7, 8 and9 are ?junk?
senses, in that the top-10 terms do6An identical result was observed for LDA.not convey a coherent sense.
These topics are anartifact of HDP: they are learnt at a much laterstage of the iterative process of Gibbs samplingand are often smaller than other topics (i.e.
havemore zero-probability terms).
We notice that theyare assigned as topics to instances very rarely (al-though they are certainly used to assign topics tonon-target words in the instances), and as such,they do not present a real issue when assigningthe sense to an instance, as they are likely to beovershadowed by the dominant senses.7 This con-clusion is born out when we experimented withmanually filtering out these topics when assign-ing instance to senses: there was no perceptiblechange in the results, reinforcing our suggestionthat these topics do not impact on target wordsense assignment.Comparing the results for HDP back to thosefor LDA, HDP tends to learn almost double thenumber of senses per target word as are in thegold-standard (and hence are used for the ?Vari-able T ?
version of LDA).
Far from hurting ourWSD F-score, however, the extra topics are dom-inated by junk topics, and boost WSD F-score forthe ?genuine?
topics.
Based on this insight, weran LDA once again with variable T (and posi-tional and dependency features), but this time set-ting T to the value learned by HDP, to give LDAthe facility to use junk topics.
This resulted in anF-score of 0.66 across all word classes (verbs =0.71, nouns = 0.62), demonstrating that, surpris-ingly, even for the same T setting, HDP achievessuperior results to LDA.
I.e., not only does HDPlearn T automatically, but the topic model learnedfor a given T is superior to that for LDA.Looking at the other senses discovered forcheat, we notice that the model has induced amyriad of senses: the relationship sense of cheat(senses 1, 3 and 4, e.g.
husband cheats); the examusage of cheat (sense 2); the competition/gameusage of cheat (sense 5); and cheating in the po-litical domain (sense 6).
Although the senses arepossibly ?split?
a little more than desirable (e.g.senses 1, 3 and 4 arguably describe the samesense), the overall quality of the produced senses7In the WSD evaluation, the alignment of induced sensesto the gold senses is learnt automatically based on the map-ping instances.
E.g.
if all instances that are assigned sensea have gold sense x, then sense a is mapped to gold sensex.
Therefore, if the proportion of junk senses in the map-ping instances is low, their influence on WSD results will benegligible.594Sense Num Top-10 Terms1 cheat think want ... love feel tell guy cheat#nsubj#include find2 cheat student cheating test game school cheat#aux#to teacher exam study3 husband wife cheat wife #1 tiger husband #-1 cheat#prep on#wife ... woman cheat#nsubj#husband4 cheat woman relationship cheating partner reason cheat#nsubj#man woman #-1 cheat#aux#to spouse5 cheat game play player cheating poker cheat#aux#to card cheated money6 cheat exchange china chinese foreign cheat #-2 cheat #2 china #-1 cheat#aux#to team7 tina bette kirk walk accuse mon pok symkyn nick star8 fat jones ashley pen body taste weight expectation parent able9 euro goal luck fair france irish single 2000 cheat#prep at#point complainTable 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positionalword and dependency features)is encouraging.
Also, we observe a spin-off ben-efit of topic modelling approaches to WSI: thehigh-ranking words in each topic can be used togist the sense, and anecdotally confirm the impactof the different feature types (i.e.
the positionalword and dependency features).3.3 Comparison with other Topic ModellingApproaches to WSIThe idea of applying topic modelling to WSI isnot entirely new.
Brody and Lapata (2009) pro-posed an LDA-based model which assigns differ-ent weights to different feature sets (e.g.
unigramtokens vs. dependency relations), using a ?lay-ered?
feature representation.
They carry out ex-tensive parameter optimisation of both the (fixed)number of senses, number of layers, and size ofthe context window.Separately, Yao and Durme (2011) proposedthe use of non-parametric topic models in WSI.The authors preprocess the instances slightly dif-ferently, opting to remove the target word fromeach instance and stem the tokens.
They alsotuned the hyperparameters of the topic model tooptimise the WSI effectiveness over the evalua-tion set, and didn?t use positional or dependencyfeatures.Both of these papers were evaluated overonly the SemEval-2007 WSI dataset (Agirre andSoroa, 2007), so we similarly apply our HDPmethod to this dataset for direct comparability.
Inthe remainder of this section, we refer to Brodyand Lapata (2009) as BL, and Yao and Durme(2011) as YVD.The SemEval-2007 dataset consists of roughly27K instances, for 65 target verbs and 35 targetnouns.
BL report on results only over the nouninstances, so we similarly restrict our attention toSystem F-ScoreBL 0.855YVD 0.857SemEval Best (I2R) 0.868Our method (default parameters) 0.842Our method (tuned parameters) 0.869Table 3: F-score for the SemEval-2007 WSI task, forour HDP method with default and tuned parameter set-tings, as compared to competitor topic modelling andother approaches to WSIthe nouns in this paper.
Training data was not pro-vided as part of the original dataset, so we fol-low the approach of BL and YVD in construct-ing our own training dataset for each target wordfrom instances extracted from the British NationalCorpus (BNC: Burnard (2000)).8 Both BL andYVD separately report slightly higher in-domainresults from training on WSJ data (the SemEval-2007 data was taken from the WSJ).
For the pur-poses of model comparison under identical train-ing settings, however, it is appropriate to report onresults for only the BNC.We experiment with both our original method(with both positional word and dependency fea-tures, and default parameter settings for HDP)without any parameter tuning, and the samemethod with the tuned parameter settings ofYVD, for direct comparability.
We present the re-sults in Table 3, including the results for the best-performing system in the original SemEval-2007task (I2R: Niu et al(2007)).The results are enlightening: with default pa-rameter settings, our methodology is slightly be-low the results of the other three models.
Bear8In creating the training dataset, each instance is madeup of the sentence the target word occurs in, as we as onesentence to either side of that sentence, i.e.
3 sentences intotal per instance.595in mind, however, that the two topic modelling-based approaches were tuned extensively to thedataset.
When we use the tuned hyperparame-ter settings of YVD, our results rise around 2.5%to surpass both topic modelling approaches, andmarginally outperform the I2R system from theoriginal task.
Recall that both BL and YVD reporthigher results again using in-domain training data,so we would expect to see further gains again overthe I2R system in following this path.Overall, these results agree with our findingsover the SemEval-2010 dataset (Section 3.2), un-derlining the viability of topic modelling to auto-mated word sense induction.3.4 DiscussionAs part of our preprocessing, we remove all stop-words (other than for the positional word and de-pendency features), as described in Section 2.1.We separately experimented with not removingstopwords, based on the intuition that prepositionssuch as to and on can be informative in determin-ing word sense based on local context.
The resultswere markedly worse, however.
We also tried ap-pending part of speech information to each wordlemma, but the resulting data sparseness meantthat results dropped marginally.When determining the sense for an instance, weaggregate the sense assignments for each word inthe instance (not just the target word).
An alter-nate strategy is to use only the target word topicassignment, but again, the results for this strategywere inferior to the aggregate method.In the SemEval-2007 experiments (Sec-tion 3.3), we found that YVD?s hyperparametersettings yielded better results than the defaultsettings.
We experimented with parameter tuningover the SemEval-2010 dataset (including YVD?soptimal setting on the 2007 dataset), but foundthat the default setting achieved the best overallresults: although the WSD F-score improved alittle for nouns, it worsened for verbs.
This obser-vation is not unexpected: as the hyperparameterswere optimised for nouns in their experiments,the settings might not be appropriate for verbs.This also suggests that their results may be due inpart to overfitting the SemEval-2007 data.4 Identifying Novel SensesHaving established the effectiveness of our ap-proach at WSI, we next turn to an application ofWSI, in identifying words which have taken onnovel senses over time, based on analysis of di-achronic data.
Our topic modelling approach isparticularly attractive for this task as, not onlydoes it jointly perform type-level WSI, and token-level WSD based on the induced senses (in as-signing topics to each instance), but it is possibleto gist the induced senses via the contents of thetopic (typically using the topic words with highestmarginal probability).The meanings of words can change over time;in particular, words can take on new senses.
Con-temporary examples of new word-senses includethe meanings of swag and tweet as used below:1.
We all know Frankie is adorable, but does hehave swag?
[swag = ?style?]2.
The alleged victim gave a description of theman on Twitter and tweeted that she thoughtshe could identify him.
[tweet = ?send a mes-sage on Twitter?
]These senses of swag and tweet are not includedin many dictionaries or computational lexicons ?e.g., neither of these senses is listed in Wordnet3.0 (Fellbaum, 1998) ?
yet appear to be in regu-lar usage, particularly in text related to pop cultureand online media.The manual identification of such new word-senses is a challenge in lexicography over andabove identifying new words themselves, andis essential to keeping dictionaries up-to-date.Moreover, lexicons that better reflect contempo-rary usage could benefit NLP applications that usesense inventories.The challenge of identifying changes in wordsense has only recently been considered in com-putational linguistics.
For example, Sagi et al(2009), Cook and Stevenson (2010), and Gulor-dava and Baroni (2011) propose type-based mod-els of semantic change.
Such models do notaccount for polysemy, and appear best-suited toidentifying changes in predominant sense.
Bam-man and Crane (2011) use a parallel Latin?English corpus to induce word senses and builda WSD system, which they then apply to studydiachronic variation in word senses.
Crucially, inthis token-based approach there is a clear connec-tion between word senses and tokens, making itpossible to identify usages of a specific sense.Based on the findings in Section 3.2, here weapply the HDP method for WSI to the task of596identifying new word-senses.
In contrast to Bam-man and Crane (2011) our token-based approachdoes not require parallel text to induce senses.4.1 MethodGiven two corpora ?
a reference corpus whichwe take to represent standard usage, and a secondcorpus of newer texts ?
we identify senses thatare novel to the second corpus compared to thereference corpus.
For a given word w, we poolall usages of w in the reference corpus and sec-ond corpus, and run the HDP WSI method on thissuper-corpus to induce the senses of w. We thentag all usages of w in both corpora with their sin-gle most-likely automatically-induced sense.Intuitively, if a word w is used in some senses in the second corpus, and w is never used inthat sense in the reference corpus, then w has ac-quired a new sense, namely s. We capture thisintuition into a novelty score (?Nov?)
that indi-cates whether a given word w has a new sense inthe second corpus, s, compared to the referencecorpus, r, as below:Nov(w) = max({ps(ti)?
pr(ti)pr(ti): ti ?
T})(1)where ps(ti) and pr(ti) are the probability ofsense ti in the second corpus and reference cor-pus, respectively, calculated using smoothed max-imum likelihood estimates, and T is the set ofsenses induced for w. Novelty is high if there issome sense t that has much higher relative fre-quency in s than r and that is also relatively infre-quent in r.4.2 DataBecause we are interested in the identification ofnovel word-senses for applications such as lexi-con maintenance, we focus on relatively newly-coined word-senses.
In particular, we take thewritten portion of the BNC ?
consisting primar-ily of British English text from the late 20th cen-tury ?
as our reference corpus, and a similarly-sized random sample of documents from theukWaC (Ferraresi et al 2008) ?
a Web corpusbuilt from the .uk domain in 2007 which in-cludes a wide range of text types ?
as our sec-ond corpus.
Text genres are represented to dif-ferent extents in these corpora with, for example,text types related to the Internet being much morecommon in the ukWaC.
Such differences are anoted challenge for approaches to identifying lex-ical semantic differences between corpora (Peirs-man et al 2010), but are difficult to avoid giventhe corpora that are available.
We use TreeTagger(Schmid, 1994) to tokenise and lemmatise bothcorpora.Evaluating approaches to identifying seman-tic change is a challenge, particularly due to thelack of appropriate evaluation resources; indeed,most previous approaches have used very smalldatasets (Sagi et al 2009; Cook and Stevenson,2010; Bamman and Crane, 2011).
Because thisis a preliminary attempt at applying WSI tech-niques to identifying new word-senses, our evalu-ation will also be based on a rather small dataset.We require a set of words that are known tohave acquired a new sense between the late 20thand early 21st centuries.
The Concise OxfordEnglish Dictionary aims to document contempo-rary usage, and has been published in numerouseditions including Thompson (1995, COD95) andSoanes and Stevenson (2008, COD08).
Althoughsome of the entries have been substantially re-vised between editions, many have not, enablingus to easily identify new senses amongst the en-tries in COD08 relative to COD95.
A manual lin-ear search through the entries in these dictionarieswould be very time consuming, but by exploit-ing the observation that new words often corre-spond to concepts that are culturally salient (Ayto,2006), we can quickly identify some candidatesfor words that have taken on a new sense.Between the time periods of our two corpora,computers and the Internet have become muchmore mainstream in society.
We therefore ex-tracted all entries from COD08 containing theword computing (which is often used as a topic la-bel in this dictionary) that have a token frequencyof at least 1000 in the BNC.
We then read theentries for these 87 lexical items in COD95 andCOD08 and identified those which have a clearcomputing sense in COD08 that was not presentin COD95.
In total we found 22 such items.
Thisprocess, along with all the annotation in this sec-tion, is carried out by a native English-speakingauthor of this paper.To ensure that the words identified from thedictionaries do in fact have a new sense in theukWaC sample compared to the BNC, we exam-ine the usage of these words in the corpora.
Weextract a random sample of 100 usages of each597lemma from the BNC and ukWaC sample andannotate these usages as to whether they corre-spond to the novel sense or not.
This binary dis-tinction is easier than fine-grained sense annota-tion, and since we do not use these annotationsfor formal evaluation ?
only for selecting itemsfor our dataset ?
we do not carry out an inter-annotator agreement study here.
We eliminate anylemma for which we find evidence of the novelsense in the BNC, or for which we do not findevidence of the novel sense in the ukWaC sam-ple.9 We further check word sketches (Kilgarriffand Tugwell, 2002)10 for each of these lemmasin the BNC and ukWaC for collocates that likelycorrespond to the novel sense; we exclude anylemma for which we find evidence of the novelsense in the BNC, or fail to find evidence of thenovel sense in the ukWaC sample.
At the endof this process we have identified the following5 lemmas that have the indicated novel senses inthe ukWaC compared to the BNC: domain (n) ?In-ternet domain?
; export (v) ?export data?
; mirror(n) ?mirror website?
; poster (n) ?one who postsonline?
; and worm (n) ?malicious program?.
Foreach of the 5 lemmas with novel senses, a sec-ond annotator ?
also a native English-speakingauthor of this paper ?
annotated the sample of100 usages from the ukWaC.
The observed agree-ment and unweighted Kappa between the two an-notators is 97.2% and 0.92, respectively, indicat-ing that this is indeed a relatively easy annotationtask.
The annotators discussed the small numberof disagreements to reach consensus.For our dataset we also require items that havenot acquired a novel sense in the ukWaC sample.For each of the above 5 lemmas we identified adistractor lemma of the same part-of-speech thathas a similar frequency in the BNC, and that hasnot undergone sense change between COD95 andCOD08.
The 5 distractors are: cinema (n); guess(v); symptom (n); founder (n); and racism (n).4.3 ResultsWe compute novelty (?Nov?, Equation 1) for all10 items in our dataset, based on the output of the9We use the IMS Open Corpus Workbench (http://cwb.sourceforge.net/) to extract the usages of ourtarget lemmas from the corpora.
This extraction process failsin some cases, and so we also eliminate such items from ourdataset.10http://www.sketchengine.co.uk/Lemma Novelty Freq.
ratio Novel sense freq.domain (n) 116.2 2.60 41worm (n) 68.4 1.04 30mirror (n) 38.4 0.53 10guess (v) 16.5 0.93 ?export (v) 13.8 0.88 28founder (n) 11.0 1.20 ?cinema (n) 9.7 1.30 ?poster (n) 7.9 1.83 4racism (n) 2.4 0.98 ?symptom (n) 2.1 1.16 ?Table 4: Novelty score (?Nov?
), ratio of frequency inthe ukWaC sample and BNC, and frequency of thenovel sense in the manually-annotated 100 instancesfrom the ukWaC sample (where applicable), for alllemmas in our dataset.
Lemmas shown in boldfacehave a novel sense in the ukWaC sample compared tothe BNC.topic modelling.
The results are shown in column?Novelty?
in Table 4.
The lemmas with a novelsense have higher novelty scores than the distrac-tors according to a one-sided Wilcoxon rank sumtest (p < .05).When a lemma takes on a new sense, it mightalso increase in frequency.
We therefore also con-sider a baseline in which we rank the lemmas bythe ratio of their frequency in the second and ref-erence corpora.
These results are shown in col-umn ?Freq.
ratio?
in Table 4.
The difference be-tween the frequency ratios for the lemmas with anovel sense, and the distractors, is not significant(p > .05).Examining the frequency of the novel senses?shown in column ?Novel sense freq.?
in Table 4?
we see that the lowest-ranked lemma with anovel sense, poster, is also the lemma with theleast-frequent novel sense.
This result is unsur-prising as our novelty score will be higher forhigher-frequency novel senses.
The identificationof infrequent novel senses remains a challenge.The top-ranked topic words for the sense cor-responding to the maximum in Equation 1 forthe highest-ranked distractor, guess, are the fol-lowing: @card@, post, ..., n?t, comment, think,subject, forum, view, guess.
This sense seemsto correspond to usages of guess in the contextof online forums, which are better representedin the ukWaC sample than the BNC.
Because ofthe challenges posed by such differences betweencorpora (discussed in Section 4.2) we are unsur-prised to see such an error, but this could be ad-dressed in the future by building comparable cor-598LemmaTopic Selection MethodologyNov Oracle (single topic) Oracle (multiple topics)Precision Recall F-score Precision Recall F-score Precision Recall F-scoredomain (n) 1.00 0.29 0.45 1.00 0.56 0.72 0.97 0.88 0.92export (v) 0.93 0.96 0.95 0.93 0.96 0.95 0.90 1.00 0.95mirror (n) 0.67 1.00 0.80 0.67 1.00 0.80 0.67 1.00 0.80poster (n) 0.00 0.00 0.00 0.44 1.00 0.62 0.44 1.00 0.62worm (n) 0.93 0.90 0.92 0.93 0.90 0.92 0.86 1.00 0.92Table 5: Results for identifying the gold-standard novel senses based on the three topic selection methodologiesof: (1) Nov; (2) oracle selection of a single topic; and (3) oracle selection of multiple topics.pora for use in this application.Having demonstrated that our method for iden-tifying novel senses can distinguish lemmas thathave a novel sense in one corpus compared to an-other from those that do not, we now considerwhether this method can also automatically iden-tify the usages of the induced novel sense.For each lemma with a gold-standard novelsense, we define the automatically-induced novelsense to be the single sense corresponding to themaximum in Equation 1.
We then compute theprecision, recall, and F-score of this novel sensewith respect to the gold-standard novel sense,based on the 100 annotated tokens for each ofthe 5 lemmas with a novel sense.
The results areshown in the first three numeric columns of Ta-ble 5.In the case of export and worm the results areremarkably good, with precision and recall bothover 0.90.
For domain, the low recall is a result ofthe majority of usages of the gold-standard novelsense (?Internet domain?)
being split across twoinduced senses ?
the top-two highest ranked in-duced senses according to Equation 1.
The poorperformance for poster is unsurprising due to thevery low frequency of this lemma?s gold-standardnovel sense.These results are based on our novelty rank-ing method (?Nov?
), and the assumption thatthe novel sense will be represented in a singletopic.
To evaluate the theoretical upper-boundfor a topic-ranking method which uses our HDP-based WSI method and selects a single topic tocapture the novel sense, we next evaluate an op-timal topic selection approach.
In the middlethree numeric columns of Table 5, we present re-sults for an experimental setup in which the sin-gle best induced sense ?
in terms of F-score ?is selected as the novel sense by an oracle.
Wesee big improvements in F-score for domain andposter.
This encouraging result suggests refiningthe sense selection heuristic could theoreticallyimprove our method for identifying novel senses,and that the topic modelling approach proposedin this paper has considerable promise for auto-matic novel sense detection.
Of particular note isthe result for poster: although the gold-standardnovel sense of poster is rare, all of its usages aregrouped into a single topic.Finally, we consider whether an oracle whichcan select the best subset of induced senses ?
interms of F-score ?
as the novel sense could of-fer further improvements.
In this case ?
resultsshown in the final three columns of Table 5 ?we again see an increase in F-score to 0.92 fordomain.
For this lemma the gold-standard novelsense usages were split across multiple inducedtopics, and so we are unsurprised to find that amethod which is able to select multiple topics asthe novel sense performs well.
Based on thesefindings, in future work we plan to consider alter-native formulations of novelty.5 ConclusionWe propose the application of topic modellingto the task of word sense induction (WSI), start-ing with a simple LDA-based methodology witha fixed number of senses, and culminating ina nonparametric method based on a Hierarchi-cal Dirichlet Process (HDP), which automaticallylearns the number of senses for a given targetword.
Our HDP-based method outperforms allmethods over the SemEval-2010WSI dataset, andis also superior to other topic modelling-basedapproaches to WSI based on the SemEval-2007dataset.
We applied the proposed WSI model tothe task of identifying words which have taken onnew senses, including identifying the token oc-currences of the new word sense.
Over a smalldataset developed in this research, we achievedhighly encouraging results.599ReferencesEneko Agirre and Aitor Soroa.
2007.
SemEval-2007Task 02: Evaluating word sense induction and dis-crimination systems.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 7?12, Prague, Czech Re-public.John Ayto.
2006.
Movers and Shakers: A Chronologyof Words that Shaped our Age.
Oxford UniversityPress, Oxford.David Bamman and Gregory Crane.
2011.
Measur-ing historical word sense variation.
In Proceedingsof the 2011 Joint International Conference on Dig-ital Libraries (JCDL 2011), pages 1?10, Ottawa,Canada.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research,3:993?1022.S.
Brody and M. Lapata.
2009.
Bayesian word senseinduction.
pages 103?111, Athens, Greece.Lou Burnard.
2000.
The British National CorpusUsers Reference Guide.
Oxford University Com-puting Services.Paul Cook and Suzanne Stevenson.
2010.
Automat-ically identifying changes in the semantic orienta-tion of words.
In Proceedings of the Seventh In-ternational Conference on Language Resources andEvaluation (LREC 2010), pages 28?34, Valletta,Malta.Marie-Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.Genoa, Italy.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,MA.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluat-ing ukwac, a very large web-derived corpus of en-glish.
In Proceedings of the 4th Web as CorpusWorkshop: Can we beat Google, pages 47?54, Mar-rakech, Morocco.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
pages233?237.Kristina Gulordava and Marco Baroni.
2011.
A dis-tributional similarity approach to the detection ofsemantic change in the Google Books Ngram cor-pus.
In Proceedings of the GEMS 2011 Workshopon GEometrical Models of Natural Language Se-mantics, pages 67?71, Edinburgh, Scotland.Adam Kilgarriff and David Tugwell.
2002.
Sketch-ing words.
In Marie-He?le`ne Corre?ard, editor, Lex-icography and Natural Language Processing: AFestschrift in Honour of B. T. S. Atkins, pages 125?137.
Euralex, Grenoble, France.Dan Klein and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in Neural Informa-tion Processing Systems 15 (NIPS 2002), pages 3?10, Whistler, Canada.Ioannis Korkontzelos and Suresh Manandhar.
2010.Uoy: Graphs of unambiguous vertices for wordsense induction and disambiguation.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, pages 355?358, Uppsala, Sweden.Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dli-gach, and Sameer Pradhan.
2010.
SemEval-2010Task 14: Word sense induction & disambiguation.In Proceedings of the 5th International Workshopon Semantic Evaluation, pages 63?68, Uppsala,Sweden.Roberto Navigli and Giuseppe Crisafulli.
2010.
In-ducing word senses to improve web search resultclustering.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 116?126, Cambridge, USA.Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.2007.
I2R: Three systems for word sense discrimi-nation, chinese word sense disambiguation, and en-glish word sense disambiguation.
In Proceedingsof the Fourth International Workshop on Seman-tic Evaluations (SemEval-2007), pages 177?182,Prague, Czech Republic.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-based construction of semanticspace models.
Comput.
Linguist., 33:161?199.Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.2010.
The automatic identification of lexical varia-tion between language varieties.
Natural LanguageEngineering, 16(4):469?491.Eyal Sagi, Stefan Kaufmann, and Brady Clark.
2009.Semantic density analysis: Comparing word mean-ing across time and space.
In Proceedings ofthe EACL 2009 Workshop on GEMS: GEometricalModels of Natural Language Semantics, pages 104?111, Athens, Greece.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49, Manchester, UK.Hinrich Schutze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Catherine Soanes and Angus Stevenson, editors.
2008.The Concise Oxford English Dictionary.
OxfordUniversity Press, eleventh (revised) edition.
OxfordReference Online.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.2006.
Hierarchical Dirichlet processes.
Journalof the American Statistical Association, 101:1566?1581.600Della Thompson, editor.
1995.
The Concise OxfordDictionary of Current English.
Oxford UniversityPress, Oxford, ninth edition.Xuchen Yao and Benjamin Van Durme.
2011.
Non-parametric bayesian word sense induction.
In Pro-ceedings of TextGraphs-6: Graph-based Methodsfor Natural Language Processing, pages 10?14,Portland, Oregon.601
