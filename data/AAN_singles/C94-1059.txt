ENGLISH GENERATION FROM INTERL INGUABY  EXAMPI~E- I~ASI~I )  METHOI )AbstractThis paper describes the experiment of the Englishgeneration from interlingua by the example-basedmethod.
The generator is implemented by using EnglishWord Dictionary ,and Concept Dictionary developed inEDR.
How to construct examples lind how to define thesimilarities are main problems.
The results of experi-ments are shown.1.
IntroductionEiji Komatsu*, Jin Cui**, lliroshi Yasuhara**(*) Oki Electric htdustry Co. Ltd. Meltimedia I.aboratory11-22, Shibaura 4-Chome, Minato-ku, Tokyo 108 Japane-mail : komatsu @okilab.oki.co.jp(**) Japan Electronic l)ictionary Research Institute l.td.
(EDR) 6th LaboratoryMita-Kokusai-Bldg.
4-28, Mira l-Chome, Minato-kv, Tokyo 108 Japane-mail : sai@edrrr.edr.co.jp, yasuhara@edr6r.edr.co.jpto mean example data of the example-based method.
Andthe terms "interlingua" and "syntactic tree" are used tomean sets, elements m~d fragments of elentents.2.
Input anti OutputThe generator t,anslates an interlingt, a to a syntactictree.
Fig.2.1 shows a sample of input interlingnae andFig.2.2, a sample of output syntactic trees.
Both samplescorrespond to the same sentence "My brother will takethe medicine".This paper describes the generator that is originallyimplemented tocorrect and evah,ate English Word Dic-tionary and Concept Dictionary being developed in EDR(El)R,1993).
To evaluate Concept Dictionary, as the firststrategy, interlingua method was introduced.
As the num-ber o1' concepts is very large and they are elements ofcomplex hierarchy, it is difficult to make roles and on theother hand the example-based method was expected to bemore effective than the rule-based method.
So, as the sec-ond strategy, the example-bused method was also intro-duced.The example-based method is usually used in MT bythe transfer method (Nagao, 1984; Sato, 1991; Stnnita,1992), though one by Sadler (1989) is by the interlinguamethod.
In this generator, the example-based method co-exists with the interlingua method because of above rea-sons, but the combination of the example-based methodand tim interlingua method is not intportant, becausel'rom another point of view, the generation frominterlingua is recognized as a translation from one hm-guage i.e.
interlingua to another i.e.
English and the gen-eration from interlingua can be seen similar as transla-tions in above MT systems.
So in this experiment, how toapply the example-based method to various natural hm-guage processing and lbr which parts the method aresuitable are the main interests.
For this purpose, the gen-erator is designed to execute the generation with maxi-mum usage of the example-based method.In this experiment, he coverage of the generation isnot complete, that is, some elements t, ch as articles andconjunctions are not generated.Below, section 2 describes the input and ot, tput of thegenerator, section 3, examples used in this system, sec-tion 4, the similarities used to retrieve xamples and toselect words, section 5, the generation algorithm, section6, the experiments for verb selections and section 7, theconclusion.The examples, similarities and the generation algo-rithm are decided a priori then modilied in response tothe output of the generator.To avoid confusions, the word "example" is used only(*) This work has been done when the author wits in EDR.
"My brother will take tile medicine.
"(non-statement).~e086)  --------liD-- {2dc30I) (no>statement)modifyagent7{ 3b f0d2~ (statement)future {3bf0lx)} (non-statement)Fig.2.1 Input lnterlingualnterlinguae consist of concepts, conceptual relationsand attributes.
Each concepts are classified as "state-ments" or "non-statements".
Concepts are represented byconcept identification umbers (To distinguish conceptseasily by men, concept illustrations are also given).
Inter-pretations of codes relating to interlinguae in this paperare shown in Table 2.1.
In the table, as for concept iden-tification umbers, concept illustrations are showed as in-terp,'etations of codes.
"My brother will take the medicine.
"brother(ENl)-M- my(EPl)M(s j )tak(EVE;EVED;I{CV9;EVDO0) - -S - -  wiII(EAV)"~S'-.
e(EEV)M((to)..,.,....~.medicine(EN1)Fig.2.2 Output Syntactic 'Free363InformationConceptIdentificationNumbersConceptRelationsTable 2.1 Codes for InterlinguaeCode Interpretation(3bfOd2) todrink something(3be086) brothers(Oe351f) sisters(2dc301) c#I(2dc304) c#1~(3bf0D) a substance used on or in the body to treat a disease(3bdbf6) a drilled liquor named wtfiskey(3bd862) a drug or agent hat reduces fever(3cee4f) to obtain a thing which one wante~l(3ceae3) to become a certain condition(0fde5f) to accept others' opinions and wishesi i ..(0c98dc) tbo first part of the day, from the time when the sunagentrises, usually tmtil the time when the midday mealts eatenSubject hat brings about a voluntary action.Conscious attd automated ntities are suclt subjects.
"Animals eat"(eat) - -  agent~ (animals)Object affected by an action or change"Eat food.
"(eat ) - -  ob jec t~ (food)Time at which an event begins"Work until a sc time"(wake up) ~ t ime~ Gin time)objecttimemodifier Other elationshlpspast Viewpoint is in the pastpresent Viewpoint is in the presentfuture Viewpoint is in the futureend The end of an action or eventalready Already occurredTable 2.2hfformatlon CodePart-of Speech EN IEP1EVE"~i;vEEVEPg.Grammatical EVSTMhffonnation I~VIIEVEDEVENECV9EVD(X)EVDO6Surface Relation: M(sj)M(do)M(adj)M(obpp)SCodes for Syntactic Treeslnteq~retatlonCommon llOll i1Personal pronounVerbAnxu!iary verbVerb elidingPrepositionArticleUninflected partInfinitivePast tensePast participlePartially irregular inflections(%" follws)Takes a direct objectTakes a direct object(the direct object is to-infinltiv(subject relationdirect object relationadjective modificationobligatory prepositional phraserelations between content wordsand functional wordsSyntactic trees consist of words, part-of-speeches,grammatic~d information and syntactic relations.The interpretations of codes relating to syntactic treesused in this paper are shown Table 2.2.3.
ExamplesAn example should be a pair of an interlingua nd asyntactic tree.
For the flexibility of usage of examples,interlinguae and syntactic trees in ex:unples are dividedinto smaller parts that are small enough to use flexiblybut have enough information for generations.Fig.3.1 shows the common form of interlinguae andsyntactic trees in examples (referred as "basic unit", be-low).
An example is a pair of fragments in this form madefrom an interlingua nd a syntactic tree.tip (near to tile root of Ihe tree lower n~lt:structure of an interlingm0lower arc lower nodeuppt r n(~le Upl~r arc ~ "attributeFig.3.1 Basic UifitsFig.3.2 shows the linguistic resources used by the gen-erator.
As the results of trying to execute as many pro-cesses as possible by the example-based method, it be-came necessary for the generator to use two differentkinds of examples (referred as "Basic Example Set" and"Example Set tbr Attribute", below).1/~~.qt  Word Dictionar~I English Generator _ EDR Concept Dictionary \]/ \Examples ~_~ I ~  IFig.3.2 Linguistic ResourcesFig.3.3 shows examples in the Basic Example Set.Circlod nodes are "central nodes".
Basic Example Set issupposod to be used for selecting content words for con-cepts.
Functional words except prepositions and grmn-matical information for inflections are removed, sincethey are unnecessary for this purpose.
In Fig.3.2, example(A) and (13) have 11o upper node and Example (C) and (D)have no lower node.
Examples in this set are accessed byconcepts in the central nodes of interlinguae; Example(A) and (B) are accessed by (3bf0d2) and (C), by(3bf0f9) and (D) by (0c98dc) .
When several ex-amples with the same key exist, by the simih'u'ity definedbelow, only one example is finally accepted.Fig.3.4 shows examples in the Example Set for At-tributes.
This example set is supposed to be used for de-ciding inflection (i.e.
selecting the word whose inflectioncorresponds to the attributes) and adding functionalwords for attributes.
Content words in lower nodes are364removed, since the upper node influences to the inflec-tion of the center word, but the lower nodes rarely don't.Functional words in lower nodes are added to the outputs.Concepts and spellings of words are also removed, sincethey can be decided by Basic Example Set and unneces-sary here.
Examples are accessed by combinations of at-tributes in interlinguae, some grammatical information ofthe upper node, those of central nodes and the surface re-lation of the upper arc; in Fig.3.4, Example (a) is accessedby (past, -, EVE; EVED, -), Example (b) by (end, already,-, EVE; EVEN, -), Example (c) by (present, -, EVE;EVSTM; ECV9, -), Example (d) by (present, , -, EVE;EVIl, -), Example (e) by (future, -, EVE; FNSTM;ECV9,- ) and Example (1) by (-, EVE; EVDO0, EN 1,M(do) ).
Example (a), (b), (c), (d) and (e) have no uppernode.
Since examples in this set don't include concepts,examples are accessed eterministically and the similar-ity is not used.4.
SimilaritiesThere are two major similarities in the example-basedmethod.
One is for the source language and used for se-lecting examples.
Anotber is for the target language andused for creating outputs.
In this generator, the lbrmer isthe similarity between interlinguae (in tile form of basict, nits) and the latter is the similarity between words.
Inthe generator, the similarity is used only for Basic Ex-.ample Set.Example (A) : Brother takes the medicine in the morning.
( 31~086} bro0~er(l!N I )..,1agent f -'M(sj)object(3bf0f9) mcdicinc(EN1 )\],Rel linguu Syntactic WlecF.xampl'e (B) : Sister drinks the whiskey.
(Oe35 lf) st~r( I';N 1 )agent ~ (  .
.
.
.
.
.
.
.
.
.  )
~ , lk (  .
.
.
.
.
.
.
.
.
.
.
.
.
R I~ M(nj)"~'J?':'....~.
""M(.,,)(31+dbf6) ~hinkcy(l{N I )Interlingua Synt~tic "l'l.ooExample (C) : Brothers takes medicine in tile morning,{ 3 b fljd2 } "~hjeet--II~ @ t ak(EVE;EV DO0) -M(do)q~.- ~(non-slalemenl)Illterlingua Synt~lic TreeExample (D) : Brothers takes medicine in the morning.
(.icAl-statement) SNIKiu(El'R) Inlerlinguz Syntactic TreeFig.3.3 Examples in Basic Example SetExample (a) : *(EVE;EVED;EVDO0)(~state  m e r i t ) ~ ~pastExample (b) :lmve *(EVE;EVEN;EVDO0)state merit) ~_(I~V E;EV EN; ECV9~S- -  have(EAV)endalreadyExample (c) : *(EVE;EVSTM;ECV9;EVDO0) estatement) Q(r'.
'W;;EVSTM;ECVg)~--S-- e(EI?V)presentExample (d) : *(FNE;FVI~;;F.VI)O0)";littelrlC-l~tt) Q( I , ;V IZ ;  f2V I t ) .~pvese- ta tExample (e) : will *(I:VE;IiVSTM;ECV9;EVDO0) eQ-~*~"~ (stalctne,,t) ~---~(F.VI?
;F.VS'I'M;ECV9)~.~,S-- wil I (EAV)c(\]2.EV)futureExample (D : *(I{VI,\];F.VI)O0) *(EN1)1. )
*(r~vJ:4EVDO0)l'tltuleFig.3.4 Examples in Example Set for AttributesThe simihu'ity between interlingt,ae is defined its follows;SiI(ILI,IL2) = (Sc(Clcent,C2cent) ?
KcentI E ,Rc(('li,C2i) ?
K(slel(i)) X (k01um(Rl f'H~2) I I)i G i l l  f /  R2ILI,IL2 : intcrlinguaeClcent, C2cent : concepts in central nodesKcent : weight of simihuity between central nodesCli, C2i : concepts in lower nodes with arc ik(x) : weight of similarity between concepts in lowernodes, x is tim number of elementsin tbe interjunctionsrel(i) : surface relation which corresponds totheconcept relation iR 1,R2 : set of conceptt,al relations each for ILl, 11.2ntun(S) : the number of elements of set SIt is always assured in adwmce by tile generator that 1) tileword in tbe upper node of tile input is already selected (ifthere is im upper node); 2) arcs of imerlingt, a which corm-spond to obligatory relations of tile syntactic tree in the ex-;nnple, exist in the interjunction fP.
1 and R2; 3) upper arcsare same (if already decided); 4) part-of-speeches of wordsin upper nodes are same.
l:,xamples that don't satisfy these365four conditions are rejected before the similarity calculation.The similarity between concepts used in the above simi-larity is defined as follows;Sc(Cl ,C2) =the ~lumber of common ancestersthe number of ancesters of CI + the number ofancesters of C2Ilere, ancestors until three layers above are used.
(Cut;1993)It is difficult to find the most similar interlingua in an ex-ample set to the input interlingua, because to find it ,  it isnecessary to calculate all similarities between interlinguae inthe ex,-unple set and the input.
To avoid this, in this genera-tor, some constraints are given for access keys i.e.
centralnodes.
For "statements" in interlingua, central nodes of ex-amples should be same with that of the input and for "non-statements" in interlingua, central nodes of examples can betile s,'u-ne concepts or sister concepts in the concept hierar-chy.
By this constraints, the search of examples can be ex-ecuted fast.The similarity between words is defined as follow;k (0  < k < 1) i f  p~t -o f - t spe .eeh  and  lg ra lnmat icnt  in fo rn l~t lon.~w(~*t 1,W2)  ~ tJ itlrG ~ i r t~11 (O< I < 1) i f  Fmrt-of-~Fm.eeh are santo0 i f  s ix=l ing,  par t -o f - speech  mild grn inmt icml  in fo rn la t i~m areLa l l  d l t  f, a re | l tk, 1 : some numbers5.
Generat ion  A lgor i thmThe generator generates fragments of a syntactic tree andtiredly combines them into a syntactic tree.The generation algorithm is as follows;Step 1 : Sets the current central node at the root node ofthe input interlingua.Step 2-1 : Cuts the basic unit for the current central node.Step 2-2 : Extracts candidate English words for conceptsof the central node and lower nodes of the current basic unit,from English Word Dictionary.Step 3-1 : Retrieves an example from Basic Example Set.Step 3-2 : Selects the same word (neglecting inflection)from the candidate word lists and checks if there is an ex-ample in Example Set for Attributes, whose attributes andwords in the central node coincide with attributes in the cur-rent basic unit and the selected word.Step 3-3 : If the word selection succeeded, accepts theexample.
Generates upper arc (if exists), lower arc (only forobligatory relations) central nodes ,and functional words forthe central node, saves the results and similarity and calcu-lates the similarity of interlingua between the input and theexample.
Prepositions are extracted from the basic example.Step 3-4 : Repeat Step 3-2 to Step 3-3 until there remainsno basic examples.Step 3-5 : Selects one example that is accepted in Step 3-3 and the simih'u'ity is largest.Step 3-6 : Puts the results.Step 4 : Move the current central node in the inputlinterlingua in depth-first order.Step 5 : Repeat Step 2-1 to Step 4 until the movement ofthe current central node ends or the word selection for a nodefails.
{2dc304} (non-statement)agent{3bf0d2} (statement)objoctp, t{3bd862} (non-statement)Figure 5.1 Inputted lnterllnguaSuppose the interlingua such as Fig.5.1 is inputted andexamples in Fig.3.3 are used as Basic Example Set andFig.3.4 used as Example Set for Attributes.The list of candidate words for {3bf0d2} is as fol-lows;tak(EVE;EVSTM;ECV9;EVDO0),took(EVE;EVED;EVDO0),taken(EVE;EVEN;EVDO0),drink(I~VE;EVB;EVDO0),drank(EVE;EVED;EVDO0),drunk(EVE;EVEN;EVDO0).From Basic Example Set, Example (A) and (B) are re-trieve(l, since central nodes are same.By Example (A) and Example (a), took(EVE; EVED;EVDO0) is selected and by Example (B) and Example (a),drank(EVE; EVED; EVDO0) is selected.As similmity between the input and Example (A) is largerthan that between the inpvt and Example (B), "took" is se-lected.
This is because similarity between {3bd862} but(3bf0fg} is 0.876535 and one between {3bd862} and{3bdbf6} is 0.6.
Exper iments  for  Verb  Se lect ionsThis chapter describes experiments oevaluate xamples,similarities and the generation algorithm.
Experiments forverb selections are executed.The generator selects one word from candklate word listretrieved from EDR English Dictionary.The experiments are (lone by Jack-knife test method(Sumita; 1992) ; 1) Specify a concept; 2) Collect examplesthat include a word in candidate word list whose meaning issame with the specified concept ; 3) Remove one examplefrom example sets; 4) Make tile input interlingua from theremoved example; 5) Generate a sentence from thisinterlingua by using remained examples; 6) Compare theoriginal word and the generated word for the verb; 7) Repeat3) - 6) by removing each example in turn.Below the results of three experiments (Experiment 1,Experiment 2, Experiment 3) me shown.
'Fable 6.1 shows specified concepts for experiments andcandidate word lists for the concepts.
As for Experiment 1and Experiment 2, words that have no examples is omittedfrom candidate word lists, since they won't never be se-lected.
Fig.6.1, Fig.6.2 and Fig.6.3 show examples and gen-erated sentences for Experiment 1, Experiment 2 and Ex-periment 3 each.
Examples in Fig.6.1 ,'rod Fig.6.2 are ex-tracted from EDR English Corpus and examples in Fig.6.3are extracted from a published printed English-Japanese dic-tionary, though some modifications (Tenses, aspects ,366modals are all same.
SI, bjects are same if possible) arc done.Sentences in the left hand sides of ,arrows are original sen-tences and those in the right hand side are generated sen-tences (In generated sentences, only verbs are generatedwords and others are copied from origimd sentences).
Un-derlined words are words for the specified concepts.
For sen-tences with a circle at the head of left hand sides, the genera-tor selects ame words with those in the original sentences.Sentences without circles include both right and wrong re-sults.In interlingua method, roughly speaking, all words corre-sponding to it concept are basically right its the generatedword if it is grammatically consistent.
So the evaluation oftire experiments i  delicate.The rates of coincides between original verbs and generoated verbs are 85% (Experiment 1), 13% (Experiment 2) and16% (Experiment 3).
Since some sentences without coin-cides can be also right, the real rates of success are lager thanabove nt, mbers.7.
ConclusionsThe English generation by the example-based meth+?l isdescrihed.
For experiments of verb sel.
'.+ctions, tile effective-hess of tile method is different for verbs to be generated.
(Inexperiment 3,for "confirn?'
and "endorse" the success rate ishigh), It also depends on concepts and the nunlber of candi-date words.Since examples are made automatically from large scalecorpus and to make examples is easier than to make rules,the effort to design the generator became little.
By removingredtmdant basic units, the efficiency of examples is not ser fOllS.In this paper, only the experiments for verb selections areshown.
But the strategies that the generator uses should wiryin response to the categories of words to be generated.
Forexample, to generate prepositions the semantic is more im-portant, bnt to generate other functiomtl words the syntax ismore important.
For verb selections, both are necessary.These strategies are also remained problems.Table 6.1 Concepts and Word ListExpriments Specified Concept Candidate Word ListE?l~riment I ~3cee4f)) acj?ev{e}(I~,VI!
)get(EVl9tak\[E) (F.V F+)(others are omitted)Ext~riment 2 (3ceae3)) get (EVE)grew (EVE)fall (EVE)(others are omitted)Experiment 3 (Ofdc5f)) accept (EVE)acknowledgie } (EVF.
)a(~fit (EVE)allow (EVE)answer (EVE)appmv{e} (EVE)confirm (EVE)endors{e} (FVE)grant (EVF+)receiv{e) (EVE)ratif{y} (EVE)recogniz{el (EVE)respond (EVE)homologat{e} (EVF.)ex.
01 : l ie had achieved a certain transquility.
'- lie had ,gin a certain transquility.ex.
02 : Q)You have ~ our keys.--'- You have ~our  keys.ex.
03 : (1)lie quietly ,got a broom.- " lle qt,ietly ~ it broom.ex.
04 : (..~lle g~ the menus.-,- lie ,tg~.the menus.ex.
05 : ~)ln the storm 1 took shelter under it lree."
In the storm I took shelter under it tree.ex.
06 : ( ) l l e  takes dangerous drugs.-- ~ lle takes dangerous drugs.ex.
07 : (.~The people look our old house.The people took our old house.Fig.6.1 l~xamples and Results of Experinmnt 1cx.
01 : Diantonds come expensive.?
~+ Diamonds become xpensive.ex.
02 : You ~rgLQw older.,- You become older.ex.
03 : A thing was bc~conairlg increasingly sure.A thing was gct_tir~ increasingly sure.ex.
04 : l{nvironment becomes individualized.
'- Enviromnent grows individualized.ex.
05 : A man ~ oM anyhow.-- '- A man becomes old anyhow.ex.
06 : These letters became the center of my existence."
These letters went the center of my existettce.ex.
07 : Almost unbearable my fantasies become.Ahnost unbearable my fantasies go.ex.
0g : Sonmthing bad ~ wrong.+ + Something had fallen wrong.ex.
09 : We had become good f,iends during my stayat the hospital.-+ We bad ~ good friends during my stayat the hospital.ex.
10 : You're the kind to go_ violent.- '- You're tile kind to become violent.ex.
11 : ( ) t le r  eyes became bright.-"  l ler eyes became bright.ex.
12 : Eventually it become a movie.- ~ Fventually it ~ a movie.ex.
13 : After a while the signal became a buzz..... After a while the signal wenl a buzz.ex.
14: It was ~ g  light.- " It was becoming light.ex.
15 : I le fell silent, its yesterday.- ~  lie went silent, as yesterday.ex.
16 : After a few jokes his speech became serious.-+ After a few jokes his speech went serious.ex.
17 : You'll gg$.
even fatter.-'- You'll ,rgre.w even fatter.ex.
18 : She became stout.--" She ~ stout.ex.
19 : The fish has ~ bad.+ The fish has become bad.ex.
20 : Q)lle suddenly became we:tlthy.+ lie suddenly became wealthy.ex.
21 : She became impatient.- " She went impatient.ex.
22 ; (.
)l ie became a priest.- "  l ie became a priest.Fig.6.2 l+.xmnples and Results of F+xperiment 2.367ex.
01: I ~ an invitation.- "  I allow an invitation.ex.
02 : 1 ~ an offer.- "  I .receive an offer.ex.
03 : I acknowledge a defeat.I acceot a defeat.ex.
04 : I acknowleclg?
his fight.-+ I ~ his right.ex.
05 : I acknowledge the truth of an argument.-.- 1 ~ the truth of an argument.ex.
06 : I admit a claim.- "  I allow a claim.ex.
07 : I admit defeat.--" I acknowledge defeat.ex.
08 : I admit my guilt.-~ 1 acknowledge my guilt.ex.
09 : I will admit no objection.- "  I will ~ no objection.ex.
10 : I allow a claim.1 ~ a claim.ex.
11 : I allow your ,argument.I confirm your argument.ex.
12 : I answer his wish.- "  I receive his wish.ex.
13 : I ~ e  a bill.- "  I acceot a bi l lex.
14 : I ap_prove a resolution.- ,  1 confirm a resolution.ex.
15 : 1 ,approve accounts.- "  I ~ accounts.ex.
16 : Q)I confirm a treaty.- "  I confirm a treaty.ex.
17 : Q)I confirm an appointment.- "  I confirm an appointment.ex.
18 : 1 .confirm a verbal promise.-~- I a_~rove a verbal promise.ex.
19 : I confirm a telegraphic order.-~- I answer a telegraphic order.ex.
20 : I confirm possession to him.- "  I ~,cknowledge possession to him.ex.
21 : 1 confirm a functionary in his new office.-~ 1 ~ a  functionary in his new office.ex.
22 : Q")I endorse his opinion.I endorse his opinion.ex.
23 : O I  endorse a policy.--~ I endorse a policy.ex.
24 : I ~ a  request.-~ I acknowledge a request.ex.
25 : The king granted the old woman her wish.- "  The king answered the old woman her wish.ex.
26 : Japan receive a treaty.- "  Japan ratifies a treaty.ex.
27 : QParl iament ratified the agreement.--, Parliament ratified the agreement.ex.
28 : I receive a proposal.--+ I ~ a  proposal.ex.
29 : I receive an offer.-~ I accepK_an offer.ex.
30 : I receive a petition.--" I answer a petition.ex.
31 : Q)Priest receives his confession.- "  Priest receives his confession.ex.
32 : Priest receives his oath.
'- Priest ratifies his oath.ex.
33 : I recognize a claim as justified.--~ I allow a claim as justified.Fig.6.3 Examples and Results of Experiment 3ex.
34 : Japan recognizes the independenceof a new state.Japan acknowledges the independence of ...ex.
35 : He ~qu ick ly  to the appealfor subscriptions.- "  He ~qu ick ly  to the appeal for ...Fig.6.3 Examples and Results of Experiment 3 (remainder)ReferenceCui, J., Komatsu, E. and Yasnhara, tl.
(1993).
A Calculationof Similarity between Words Using EDR ElectronicDictionary.
Reprint of ll'SJ, Vol.93, No.1 (in Japanese)EDR (1993a).
EDR Electronic Dictionary SpecificationGuide.TR.04 l.EDR (1993b).
English Word Concept Dictionary.
TR-026Komatsu, E., Cni, J. and Yasuhara, II.
(1993).
A Mono-lin-gual Corpus-Based Machine Translation of the InterlinguaMethod.
Fifth International Conference on Theoretical andMethodological IssuesNagao, M. (1984).
A Framework of A Mechanical Transla-tion between Japmlese and English by Analogy Principle.Artificial and Human Intelligence (A. Elithorn and R.Banerji, editors) Elsevier Science Publishers, B.V.Sadler, V. (1989).
Working with Analogical Semantics, Dis-ambiguation Tect, niques in DLT, Foris Publ icat ions,Dordrecht Holland.Sato, S. (1991).
Example-Based Translation Approach.Proc.
(g'International Workshop on Fundamental Researchfor theFuture Generation of Natural Language Processing, ATRInterpreting Telephony Research Laboratories, pp.
1-16.Sumita, g. and Iida, 11.
(1992).
Example-Based Transfer ofJapanese Adnominal Particles into English.
IEICE TRANS.INF.
&SYST., VOL.
E75-D, NO.4368
