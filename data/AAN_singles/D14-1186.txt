Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774?1778,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsJoint Learning of Chinese Words, Terms and KeywordsZiqiang Cao1Sujian Li1Heng Ji21Key Laboratory of Computational Linguistics, Peking University, MOE, China2Computer Science Department, Rensselaer Polytechnic Institute, USA{ziqiangyeah, lisujian}@pku.edu.cn jih@rpi.eduAbstractPrevious work often used a pipelinedframework where Chinese word segmen-tation is followed by term extraction andkeyword extraction.
Such framework suf-fers from error propagation and is un-able to leverage information in later mod-ules for prior components.
In this paper,we propose a four-level Dirichlet Processbased model (DP-4) to jointly learn theword distributions from the corpus, do-main and document levels simultaneously.Based on the DP-4 model, a sentence-wiseGibbs sampler is adopted to obtain propersegmentation results.
Meanwhile, termsand keywords are acquired in the samplingprocess.
Experimental results have shownthe effectiveness of our method.1 IntroductionFor Chinese language which does not contain ex-plicitly marked word boundaries, word segmenta-tion (WS) is usually the first important step formany Natural Language Processing (NLP) tasksincluding term extraction (TE) and keyword ex-traction (KE).
Generally, Chinese terms and key-words can be regarded as words which are repre-sentative of one domain or one document respec-tively.
Previous work of TE and KE normally usedthe pipelined approaches which first conductedWS and then extracted important word sequencesas terms or keywords.It is obvious that the pipelined approaches areprone to suffer from error propagation and fail toleverage information for word segmentation fromlater stages.
Here, we provide one example in thedisease domain, to demonstrate the common prob-lems in current pipelined approaches and proposethe basic idea of our joint learning of words, termsand keywords.Example: @??
(thrombocytopenia) (with){?
(heparinoid) 	(have) s?
(relation).This is a correctly segmented Chinese sen-tence.
The document containing the example sen-tence mainly talks about the property of ?{?(heparinoid)?
which can be regarded as one key-word of the document.
At the same time, theword@??
(thrombocytopenia) appears fre-quently in the disease domain and can be treatedas a domain-specific term.However, for such a simple sentence, currentsegmentation tools perform poorly.
The segmen-tation result with the state-of-the-art ConditionalRandom Fields (CRFs) approach (Zhao et al.,2006) is as follows:@(blood platelet) ?(reduction) ?
(symptom){(of same kind) ?
(liver) 	(always)s?
(relation)where @??
is segmented into three com-mon Chinese words and {?
is mixed with itsneighbors.In a text processing pipeline of WS, TE andKE, it is obvious that imprecise WS results willmake the overall system performance unsatisfy-ing.
At the same time, we can hardly make use ofdomain-level and document-level information col-lected in TE and KE to promote the performanceof WS.
Thus, one question comes to our minds:can words, terms and keywords be jointly learnedwith consideration of all the information from thecorpus, domain, and document levels?Recently, the hierarchical Dirichlet process(HDP) model has been used as a smoothed bigrammodel to conduct word segmentation (Goldwateret al., 2006; Goldwater et al., 2009).
Meanwhile,one strong point of the HDP based models is thatthey can model the diversity and commonality inmultiple correlated corpora (Ren et al., 2008; Xuet al., 2008; Zhang et al., 2010; Li et al., 2012;Chang et al., 2014).
Inspired by such existingwork, we propose a four-level DP based model,17740G1GwH mwH imjw imwH1mwH NmmwH3?
?????jmN2?1?0?
M| |VFigure 1: DP-4 Modelnamed DP-4, to adapt to three levels: corpus, do-main and document.
In our model, various DPsare designed to reflect the smoothed word distri-butions in the whole corpus, different domains anddifferent documents.
Same as the DP based seg-mentation models, our model can be easily usedas a semi-supervised framework, through exertingon the corpus level the word distributions learnedfrom the available segmentation results.
Refer-ring to the work of Mochihashi et al.
(2009), weconduct word segmentation using a sentence-wiseGibbs sampler, which combines the Gibbs sam-pling techniques with the dynamic programmingstrategy.
During the sampling process, the impor-tance values of segmented words are measured indomains and documents respectively, and words,terms and keywords are jointly learned.2 DP-4 ModelGoldwater et al.
(2006) applied the HDP model onthe word segmentation task.
In essence, Goldwa-ter?s model can be viewed as a bigram languagemodel with a unigram back-off.
With the lan-guage model, word segmentation is implementedby a character-based Gibbs sampler which repeat-edly samples the possible word boundary posi-tions between two neighboring words, conditionedon the current values of all other words.
How-ever, Goldwater?s model can be deemed as mod-eling the whole corpus only, and does not distin-guish between domains and documents.
To jointlylearn the word information from the corpus, do-main and document levels, we extend Goldwater?smodel by adding two levels (domain level and doc-ument level) of DPs, as illustrated in Figure 1.2.1 Model DescriptionM DPs (Hmw;1 ?
m ?
M ) are designed specif-ically to word w to model the bigram distribu-tions in each domain and these DPs share anoverall base measure Hw, which is drawn fromDP (?0, G1) and gives the bigram distribution forthe whole corpus.
Assuming the mthdomain in-cludes Nmdocuments, we use Hmjw(1 ?
j ?Nm) to model the bigram distribution of the ithdocument in the domain.
Usually, given a do-main, the bigram distributions of different docu-ments are not conditionally independent and simi-lar documents exhibit similar bigram distributions.Thus, the bigram distribution of one document isgenerated according to both the bigram distribu-tion of the domain and the bigram distributionsof other documents in the same domain.
That is,Hmjw?
g(?3, Hmw, Hm?jw) where Hm?jwrepre-sents the bigram distributions of the documents inthe mthdomain except the jthdocument.
Assum-ing the jthdocument in the mthdomain containsNjmwords, each word is drawn according toHmjw.That is, wmji?
Hmjw(1 ?
i ?
Njm).
Thus, ourfour-level DP model can be summarized formallyas follows:G1?
DP (?0, G0) ;Hw?
DP (?1, G1)Hmw?
DP (?2, Hw) ;Hmjw?
g(?3, Hmw, Hm?jw)wmji|wi?1= w ?
HdwHere, we provide for our model the ChineseRestaurant Process (CRP) metaphor, which cancreate a partition of items into groups.
In ourmodel, the word type of the previous word wi?1corresponds to a restaurant and the current wordwicorresponds to a customer.
Each domain isanalogous to a floor in a restaurant and a room de-notes a document.
Now, we can see that there are|V | restaurants and each restaurant consists of Mfloors.
Themthfloor containsNmrooms and eachroom has an infinite number of tables with infiniteseating capacity.
Customers enter a specific roomon a specific floor of one restaurant and seat them-selves at a table with the label of a word type.
Dif-ferent from the standard HDP, each customer sitsat an occupied table with probability proportionalto both the numbers of customers already seatedthere and the numbers of customers with the sameword type seated in the neighboring rooms, and atan unoccupied table with probability proportionalto both the constant ?3and the probability that the1775customers with the same word type are seated onthe same floor.2.2 Model InferenceIt is important to build an accurate G0which de-termines the prior word distribution p0(w).
Sim-ilar to the work of Mochihashi et al.
(2009), weconsider the dependence between characters andcalculate the prior distribution of a word wiusingthe string frequency statistics (Krug, 1998):p0(wi) =ns(wi)?ns(.
)(1)where ns(wi) counts the character string com-posed of wiand the symbol ?.?
represents anyword in the vocabulary V .Then, with the CRP metaphor, we can obtain theexpected word unigram and bigram distributionson the corpus level according to G1and Hw:p1(wi) =n (wi) + ?0p0(wi)?n (.)
+ ?0(2)p2(wi|wi?1= w) =nw(wi) + ?1p1(wi)?nw(.)
+ ?1(3)where the subscript numbers indicate the corre-sponding DP levels.
n(wi) denotes the number ofwiand nw(wi) denotes the number of the bigram< w,wi> occurring in the corpus.
Next, we caneasily get the bigram distribution on the domainlevel by extending to the third DP.pm3(wi|wi?1= w) =nmw(wi) + ?2p2(wi|wi?1)?nmw(.)
+ ?2(4)where nmw(wi) is the number of the bigram <w,wi> occurring in the mthdomain.To model the bigram distributions on the docu-ment level, it is beneficial to consider the influenceof related documents in the same domain (Wanand Xiao, 2008).
Here, we only consider the in-fluence from theK most similar documents with asimple similarity metric s(d1, d2) which calculatesthe Chinese character overlap ratio of two docu-ments d1and d2.
Let djmdenote the jthdocumentin the mthdomain and djm[k](1 ?
k ?
K) the Kmost similar documents.
djmcan be deemed to be?lengthened?
by djm[k](1 ?
k ?
K).
Therefore,we estimate the count of wiin djmas:tdjmw(wi) = ndjmw(wi)+?ks(djm[k], djm)ndjm[k]w(wi)(5)where ndjm[k]w(wi) denotes the count of the bigram< w,wi> occurring in djm[k].
Next, we modelthe bigram distribution in djmas a DP with the basemeasure Hmw:pdjm4(wi|wi?1= w) =tdjmw(wi) + ?3pm3(wi|wi?1)?tdjmw(.)
+ ?3(6)With CRP, we can also easily estimate the un-igram probabilities pm3(wi) and pdjm4(wi) respec-tively on the domain and document levels, throughcombining all the restaurants.To measure whether a word is eligible to be aterm, the score function THm(?)
is defined as:THm(wi) =pm3(wi)p1(wi)(7)This equation is inspired by the work of Nazar(2011), which extracts terms with consideration ofboth the frequency in the domain corpus and thefrequency in the general reference corpus.
Similarto Eq.
7, we define the functionKHdjm(?)
to judgewhether wiis an appropriate keyword.KHdjm(wi) =pdjm4(wi)p1(wi)(8)During each sampling, we make use of Eqs.
(7)and (8) to identify the most possible terms andkeywords.
Once a word is identified as a termor keyword, it will drop out of the sampling pro-cess in the following iterations.
Its CRP explana-tion is that some customers (terms and keywords)find their proper tables and keep sitting there after-wards.2.3 Sentence-wise Gibbs SamplerThe character-based Gibbs sampler for word seg-mentation (Goldwater et al., 2006) is extremelyslow to converge, since there exists high correla-tion between neighboring words.
Here, we intro-duce the sentence-wise Gibbs sampling techniqueas well as efficient dynamic programming strat-egy proposed by Mochihashi et al.
(2009).
Thebasic idea is that we randomly select a sentencein each sampling process and use the Viterbi al-gorithm (Viterbi, 1967) to find the optimal seg-mentation results according to the word distribu-tions derived from other sentences.
Different fromMochihashi?s work, once terms or keywords are1776identified, we do not consider them in the segmen-tation process.
Due to space limitation, the algo-rithm is not detailed here and can be referred in(Mochihashi et al., 2009).3 Experiment3.1 Data and SettingIt is indeed difficult to find a standard evaluationcorpus for our joint tasks, especially in differentdomains.
As a result, we spent a lot of time to col-lect and annotate a new corpus1composed of tendomains (including Physics, Computer, Agricul-ture, Sports, Disease, Environment, History, Art,Politics and Economy) and each domain is com-posed of 200 documents.
On average each doc-ument consists of about 4800 Chinese characters.For these 2000 documents, three annotators havemanually checked the segmented words, terms andkeywords as the gold standard results for evalu-ation.
As we know, there exists a large amountof manually-checked segmented text for the gen-eral domain, which can be used as the training datafor further segmentation.
As with other nonpara-metric Bayesian models (Goldwater et al., 2006;Mochihashi et al., 2009), our DP-4 model can beeasily amenable to semi-supervised learning byimposing the word distributions of the segmentedtext on the corpus level.
The news texts pro-vided by Peking University (named PKU corpus)2is used as the training data.
This corpus containsabout 1,870,000 Chinese characters and has beenmanually segmented into words.In our experiments, the concentration coeffi-cient (?0) is finally set to 20 and the other three(?1?3) are set to 15.
The parameter K which con-trols the number of similar documents is set to 3.3.2 Performance EvaluationThe following baselines are implemented for com-parison of segmentation results: (1) Forward max-imum matching (FMM) algorithm with a vocab-ulary compiled from the PKU corpus; (2) Re-verse maximum matching (RMM) algorithm withthe compiled vocabulary; (3) Conditional RandomFields (CRFs)3based supervised algorithm trainedfrom the PKU corpus; (4) HDP based semi-supervised algorithm (Goldwater et al., 2006) us-1Nine domains are from http://www.datatang.com/data/44139 and we add an extra Disease domain.2http://icl.pku.edu.cn3We adopt CRF++(http://crfpp.googlecode.com/svn/trunk/doc/index.html)ing the PKU corpus.
The strength of Mochi-hashi et al.
(2009)?s NPYLM based segmentationmodel is its speed due to the sentence-wise sam-pling technique, and its performance is similar toGoldwater et al.
(2006)?s model.
Thus, we do notconsider the NPYLM based model for compari-son here.
Then, the segmentation results of FMM,RMM, CRF, and HDP methods are used respec-tively for further extracting terms and keywords.We use the mutual information to identify the can-didate terms or keywords composed of more thantwo segmented words.
As for DP-4, this recogni-tion process has been done implicitly during sam-pling.
To measure the candidate terms or key-words, we refer to the metric in Nazar (2011) tocalculate their importance in some specific domainor document.The metrics of F1and the out-of-vocabularyRecall (OOV-R) are used to evaluate the segmenta-tion results, referring to the gold standard results.The second and third columns of Table 1 show theF1and OOV-R scores averaged on the 10 domainsfor all the compared methods.
Our method sig-nificantly outperforms FMM, RMM and HDP ac-cording to t-test (p-value ?
0.05).
From the seg-mentation results, we can see that the FMM andRMM methods are highly dependent on the com-piled vocabulary and their identified OOV wordsare mainly the ones composed of a single Chinesecharacter.
The HDP method is heavily influencedby the segmented text, but it also exhibits the abil-ity of learning new words.
Our method only showsa slight advantage over the CRF approach.
Wecheck our segmentation results and find that theperformance of the DP-4 model is depressed bythe identified terms and keywords which may becomposed of more than two words in the goldstandard results, because the DP-4 model alwaystreats the term or keyword as a single word.
Forexample, in the gold standard, ??W?((LingnanCulture)?
is segmented into two words ??W?
and?
??, ?pn??
(data interface)?
is segmentedinto ?pn?
and ????
and so on.
In fact, our seg-mentation results correctly treat ??W??
and ?pn???
as words.To evaluate the TE and KE performance, the top50 (TE-50) and 100 (TE-100) accuracy are mea-sured for the identified terms of one domain, whilethe top 5 (KE-5) and 10 (KE-10) accuracy for thekeywords in one document, are shown in the rightfour columns of Table 1.
We can see that DP-17774 performs significantly better than all the othermethods in TE and KE results.As for the ten domains, we find our approachbehaves much better than the other approaches onthe following three domains: Disease, Physics andComputer.
It is because the language of thesethree domains is much different from that of thegeneral domain (PKU corpus), while the rest do-mains are more similar to the general domain.Method F1 OOV-R TE-50 TE-100 KE-5 KE-10FMM 0.796 0.136 0.420 0.360 0.476 0.413RMM 0.794 0.136 0.424 0.352 0.478 0.414HDP 0.808 0.356 0.672 0.592 0.552 0.506CRF 0.817 0.330 0.624 0.560 0.543 0.511DP-4 0.821 0.374 0.704 0.640 0.571 0.545Table 1: Comparison of WS, TE and KE Perfor-mance (averaged on the 10 domains).4 ConclusionThis paper proposes a four-level DP based modelto construct the word distributions from the cor-pus, domain and document levels simultaneously,through which Chinese words, terms and key-words can be learned jointly and effectively.
Inthe future, we plan to explore how to combinemore features such as part-of-speech tags into ourmodel.AcknowledgmentsWe thank the three anonymous reviewers fortheir helpful comments.
This work was par-tially supported by National High Technology Re-search and Development Program of China (No.2012AA011101), National Key Basic ResearchProgram of China (No.
2014CB340504), Na-tional Natural Science Foundation of China (No.61273278), and National Key Technology R&DProgram (No: 2011BAH10B04-03).
The contactauthor of this paper, according to the meaninggiven to this role by Peking University, is SujianLi.ReferencesBaobao Chang, Wenzhe Pei, and Miaohong Chen.2014.
Inducing word sense with automaticallylearned hidden concepts.
In Proceedings of COL-ING 2014, pages 355?364, Dublin, Ireland, August.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Contextual dependencies in un-supervised word segmentation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 673?680.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A bayesian framework for wordsegmentation: Exploring the effects of context.Cognition, 112(1):21?54.Manfred Krug.
1998.
String frequency: A cognitivemotivating factor in coalescence, language process-ing, and linguistic change.
Journal of English Lin-guistics, 26(4):286?320.Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and BaobaoChang.
2012.
Update summarization using a multi-level hierarchical dirichlet process model.
In Pro-ceedings of Coling 2012, pages 1603?1618, Mum-bai, India.Daichi Mochihashi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian unsupervised word segmen-tation with nested pitman-yor language modeling.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP: Volume 1-Volume 1, pages 100?108.Rogelio Nazar.
2011.
A statistical approach to termextraction.
IJES, International Journal of EnglishStudies, 11(2):159?182.Lu Ren, David B. Dunson, and Lawrence Carin.
2008.The dynamic hierarchical dirichlet process.
In Pro-ceedings of the 25th international conference onMachine learning, pages 824?831.Andrew J. Viterbi.
1967.
Error bounds for convolu-tional codes and an asymptotically optimum decod-ing algorithm.
IEEE Transactions on InformationTheory, pages 260?269.Xiaojun Wan and Jianguo Xiao.
2008.
Singledocument keyphrase extraction using neighborhoodknowledge.
In AAAI, volume 8, pages 855?860.Tianbing Xu, Zhongfei Zhang, Philip S. Yu, andBo Long.
2008.
Dirichlet process based evolution-ary clustering.
In ICDM?08, pages 648?657.Jianwen Zhang, Yangqiu Song, Changshui Zhang, andShixia Liu.
2010.
Evolutionary hierarchical dirich-let processes for multiple correlated time-varyingcorpora.
In Proceedings of the 16th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 1079?1088, New York, NY,USA.Hai Zhao, Chang-Ning Huang, and Mu Li.
2006.
Animproved chinese word segmentation system withconditional random field.
In Proceedings of the FifthSIGHAN Workshop on Chinese Language Process-ing, volume 1082117.1778
