Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 23?33,Dublin, Ireland, August 23rd 2014.Towards Model Driven Architectures for Human Language TechnologiesAlessandro Di BariIBM Center forAdvanced Studies of TrentoPovo di TrentoPiazza Manci 12alessandro dibari@it.ibm.comKateryna TymoshenkoTrento RISEPovo di TrentoVia Sommarive 18k.tymoshenko@trentorise.euGuido VetereIBM Center forAdvanced Studies of TrentoPovo di TrentoPiazza Manci 12guido vetere@it.ibm.comAbstractDeveloping multi-purpose Human Language Technologies (HLT) pipelines and integrating theminto the large scale software environments is a complex software engineering task.
One needsto orchestrate a variety of new and legacy Natural Language Processing components, languagemodels, linguistic and encyclopedic knowledge resources.
This requires working with a varietyof different APIs, data formats and knowledge models.
In this paper, we propose to employthe Model Driven Development (MDD) approach to software engineering, which provides richstructural and behavioral modeling capabilities and solid software support for model transforma-tion and code generation.
These benefits help to increase development productivity and qualityof HLT assets.
We show how MDD techniques and tools facilitate working with different dataformats, adapting to new languages and domains, managing UIMA type systems, and accessingthe external knowledge bases.1 IntroductionModern architectures of knowledge-based computing (cognitive computing) require HLT componentsto interact with increasingly many sources and services, such as Open Data and APIs, which may notbe known before the system is designed.
IBM?s Watson1, for instance, works on textual documents toprovide question answering and other knowledge-based services by integrating lexical resources, ontolo-gies, encyclopaedic data, and potentially any available information source.
Also, they combine a varietyof analytical procedures, which may use search, reasoning services, database queries, to provide answersbased on many kinds of evidence (IJRD, 2012).
Development platforms such as UIMA2or GATE3facil-itate the development of HLT components to a great extent, by providing tools for annotating texts, basedon vocabularies and ontologies, training and evaluating pipeline components, etc.
However, in general,they focus on working with specific analytical structures (annotations), rather than integrating distributedservices and heterogeneous resources.
Such integration requires great flexibility in the way linguistic andconceptual data are encoded and exchanged, since each independent service or resource may adopt dif-ferent representations for notions whose standardization is still in progress.
Therefore, developing HLTsystems and working with them in such environments requires modeling, representing, mapping, manip-ulating, and exchanging linguistic and conceptual data in a robust and flexible way.
Supporting thesetasks with mature methodologies, representational languages, and development environments appears tobe of paramount importance.Since the early 90s, Computer Sciences have envisioned methodologies, languages, practices, andtools for driving the development of software architectures by models (Model Driven Architecture,MDA)4.
The MDA approach starts from providing formal descriptions (models) of requirements, inter-actions, data structures, protocols and many other aspects of the desired system.
Then, models are turnedThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1http://www.ibm.com/innovation/us/watson/2http://uima.apache.org/3https://gate.ac.uk/4http://www.omg.org/mda/23into technical resources (e.g.
schemes and software modules) by means of programmable transforma-tion procedures.
Model-to-model transformations, both at schema and instance level, are also supported,based on model correspondence rules (mappings) that can be programmed in a declarative way.As part of the development of UIMA-based NLP components, easily pluggable in services ecosystems,we are working on an open, flexible and interoperable pipeline, based on MDA.
We want our platformto be language agnostic and domain independent, to facilitate its use across projects and geographies.To achieve this, we adopted the Eclipse Modeling Framework5as modeling and developing platform,and we generate UIMA resources (Type System) as a Platform Specific Model, by means of a specifictransformation.
We started by designing models of all the required components, and analyzed the way toimprove usability and facilitate interoperability by providing appropriate abstraction and layering.In the present paper we provide the motivation of our architectural choices, we illustrate the basicfeatures, and discuss relevant issues.
Also, we provide some example of how MDA facilitates the designand the implementation of open and easily adaptable HLT functionalities.2 Model-driven Development and NLP2.1 Model-driven DevelopmentGenerally speaking, we talk about a ?modeling?
language when it is possible to visually represent (ormodel) objects under construction (such as services and objects) from both a structural and behav-ioral point of view.
The most popular (software) modeling language is the Unified Modeling Language(UML)(Rumbaugh et al., 2005).
One of the strengths of such language is that it allows clear and trans-parent communication among different stakeholders and roles such as developers, architects, analyst,project managers and testers.Model Driven Development (MDD) is a software development approach aimed at improving qualityand productivity by raising the level of abstraction of services and related objects under development.Given an application domain, we usually have a ?business?
model that allows for fast and highly expres-sive representation of specific domain objects.
Based on business models, the MDD tooling providesfacilities to generate a variety of platform specific ?executable models?62.2 Model-driven ArchitectureModel Driven Architecture (MDA)(Miller and Mukerji, 2003) is a development approach, strictly basedon formal specifications of information structures and behaviors, and their semantics.
MDA is promotedby the Object Management Group (OMG)7based on several modeling standards such as: Unified Mod-eling Language (UML)8, Meta-Object Facility (MOF9), XML Metadata Interchange (XMI) and others.The aim is to provide complex software environments with a higher level of abstraction, so that theapplication (or service) can be completely designed independently from the underlying technologicalplatform.
To this end, MDA defines three macro modeling layers:?
Computation Independent Model (CIM) abstract from any possible (software) automation; usu-ally business processes are represented at this layer?
Platform Independent Model (PIM) represents any software automation (that supports the CIM)but this representation is really independent from any technological constraint such as the runningplatform or related middleware, or any third party software.
Usually, the skeleton or structure ofsuch a model is automatically generated from the CIM but it is expected to be further expanded for5http://www.eclipse.org/modeling/emf/6For a typical MDD approach, there are two different possible implementation strategies: the first one is to customizegeneric modeling languages (such as UML) by providing specific profiles (representing the business domain) for the language(UML is very generic and it offers several extensibility mechanisms such as profiles); the second one is a stronger approachthat leads to what we call a DSL (Domain Specific Language), a fully specified, vertical language for a particular domain.
Sucha language is usually built in order to allow business experts to directly work on it, without the need of technological skills.7http://omg.org/8http://www.uml.org/9http://www.omg.org/mof/24a fully specification.
PIM can be expressed in UML (Rumbaugh et al., 2005) or EMF10but also inany peculiar DSL (domain specific language).?
Platform Specific Model (PSM) can be completely generated from the PIM.
Among other things,this requires PIM to be able to represent every aspect of the solution under development: bothstructural and behavioral parts have to be fully specified at this level.2.3 Eclipse Modeling Framework (EMF)We chose to adopt EMF as the underlying modeling framework and tooling for our model-driven ap-proach.
EMF is an Eclipse11-based modeling framework and code generation facility.
Ecore is the coremeta-model for EMF.
Ecore represents the reference implementation of OMG?s EMOF (Essential Meta-Object Facility).
EMF is at the heart of several important tools and applications and it is often usedto represent meta-models and their instances (models) management.
Just as an example, UML opensource implementation defines its meta-model in terms of EMF-Ecore.
Applications or tools that useEcore to represent their meta model can leverage the EMF tooling to accomplish several goals such asthe code generation for model management, diagramming and editing of models, transformations visualdevelopment and so on.2.4 Model-driven NLPWe considered the main features of the Model Driven approach as a powerful way to handle the com-plexity of modern HLT use cases (Di Bari et al., 2013).
In adopting this approach, we chose UIMA12asa standardized and well supported tool to deploy our Platform Specific models.With respect to the basic features of the UIMA platform, we wanted to enhance:?
The representation, visualization and management of complex linguistic and conceptual models?
The use of many kinds of data specifications?
The interaction with many kinds of platforms and servicesTherefore, we decided to design a set of models in EMF, considering this as our PIM layer.
From thesemodels, we can generate PSM components to support a variety of tasks, including:?
A UIMA Type System for implementing NLP pipelines?
A set of data format transformations for working with linguistic corpora?
A set of basic interfaces to access linguistic and knowledge services?
A Business Object Model (BOM) to work with rule engines (business rules management systems)3 Modeling NLP with EMF and UIMA3.1 Working with data formatsIn order to train our statistical parser based on OpenNLP13and UIMA, we had to adapt different corporaformats, (such as PENN14and CONLL15), because OpenNLP requests them for different analysis classes(such as named entity, part-of-speech, chunking, etc.).
In fact, we had to transform available corporafrom standard formats to OpenNLP specific ones.
We represented standard formats with EMF (an Ecoremodel for each one) and we created specific transformations using Java Emitter Templates (JET)16, a10http://www.eclipse.org/modeling/emf/11http://eclipse.org/12http://uima.apache.org/13http://opennlp.apache.org/14http://www.cis.upenn.edu/?treebank/15http://ilk.uvt.nl/conll/#dataformat16http://www.eclipse.org/modeling/m2t/?project=jet25framework for fast code generation based on EMF.
This solution gives us a lot of flexibility: if the parseror corpora formats change, we just have to adapt EMF models and/or JET templates consistently.
For abetter understanding, we show here an example of the JET template used for transforming from CONLLto OpenNLP POSTag format:<c:setVariable select="/contents" var="root"/><c:iterate select="$root/sentence" var="sentence"><c:iterate select="$sentence/token" var="token"><c:get select="$token/@FORM"/>_<c:get select="$token/@CPOSTAG"/></c:iterate></c:iterate>Having the simple CONLL Ecore model (in Figure 1) available, this template is the only artifact realizingthe needed transformation.
This template simply iterates over all sentences of a document (root),then over all tokens of a sentence and print out the form and its postag in the requested format.Other format conversions are done with the same technique.
Notice that this tool allows to write (evencomplex) transformation without requiring programming skills, thus ensuring a greater maintainabilityand lowering the overall cost of the project.Figure 1: A simple EMF model representing CONLL formatFigure 2: A sentence represented in PENN format (EMF editor on the left)Further, as we can notice from the Figure 2, EMF provides very powerful MDD capabilities also fromthe modeling instantiation and editing point of view.
Notice that these are two view of the very sameresource (a PENN file).
This is done by ?teaching?
the EMF framework how to parse the underlying per-sisted data, by providing an implementation (that can be done manually or using some parser generationtool in its turn) through a specific EMF extension point.
From that point on, the framework will always26be able to open and manage PENN resources as instances (with all semantic constraints and validationavailable) of the PENN Ecore model.
Furthermore, the editor and all the specific model managementtooling are automatically generated by the EMF platform.
This way, all instance management (and edit-ing) tasks are transferred to the EMF framework, which reduces costs and helps developers focusing onNLP tasks.3.2 Managing the UIMA Type SystemIn order to manage a (complex enough) UIMA Type System, we fully leveraged the EMF/UIMA in-teroperability provided by the UIMA framework.
UIMA already provides simple transformations fromEMF to a UIMA type system and viceversa; furthermore, UIMA provides the XMI serialization so thatan instance of a type system can be read as an instance of the corresponding EMF model.
However,we had to modify the transformation from EMF to UIMA (type system) in order to reach our ?modeldriven?
goals and also to handle several dependent, but separated EMF modules.
Specifically, our modelis organized around the following modules (packages):?
Text.
A model of linguistic occurrences in their context, which are given to the parser (token,sentences, named entities, parse) and get annotated by the underlying document analysis framework(e.g.
UIMA).?
Linguistics A model that abstracts the linguistic apparatus, including categories, morphological andsyntactic features, and relationships.
This is the key for the multilinguality we walk through in thenext section.
We can see a fragment of this model in Figure 3?
Ontology An abstract representation of any entity and concept, along with a uniform interface to avariety of existing knowledge bases?
Mapping A model that allows bridging any tag-set, also explained in next section.Figure 3: A fragment of the Linguistic model3.3 Adapting to new languages and domainsGiven this basis, we can now illustrate how we are able to incorporate new linguistic resources (such asvocabularies, ontologies, training corpora with different formats, tag-sets, etc) obtaining UIMA-basedpipeline components.
Given an (untrained) statistical library for parsing (such as OpenNLP or others)and a new natural language to represent, we do the following:1.
Analyze the format requested for the training corpora by the NLP parsing library versus the availabletraining data for the new language.
Most likely the training data will be in some standard format(e.g.
CONLL, PENN, IOB) that is already represented by a suitable Ecore (EMF) model.
If not yetpresent, we have to write a transformation (usually a simple JET template) from the standard to thespecific one requested by the parser.272.
Train the parser for the specific language and tag-set.3.
Given the linguistic knowledge for the new language, and the task at hand, we adopt/modify/extenda suitable Linguistics model.
If needed, we generate the UIMA Type System portion corre-sponding to the requested features.174.
In case of a new business domain also, we adopt/modify/extend the ontology.
If needed, we wrapbusiness knowledge bases where the ontology is instantiated.185.
Given the tag-set in use, we represent the mapping with the linguistic model, by instantiating asuitable Mapping model.
At run-time, the (UIMA-based) parser asks (the mapping manager) toget a Linguistic Category (see Figure 4) of a given tag (the key feature in the Mapping model).Given the workflow above, we can figure out how the Linguistics and Mapping model are play-ing a key role for achieving a multi-language solution.
Just as an example, for the Italian language, theLinguistics model defines 55 Word Classes (part of speech), 6 morpho-syntactic attributes com-posed by 22 morpho-syntactic features and 27 syntactic dependencies.
The mapping model for theEVALITA19tagset, contains 115 mapping elements.Figure 4: A fragment of the mapping between tag-set and the Linguistic modelWe can now prove how the specific language knowledge has been encapsulated in the suitable model sothat for example, the code that manages the parser results and creates UIMA annotations does not changefor a new language.
Correspondingly, no programming skills are needed to represent this linguisticknowledge.3.4 Benefits for NLP developmentTo show the benefits of our approach, we summarize here the following remarks:?
Whereas we consider UIMA the reference framework for document management and annotation,we also leveraged the features of a mature and stable modeling framework such as EMF.
For in-stance, we could exploit diagramming, model and instance management and code generation.
Insum, we saved significant development time by means of higher level of abstraction that allowed usto easily create transformations, create mapping models, smoothly use different format for the samedata and so on.?
Having an additional level of abstraction can lead in thinking there is an additional cost; however, asstated above, this is not the case for transforming from EMF to UIMA.
The only, real additional costis represented by the effort of developing transformations.
Nevertheless, this is quickly absorbed assoon as the transformation is re-used.
In our case, just for data conversions, we re-used the same17Steps 2, 3, 4 are done through our Eclipse-based tooling.18The benefit of abstracting semantic information from the Type System has been illustrated in (Verspoor et al., 2009)19http://www.evalita.it/28EMF models (and their underlying data parsing capabilities we implemented) several times.
Eachtime we wanted to change a parser library, or new corpora were available, we reused the same EMFmodels and techniques.?
The abstraction introduced by the Linguistic model, allowed us to create a language-independent parsing software layer.
Furthermore, the same model was leveraged in order to allowinteroperability between different parsers that were using different tagsets.4 Using an External Knowledge BaseState-of-the-art NLP algorithms frequently employ semantic knowledge about entities or concepts, men-tioned in the text being processed.
This kind of knowledge is typically extracted from the externallexicons and knowledge bases (KB)20, e.g.
WordNet (Fellbaum, 1998) or Wikipedia21.
Therefore, inour NLP stack, we have to model extraction, usage and representation of semantic knowledge from theexternal KBs each of which might have different structure and access protocols.
Moreover, we mayneed to plug new KBs into our software environment, with the least effort.
Finally, in complex servicesecosystems, we might also need to use the KB outside of the NLP system, e.g.
in a remote reasoningsystem, and we should be able to reuse the same model for these purposes.MDD allows us to define the a single ontology/knowledge base (KB) abstraction, i.e.
KB PIM, basedon the high-level requirements of our software system, regardless of the actual implementation details ofthe KBs to be used.
In contrast to UIMA type systems, which are limited to type hierarchies and typefeature structures with the respective getters and setters, MDD allows to specify also functionalities, i.e.methods, such as querying and updating the KB.
Figure 5 demonstrates a diagram of a KB abstraction thatwe employ in the Ontology module.
In this abstraction KnowledgeBase is a collection of Individualsand Concepts, Relationships and Roles, which all are subclasses of the Entity abstraction.
The figurecontains also the visualizations of some components from the Text module, which show how we modelannotating text with references to the KB elements.
For this purpose we have a special kind of TextUnitcalled EntityToken used to encode the fact that a certain span in a text denotes a specific Entity.
Note thatthis is a high-level abstract schema without any platform-related details.
This KB PIM may be invokedin the various parts of the full system PIM model, not necessarily within the NLP stack.
We can use thisschema to generate different PSMs depending on our needs.One of the core benefits of MDD are the transformations.
After we have defined our high-level KBPIM conceptual model we may define a set of transformations which will convert it to the PSM mod-els, which contain the implementation details of the conceptual model within a specific platform.
Forexample, it can be Jena TDB22, a framework for storing and managing structured knowledge models, ora SQL relational database.
PIM-to-PSM transformations can be defined programmatically, by means ofspecial tools such as IBM Rational Software Architect23, or by means of a specific modeling language,e.g.
ATL Transformation Language (Jouault et al., 2006).
Finally, we can use a number of tools for codegeneration from the PSM model, thus facilitating and speeding up the software development process.Depending on the task, our Ontology PIM may be instantiated both as an UIMA PSM or as a PSMfor another platform.
More specifically, we have the following scenarios for the KB usage.Within NLP stack.
This may be required, for example, if some annotators in the NLP UIMA pipeline,such as a relation extractor or a coreference resolver, require knowledge about types of the individuals(entities) mentioned in a text.
In such case, in order to annotate the text with information about individu-als and their classes from an external KB we can transform the PIM model to a UIMA Type System (TS).We define a transformation which converts TextUnit to a subclass of UIMA Annotation24, and the Entityelement in Ontology to a direct subclass of UIMA TOP25.
Therefore, for example, within our model20Here we use the term ?knowledge base?
to refer to any external resource containing structured semantic knowledge21http://en.wikipedia.org/22http://jena.apache.org/documentation/tdb/index.html23http://www-03.ibm.com/software/products/en/ratisoftarch24https://uima.apache.org/d/uimaj-2.6.0/apidocs/org/apache/uima/jcas/tcas/Annotation.html25https://uima.apache.org/d/uimaj-2.6.0/apidocs/org/apache/uima/jcas/cas/TOP.html29Figure 5: Fragments of the Ontology and Text modelsillustrated in Figure 5, Individual abstraction, which is a subclass of Entity, becomes a subclass of UIMATOP.
The EntityToken annotations from Text are converted to a subclass of UIMA Annotation and areused to mark the spans of the Individual?s mentions in a text.
In the original PIM model EntityTokenshave property references which is a collection of pointers to the respective entities.
In UIMA PSM thisproperty is converted to a UIMA TS feature.KnowledgeBase is an abstraction for an external KB resource to be plugged into the UIMA NLP stack.UIMA contains a mechanism for plugging in external resources26such as KBs.
Each resource is definedby its name, location and a name of the class which implements handling this resource.
MDA and PIM-to-PSM transformations can simplify modeling the resource implementations for different platforms,and EMF tools can further help with automatic code generation.
We can define a transformation whichwould convert platform independent KnowledgeBase model elements to a platform-specific models, forinstance a class diagram for implementing a KnowledgeBase within Jena TDB platform.
Then we canuse code generation tools for further facilitation of software development.Within a reasoning component of a QA system.
We may need to use both the output of the NLPstack and information stored in a KB within some reasoning platform.
For instance, this could be neededwithin a Question Answering system which provides an answer to the input question based on both textevidence coming from the UIMA pipeline, e.g.
syntactic parse information, and a KB.
In this case, thePIM representations of linguistic annotations (Text and Linguistics packages) and KB knowledge(Ontology package) may be instantiated as PSMs relative to the specific reasoning (rule- or statistic-based) framework.
UIMA annotations previously obtained within the UIMA can be converted to theformat required by the reasoning framework by means of model-to-model transformations.5 Related WorkIn the recent years, a number of approaches to modeling annotations and language in NLP softwaresystems and increasing interoperability of distinct NLP components have been proposed (Hellmann etal., 2013; Ide and Romary, 2006; McCrae et al., 2011).The most widely-accepted solutions for assembling NLP pipelines are UIMA and the GATE (Cun-ningham et al., 2011) frameworks.
They both use annotation models based on referential and featurestructures and allow to define custom language models called UIMA type system (TS) or GATE annota-tion schema in an XML descriptor file.
There are ongoing efforts to develop all-purpose UIMA-based26http://uima.apache.org/downloads/releaseDocs/2.1.0-incubating/docs/html/tutorials_and_users_guides/tutorials_and_users_guides.html#ugr.tug.aae.accessing_external_resource_files30NLP toolkits , such as DKPro27or ClearTK (Ogren et al., 2009), with TSs describing a variety of lin-guistic phenomena.
UIMA is accompanied with a UIMAfit toolkit (Ogren and Bethard, 2009), whichcontains a set of utilities that facilitate construction and testing of UIMA pipelines.
The two frameworksare compatible, and GATE provides means to integrate GATE with UIMA and vice versa by using XMLmapping files to reconcile the annotation schemes and software wrappers to integrate the components28.From the MDA perspective, UIMA and GATE type/annotation systems are platform specific models.Defining a language model as a platform independent EMF model results in greater expressivity.
Forexample, in UIMA TS one can model only type class hierarchy and type features, while in EMF modelwe can also encode the types behavior, i.e.
their methods.
Moreover, MDA allows to model the usage ofthe annotation produced by an NLP pipeline within a larger system.
For instance, this could be a questionanswering system which uses both information extracted from text (e.g.
question parse) and informationextracted from a knowledge base (e.g.
query results) and provides tools to facilitate the code generation.Certain effort has been made on reconciling the different annotation formats.
For example, Ide et al.
(2003) and Ide and Suderman (2007) proposed Linguistic Annotation Framework (LAF), a graph-basedannotation model, and its extension, Graph Annotation Format (GrAF), an XML serialization of LAF, forrepresenting various corpora annotations.
It can be used as a pivot format to run the transformations be-tween different annotation models that adhere to the abstract data model.
The latter contains referentialstructures for associating annotations with the original data and uses feature structure graphs to describethe annotations.
Ide and Suderman (2009) show how one may perform GrAF-UIMA-GrAF and GrAF-GATE-GrAF transformations and provide the corresponding software.
However, in GrAF representationfeature values are strings, and additionally, it is not possible to derive information about annotationstypes hierarchy from the GrAF file only, therefore, the resulting UIMA TS would be shallow and with allfeature values types being string, unless additional information is provided (Ide and Suderman, 2009).At the same time, MDD tools like EMF provide a both a modeling language with high expressivenessand solid support to any transformation.
We show how MDD facilitates conversion between differentformats in Section 3.1.
Additionaly, MDD tools like EMF have a solid sofware support for complexmodel visualization, this helps to facilitate understanding and managing large models.After the emergence of the Semantic Web, RDF/OWL formalisms have been used to describe languageand annotation models (Hellmann et al., 2013; McCrae et al., 2011; Liu et al., 2012).
For instance, NLPInterchange Format, NIF, (Hellmann et al., 2013) is intended to allow various NLP tools to interact onweb in a decentralized manner.
Its annotation model, NIF Core Ontology, encoded in OWL, providesmeans to describe strings, documents, spans, and their relations.
Choice of a language model dependson the developers of the NLP tools, however, they are recommended to reuse existing ontologies, e.g.Ontologies of Linguistic Annotations (OLiA) (Chiarcos, 2012).
Another effort, Lemon (McCrae et al.,2011), is a common RDF meta-model for describing lexicons for ontologies and linking them withontologies.
Its core element is a lexical entry which has a number of properties, e.g.
semantic rolesor morpho-syntactic properties.
There has also been research on converting UIMA type systems to theOWL format (Liu et al., 2012).
OWL is highly expressive, and a number of tools exists for reasoningupon OWL/RDF models or visualizing them, e.g.
Prot?eg?e29, or for aligning them (Volz et al., 2009).Expressiveness of UML, which is typically used to encode the PIM models in MDD, is comparable to thatof the ontology description languages (Guizzardi et al., 2004), and it may be reasoned upon (Calvanese etal., 2005).
However, differently from the OWL models, there is a number of software solutions which areable to generate code on top of UML representations.
Therefore, when using MDD we benefit both fromthe high expressiveness of the modeling language and the solid software engineering support providedby the MDD tools.27http://www.ukp.tu-darmstadt.de/software/dkpro-core/28http://gate.ac.uk/sale/tao/splitch21.html29http://protege.stanford.edu/316 ConclusionModel Driven Development and Architecture is a successful paradigm for tackling the complexity ofmodern software infrastructures, well supported by tools and standards.
We have discussed how thebasic principles behind MDD/A are relevant for Human Language Technologies, when approaching thecoming era of high-level cognitive functionalities delivered by interconnected software services, andgrounded on open data.
Model-to-model transformations, supported by specific tools, offer the possi-bility to rapidly integrate different platforms, and to work with many kinds of data representations.
Toget the best of this approach, it is important to carefully analyze the layering and interconnections ofdifferent models, and to provide them with a suitable design.
In our work, we are learning the benefitof modeling aspects such as morpho-syntax and semantics separately, to foster adaptability across lan-guages and domains.
A complete and principled analysis of such general design is yet to come.
Here wehave presented some preliminary result of our experiences, and shared what we achieved so far.AcknowledgementsThis research is partially supported by the EU FP7 / Marie Curie Industry-Academia Partnerships andPathways schema / PEOPLE Work Programme (Grant agreement no.
: 286348, K-Drive project) and bythe IBM Center for Advanced Studies of Trento, Italy.ReferencesD.
Calvanese, G. De Giacomo, D. Lembo, M. Lenzerini, and R. Rosati.
2005.
DL-Lite: Tractable descriptionlogics for ontologies.
In AAAI, pages 602?607.C.
Chiarcos.
2012.
Ontologies of linguistic annotation: Survey and perspectives.
In LREC, pages 303?310.H.
Cunningham, D. Maynard, K. Bontcheva, V. Tablan, N. Aswani, I. Roberts, G. Gorrell, A. Funk, A. Roberts,D.
Damljanovic, T. Heitz, M. A. Greenwood, H. Saggion, J. Petrak, Y. Li, and W. Peters.
2011.
Text Processingwith GATE (Version 6).
University of Sheffield Department of Computer Science.A.
Di Bari, A. Faraotti, C. Gambardella, and G. Vetere.
2013.
A Model-driven approach to NLP programmingwith UIMA.
In 3rd Workshop on Unstructured Information Management Architecture, pages 2?9.C.
Fellbaum.
1998.
WordNet: An electronic lexical database.
The MIT press.G.
Guizzardi, G. Wagner, and H. Herre.
2004.
On the foundations of uml as an ontology representation language.In E. Motta, N. Shadbolt, A. Stutt, and N. Gibbins, editors, EKAW, volume 3257 of Lecture Notes in ComputerScience, pages 47?62.
Springer.S.
Hellmann, J. Lehmann, S. Auer, and M. Br?ummer.
2013.
Integrating NLP using Linked Data.
In ISWC.N.
Ide and L. Romary.
2006.
Representing linguistic corpora and their annotations.
In LREC.N.
Ide and K. Suderman.
2007.
GrAF: A graph-based format for linguistic annotations.
In Proceedings of theLinguistic Annotation Workshop, pages 1?8.
Association for Computational Linguistics.N.
Ide and K. Suderman.
2009.
Bridging the gaps: interoperability for GrAF, GATE, and UIMA.
In ThirdLinguistic Annotation Workshop, pages 27?34.
Association for Computational Linguistics.N.
Ide, L. Romary, and E. de la Clergerie.
2003. International standard for a linguistic annotation framework.
InHLT-NAACL workshop on Software engineering and architecture of language technology systems, pages 25?30.Association for Computational Linguistics.IJRD.
2012.
This is Watson [Special issue].
IBM Journal of Research and Development, editor Clifford A.Pickover, 56(3.4).F.
Jouault, F. Allilaire, J.
B?ezivin, I. Kurtev, and P. Valduriez.
2006.
ATL: a QVT-like transformation language.
InCompanion to the 21st ACM SIGPLAN symposium on Object-oriented programming systems, languages, andapplications, pages 719?720.
ACM.32H.
Liu, S. Wu, C. Tao, and C. Chute.
2012.
Modeling UIMA type system using web ontology language: towardsinteroperability among UIMA-based NLP tools.
In 2nd international workshop on Managing interoperabilityand compleXity in health systems, pages 31?36.
ACM.J.
McCrae, D. Spohr, and P. Cimiano.
2011.
Linking lexical resources and ontologies on the semantic web withlemon.
In The Semantic Web: Research and Applications, pages 245?259.
Springer.J.
Miller and J. Mukerji.
2003.
MDA Guide Version 1.0.1.
Technical report, Object Management Group (OMG).P.
Ogren and S. Bethard.
2009.
Building test suites for UIMA components.
In Workshop on Software Engineering,Testing, and Quality Assurance for Natural Language Processing, pages 1?4.
Association for ComputationalLinguistics, June.P.
V. Ogren, P.G.
Wetzler, and S. J. Bethard.
2009.
ClearTK: a framework for statistical natural language process-ing.
In Unstructured Information Management Architecture Workshop at the Conference of the German Societyfor Computational Linguistics and Language Technology.J.
Rumbaugh, I. Jacobson, and G. Booch.
2005.
The Unified Modeling Language Reference Manual.
Addison-Wesley, Boston, MA, 2 edition.K.
Verspoor, W. Baumgartner Jr, C. Roeder, and L. Hunter.
2009.
Abstracting the types away from a UIMA typesystem.
From Form to Meaning: Processing Texts Automatically.
T?ubingen:Narr, pages 249?256.J.
Volz, C. Bizer, M. Gaedke, and G. Kobilarov.
2009.
Silk - A Link Discovery Framework for the Web of Data.In LDOW.33
