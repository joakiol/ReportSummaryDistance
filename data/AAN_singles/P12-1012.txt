Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 107?116,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsTemporally Anchored Relation ExtractionGuillermo Garrido, Anselmo Pen?as, Bernardo Cabaleiro, and A?lvaro RodrigoNLP & IR Group at UNEDMadrid, Spain{ggarrido,anselmo,bcabaleiro,alvarory}@lsi.uned.esAbstractAlthough much work on relation extractionhas aimed at obtaining static facts, many ofthe target relations are actually fluents, as theirvalidity is naturally anchored to a certain timeperiod.
This paper proposes a methodologi-cal approach to temporally anchored relationextraction.
Our proposal performs distant su-pervised learning to extract a set of relationsfrom a natural language corpus, and anchorseach of them to an interval of temporal va-lidity, aggregating evidence from documentssupporting the relation.
We use a rich graph-based document-level representation to gener-ate novel features for this task.
Results showthat our implementation for temporal anchor-ing is able to achieve a 69% of the upperbound performance imposed by the relationextraction step.
Compared to the state of theart, the overall system achieves the highestprecision reported.1 IntroductionA question that arises when extracting a relation ishow to capture its temporal validity: Can we assign aperiod of time when the obtained relation held?
Aspointed out in (Ling and Weld, 2010), while muchresearch in automatic relation extraction has focusedon distilling static facts from text, many of the tar-get relations are in fact fluents, dynamic relationswhose truth value is dependent on time (Russell andNorvig, 2010).The Temporally anchored relation extractionproblem consists in, given a natural language textdocument corpus, C, a target entity, e, and a targetrelation, r, extracting from the corpus the value ofthat relation for the entity, and a temporal intervalfor which the relation was valid.In this paper, we introduce a methodological ap-proach to temporal anchoring of relations automat-ically extracted from unrestricted text.
Our system(see Figure 1) extracts relational facts from text us-ing distant supervision (Mintz et al, 2009) and thenanchors the relation to an interval of temporal va-lidity.
The intuition is that a distant supervised sys-tem can effectively extract relations from the sourcetext collection, and a straightforward date aggrega-tion can then be applied to anchor them.
We pro-pose a four step process for temporal anchoring:(1) represent temporal evidence; (2) select tempo-ral information relevant to the relation; (3) decidehow a relational fact and its relevant temporal in-formation are themselves related; and (4) aggregateimprecise temporal intervals across multiple docu-ments.
In contrast with previous approaches thataim at intra-document temporal information extrac-tion (Ling and Weld, 2010), we focus on mininga corpus aggregating temporal evidences across thesupporting documents.We address the following research questions:(1) Validate whether distant supervised learning issuitable for the task, and evaluate its shortcomings.
(2) Explore whether the use of features extractedfrom a document-level rich representation could im-prove distant supervised learning.
(3) Compare theuse of document metadata against temporal expres-sions within the document for relation temporal an-choring.
(4) Analyze how, in a pipeline architecture,the propagation of errors limits the overall system?s107Training(1) IR candidate documentretrieval(3) Distant supervisedlearning(5) Relation Extraction(6) Temporal AnchoringDocumentCollectionDocumentIndex(2) DocumentRepresentation(4) ClassifiersKnowledgeBaseTraining seeds< entity, relation name, value >Trainingexamples+ / -relationinstancesunlabelledcandidateTrainingApplicationDate Extractionoutput:temporallyanchoredrelationsDateAggregationInput: QueryentityFigure 1: System overview diagram.performance.The representation we use for temporal informa-tion is detailed in section 2; the rich document-levelrepresentation we exploit is described in section 3.For a query entity and target relation, the system firstperforms relation extraction (section 4); then, wefind and aggregate time constraint evidence for thesame relation across different documents, to estab-lish a temporal validity anchor interval (section 5).Empirical comparative evaluation of our approach isintroduced in section 6; while some related work isshown in section 7 and conclusions in section 8.2 Temporal AnchorsWe will denominate relation instance a triple?entity, relation name, value?.
We aim at anchor-ing relation instances to their temporal validity.
Weneed a representation flexible enough to capture theimprecise temporal information available in text,but expressed in a structured style.
Allen?s (1983)interval-based algebra for temporal representationand reasoning, underlies much research, such as theTempeval challenges (Verhagen et al, 2007; Puste-jovsky and Verhagen, 2009).
Our task is different,as we focus on obtaining the temporal interval as-sociated to a fact, rather than reasoning about thetemporal relations among the events appearing in asingle text.Let us assume that each relation instance is validduring a certain temporal interval, I = [t0, tf ].
Thissharp temporal interval fails to capture the impreci-sion of temporal boundaries conveyed in natural lan-guage text.
The Temporal Slot Filling task at TAC-KBP 2011 (Ji et al, 2011) proposed a 4-tuple rep-resentation that we will refer to as imprecise anchorintervals.
An imprecise temporal interval is definedas an ordered 4-tuple of time points: (t1, t2, t3, t4),with the following semantics: the relation is true fora period which starts at some point between t1 andt2 and ends between t3 and t4.
It should hold that:t1 ?
t2, t3 ?
t4, and t1 ?
t4.
Any of the fourendpoints can be left unconstrained (t1 or t3 wouldbe ?
?, and t2 or t4 would be +?).
This represen-tation is flexible and expressive, although it cannotcapture certain types of information (Ji et al, 2011).3 Document RepresentationWe use a rich document representation that employsa graph structure obtained by augmenting the syn-tactic dependency analysis of the document with se-mantic information.A document D is represented as a documentgraph GD; with node set VD and edge set, ED.
Eachnode v ?
VD represents a chunk of text, which is asequence of words1.
Each node is labeled with adictionary of attributes, some of which are commonfor every node: the words it contains, their part-of-speech annotations (POS) and lemmas.
Also, a rep-resentative descriptor, which is a normalized stringvalue, is generated from the chunks in the node.
Cer-tain nodes are also annotated with one or more types.There are three families of types: Events (verbsthat describe an action, annotated with tense, polar-ity and aspect); standardized Time Expressions; andNamed Entities, with additional annotations such asgender or age.Edges in the document graph, e ?
ED, representfour kinds of relations between the nodes:?
Syntactic: a dependency relation.?
Coreference: indicates that two chunks refer to1Most chunks consist in one word; we join words into achunk (and a node) in two cases: a multi-word named entityand a verb and its auxiliaries.108David[NNP,David]NER: PERSONDESCRIPTOR:DavidPOS: NJulia[NNP,Julia]CLASS:WIFENER: PERSONDESCRIPTOR:JuliaPOS: NGENDER:FEMALESeptember[NNP,September] 1979[CD,1979]NER:DATETIMEVALUE:197909DESCRIPTOR: September 1979POS: NNPwife[NN,wife]DESCRIPTOR:wifePOS: NNis[VBZ,be] celebrating[VBG,celebrate]ASPECT:PROGRESSIVETENSE:PRESENTPOLARITY:POSDESCRIPTOR: celebratePOS: Vbirthday[NN,birthday]DESCRIPTOR:birthdayPOS: NNwas[VBD,be] born[VBN,bear]ASPECT:NONETENSE:PASTPOLARITY:POSDESCRIPTOR: bearPOS: Varg0hasClassprep_inarg1arg1hasINCLUDEShas_wifeFigure 2: Collapsed document graph representation, GC ,for the sample text document ?David?s wife, Julia, is cel-ebrating her birthday.
She was born in September 1979?.the same discourse referent.?
Semantic relations between two nodes, such ashasClass, hasProperty and hasAge.?
Temporal relations between events and time ex-pressions.The processing includes dependency parsing,named entity recognition and coreference reso-lution, done with the Stanford CoreNLP soft-ware (Klein and Manning, 2003); and events andtemporal information extraction, via the TARSQIToolkit (Verhagen et al, 2005).The document graph GD is then further trans-formed into a collapsed document graph, GC .
Eachnode of GC clusters together coreferent nodes, rep-resenting a discourse referent.
Thus, a node u in GCis a cluster of nodes u1, .
.
.
, uk of GD.
There is anedge (u, v) in GC if there was an edge between anyof the nodes clustered into u and any of the nodesv1, .
.
.
, vk?
.
The coreference edges do not appear inthis representation.
Additional semantic informationis also blended into this representation: normaliza-tion of genitives, semantic class indicators inferredfrom appositions and genitives, and gender annota-tion inferred from pronouns.
A final graph examplecan be seen in Figure 2.4 Distant Supervised Relation ExtractionTo perform relation extraction, our proposal fol-lows a distant supervision approach (Mintz et al,2009), which has also inspired other slot filling sys-tems (Agirre et al, 2009; Surdeanu et al, 2010).We capture long distance relations by introducinga document-level representation and deriving novelfeatures from deep syntactic and semantic analysis.Seed harvesting.
From a reference KnowledgeBase (KB), we extract a set of relation triplesor seeds: ?entity, relation, value?, where therelation is one of the target relations.
Ourdocument-level distant supervision assumption isthat if entity and value are found in a documentgraph (see section 3), and there is a path connect-ing them, then the document expresses the relation.Relation candidates gathering.
From a seed triple,we retrieve candidate documents that contain boththe entity and value, within a span of 20 tokens,using a standard IR approach.
Then, entity andvalue are matched to the document graph represen-tation.
We first use approximate string comparisonto find nodes matching the seed entity.
After an en-tity node has been found we use local breadth-first-search (BFS) to find a matching value and the short-est connecting path between them.
We enforce theNamed Entity type of entity and value to match aexpected type, predefined for the relation.Our procedure traverses the document graph look-ing for entity and value nodes meeting those condi-tions; when found, we generate features for a pos-itive example for the relation2.
If we encounter anode that matches the expected NE type of the rela-tion, but does not match the seed value, we generatea negative example for that relation.Training.
From positive and negative examples, wegenerate binary features; some of them are inspiredby previous work (Surdeanu and Ciaramita, 2007;Mintz et al, 2009; Riedel et al, 2010; Surdeanu etal., 2010), and others are novel, taking advantage ofour graph representation.
Table 1 summarizes ourchoice of features.
Features appearing in less than 5training examples were discarded.Relation instance extraction.
Given an input entityand a target relation, we aim at finding a filler valuefor a relation instance.
This task is known as SlotFilling.
From the set of retrieved documents relevantto the query entity, represented as document graphs,2From the collapsed document graph representation we ob-tained an average of 9213 positive training examples per slot;from the uncollapsed document graph, a slightly lower averageof 8178.5 positive examples per slot.109Feature name Descriptionpath dependency path between ENTITY andVALUE in the sentenceX-annotation NE annotations for XX-pos Part-of-speech annotations for XX-gov Governor of X in the dependency pathX-mod Modifiers of X in the dependency pathX-has age X is a NE, with an age attributeX-has class-C X is a NE, with a class CX-property-P X is a NE, and it has a property PX-has-Y X is a NE, with a possessive relation withanother NE, YX-is-Y X is a NE, in a copula with another NE, YX-gender-G X is a NE, and it has gender GV -tense Tense of the verb V in the pathV -aspect Aspect of the verb V in the pathV -polarity Polarity (positive or negative) of the verb VTable 1: Features included in the model.
X stands forENTITY and VALUE.
Verb features are generated fromthe verbs, V , identified in the path between ENTITY andVALUE.we locate matching entities and start a local BFS ofcandidate values, generating for them an unlabelledexample.
For each of the relations to extract, a bi-nary classifier (extractor) decides whether the exam-ple is a valid relation instance.
For each particularrelation classifier, only candidates with the expectedentity and value types for the relation were used inthe application phase.
Each extractor was a SVMclassifier with linear kernel (Joachims, 2002).
Alllearning parameters were set to their default values.The classification process yields a predicted classlabel, plus a real number indicating the margin.
Weperformed an aggregation phase to sum the mar-gins over distinct occurrences of the same extractedvalue.
The rationale is that when the same value isextracted from more than one document, we shouldaccumulate that evidence.The output of this phase is the set of extracted re-lations (positive for each of the classifiers), plus thedocuments where the same fact was detected (sup-porting documents).5 Temporal Anchoring of RelationsIn this section, we propose and discuss a unifiedmethodological approach for temporal anchoring ofrelations.
We assume the input is a relation instanceand a set of supporting documents.
The task is es-tablishing a imprecise temporal anchor interval forthe relation.We present a four-step methodological approach:(1) representation of intra-document temporal infor-mation; (2) selection of relevant temporal informa-tion for the relation; (3) mapping of the link betweenrelational fact and temporal information into an in-terval; and (4) aggregation of imprecise intervals.Temporal representation.
The first methodologi-cal step is to obtain and represent the available intra-document temporal information; the input is a doc-ument, and the task is to identify temporal signalsand possible links among them.
We use the term linkfor a relation between a temporal expression (a date)and an event; we want to avoid confusion with theterm relation (a relational fact extracted from text).In our particular implementation:?
We use TARSQI to extract temporal expressionsand link them to events.
In particular, TARSQIuses the following temporal links: included, si-multaneous, after, before, begun by or ended.?
We focus also on the syntactic pattern [Event-preposition-Time] within the lexical context of thecandidate entity and value.?
Both are normalized into one from a set of prede-fined temporal links: within, throughout, begin-ning, ending, after and before.Selection of temporal evidence.
For each docu-ment and relational instance, we have to select thosetemporal expressions that are relevant.a.
Document-level metadata.
The default valuewe use is the document creation time (DCT),if available.
The underlying assumption is thatthere is a within link from each fact expressed inthe text and the document creation time.b.
Temporal expressions.
Temporal evidencecomes also from the temporal expressionspresent in the context of a relation.
In our par-ticular implementation, we followed a straight-forward approach, looking for the time expres-sion closest in the document graph to the short-est path between the entity and value nodes.
Thissearch is performed via a limited depth BFS,starting from the nodes in the path, in order fromvalue to entity.Mapping of temporal links into intervals.
Thethird step is deciding how a relational fact and its rel-evant temporal information are themselves related.We have to map this information, expressed in text,110Temporal link Constraints mappingBefore t4 = firstAfter t1 = lastWithin and Throughout t2 = first and t3 = lastBeginning t1 = first and t2 = lastEnding t3 = first and t4 = lastTable 2: Mapping from time expression and temporal re-lation to temporal constraints.to a temporal representation.
We will use the impre-cise anchor intervals described is section 2.Let T be a temporal expression identified in thedocument or its metadata.
Now, the mapping of tem-poral constraints depends on the temporal link to thetime expression identified; also, the semantics of theevent have to be considered in order to decide thetime period associated to a relation instance.
Thisstep is important because the event could refer just tothe beginning of the relation, its ending, or both.
Forinstance, it is obvious that having the event marryis different to having the event divorce, when decid-ing the temporal constraints associated to the spouserelation.Table 2 shows our particular mapping betweentemporal links and constraints.
In particular, for thedefault document creation time, we suppose that arelation which appears in a document with creationtime d held true at least in that date; that is, we areassuming a within link, and we map t2 = d, t3 = d.Inter-document temporal evidence aggregation.The last step is aggregating all the time constraintsfound for the same relation and value across differ-ent documents.
If we found that a relation started af-ter two dates d and d?, where d?
> d, the closest con-straint to the real start of the relation is d?.
Mapped totemporal constraints, it means that we would choosethe biggest t1 possible.
Following the same reason-ing, we would want to maximize t3.
On the otherside, when a relation started before two dates d2 andd?2, where d?2 > d2, the closest constraint is d2 andwe would choose the smallest t2.
In summary, wewill maximize t1 and t3 and minimize t2 and t4, sowe will narrow the margins.6 EvaluationWe have used for our evaluation the dataset com-piled within the TAC-KBP 2011 Temporal Slot Fill-ing Task (Ji et al, 2011).
We employed as initialKB the one distributed to participants in the task,which has been compiled from Wikipedia infoboxes.It contains 898 triples ?entity, slot type, value?
for100 different entities and up to 8 different slots (re-lations) per entity3.
This gold standard contains thecorrect responses pooled from the participant sys-tems plus a set of responses manually found byannotators.
Each triple has associated a temporalanchor.
The relations had to be extracted from adomain-general collection of 1.7 million documents.Our system was one of the five that took part inthe task.We have evaluated the overall system andthe two main components of the architecture: Rela-tion Extraction, and Temporal Anchoring of the re-lations.
Due to space limitations, the description ofour implementation is very concise; refer to (Garridoet al, 2011) for further details.6.1 Evaluation of Relation ExtractionSystem response in the relation extraction step con-sists in a set of triples ?entity, slot type, value?.Performance is measured using precision, recall andF-measure (harmonic mean) with respect to the 898triples in the key.
Target relations (slots) are poten-tially list-valued, that is, more than one value canbe valid for a relation (possibly at different pointsin time).
Only correct values yield any score, andredundant triples are ignored.Experiments.
We run two different system settingsfor the relation extraction step.
They differ in thedocument representation used (detailed in section3),in order to empirically assess whether clustering ofdiscourse referents into single nodes benefits the ex-traction.
In SETTING 1, each document is repre-sented as a document graph, GD, while in SETTING2 collapsed document graph representation, GC , isemployed.Results.
Results are shown in Table 3 in the col-umn Relation Extraction.
Both settings have a sim-ilar performance with a slight increase in the caseof graphs with clustered referents.
Although preci-sion is close to 0.5, recall is lower than 0.1.
We havestudied the limits of the assumptions our approach3There are 7 person relations: cities of residence, state-orprovinces of residence, countries of residence, employee of,member of, title, spouse, and an organization relation:top members/employees.111is based on.
First, our standard retrieval componentperformance limits the overall system?s.
As a matterof example, if we retrieve the first 100 documentsper entity, we find relevant documents only for 62%of the triples in the key.
This number means that nomatter how good relation extraction method is, 38%of relations will not be found.Second, the distant supervision assumption un-derlying our approach is that for a seed relation in-stance ?entity, relation, value?, any textual men-tion of entity and value expresses the relation.
Ithas been shown that this assumption is more oftenviolated when training knowledge base and docu-ment collection are of different type, e.g.
Wikipediaand news-wire (Riedel et al, 2010).
We have real-ized that a more determinant factor is the relationitself and the type of arguments it takes.
We ran-domly sampled 100 training examples per relation,and manually inspected them to assess if they wereindeed mentions of the relation.
While for the re-lation cities of residence only 30% of the trainingexamples are expressing the relation, for spouse thenumber goes up to 59%.
For title, up to 90% of theexamples are correct.
This fact explains, at least par-tially, the zeros we obtain for some relations.6.2 Evaluation of Temporal AnchoringUnder the evaluation metrics proposed by TAC-KBP2011, if the value of the relation instance is judgedas correct, the score for temporal anchoring dependson how well the returned interval matches the oneprovided in the key.
More precisely, let the correctimprecise anchor interval in the gold standard keybe Sk = (k1, k2, k3, k4) and the system response beS = (r1, r2, r3, r4).
The absence of a constraint int1 or t3 is treated as a value of ??
; the absence ofa constraint in t2 or t4 is treated as a value of +?.Then, let di = |ki ?
ri|, for i ?
1, .
.
.
, 4, be thedifference, a real number measured in years.
Thescore for the system response is:Q(S) =144?i=111 + diThe score for a target relation Q(r) is computedby summing Q(S) over all unique instances of therelation whose value is correct.
If the gold standardcontains N responses, and the system output M re-sponses, then precision is: P = Q(r)/M , and recall:R = Q(r)/N ; F1 is the harmonic mean of P and R.Experiments.
We evaluated two different set-tings for the temporal anchoring step; both usethe collapsed document graph representation, GC(SETTING 2).
The goal of the experiment is two-fold.
First, test the strength of the document creationtime as evidence for temporal anchoring.
Second,test how hard this metadata-level baseline is to beatusing contextual temporal expressions.The SETTING 2-I assumes a within temporal linkbetween the document creation time and any relationexpressed inside the document, and aggregates thisinformation across the documents that we have iden-tified as supporting the relation.
The SETTING 2-IIconsiders documents content in order to extract tem-poral links from the context of the text that expressesthe relation.
If no temporal expression is found, thedate of the document is used as default.
Temporallinks from all supporting documents are mapped intointervals and aggregated as detailed in section 5.The performance on relation extraction is an up-per bound for temporal anchoring, attainable if tem-poral anchoring is perfect.
Thus, we also evaluatethe temporal anchoring performance as the percent-age the final system achieves with respect to the re-lation extraction upper bound.Results.
Results are shown in Table 3 under columnTemporal Anchoring.
They are low, due to the upperbound that error propagation in candidate retrievaland relation extraction imposes upon this step: tem-porally anchoring alone achives 69% of its upperbound.
This value corresponds to the baseline SET-TING 2-I, showing its strength.
The difference withSETTING 2-II shows that this baseline is difficultto beat by considering temporal evidence inside thedocument content.
There is a reason for this.
Thetemporal link mapping into time intervals does notdepend only on the type of link, but also on the se-mantics of the text that expresses the relation as wepointed out above.
We have to decide how to trans-form the link between relation and temporal expres-sion into a temporal interval.
Learning a model forthis is a hard open research problem that has a strongadversary in the baseline proposed.112Relation Extraction Temporal AnchoringSETTING 1 SETTING 2 SETTING 2-I SETTING 2-IIP R F P R F P R F % P R F %(1) 0 0 0 0 0 0 0 0 0 0 0 0 0 0(2) 0 0 0 0 0 0 0 0 0 0 0 0 0 0(3) 0.33 0.02 0.03 0 0 0 0 0 0 0 0 0 0 0(4) 0.22 0.09 0.13 0.29 0.11 0.16 0.23 0.09 0.13 79 0.21 0.08 0.11 72(5) 0.53 0.13 0.20 0.54 0.12 0.19 0.34 0.07 0.12 63 0.30 0.06 0.11 56(6) 0.70 0.12 0.20 0.75 0.13 0.22 0.57 0.10 0.16 76 0.50 0.08 0.14 67(7) 0.50 0.06 0.10 0.50 0.07 0.12 0.29 0.04 0.07 58 0.25 0.04 0.06 50(8) 0.25 0.04 0.07 0.20 0.04 0.07 0.15 0.03 0.05 75 0.06 0.01 0.02 30(9) 0.42 0.08 0.14 0.45 0.08 0.14 0.31 0.06 0.10 69 0.27 0.05 0.09 60Table 3: Results of experiments for each relation: (1) per:stateorprovinces of residence; (2) per:employee of; (3)per:countries of residence; (4) per:member of; (5) per:title; (6) org:top members/employees; (7) per:spouse; (8)per:cities of residence; (9) overall results (calculated as a micro-average).System # Filled Precision Recall F1BLENDER2 1206 0.1789 0.3030 0.2250BLENDER1 1116 0.1796 0.2942 0.2231BLENDER3 1215 0.1744 0.2976 0.2199IIRG1 346 0.2457 0.1194 0.1607Setting 2-1 167 0.2996 0.0703 0.1139Setting 2-2 167 0.2596 0.0609 0.0986Stanford 12 5140 0.0233 0.1680 0.0409Stanford 11 4353 0.0238 0.1453 0.0408USFD20112 328 0.0152 0.0070 0.0096USFD20113 127 0.0079 0.0014 0.0024Table 4: System ID, number of filled responses of thesystem, precision, recall and F measure.6.3 Comparative EvaluationOur approach was compared with the other fourparticipants at the KBP Temporal Slot Filling Task2011.
Table 4 shows results sorted by F-measure incomparison to our two settings (described above).These official results correspond to a previousdataset containing 712 triples4.As shown in column Filled our approach returnsless triples than other systems, explaining low recall.However, our system achieves the highest precisionfor the complete task of temporally anchored rela-tion extraction.
Despite low recall, our system ob-tains the third best F1 value.
This is a very promis-ing result, since several directions can be exploredto consider more candidates and increase recall.7 Related WorkCompiling a Knowledge Base of temporally an-chored facts is an open research challenge (Weikumet al, 2011).
Despite the vast amount of research fo-cusing on understanding temporal expressions and4Slot-fillers from human assessors were not consideredtheir relation to events in natural language, the com-plete problem of temporally anchored relation ex-traction remains relatively unexplored.
Also, whilemuch research has focused on single-document ex-traction, it seems clear that extracting temporally an-chored relations needs the aggregation of evidencesacross multiple documents.There have been attempts to extend an existingknowledge base.
Wang et al (2010) use regularexpressions to mine Wikipedia infoboxes and cat-egories and it is not suited for unrestricted text.
Anearlier attempt (Zhang et al, 2008), is specific forbusiness and difficult to generalize to other relations.Two recent promising works are more related to ourresearch.
Wang et al (2011) uses manually definedpatterns to collect candidate facts and explicit dates,and re-rank them using a graph label propagation al-gorithm; their approach is complementary to ours,as our aim is not to harvest temporal facts but toextract the relations in which a query entity takespart; unlike us, they require entity, value, and a ex-plicit date to appear in the same sentence.
Talukdaret al (2012) focus on the partial task of temporallyanchoring already known facts, showing the useful-ness of the document creation time as temporal sig-nal, aggregated across documents.Earlier work has dealt mainly with partial aspectsof the problem.
The TempEval community focusedon the classification of the temporal links betweenpairs of events, or an event and a temporal expres-sion; using shallow features (Mani et al, 2003; La-pata and Lascarides, 2004; Chambers et al, 2007),or syntactic-based structured features (Bethard andMartin, 2007; Pus?cas?u, 2007; Cheng et al, 2007).Aggregating evidence across different documents113to temporally anchor facts has been explored in set-tings different to Information Extraction, such asanswering of definition questions (Pas?ca, 2008) orextracting possible dates of well-known historicalevents (Schockaert et al, 2010).Temporal inference or reasoning to solve con-flicting temporal expressions and induce temporalorder of events has been used in TempEval (Tatuand Srikanth, 2008; Yoshikawa et al, 2009) andACE (Gupta and Ji, 2009) tasks, but focused onsingle-document extraction.
Ling et al (2010), usecross-event joint inference to extract temporal facts,but only inside a single document.Evaluation campaigns, such as ACE and TAC-KBP 2011 have had an important role in promotingthis research.
While ACE required only to identifytime expressions and classify their relation to events,KBP requires to infer explicitly the start/end time ofrelations, which is a realistic approach in the contextof building time-aware knowledge bases.
KBP rep-resents an important step for the evaluation of tem-poral information extraction systems.
In general, theparticipant systems adapted existing slot filling sys-tems, adding a temporal classification component:distant supervised (Chen et al, 2010; Surdeanu etal., 2010) on manually-defined patterns (Byrne andDunnion, 2010).8 ConclusionsThis paper introduces the problem of extracting,from unrestricted natural language text, relationalknowledge anchored to a temporal span, aggregat-ing temporal evidence from a collection of docu-ments.
Although compiling time-aware knowledgebases is an important open challenge (Weikum etal., 2011), it has remained unexplored until very re-cently (Wang et al, 2011; Talukdar et al, 2012).We have elucidated the two challenges of the task,namely relation extraction and temporal anchoringof the extracted relations.We have studied how, in a pipeline architecture,the propagation of errors limits the overall system?sperformance.
The performance attainable in the fulltask is limited by the quality of the output of thethree main phases: retrieval of candidate passages/documents, extraction of relations and temporal an-choring of those.We have also studied the limits of the distant su-pervision approach to relation extraction, showingempirically that its performance depends not onlyon the nature of reference knowledge base and doc-ument corpus (Riedel et al, 2010), but also on therelation to be extracted.
Given a relation betweentwo arguments, if it is not dominant among textualexpressions of those arguments, the distant supervi-sion assumption will be more often violated.We have introduced a novel graph-based docu-ment level representation, that has allowed us to gen-erate new features for the task of relation extraction,capturing long distance structured contexts.
Our re-sults show how, in a document level syntactic repre-sentation, it yields better results to collapse corefer-ent nodes.We have presented a methodological approachto temporal anchoring composed of: (1) intra-document temporal information representation; (2)selection of relation-dependent relevant temporal in-formation; (3) mapping of temporal links to an inter-val representation; and (4) aggregation of impreciseintervals.Our proposal has been evaluated within a frame-work that allows for comparability.
It has been ableto extract temporally anchored relational informa-tion with the highest precision among the partici-pant systems taking part in the competitive evalu-ation TAC-KBP 2011.For the temporal anchoring sub-problem, we havedemonstrated the strength of the document creationtime as a temporal signal.
It is possible to achievea performance of 69% of the upper-bound imposedby relation extraction by assuming that any relationmentioned in a document held at the document cre-ation time (there is a within link between the rela-tional fact and the document creation time).
Thisbaseline has proved stronger than extracting and an-alyzing the temporal expressions present in the doc-ument content.AcknowledgmentsThis work has been partially supported by the Span-ish Ministry of Science and Innovation, throughthe project Holopedia (TIN2010-21128-C02), andthe Regional Government of Madrid, through theproject MA2VICMR (S2009/TIC1542).114ReferencesEneko Agirre, Angel X. Chang, Daniel S. Jurafsky,Christopher D. Manning, Valentin I. Spitkovsky, andEric Yeh.
2009.
Stanford-UBC at TAC-KBP.
In TAC2009, November.James F. Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Commun.
ACM, 26:832?843,November.Steven Bethard and James H. Martin.
2007.
Cu-tmp:temporal relation classification using syntactic and se-mantic features.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, SemEval?07, pages 129?132, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Lorna Byrne and John Dunnion.
2010.
UCD IIRG atTAC 2010 KBP Slot Filling Task.
In Proceedings ofthe Third Text Analysis Conference (TAC 2010).
NIST,November.Nathanael Chambers, Shan Wang, and Dan Jurafsky.2007.
Classifying temporal relations between events.In Proceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 173?176, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,Wen-Pin Lin, Matthew Snover, Javier Artiles, MarissaPassantino, and Heng Ji.
2010.
CUNY-BLENDERTAC-KBP2010: Entity linking and slot filling systemdescription.
In Proceedings of the Third Text AnalysisConference (TAC 2010).
NIST, November.Yuchang Cheng, Masayuki Asahara, and Yuji Mat-sumoto.
2007.
Naist.japan: temporal relation identifi-cation using dependency parsed tree.
In Proceedingsof the 4th International Workshop on Semantic Evalu-ations, SemEval ?07, pages 245?248, Stroudsburg, PA,USA.
Association for Computational Linguistics.Guillermo Garrido, Bernardo Cabaleiro, Anselmo Peas,varo Rodrigo, and Damiano Spina.
2011.
A distantsupervised learning system for the TAC-KBP Slot Fill-ing and Temporal Slot Filling Tasks.
In Text AnalysisConference, TAC 2011 Proceedings Papers.Prashant Gupta and Heng Ji.
2009.
Predicting un-known time arguments based on cross-event propaga-tion.
In Proceedings of the ACL-IJCNLP 2009 Con-ference Short Papers, ACLShort ?09, pages 369?372,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Heng Ji, Ralph Grishman, and Hoa Trang Dang.
2011.Overview of the tac2011 knowledge base populationtrack.
In Text Analysis Conference, TAC 2011 Work-shop, Notebook Papers.T.
Joachims.
2002.
Learning to Classify Text Us-ing Support Vector Machines ?
Methods, Theory, andAlgorithms.
Kluwer/Springer.
We used Joachim?sSVMLight implementation available at http://svmlight.joachims.org/.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL 2003, pages 423?430.Mirella Lapata and Alex Lascarides.
2004.
Inferringsentence-internal temporal relations.
In HLT 2004.Xiao Ling and Daniel S. Weld.
2010.
Temporal informa-tion extraction.
In Proceedings of the Twenty-FourthAAAI Conference on Artificial Intelligence (AAAI-10).Inderjeet Mani, Barry Schiffman, and Jianping Zhang.2003.
Inferring temporal ordering of events in news.In NAACL-Short?03.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extractionwithout labeled data.
In ACL 2009, pages 1003?1011,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.M Pas?ca.
2008.
Answering Definition Questions viaTemporally-Anchored Text Snippets.
Proc.
of IJC-NLP2008.Georgiana Pus?cas?u.
2007.
Wvali: temporal relationidentification by syntactico-semantic analysis.
In Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations, SemEval ?07, pages 484?487,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.James Pustejovsky and Marc Verhagen.
2009.
SemEval-2010 task 13: evaluating events, time expressions,and temporal relations (TempEval-2).
In Proceed-ings of the Workshop on Semantic Evaluations: Re-cent Achievements and Future Directions, DEW ?09,pages 112?116, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Jose?
Balca?zar, Francesco Bonchi,Aristides Gionis, and Miche`le Sebag, editors, MachineLearning and Knowledge Discovery in Databases,volume 6323 of LNCS, pages 148?163.
SpringerBerlin / Heidelberg.Stuart J. Russell and Peter Norvig.
2010.
Artificial Intel-ligence - A Modern Approach (3. internat.
ed.).
Pear-son Education.Steven Schockaert, Martine De Cock, and Etienne Kerre.2010.
Reasoning about fuzzy temporal informationfrom the web: towards retrieval of historical events.Soft Computing - A Fusion of Foundations, Method-ologies and Applications, 14:869?886.Mihai Surdeanu and Massimiliano Ciaramita.
2007.Robust information extraction with perceptrons.
InACE07, March.115Mihai Surdeanu, David McClosky, Julie Tibshirani, JohnBauer, Angel X. Chang, Valentin I. Spitkovsky, andChristopher D. Manning.
2010.
A simple distantsupervision approach for the tac-kbp slot filling task.In Proceedings of the Third Text Analysis Conference(TAC 2010), Gaithersburg, Maryland, USA, Novem-ber.
NIST.Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.2012.
Coupled temporal scoping of relational facts.
InProceedings of the Fifth ACM International Confer-ence on Web Search and Data Mining (WSDM), Seat-tle, Washington, USA, February.
Association for Com-puting Machinery.Marta Tatu and Munirathnam Srikanth.
2008.
Experi-ments with reasoning for temporal relations betweenevents.
In COLING?08.Marc Verhagen, Inderjeet Mani, Roser Sauri, RobertKnippen, Seok Bae Jang, Jessica Littman, AnnaRumshisky, John Phillips, and James Pustejovsky.2005.
Automating temporal annotation with TARSQI.In ACLdemo?05.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
SemEval-2007 task 15: TempEval temporal re-lation identification.
In SemEval?07.Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,and Gerhard Weikum.
2010.
Timely YAGO: har-vesting, querying, and visualizing temporal knowledgefrom Wikipedia.
In Proceedings of the 13th Inter-national Conference on Extending Database Technol-ogy, EDBT ?10, pages 697?700, New York, NY, USA.ACM.Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol, andGerhard Weikum.
2011.
Harvesting facts from textualweb sources by constrained label propagation.
In Pro-ceedings of the 20th ACM international conference onInformation and knowledge management, CIKM ?11,pages 837?846, New York, NY, USA.
ACM.Gerhard Weikum, Srikanta Bedathur, and Ralf Schenkel.2011.
Temporal knowledge for timely intelligence.In Malu Castellanos, Umeshwar Dayal, Volker Markl,Wil Aalst, John Mylopoulos, Michael Rosemann,Michael J. Shaw, and Clemens Szyperski, editors, En-abling Real-Time Business Intelligence, volume 84of Lecture Notes in Business Information Processing,pages 1?6.
Springer Berlin Heidelberg.Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-hara, and Yuji Matsumoto.
2009.
Jointly identifyingtemporal relations with Markov Logic.
In Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume1 - Volume 1, ACL ?09, pages 405?413, Stroudsburg,PA, USA.
Association for Computational Linguistics.Qi Zhang, Fabian M. Suchanek, Lihua Yue, and GerhardWeikum.
2008.
TOB: Timely ontologies for businessrelations.
In 11th International Workshop on the Weband Databases, WebDB.116
