Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787?794,Sydney, July 2006. c?2006 Association for Computational LinguisticsMinimum Risk Annealing for Training Log-Linear Models?David A. Smith and Jason EisnerDepartment of Computer ScienceCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USA{dasmith,eisner}@jhu.eduAbstractWhen training the parameters for a natural language system,one would prefer to minimize 1-best loss (error) on an eval-uation set.
Since the error surface for many natural languageproblems is piecewise constant and riddled with local min-ima, many systems instead optimize log-likelihood, which isconveniently differentiable and convex.
We propose traininginstead to minimize the expected loss, or risk.
We define thisexpectation using a probability distribution over hypothesesthat we gradually sharpen (anneal) to focus on the 1-best hy-pothesis.
Besides the linear loss functions used in previouswork, we also describe techniques for optimizing nonlinearfunctions such as precision or the BLEU metric.
We presentexperiments training log-linear combinations of models fordependency parsing and for machine translation.
In machinetranslation, annealed minimum risk training achieves signif-icant improvements in BLEU over standard minimum errortraining.
We also show improvements in labeled dependencyparsing.1 Direct Minimization of ErrorResearchers in empirical natural language pro-cessing have expended substantial ink and effort indeveloping metrics to evaluate systems automati-cally against gold-standard corpora.
The ongoingevaluation literature is perhaps most obvious in themachine translation community?s efforts to betterBLEU (Papineni et al, 2002).Despite this research, parsing or machine trans-lation systems are often trained using the muchsimpler and harsher metric of maximum likeli-hood.
One reason is that in supervised training,the log-likelihood objective function is generallyconvex, meaning that it has a single global max-imum that can be easily found (indeed, for su-pervised generative models, the parameters at thismaximum may even have a closed-form solution).In contrast to the likelihood surface, the error sur-face for discrete structured prediction is not onlyriddled with local minima, but piecewise constant?This work was supported by an NSF graduate researchfellowship for the first author and by NSF ITR grant IIS-0313193 and ONR grant N00014-01-1-0685.
The views ex-pressed are not necessarily endorsed by the sponsors.
Wethank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, andthe reviewers for helpful discussions and comments.and not everywhere differentiable with respect tothe model parameters (Figure 1).
Despite thesedifficulties, some work has shown it worthwhileto minimize error directly (Och, 2003; Bahl et al,1988).We show improvements over previous work onerror minimization by minimizing the risk or ex-pected error?a continuous function that can bederived by combining the likelihood with any eval-uation metric (?2).
Seeking to avoid local min-ima, deterministic annealing (Rose, 1998) gradu-ally changes the objective function from a convexentropy surface to the more complex risk surface(?3).
We also discuss regularizing the objectivefunction to prevent overfitting (?4).
We explainhow to compute expected loss under some evalu-ation metrics common in natural language tasks(?5).
We then apply this machinery to traininglog-linear combinations of models for dependencyparsing and for machine translation (?6).
Finally,we note the connections of minimum risk trainingto max-margin training and minimum Bayes riskdecoding (?7), and recapitulate our results (?8).2 Training Log-Linear ModelsIn this work, we focus on rescoring with log-linear models.
In particular, our experiments con-sider log-linear combinations of a relatively smallnumber of features over entire complex structures,such as trees or translations, known in some pre-vious work as products of experts (Hinton, 1999)or logarithmic opinion pools (Smith et al, 2005).A feature in the combined model might thus bea log probability from an entire submodel.
Giv-ing this feature a small or negative weight candiscount a submodel that is foolishly structured,badly trained, or redundant with the other features.For each sentence xi in our training corpus S,we are given Ki possible analyses yi,1, .
.
.
yi,Ki .
(These may be all of the possible translations orparse trees; or only the Ki most probable under787Figure 1: The loss surface for a machine translation sys-tem: while other parameters are held constant, we vary theweights on the distortion and word penalty features.
Note thepiecewise constant regions with several local maxima.some other model; or only a random sample ofsize Ki.)
Each analysis has a vector of real-valuedfeatures (i.e., factors, or experts) denoted fi,k.
Thescore of the analysis yi,k is ?
?
fi,k, the dot prod-uct of its features with a parameter vector ?.
Foreach sentence, we obtain a normalized probabilitydistribution over the Ki analyses asp?
(yi,k | xi) =exp ?
?
fi,k?Kik?=1 exp ?
?
fi,k?
(1)We wish to adjust this model?s parameters ?to minimize the severity of the errors we makewhen using it to choose among analyses.
A lossfunction Ly?
(y) assesses a penalty for choosingy when y?
is correct.
We will usually write thissimply as L(y) since y?
is fixed and clear fromcontext.
For clearer exposition, we assume belowthat the total loss over some test corpus is the sumof the losses on individual sentences, although wewill revisit that assumption in ?5.2.1 Minimizing Loss or Expected LossOne training criterion directly mimics test condi-tions.
It looks at the loss incurred if we choose thebest analysis of each xi according to the model:min??iL(argmaxyip?
(yi | xi)) (2)Since small changes in ?
either do not changethe best analysis or else push a different analy-sis to the top, this objective function is piecewiseconstant, hence not amenable to gradient descent.Och (2003) observed, however, that the piecewise-constant property could be exploited to character-ize the function exhaustively along any line in pa-rameter space, and hence to minimize it globallyalong that line.
By calling this global line mini-mization as a subroutine of multidimensional opti-mization, he was able to minimize (2) well enoughto improve over likelihood maximization for train-ing factored machine translation systems.Instead of considering only the best hypothesisfor any ?, we can minimize risk, i.e., the expectedloss under p?
across all analyses yi:min?Ep?L(yi,k)def= min??i?kL(yi,k)p?
(yi,k | xi)(3)This ?smoothed?
objective is now continuous anddifferentiable.
However, it no longer exactly mim-ics test conditions, and it typically remains non-convex, so that gradient descent is still not guaran-teed to find a global minimum.
Och (2003) foundthat such smoothing during training ?gives almostidentical results?
on translation metrics.The simplest possible loss function is 0/1 loss,where L(y) is 0 if y is the true analysis y?i and1 otherwise.
This loss function does not at-tempt to give partial credit.
Even in this sim-ple case, assuming P 6= NP, there exists no gen-eral polynomial-time algorithm for even approx-imating (2) to within any constant factor, evenfor Ki = 2 (Hoffgen et al, 1995, from Theo-rem 4.10.4).1 The same is true for for (3), sincefor Ki = 2 it can be easily shown that the min 0/1risk is between 50% and 100% of the min 0/1 loss.2.2 Maximizing LikelihoodRather than minimizing a loss function suited tothe task, many systems (especially for languagemodeling) choose simply to maximize the prob-ability of the gold standard.
The log of this likeli-hood is a convex function of the parameters ?:max?
?ilog p?
(y?i | xi) (4)where y?i is the true analysis of sentence xi.
Theonly wrinkle is that p?
(y?i | xi) may be left unde-fined by equation (1) if y?i is not in our set of Kihypotheses.
When maximizing likelihood, there-fore, we will replace y?i with the min-loss analy-sis in the hypothesis set; if multiple analyses tie1Known algorithms are exponential but only in the dimen-sionality of the feature space (Johnson and Preparata, 1978).788?10 ?5 0 5 1017.518.018.519.0Translation model 1Bleu %?
= ??
= 0.1?
= 1?
= 10Figure 2: Loss and expected loss as one translation model?sweight varies: the gray line (?
= ?)
shows true BLEU (to beoptimized in equation (2)).
The black lines show the expectedBLEU as ?
in equation (5) increases from 0.1 toward?.for this honor, we follow Charniak and Johnson(2005) in summing their probabilities.2Maximizing (4) is equivalent to minimizing anupper bound on the expected 0/1 loss?i(1 ?p?
(y?i | xi)).
Though the log makes it tractable,this remains a 0/1 objective that does not give par-tial credit to wrong answers, such as imperfect butuseful translations.
Most systems should be eval-uated and preferably trained on less harsh metrics.3 Deterministic AnnealingTo balance the advantages of direct loss minimiza-tion, continuous risk minimization, and convexoptimization, deterministic annealing attemptsthe solution of increasingly difficult optimizationproblems (Rose, 1998).
Adding a scale hyperpa-rameter ?
to equation (1), we have the followingfamily of distributions:p?,?
(yi,k | xi) =(exp ?
?
fi,k)?
?Kik?=1(exp ?
?
fi,k?)?
(5)When ?
= 0, all yi,k are equally likely, givingthe uniform distribution; when ?
= 1, we recoverthe model in equation (1); and as ?
?
?, weapproach the winner-take-all Viterbi function thatassigns probability 1 to the top-scoring analysis.For a fixed ?, deterministic annealing solvesmin?Ep?,?
[L(yi,k)] (6)2An alternative would be to artificially add y?i (e.g., thereference translation(s)) to the hypothesis set during training.We then increase ?
according to some scheduleand optimize ?
again.
When ?
is low, the smoothobjective might allow us to pass over local min-ima that could open up at higher ?.
Figure 3 showshow the smoothing is gradually weakened to reachthe risk objective (3) as ?
?
1 and approach thetrue error objective (2) as ?
?
?.Our risk minimization most resembles the workof Rao and Rose (2001), who trained an isolated-word speech recognition system for expectedword-error rate.
Deterministic annealing has alsobeen used to tackle non-convex likelihood sur-faces in unsupervised learning with EM (Ueda andNakano, 1998; Smith and Eisner, 2004).
Otherwork on ?generalized probabilistic descent?
mini-mizes a similar objective function but with ?
heldconstant (Katagiri et al, 1998).Although the entropy is generally higher atlower values of ?, it varies as the optimizationchanges ?.
In particular, a pure unregularized log-linear model such as (5) is really a function of ?
?
?,so the optimizer could exactly compensate for in-creased ?
by decreasing the ?
vector proportion-ately!3 Most deterministic annealing procedures,therefore, express a direct preference on the en-tropy H , and choose ?
and ?
accordingly:min?,?Ep?,?
[L(yi,k)] ?
T ?H(p?,?)
(7)In place of a schedule for raising ?, we now usea cooling schedule to lower T from ?
to ?
?,thereby weakening the preference for high en-tropy.
The Lagrange multiplier T on entropy iscalled ?temperature?
due to a satisfying connec-tion to statistical mechanics.
Once T is quite cool,it is common in practice to switch to raising ?
di-rectly and rapidly (quenching) until some conver-gence criterion is met (Rao and Rose, 2001).4 RegularizationInformally, high temperature or ?
< 1 smoothsour model during training toward higher-entropyconditional distributions that are not so peaked atthe desired analyses y?i .
Another reason for suchsmoothing is simply to prevent overfitting to thesetraining examples.A typical way to control overfitting is to use aquadratic regularizing term, ||?||2 or more gener-ally?d ?2d/2?2d.
Keeping this small keeps weights3For such models, ?
merely aids the nonlinear optimizerin its search, by making it easier to scale all of ?
at once.789low and entropy high.
We may add this regularizerto equation (6) or (7).
In the maximum likelihoodframework, we may subtract it from equation (4),which is equivalent to maximum a posteriori esti-mation with a diagonal Gaussian prior (Chen andRosenfeld, 1999).
The variance ?2d may reflect aprior belief about the potential usefulness of fea-ture d, or may be tuned on heldout data.Another simple regularization method is to stopcooling before T reaches 0 (cf.
Elidan and Fried-man (2005)).
If loss on heldout data begins toincrease, we may be starting to overfit.
Thistechnique can be used along with annealing orquadratic regularization and can achieve addi-tional accuracy gains, which we report elsewhere(Dreyer et al, 2006).5 Computing Expected LossAt each temperature setting of deterministic an-nealing, we need to minimize the expected loss onthe training corpus.
We now discuss how this ex-pectation is computed.
When rescoring, we as-sume that we simply wish to combine, in someway, statistics of whole sentences4 to arrive at theoverall loss for the corpus.
We consider evalua-tion metrics for natural language tasks from twobroadly applicable classes: linear and nonlinear.A linear metric is a sum (or other linear combi-nation) of the loss or gain on individual sentences.Accuracy?in dependency parsing, part-of-speechtagging, and other labeling tasks?falls into thisclass, as do recall, word error rate in ASR, andthe crossing-brackets metric in parsing.
Thanks tothe linearity of expectation, we can easily computeour expected loss in equation (6) by adding up theexpected loss on each sentence.Some other metrics involve nonlinear combi-nations over the sentences of the corpus.
Onecommon example is precision, Pdef=?i ci/?i ai,where ci is the number of correctly posited ele-ments, and ai is the total number of posited ele-ments, in the decoding of sentence i.
(Depend-ing on the task, the elements may be words, bi-grams, labeled constituents, etc.)
Our goal is tomaximize P , so during a step of deterministic an-nealing, we need to maximize the expectation ofP when the sentences are decoded randomly ac-cording to equation (5).
Although this expectationis continuous and differentiable as a function of4Computing sentence xi?s statistics usually involves iter-ating over hypotheses yi,1, .
.
.
yi,Ki .
If these share substruc-ture in a hypothesis lattice, dynamic programming may help.
?, unfortunately it seems hard to compute for anygiven ?.
We observe however that an equivalentgoal is to minimize ?
logP .
Taking that as ourloss function instead, equation (6) now needs tominimize the expectation of ?
logP ,5 which de-composes somewhat more nicely:E[?
logP ] = E[log?iai ?
log?ici]= E[logA] ?
E[logC] (8)where the integer random variables A =?i aiand C =?i ci count the number of posited andcorrectly posited elements over the whole corpus.To approximate E[g(A)], where g is any twice-differentiable function (here g = log), we can ap-proximate g locally by a quadratic, given by theTaylor expansion of g aboutA?s mean ?A = E[A]:E[g(A)] ?
E[g(?A) + (A?
?A)g?(?A)+12(A?
?A)2g??
(?A)]= g(?A) + E[A?
?A]g?(?A)+12E[(A?
?A)2]g??
(?A)= g(?A) +12?2Ag??
(?A).Here ?A =?i ?ai and ?2A =?i ?2ai , since Ais a sum of independent random variables ai (i.e.,given the current model parameters ?, our ran-domized decoder decodes each sentence indepen-dently).
In other words, given our quadratic ap-proximation to g, E[g(A)] depends on the (true)distribution of A only through the single-sentencemeans ?ai and variances ?2ai , which can be foundby enumerating the Ki decodings of sentence i.The approximation becomes arbitrarily good aswe anneal ?
?
?, since then ?2A ?
0 andE[g(A)] focuses on g near ?A.
For equation (8),E[g(A)] = E[logA] ?
log(?A) ?
?2A2?2Aand E[logC] is found similarly.Similar techniques can be used to compute theexpected logarithms of some other non-linear met-rics, such as F-measure (the harmonic mean ofprecision and recall)6 and Papineni et al (2002)?s5This changes the trajectory that DA takes through pa-rameter space, but ultimately the objective is the same: as?
?
?
over the course of DA, minimizing E[?
logP ] be-comes indistinguishable from maximizing E[P ].6Rdef= C/B; the count B of correct elements is known.So logFdef= log 2PR/(P + R) = log 2R/(1 + R/P ) =log 2C/B ?
log(1 +A/B).
Consider g(x) = log 1 + x/B.790BLEU translation metric (the geometric mean ofseveral precisions).
In particular, the expectationof log BLEU distributes over its N +1 summands:log BLEU = min(1 ?rA1, 0) +N?n=1wn logPnwhere Pn is the precision of the n-gram elementsin the decoding.7 As is standard in MT research,we take wn = 1/N and N = 4.
The first term inthe BLEU score is the log brevity penalty, a con-tinuous function of A1 (the total number of uni-gram tokens in the decoded corpus) that fires onlyifA1 < r (the average word count of the referencecorpus).
We again use a Taylor series to approxi-mate the expected log brevity penalty.We mention an alternative way to compute (say)the expected precisionC/A: integrate numericallyover the joint density of C and A.
How can weobtain this density?
As (C,A) =?i(ci, ai) is asum of independent random length-2 vectors, itsmean vector and 2 ?
2 covariance matrix can berespectively found by summing the means and co-variance matrices of the (ci, ai), each exactly com-puted from the distribution (5) over Ki hypothe-ses.
We can easily approximate (C,A) by the(continuous) bivariate normal with that mean andcovariance matrix8?or else accumulate an exactrepresentation of its (discrete) probability massfunction by a sequence of numerical convolutions.6 ExperimentsWe tested the above training methods on twodifferent tasks: dependency parsing and phrase-based machine translation.
Since the basic setupwas the same for both, we outline it here beforedescribing the tasks in detail.In both cases, we start with 8 to 10 models(the ?experts?)
already trained on separate trainingdata.
To find the optimal coefficients ?
for a log-linear combination of these experts, we use sepa-rate development data, using the following proce-dure due to Och (2003):1.
Initialization: Initialize ?
to the 0 vector.
Foreach development sentence xi, set its Ki-bestlist to ?
(thus Ki = 0).7BLEU is careful when measuring ci on a particular de-coding yi,k.
It only counts the first two copies of the (e.g.)
ascorrect if the occurs at most twice in any reference translationof xi.
This ?clipping?
does not affect the rest of our method.8Reasonable for a large corpus, by Lyapunov?s centrallimit theorem (allows non-identically distributed summands).2.
Decoding: For each development sentencexi, use the current ?
to extract the 200 anal-yses yi,k with the greatest scores exp ?
?
fi,k.Calcuate each analysis?s loss statistics (e.g.,ci and ai), and add it to the Ki-best list if it isnot already there.3.
Convergence: If Ki has not increased forany development sentence, or if we havereached our limit of 20 iterations, stop: thesearch has converged.4.
Optimization: Adjust ?
to improve our ob-jective function over the whole developmentcorpus.
Return to step 2.Our experiments simply compare three proce-dures at step 4.
We may either?
maximize log-likelihood (4), a convex func-tion, at a given level of quadratic regulariza-tion, by BFGS gradient descent;?
minimize error (2) by Och?s line searchmethod, which globally optimizes each com-ponent of ?
while holding the others con-stant;9 or?
minimize the same error (2) more effectively,by raising ?
?
?
while minimizing the an-nealed risk (6), that is, cooling T ?
??
(or?
?
?)
and at each value, locally minimiz-ing equation (7) using BFGS.Since these different optimization procedureswill usually find different ?
at step 4, their K-bestlists will diverge after the first iteration.For final testing, we selected among severalvariants of each procedure using a separate smallheldout set.
Final results are reported for a larger,disjoint test set.6.1 Machine TranslationFor our machine translation experiments, wetrained phrase-based alignment template modelsof Finnish-English, French-English, and German-English, as follows.
For each language pair, wealigned 100,000 sentence pairs from EuropeanParliament transcripts using GIZA++.
We thenused Philip Koehn?s phrase extraction softwareto merge the GIZA++ alignments and to extract9The component whose optimization achieved the lowestloss is then updated.
The process iterates until no lower losscan be found.
In contrast, Papineni (1999) proposed a linearprogramming method that may search along diagonal lines.791and score the alignment template model?s phrases(Koehn et al, 2003).The Pharaoh phrase-based decoder uses pre-cisely the setup of this paper.
It scores a candidatetranslation (including its phrasal alignment to theoriginal text) as ?
?
f , where f is a vector of thefollowing 8 features:1. the probability of the source phrase given thetarget phrase2.
the probability of the target phrase given thesource phrase3.
the weighted lexical probability of the sourcewords given the target words4.
the weighted lexical probability of the targetwords given the source words5.
a phrase penalty that fires for each templatein the translation6.
a distortion penalty that fires when phrasestranslate out of order7.
a word penalty that fires for each Englishword in the output8.
a trigram language model estimated on theEnglish side of the bitextOur goal was to train the weights ?
of these 8features.
We used the method described above,employing the Pharaoh decoder at step 2 to gener-ate the 200-best translations according to the cur-rent ?.
As explained above, we compared threeprocedures at step 4: maximum log-likelihood bygradient ascent; minimum error using Och?s line-search method; and annealed minimum risk.
Asour development data for training ?, we used 200sentence pairs for each language pair.Since our methods can be tuned with hyperpa-rameters, we used performance on a separate 200-sentence held-out set to choose the best hyper-parameter values.
The hyperparameter levels foreach method were?
maximum likelihood: a Gaussian prior withall ?2d at 0.25, 0.5, 1, or ??
minimum error: 1, 5, or 10 different ran-dom starting points, drawn from a uniformOptimization Finnish- French- German-Procedure English English EnglishMax.
like.
5.02 5.31 7.43Min.
error 10.27 26.16 20.94Ann.
min.
risk 16.43 27.31 21.30Table 1: BLEU 4n1 percentage on translating 2000-sentence test corpora, after training the 8 experts on 100,000sentence pairs and fitting their weights ?
on 200 more, usingsettings tuned on a further 200.
The current minimum risk an-nealing method achieved significant improvements over min-imum error and maximum likelihood at or below the 0.001level, using a permutation test with 1000 replications.distribution on [?1, 1]?
[?1, 1]?
?
?
?
, whenoptimizing ?
at an iteration of step 4.10?
annealed minimum risk: with explicit en-tropy constraints, starting temperature T ?
{100, 200, 1000}; stopping temperature T ?
{0.01, 0.001}.
The temperature was cooledby half at each step; then we quenched bydoubling ?
at each step.
(We also ran exper-iments with quadratic regularization with all?2d at 0.5, 1, or 2 (?4) in addition to the en-tropy constraint.
Also, instead of the entropyconstraint, we simply annealed on ?
whileadding a quadratic regularization term.
Noneof these regularized models beat the best set-ting of standard deterministic annealing onheldout or test data.
)Final results on a separate 2000-sentence test setare shown in table 1.
We evaluated translation us-ing BLEU with one reference translation and n-grams up to 4.
The minimum risk annealing pro-cedure significantly outperformed maximum like-lihood and minimum error training in all three lan-guage pairs (p < 0.001, paired-sample permuta-tion test with 1000 replications).Minimum risk annealing generally outper-formed minimum error training on the held-outset, regardless of the starting temperature T .
How-ever, higher starting temperatures do give betterperformance and a more monotonic learning curve(Figure 3), a pattern that held up on test data.
(In the same way, for minimum error training,10That is, we run step 4 from several starting points, finish-ing at several different points; we pick the finishing point withlowest development error (2).
This reduces the sensitivity ofthis method to the starting value of ?.
Maximum likelihoodis not sensitive to the starting value of ?
because it has only aglobal optimum; annealed minimum risk is not sensitive to iteither, because initially ?
?
0, making equation (6) flat.7925 10 15 2016182022IterationBleuT=1000T=200T=100Min.
errorFigure 3: Iterative change in BLEU on German-English de-velopment (upper) and held-out (lower), under annealed min-imum risk training with different starting temperatures, ver-sus minimum error training with 10 random restarts.5 10 15 205101520IterationBleu10 restarts1 restartFigure 4: Iterative change in BLEU on German-Englishdevelopment (upper) and held-out (lower), using 10 randomrestarts vs. only 1.more random restarts give better performance anda more monotonic learning curve?see Figure 4.
)Minimum risk annealing did not always win onthe training set, suggesting that its advantage isnot superior minimization but rather superior gen-eralization: under the risk criterion, multiple low-loss hypotheses per sentence can help guide thelearner to the right part of parameter space.Although the components of the translation andlanguage models interact in complex ways, the im-provement on Finnish-English may be due in partto the higher weight that minimum risk annealingfound for the word penalty.
That system is there-fore more likely to produce shorter output like ihave taken note of your remarks and i also agreewith that .
than like this longer output from theminimum-error-trained system: i have taken noteof your remarks and i shall also agree with all thatthe union .We annealed using our novel expected-BLEUapproximation from ?5.
We found this to performsignificantly better on BLEU evaluation than if wetrained with a ?linearized?
BLEU that summedper-sentence BLEU scores (as used in minimumBayes risk decoding by Kumar and Byrne (2004)).6.2 Dependency ParsingWe trained dependency parsers for three differentlanguages: Bulgarian, Dutch, and Slovenian.11 In-put sentences to the parser were already tagged forparts of speech.
Each parser employed 10 experts,each parameterized as a globally normalized log-linear model (Lafferty et al, 2001).
For example,the 9th component of the feature vector fi,k (whichdescribed the kth parse of the ith sentence) was thelog of that parse?s normalized probability accord-ing to the 9th expert.Each expert was trained separately to maximizethe conditional probability of the correct parsegiven the sentence.
We used 10 iterations of gradi-ent ascent.
To speed training, for each of the first9 iterations, the gradient was estimated on a (dif-ferent) sample of only 1000 training sentences.We then trained the vector ?, used to combinethe experts, to minimize the number of labeled de-pendency attachment errors on a 200-sentence de-velopment set.
Optimization proceeded over listsof the 200-best parses of each sentence producedby a joint decoder using the 10 experts.Evaluating on labeled dependency accuracy on200 test sentences for each language, we see thatminimum error and annealed minimum risk train-ing are much closer than for MT.
For Bulgarianand Dutch, they are statistically indistinguishableusing a paired-sample permutations test with 1000replications.
Indeed, on Dutch, all three opti-mization procedures produce indistinguishable re-sults.
On Slovenian, annealed minimum risk train-ing does show a significant improvement over theother two methods.
Overall, however, the resultsfor this task are mediocre.
We are still working onimproving the underlying experts.7 Related WorkWe have seen that annealed minimum risk train-ing provides a useful alternative to maximum like-lihood and minimum error training.
In our ex-periments, it never performed significantly worse11For information on these corpora, see the CoNLL-Xshared task on multilingual dependency parsing: http://nextens.uvt.nl/?conll/.793Optimization labeled dependency acc.
[%]Procedure Slovenian Bulgarian DutchMax.
like.
27.78 47.23 36.78Min.
error 22.52 54.72 36.78Ann.
min.
risk 31.16 54.66 36.71Table 2: Labeled dependency accuracy on parsing 200-sentence test corpora, after training 10 experts on 1000 sen-tences and fitting their weights ?
on 200 more.
For Slove-nian, minimum risk annealing is significantly better than theother training methods, while minimum error is significantlyworse.
For Bulgarian, both minimum error and annealed min-imum risk training achieve significant gains over maximumlikelihood, but are indistinguishable from each other.
ForDutch, the three methods are indistinguishable.than either and in some cases significantly helped.Note, however, that annealed minimum risk train-ing results in a deterministic classifier just as theseother training procedures do.
The orthogonaltechnique of minimum Bayes risk decoding hasachieved gains on parsing (Goodman, 1996) andmachine translation (Kumar and Byrne, 2004).
Inspeech recognition, researchers have improved de-coding by smoothing probability estimates numer-ically on heldout data in a manner reminiscent ofannealing (Goel and Byrne, 2000).
We are inter-ested in applying our techniques for approximat-ing nonlinear loss functions to MBR by perform-ing the risk minimization inside the dynamic pro-gramming or other decoder.Another training approach that incorporates ar-bitrary loss functions is found in the structuredprediction literature in the margin-based-learningcommunity (Taskar et al, 2004; Crammer et al,2004).
Like other max-margin techniques, theseattempt to make the best hypothesis far away fromthe inferior ones.
The distinction is in using a lossfunction to calculate the required margins.8 ConclusionsDespite the challenging shape of the error sur-face, we have seen that it is practical to opti-mize task-specific error measures rather than op-timizing likelihood?it produces lower-error sys-tems.
Different methods can be used to attemptthis global, non-convex optimization.
We showedthat for MT, and sometimes for dependency pars-ing, an annealed minimum risk approach to opti-mization performs significantly better than a pre-vious line-search method that does not smooth theerror surface.
It never does significantly worse.With such improved methods for minimizing er-ror, we can hope to make better use of task-specifictraining criteria in NLP.ReferencesL.
R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mer-cer.
1988.
A new algorithm for the estimation of hiddenMarkov model parameters.
In ICASSP, pages 493?496.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-bestparsing and maxent discriminative reranking.
In ACL,pages 173?180.S.
F. Chen and R. Rosenfeld.
1999.
A gaussian prior forsmoothing maximum entropy models.
Technical report,CS Dept., Carnegie Mellon University.K.
Crammer, R. McDonald, and F. Pereira.
2004.
New largemargin algorithms for structured prediction.
In Learningwith Structured Outputs (NIPS).M.
Dreyer, D. A. Smith, and N. A. Smith.
2006.
Vine parsingand minimum risk reranking for speed and precision.
InCoNLL.G.
Elidan and N. Friedman.
2005.
Learning hidden variablenetworks: The information bottleneck approach.
JMLR,6:81?127.V.
Goel and W. J. Byrne.
2000.
Minimum Bayes-Risk au-tomatic speech recognition.
Computer Speech and Lan-guage, 14(2):115?135.J.
T. Goodman.
1996.
Parsing algorithms and metrics.
InACL, pages 177?183.G.
Hinton.
1999.
Products of experts.
In Proc.
of ICANN,volume 1, pages 1?6.K.-U.
Hoffgen, H.-U.
Simon, and K. S. Van Horn.
1995.Robust trainability of single neurons.
J. of Computer andSystem Sciences, 50(1):114?125.D.
S. Johnson and F. P. Preparata.
1978.
The densest hemi-sphere problem.
Theoretical Comp.
Sci., 6(93?107).S.
Katagiri, B.-H. Juang, and C.-H. Lee.
1998.
Pattern recog-nition using a family of design algorithms based upon thegeneralized probabilistic descent method.
Proc.
IEEE,86(11):2345?2373, November.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In HLT-NAACL, pages 48?54.S.
Kumar and W. Byrne.
2004.
Minimum bayes-risk decod-ing for statistical machine translation.
In HLT-NAACL.J.
Lafferty, A. McCallum, and F. C. N. Pereira.
2001.
Condi-tional random fields: Probabilistic models for segmentingand labeling sequence data.
In ICML.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: A method for automatic evaluation of machinetranslation.
In ACL, pages 311?318.K.
A. Papineni.
1999.
Discriminative training via linearprogramming.
In ICASSP.A.
Rao and K. Rose.
2001.
Deterministically annealed de-sign of Hidden Markov Model speech recognizers.
IEEETrans.
on Speech and Audio Processing, 9(2):111?126.K.
Rose.
1998.
Deterministic annealing for clustering, com-pression, classification, regression, and related optimiza-tion problems.
Proc.
IEEE, 86(11):2210?2239.N.
A. Smith and J. Eisner.
2004.
Annealing techniques forunsupervised statistical language learning.
In ACL, pages486?493.A.
Smith, T. Cohn, and M. Osborne.
2005.
Logarithmicopinion pools for conditional random fields.
In ACL, pages18?25.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.2004.
Max-margin parsing.
In EMNLP, pages 1?8.N.
Ueda and R. Nakano.
1998.
Deterministic annealing EMalgorithm.
Neural Networks, 11(2):271?282.794
