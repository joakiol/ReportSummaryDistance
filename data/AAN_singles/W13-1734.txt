Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 266?269,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsNative Language Identification using large scale lexical featuresAndre?
LynumNorwegian University of Science and TechnologyDepartment of Computer and Information and ScienceSem S?lands vei 7-9NO-7491 Trondheim, Norwayandrely@idi.ntnu.noAbstractThis paper describes an effort to perform Na-tive Language Identification (NLI) using ma-chine learning on a large amount of lexicalfeatures.
The features were collected from se-quences and collocations of bare word forms,suffixes and character n-grams amounting toa feature set of several hundred thousand fea-tures.
These features were used to train a lin-ear Support Vector Machine (SVM) classifierfor predicting the native language category.1 IntroductionMuch effort in Native Language Identification (NLI)has focused on identifying specific characteristicsof the errors in texts produced by English SecondLanguage (ESL) learners, like the work presentedin (Bestgen et al 2012) and (Koppel et al 2005).This might be specific spelling errors, syntactic ormorphological mistakes.
One motivation for this ap-proach has been the notion that aspects of the L1 lan-guage influences which errors and mistakes are pro-duced by L2 learners, which has guided the modelbuilding towards a smaller number of features andmodels which lend themselves to interpretation interms of linguistic knowledge.Research so far has shown mixed support that thisnotion of language transfer is the best indicator ofL1 language.
While many such features are highlypredictive, features that are usually indicative of thetext topic has shown strong performance when ap-plied to the NLI task as demonstrated in (Ahn, 2011)and (Koppel et al 2005).
This is largely lexical fea-tures such as frequency measures of token, lemmaor character n-grams.
There has been some effortin identifying if this is an artifact of biases in theavailable corpora or if it is indeed an indication ofa substantial phenomenon in ESL language use bydifferent L1 learners (Ahn, 2011).The approach in this paper extends the use of lexi-calized features and shows that such lexicalized fea-tures can by themselves form the basis of a compet-itive and robust NLI system.
This approach entailspossibly abondoning interpretability and other lin-guistic considerations in order to build an as efficientas possible system on the NLI classification tasks it-self.
It is also motivated by the possibility that sim-ple lexicalized features can be applied efficiently ina task that on the face of it requires the system toon some level learn differences syntactic relations inaddition to the differences in morphology found intext produced by the ESL learners.The experiments presented in this paper are a re-sult of exploring a range of features and machinelearning approaches.
The best systems found useda combination of bareword features, character n-grams, suffix and bareword collocations with TF-IDF weighting.
The resulting feature space containsseveral hundred thousand features which were usedto train a linear Support Vector Machine (SVM)classifier.
I will first present the features an howthey were extracted in section 2, details of the SVMmodel is presented in section 3, the different systemssubmitted to the shared task are described in section4, along with the results in section 5.
I have also in-cluded som discussion of issues encountered duringthe development of features and models in section 6.2662 Model featuresThis section describes the features used in the sub-mitted systems.
All the different text features are de-rived from the surface form of the training and devel-opment corpora without any additional processing orannotation.
The provided tokenization was used andno steming, lemmatization or syntactic parsing wasperformed on the data.2.1 Bareword featuresThe frequency of each token by itself was used asa feature, without any processing or normalization.I.e.
no stemming was used, and any capitalizationwas kept.2.2 Character n-gram featuresThese features consists of n-grams of length n.Character n-grams includes single spaces betweentokens and newlines between lines.
The systemspresented in this paper uses n-gram orders 3-6 or 1-7.2.3 Bareword directed collocation featuresThese are frequencies of the collocations of the baretokens.
The features includes the direction of thecollocation, such that a different feature is generatedif a token is collocated to the left or right of anothertoken.
The collocations are restricted to a windowaround the target token, and all the systems in thispaper uses a window of one token making this fea-ture identical to token bigrams.2.4 Suffix directed collocation featuresThese features are constructed in the same man-ner as the directed bareword collocation features de-scribed in 2.3 except that they are based on the 4-character long suffix of each token.2.5 Feature filtering and TF-IDF weightingFeatures that are presumed to be uninformative arefiltered out before classifier training and prediction.Features with a document count less than a cer-tain limit varying between the systems were ignored,along with features which appears in more than 50%of the documents, i.e.
with a Document Frequency(DF) over 0.5.All the features based on character n-gram orword counts from the corpus was scaled using sub-linear Term Frequency (TF) scaling as described infor exeample (Manning et al 2008).
In addition theIDF was adjusted using add-one smoothing, i.e.
onewas added to all DF counts1.2.6 Proficiency and prompt featuresBoth proficiency value and prompt value for the doc-ument are used as features in the form of 0?
1 indi-cators for the possible values.23 SVM classificationThe system uses an SVM multiclass classifier.
TheSVM classifier was trained without a kernel, i.e.
lin-ear, and with the cost parameter optimized throughcross validation.
SVM was used since it can trainmodels with a large number of features efficiently,and has been successsfully used to construct high-dimensional models in many NLP tasks (Joachims,1998), including NLI (Tsur and Rappoport, 2007;Koppel et al 2005).The cost hyperparameter of the SVM models wasoptimized over 5-fold cross validation on the train-ing set.4 Systems submittedFour systems were submitted to the shared task.
Ofthese three share the same feature types and differ inthe DF cutoff used to prune individual features.
Thefourth system adds additional character n-grams tothe features found in the other three systems.The first three systems are based on the followingfeatures:?
Weighted token counts.?
Weighted character n-grams of orders 3through 6.?
Prompt and proficiency values.1The documentation of the software used for feature ex-traction notes that this smoothing is mainly for numerical con-siderations, i.e.
avoiding division by zero errors (http://scikit-learn.org/stable/modules/generated/sklearn.feature extraction.text.TfidfTransformer.html)2While the prompt value is included in the submitted sys-tems it was not found to be an effective feature and did not haveany effect on the performance of the systems.
Its inclusion inthe feature set is an oversight.267?
Weighted directed token collocation countswith a window size of one, i.e.
token bigrams.?
Weighted directed 4 character suffix colloca-tion counts with a window size of 1, i.e.
4 char-acter suffix bigrams.The three systems vary in the DF cutoff with nocutoff in systmem 1, a cutoff of 5 in system 2 and acutoff of 10 in system 3.System 4 uses different cutoffs for different fea-tures; 10 for token and character n-gram frequenciesand 5 for the token and suffix collocation features.It also uses character n-grams of order 1 through 7instead of 3 through 6.Table 1 show the performance of the four systemson the development data set in addition to the featurecount for each of the systems.
The table shows bothclassification accuracy on the development data setin addition to average and standard deviation for 10-fold cross validation scores over the combined train-ing and development data sets.The software used to generate the systemsis available at https://github.com/andrely/NLI2013-submission.5 ResultsThe final results shows competitive performancefrom all the submitted systems with little variationin performance between them.
Both test set accura-cies and average 10-fold cross validation scores withstandard deviation for the shared tasks fixed foldsare given in table 2.6 Some impressionsPerformance stability: When developing the vari-ous systems the performance was always robust forthe features described in this paper and variations onthem.
There were little variation in 5-fold cross vali-dation scores, or difference between cross validationand held out scores.
This was taken as an indicationthat the system was not being overfitted despite theamount of and specificity of the features.Feature comparison: All the lexical featuresused were highly predictive also in isolation, andcould be used for a competetive system by them-selves.POS tags and lemmatization: Similar featuresbased on POS tags or lemmatized tokens turned outto be much less predictive than the lexical features.This could be caused by low quality of such annota-tion on data with many spelling or other errors.AcknowledgmentsThe Python software package Scikit-learn3 (Pe-dregosa et al 2011) and libSVM4 (Chang and Lin,2011) was used to implement the systems describedin this paper.ReferencesCharles S. Ahn.
2011.
Automatically Detecting Authors?Native Language.
Master?s thesis, Naval PostgraduateSchool, Monterey, CA.Yves Bestgen, Sylviane Granger, and Jennifer Thewis-sen. 2012.
Error Patterns and Automatic L1 Identifi-cation.
In Scott Jarvis and Scott A. Crosley, editors,Approaching Language Transfer through Text Classi-fication, pages 127?153.
Multilingual Matters.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.ACM Transactions on Intelligent Systems and Tech-nology, 2:27:1?27:27.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.Thorsten Joachims.
1998.
Text categorization with su-port vector machines: Learning with many relevantfeatures.
In Proceedings of the 10th European Con-ference on Machine Learning, ECML ?98, pages 137?142, London, UK, UK.
Springer-Verlag.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Automatically determining an anonymous author?s na-tive language.
Intelligence and Security Informatics,pages 41?76.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, New York, NY,USA.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Oren Tsur and Ari Rappoport.
2007.
Using ClassifierFeatures for Studying the Effect of Native Language3http://scikit-learn.org/4http://www.csie.ntu.edu.tw/ cjlin/libsvm/268System # of features Dev.
10-fold accuracy Dev.
accuracy1 867479 0.841?
0.010 0.8272 439063 0.839?
0.012 0.8243 282797 0.838?
0.012 0.8234 510191 0.836?
0.011 0.824Table 1: Performance and number of features for the submitted systems.
Performance is shown as accuracy on thedevelopment data set and 10-fold cross validation on the training and test set.
The feature counts shown are for thefinal systems trained on the training and development data sets.
The systems are described in section 4.System Accuracy 10-fold accuracy1 0.833 0.839?
0.0132 0.834 0.837?
0.0113 0.833 0.835?
0.0124 0.830 0.835?
0.012Table 2: Final accuracy scores on the test set and 10-fold cross validation for the submitted systems.
The systems aredescribed in section 4.on the Choice of Written Second Language Words.In Proceedings of the Workshop on Cognitive Aspectsof Computational Language Acquisition, pages 9?16,Prague, Czech Republic, June.
Association for Com-putational Linguistics.269
