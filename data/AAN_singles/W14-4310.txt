Proceedings of the SIGDIAL 2014 Conference, pages 74?78,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsUser Modeling by Using Bag-of-Behaviors for Building a Dialog SystemSensitive to the Interlocutor?s Internal StateYuya Chiba, Takashi Nose, Akinori ItoGraduate School of Engineering,Tohoku University, JapanMasashi ItoFaculty of EngineeringTohoku Institute of Technology, JapanAbstractWhen using spoken dialog systems in ac-tual environments, users sometimes aban-don the dialog without making any in-put utterance.
To help these users beforethey give up, the system should know whythey could not make an utterance.
Thus,we have examined a method to estimatethe state of a dialog user by capturing theuser?s non-verbal behavior even when theuser?s utterance is not observed.
The pro-posed method is based on vector quan-tization of multi-modal features such asnon-verbal speech, feature points of theface, and gaze.
The histogram of the VQcode is used as a feature for determiningthe state.
We call this feature ?the Bag-of-Behaviors.?
According to the experi-mental results, we prove that the proposedmethod surpassed the results of conven-tional approaches and discriminated thetarget user?s states with an accuracy ofmore than 70%.1 IntroductionSpoken dialog systems have an advantage of be-ing a natural interface since speech commands areless subject to the physical constraints imposed bydevices.
On the other hand, if the system acceptsonly a limited expression, the user need to learnhow to use the system.
If the user is not familiarwith the system, he/she cannot even make an in-put utterance.
Not all users are motivated to con-verse with the system in actual environments, andsometimes a user will abandon the dialog with-out making any input utterance.
When the userhas difficulty to make the utterance, conventionalsystems just repeat the prompt at fixed interval(Yankelovich, 1996) or taking the initiative in thedialog to complete the task (Chung, 2004; Bo-hus and Rudnicky, 2009).
However, we think thatthe system has to cope with the user?s implicit re-quests to help the user more adequately.
To solvethis problem, Chiba and Ito (2012) proposed amethod to estimate two ?user?s states?
by captur-ing their non-verbal cues.
Here, the state A iswhen the user does not know what to input, andthe state B is when the user is considering how toanswer the system?s prompt.
These states have notbeen distinguished by the conventional dialog sys-tems so far, but should be handled differently.The researchers of spoken dialog systems havefocused on the various internal states of userssuch as emotion (Forbes-Riley and Litman, 2011a;Metallinou et al., 2012), preference (Pargellis etal., 2004) and familiarity with the system (Jokinenand Kanto, 2004; Rosis et al., 2006) to build natu-ral dialog system.
In particular, the user?s ?uncer-tainty?
is assumed to be the nearest user?s statesthat we wish to study.
Forbes-Riley and Litman(2011b) and Pon-Barry et al.
(2005) introduced aframework for estimating the user?s uncertainty toa tutor system.The above-mentioned researches have a cer-tain result by employing linguistic informationfor the estimation, but it remains difficult to as-sist a user who does not make any input utter-ance.
By contrast, the method by Chiba and Ito(2012) estimated the target user?s state by onlyusing the user?s non-verbal information.
In theirwork, the user?s multi-modal behaviors were de-fined empirically, and the labels of the behaviorswere annotated manually.
Based on this result, thepresent paper proposes the method that does notuse manually-defined labels nor manual annota-tion.
The multi-modal behaviors are determinedautomatically using the vector quantization, andthe frequency distribution of the VQ code is usedfor estimation of the user?s state.
Because this ap-proach expects to construct clusters of the speechevents or behaviors of the user, we called it as Bag-of-Behaviors approach.2 Data collectionThe experimental data (video clips) were the sameas those used in the experiment by Chiba et al.
(Chiba and Ito, 2012; Chiba et al., 2012).
Thevideo clips contained the frontal image of the user74and their speech, which were recorded with a webcamera and a lapel microphone, respectively.
Thetask of the dialog was a question-and-answer taskto ask users to answer common knowledge ora number they remembered in advance, such as?Please input your ID.?
16 users (14 males and 2females) participated in the dialog collection.Recorded clips were divided into sessions,where one session included one interchange of thesystem?s prompt and the user?s response.
The totalnumber of sessions was 792.
Then we employedevaluators to label each video clip as either state A,B or C, where state A and B were that described inthe previous section, and state C is the state wherethe user had no problem answering the system.
Wetook the majority vote of the evaluators?
decisionsto determine the final label of a clip.
Fleiss?
?among the evaluators was 0.22 (fair agreement).Finally, we obtained 59, 195 and 538 sessions ofstate A, B and C, respectively.3 Discrimination method by usingBag-of-BehaviorsIn the work of Chiba et al.
(2013), the user?sstate was determined using the labels of the multi-modal events such as fillers or face orientation,which were estimated from the low-level acousticand visual features.Here, inventory of multi-modal events was de-termined empirically.
There were, however, twoproblems with this method.
The first one was thatthe optimality of the inventory was not guaran-teed.
The second one is that it was difficult to esti-mate the events from the low-level features, whichmade the final decision more difficult.
Therefore,we propose a new method for discriminating theuser?s state using automatically-determined eventsobtained by the vector quantization.First, a codebook of the low-level features(which will be described in detail in the nextsection) is created using k-means++ algorithm(Arthur and Vassilvitskii, 2007).
Let a low-levelfeature vector at time t of session s of the trainingdata be x(s)t. Then we perform the clustering ofthe low-level feature vectors for all of t and s, andcreate a codebook C = {c1, .
.
.
, cK}, where ckdenotes the k-th centroid of the codebook.Then the input feature vectors are quantizedframe-by-frame using the codebook.
When a ses-sion for evaluation sEis given, we quantize the in-put low-level feature vectors x(sE)1, .
.
.
,x(sE)Tintoq1, .
.
.
, qT, whereqt= arg minq||x(sE)t?
cq||.
(1)Then we calculate the histogram Q0(sE) =(Q1, .
.
.
, QK) whereQk=T?t=1?
(k, qt) (2)?
(x, y) ={1 x = y0 x ?= y(3)Then Q(sE) = Q0(sE)/||Q0(sE)|| is used asthe feature of the discrimination.
The similar fea-tures based on the vector quantization were usedfor image detection and scene analysis (Csurkaet al., 2004; Jiang et al., 2007; Natarajan et al.,2012) and called ?Bag-of-Features?
or ?Bag-of-Keypoints.?
In our research, each cluster of thelow-level features is expected to represent somekind of user?s behavior.
Therefore, we call the pro-posed method the ?Bag-of-Behaviors?
approach.After calculating the Bag-of-Behaviors, we em-ploy an appropriate classifier to determine theuser?s state in the given session.
In this research,the support vector machine (SVM) is used as aclassifier.4 The low-level featuresIn this section, we describe the acoustic and visualfeatures employed as the low-level features.The target user?s states are assumed to have sim-ilar aspects to emotion.
Collignon et al.
(2008)suggested that emotion has a multi-modality na-ture.
For example, W?ollmer et al.
(2013) showedthat the acoustic and visual features contributed todiscriminate arousal and expectation, respectively.Several other researches also have reported thatrecognition accuracy of emotion was improved bycombining multi-modal information (Lin et al.,2012; Wang and Venetsanopoulos, 2012; Paul-mann and Pell, 2011; Metallinou et al., 2012).Therefore, we employed similar features as thoseused in these previous works, such as the spectralfeatures and intonation of the speech, and facialfeature points, etc.4.1 Audio featuresTo represent spectral characteristics of the speech,MFCC was employed as an acoustic feature.
Weused a 39-dimension MFCC including the veloc-ity and acceleration of the lower 12th-order coef-ficients and log power.
In addition, a differentialcomponent of log F0 was used to represent theprosodic feature of the speech, and zero cross (ZC)was used to distinguish voiced and unvoiced seg-ments.
Therefore, total number of audio featureswas 3.
The basic conditions for extracting eachfeature are shown in Table 1.
Here, five frames75(the current frame, the two previous frames andtwo following frames) were used to calculate the?
and ??
components of MFCC and ?
compo-nent of log F0.4.2 Face featureFace feature (Chiba et al., 2013) was extracted bythe Constraint Local Model (CLM) (Saragih et al.,2011) frame by frame.
The coordinates of thepoints relative to the center of the face were usedas the face features.
The scale of the feature pointswas normalized by the size of the facial region.The number of feature points was 66 and the di-mension of the feature was 132.4.3 Gaze featureThe evaluators of the dialogs declared that move-ment of the user?s eyes seems to express their in-ternal state.
The present paper used the Haar-like feature which has a fast calculation algo-rithm using the integral image to represent thebrightness of the user?s eye regions.
This featurewas extracted by applying filters comprehensivelychanged the size and location to the image (eyeregions in our case).
The eye regions were de-tected by the facial feature points.
Because thisfeature had large dimensions, the principal com-ponent analysis (PCA) was conducted to reducethe dimensionality.
Finally, gaze feature had 34 di-mensions and the cumulative contribution rate wasabout 95%.4.4 Feature synchronizationThe audio features were calculated every 10 ms(see Table 1) while the visual features were ex-tracted every 33 ms.
Therefore, the features weresynchronized by copying the visual features of theprevious frame in every 10 ms.5 Discrimination examination5.1 Conditions of the Bag-of-BehaviorsconstructionWe built the Bag-of-Behaviors under two condi-tions described below.Let x(s)at,x(s)ftand x(s)etrepresent the audio fea-ture, face feature and gaze feature of the session sat time t, respectively.Table 1: Conditions of audio feature extractionMFCC logF0 ZCFrame width 25.0 ms 17.0 ms 10.0 msFrame shift 10.0 ms 10.0 ms 10.0 msTable 2: Experimental conditions# of sessions State A(59), State B(195)Codebook size K 4, 8, 16, 32, 64Ka4, 8, 16, 32, 64Kf4, 8, 16, 32, 64Ke4, 8, 16, 32, 64In Condition (1), the three features are com-bined to single feature vector x(s)t:x(s)t= (x(s)at,x(s)ft,x(s)et) (4)Then, the low-level feature vectors x(s)tare clus-tered to construct one codebook C with size K.When an input session sEis given, we calculatethe combined feature vector x(sE)t, and generatethe Bag-of-Behaviors Q(sE).
This method is akind of the feature-level fusion method.In Condition (2), the three features are used sep-arately.
First, we generate three codebooks Ca, Cfand Ceusing the audio, face and gaze features, re-spectively.
Size of those codebooks were Ka,Kfand Ke.
When an input session sEis given,we generate three Bag-of-Behaviors feature vec-tors Qa(sE),Qf(sE) and Qe(sE) using the threecodebooks.
Finally, we combine those features asQ(sE) = (Qa(sE),Qf(sE),Qe(sE)).
(5)5.2 Experimental conditionWe employed the SVM with RBF-kernel as a clas-sifier.
The experimental conditions are summa-rized in Table 2.
The hyperparameters of the clas-sifier were decided by grid-searching.
Since thesession of state C and the other states (state A andstate B) were clearly distinguished by the durationof the session, we used only the session of stateA and state B for the experiments.
Hence, eachexperiment was a two-class discrimination task.As explained, the experimental data were un-balanced.
Since it is desirable that the system candiscriminate the user?s state without deviation, theharmonic meanH of the accuracy of the two stateswas used for measuring the performance.
This iscalculated byH =2CACBCA+ CB, (6)where CAand CBrepresent the discrimination ac-curacy of state A and state B, respectively.
The ex-periments were conducted based on a 5-fold crossvalidation.764 8 16 32 64Number of clusters K30405060708090Discriminationresult[%]State A(CA)State B(CB)Harm.
(H)Figure 1: Discrimination results of condition (1)0 20 40 60 80 100 120Order of H505560657075Discriminationresult(H)[%]Harm.
(H)Figure 2: Discrimination results of condition (2)arranged in descending order5.3 Experimental resultsThe results of condition (1) are shown in Figure1.
The figure shows the best H of each num-ber of clusters.
In condition (1), the best result(H = 70.0%) was obtained when the number ofclusters K was 64.
Figure 2 shows the results ofcondition (2).
In this figure, the results are shownin descending order of the harmonic mean for allcombination of codebook size of the three code-books (there were 53= 125 conditions).
The bestH = 70.7% was obtained when Ka= 8,Kf= 8and Ke= 64.The best results of the tested methods are sum-marized in Table 3.
Here, ?Baseline + NN?
inthe table denotes the result in Chiba et al.
(2013),where the visual events and acoustic events wereannotated manually, and the manual labels wereTable 3: Comparison of estimation methodsState A State B Harm.Baseline + NN 52.5 65.1 58.2Baseline + Gaze + NN 64.5 59.5 61.9Condition (1) + RBF-SVM 67.9 72.3 70.0Condition (2) + RBF-SVM 67.7 73.8 70.7Condition (2) + MKL-SVM 68.0 76.4 72.0used as input for a neural network for the classi-fication.
The gaze feature was not used in ?Base-line + NN.?
We added the result when includingthe gaze feature, shown as ?Baseline + Gaze +NN.?
As shown in Table 3, the performance of themethod proposed in this paper surpassed the base-line methods.
Therefore, the proposed methodcould not only automatically determine the inven-tory of the audio-visual events, but also achievedbetter discrimination accuracy.
One of the reasonsof the improvement is VQ can construct the clus-ters in proper quantities.Comparing the two conditions of feature combi-nation, H of condition (2) (denoted as ?Condition(2) + RBF-SVM?)
was slightly higher than that ofcondition (1) (denoted as ?Condition (1) + RBF-SVM?).
This result was similar to Split-VQ (Pari-wal and Atal, 1991) where a single feature vec-tor split into subvectors and the input vector wasquantized subvector by subvector.We conducted additional experiments for con-dition (2) by using SVM with combined kerneltrained by Multiple Kernel Learning (MKL) (Son-nenburg et al., 2006).
The combined kernel is rep-resented as a linear combination of several sub-kernels.
The distinct kernel was employed forthe speech, face feature and gaze feature, respec-tively.
This paper used the RBF-kernel having thesame width as the sub-kernels?The best result wasshown as ?Condition (2) +MKL-SVM?
in Table 3.As shown in the table, the MKL-SVM showed thehighest performance of 72.0 %.
The weights of theaudio, face and gaze feature were 0.246, 0.005 and0.749, respectively.
This result suggested that thecontribution of the face feature was weaker thanthe other features.6 ConclusionIn this paper, we proposed a method to estimatethe state of the user of the dialog system by us-ing non-verbal features.
We proposed the Bag-of-Behaviors approach, in which the user?s mult-modal behavior was first classified by vector quan-tization, and then the histogram of the VQ codewas used as a feature of the discrimination.
Weverified that the method could discriminate the tar-get user?s state with an accuracy of 70% or more.One of the disadvantages of the current frame-work is that it requires to observe the session untiljust before the user?s input utterance.
This prob-lem makes it difficult to apply this method to anactual system, because the system has to be ableto evaluate the user?s state successively in order tohelp the user at an appropriate timing.
Therefore,we will examine a sequential estimation methodby using the Bag-of-Behaviors in a future work.77ReferencesDavid Arthur and Sergei Vassilvitskii.
2007. k-means++:The advantages of careful seeding.
In Proc.
the 18thannual ACM-SIAM symposium on Discrete algorithms,pages 1027?1035.Dan Bohus and Alexander I. Rudnicky.
2009.
The raven-claw dialog management framework: Architecture andsystems.
Computer Speech & Language, 23(3):332?361.Yuya Chiba and Akinori Ito.
2012.
Estimating auser?s internal state before the first input utterance.Advances in Human-Computer Interaction, 2012:11,DOI:10.1155/2012/865362, 2012.Yuya Chiba, Masashi Ito, and Akinori Ito.
2012.
Effect oflinguistic contents on human estimation of internal stateof dialog system users.
In Proc.
Feedback Behaviors inDialog, pages 11?14.Yuya Chiba, Masashi Ito, and Akinori Ito.
2013.
Estima-tion of user?s state during a dialog turn with sequentialmulti-modal features.
In HCI International 2013-Posters?Extended Abstracts, pages 572?576.Grace Chung.
2004.
Developing a flexible spoken dialogsystem using simulation.
In Proc.
the 42nd Annual Meet-ing on Association for Computational Linguistics, pages63?70.Olivier Collignon, Simon Girard, Frederic Gosselin, Syl-vain Roy, Dave Saint-Amour, Maryse Lassonde, and Lep-ore Franco.
2008.
Audio-visual integration of emotionexpression.
Brain research, 1242:126?135.Gabriella Csurka, Christopher Dance, Lixin Fan,Jutta Willamowski, and C?edric Bray.
2004.
Visualcategorization with bags of keypoints.
In Proc.
workshopon statistical learning in computer vision, ECCV, pages1?2.Kate Forbes-Riley and Diane Litman.
2011a.
Benefits andchallenges of real-time uncertainty detection and adapta-tion in a spoken dialogue computer tutor.
Speech Commu-nication, 53:1115?1136.Kate Forbes-Riley and Diane Litman.
2011b.
Designingand evaluating a wizarded uncertainty-adaptive spoken di-alogue tutoring system.
Computer Speech & Language,25(1):105?126.Yu-Gang Jiang, Chong-Wah Ngo, and Jun Yang.
2007.
To-wards optimal bag-of-features for object categorizationand semantic video retrieval.
In Proc.
of the 6th ACMinternational conference on Image and video retrieval,pages 494?501.Kristiina Jokinen and Kari Kanto.
2004.
User expertise mod-elling and adaptivity in a speech-based e-mail system.
InProc.
the 42nd Annual Meeting on Association for Com-putational Linguistics, pages 88?95.Jen-Chun Lin, Chung-Hsien Wu, and Wen-Li Wei.
2012.Error weighted semi-coupled hidden markov model foraudio-visual emotion recognition.
IEEE Trans.
Multime-dia, 14(1):142?156.Angeliki Metallinou, Martin W?ollmer, Athanasios Kat-samanis, Florian Eyben, Bj?orn Schuller, andShrikanth Narayanan.
2012.
Context-sensitive learningfor enhanced audiovisual emotion classification.
IEEETrans.
Affective Computing, 3(2):184?198.Pradeep Natarajan, Shuang Wu, Shiv Vitaladevuni, Xiao-dan Zhuang, Stavros Tsakalidis, and Unsang Park, Ro-hit Prasad, and Premkumar Natarajan.
2012.
Multimodalfeature fusion for robust event detection in web videos.In Proc.
Computer Vision and Pattern Recognition, pages1298?1305.Andrew Pargellis, Hong-Kwang Jeff Kuo, and Chin-Hui Lee.2004.
An automatic dialogue generation platform for per-sonalized dialogue applications.
Speech Communication,42:329?351.Kuldip Paliwal and Bishnu Atal.
1993.
Efficient vector quan-tization of lpc parameters at 24 bits/frame.
In IEEE Trans.Speech and Audio Processing, 1(1):3?14.Silke Paulmann and Marc Pell.
2011.
Is there an advantagefor recognizing multi-modal emotional stimuli?
Motiva-tion and Emotion, 35(2):192?201.Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt,Brady Clark, and Stanley Peters.
2005.
Responding tostudent uncertainty in spoken tutorial dialogue systems.Int.
J. Artif.
Intell.
Edu., 16:171?194.Fiorella Rosis, Nicole Novielli, Valeria Carofiglio, Addo-lorata Cavalluzzi, and Berardina Carolis.
2006.
Usermodeling and adaptation in health promotion dialogswith an animated character.
J. Biomedical Informatics,39:514?531.Jason Saragih, Simon Lucey, and Jeffrey Cohn.
2011.
De-formable model fitting by regularized landmark mean-shift.
Int.
J.
Computer Vision, 91(2):200?215.Yongjin Wang and Anastasios Venetsanopoulos.
2012.
Ker-nel cross-modal factor analysis for information fusionwith application to bimodal emotion recognition.
IEEETrans.
Multimedia, 14(3):597?607.Martin W?ollmer, Moritz Kaiser, Florian Eyben,Bj?orn Schuller, and Gerhard Rigoll.
2013.
Lstm-modeling of continuous emotions in an audiovisual affectrecognition framework.
Image and Vision Computing,31(2):153?163.Nicole Yankelovich.
1996.
How do users know what to say?Interactions, 3(6):32?43.78
