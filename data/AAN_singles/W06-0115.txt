Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108?117,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe Third International Chinese Language Processing Bakeoff:Word Segmentation and Named Entity RecognitionGina-Anne LevowUniversity of Chicago1100 E. 58th St.Chicago, IL 60637 USAlevow@cs.uchicago.eduAbstractThe Third International Chinese LanguageProcessing Bakeoff was held in Spring2006 to assess the state of the art in twoimportant tasks: word segmentation andnamed entity recognition.
Twenty-ninegroups submitted result sets in the twotasks across two tracks and a total of fivecorpora.
We found strong results in bothtasks as well as continuing challenges.1 IntroductionMany important natural language processing tasksranging from part of speech tagging to parsingto reference resolution and machine translationassume the ready availability of a tokenizationinto words.
While such tokenization is relativelystraight-forward in languages which use whites-pace to delimit words, Chinese presents a signif-icant challenge since it is typically written with-out such separation.
Word segmentation has thuslong been the focus of significant research becauseof its role as a necessary pre-processing phase forthe tasks above.
However, word segmentation re-mains a significant challenge both for the difficultyof the task itself and because standards for seg-mentation vary and human segmenters may oftendisagree.SIGHAN, the Special Interest Group for Chi-nese Language Processing of the Associationfor Computational Linguistics, conducted twoprior word segmentation bakeoffs, in 2003 and2005(Emerson, 2005), which established bench-marks for word segmentation against which othersystems are judged.
The bakeoff presentations atSIGHAN workshops highlighted new approachesin the field as well as the crucial importance ofhandling out-of-vocabulary (OOV) words.A significant class of OOV words is Named En-tities, such as person, location, and organizationnames.
These terms are frequently poorly coveredin lexical resources and change over time as newindividuals, institutions, or products appear.
Theseterms also play a particularly crucial role in infor-mation retrieval, reference resolution, and ques-tion answering.
As a result of this importance, andinterest in expanding the scope of the bakeoff ex-pressed at the Fourth SIGHAN Workshop, in theWinter of 2005 it was decided to hold a new bake-off to evaluate both continued progress in WordSegmentation (WS) and the state of the art in Chi-nese Named Entity Recognition (NER).2 Details of the Evaluation2.1 CorporaFive corpora were provided for the evaluation:three in Simplified characters and two in tradi-tional characters.
The Simplified character cor-pora were provided by Microsoft Research Asia(MSRA) for WS and NER, by University of Penn-sylvania/University of Colorado (UPUC) for WS,and by the Linguistic Data Consortium (LDC) forNER.
The Traditional character corpora were pro-vided by City University of Hong Kong (CITYU)for WS and NER and by the Chinese Knowl-edge Information Processing Laboratory (CKIP)of the Academia Sinica, Taiwan for WS.
Each dataprovider offered separate training and test corpora.General information for each corpus appears inTable 1.All data providers were requested to supplythe training and test corpora in both the stan-dard local encoding and in Unicode (UTF-8) ina standard XML format with sentence and wordtags, and named entity tags if appropriate.
For108Source Encodings Training (Wds/Types) Test (Wds/Types)CITYU BIG5HKSCS/Unicode 1.6M/76K 220K/23KCKIP BIG5/Unicode 5.5M/146K 91K/15KLDC Unicode 632K (est.
wds) 61K (est.
wds)MSRA GB18030/Unicode 1.3M/63K 100K/13KUPUC GB/Unicode 509K/37K 155K/17KTable 1: Overall corpus statisticsall providers except the LDC, missing encodingswere transcoded by the organizers using the ap-propriate Python CJK codecs.Primary training and truth data for word seg-mentation were generated by the organizers via aPython script by replacing sentence end tags withnewlines and word end tags with a single whites-pace character, deleting all other tags and associ-ated newlines.
For test data, end of sentence tagswere replaced with newlines and all other tags re-moved.
Since the UPUC truth corpus was onlyprovided in white-space separated form, test datawas created by automatically deleting line-internalwhitespace.Primary training and truth data for namedentity recognition were converted from theprovided XML format to a two-column formatsimilar to that used in the CoNLL 2002 NERtask(Sang, 2002) adapted for Chinese, wherethe first column is the current character andthe second column the corresponding tag.
For-mat details may be found at the bakeoff website(http://www.sighan.org/bakeoff2006/).For consistency, we tagged only ?<NAMEX>?mentions, of either (PER)SON, (LOC)ATION,(ORG)ANIZATION, or (G)EO-(P)OLITICAL(E)NTITY as annotated in the corpora.1 Test wasgenerated as above.The LDC required sites to download trainingdata from their website directly in the ACE2 eval-uation format, restricted to ?NAM?
mentions.
Theorganizers provided the sites with a Python scriptto convert the LDC data to the CoNLL formatabove, and the same script was used to create thetruth data.
Test data was created by splitting onnewlines or Chinese period characters.Comparable XML format data was also pro-vided for all corpora and both tasks.The segmentation and NER annotation stan-dard, as appropriate, for each corpus was made1Only the LDC provided GPE tags.2http://www.ldc.upenn.edu/projects/ACEavailable on the bakeoff website.
As observedin previous evaluations, these documents variedwidely in length, detail, and presentation lan-guage.Except as noted above, no additional changeswere made to the data furnished by the providers.2.2 Rules and ProceduresThe Third Bakeoff followed the structure of thefirst two word segmentation bakeoffs.
Participat-ing groups (?sites?)
registered by email form; onlythe primary contact was required to register, iden-tifying the corpora and tasks of interest.
Train-ing data was released for download from the web-sites (both SIGHAN and LDC) on April 17, 2006.Test data was released on May 15, 2006 and re-sults were due 14:00 GMT on May 17.
Scores forall submitted runs were emailed to the individualgroups by May 19, and were made available to allgroups on a web page a few days later.Groups could participate in either or both of twotracks for each task and corpus:?
In the open track, participants could use anyexternal data they chose in addition to theprovided training data.
Such data could in-clude external lexica, name lists, gazetteers,part-of-speech taggers, etc.
Groups were re-quired to specify this information in their sys-tem descriptions.?
In the closed track, participants could onlyuse information found in the provided train-ing data.
Information such as externally ob-tained word counts, part of speech informa-tion, or name lists was excluded.Groups were required to submit fully automaticruns and were prohibited from testing on corporawhich they had previously used.Scoring was performed automatically using acombination of Python and Perl scripts, facilitatedby stringent file naming conventions.
In cases109where naming errors or minor divergences fromrequired file formats arose, a mix of manual inter-vention and automatic conversion was employedto enable scoring.
The primary scoring scriptswere made available to participants for followupexperiments.3 Participating SitesA total of 36 sites registered, and 29 submitted re-sults for scoring.
The greatest number of partici-pants came from the People?s Republic of China(11), followed by Taiwan (7), the United States(5), Japan (2), with one team each from Singapore,Korea, Hong Kong, and Canada.
A summary ofparticipating groups with task and track informa-tion appears in Table 2.
A total of 144 official runswere scored: 101 for word segmentation and 43for named entity recognition.4 Results & DiscussionWe report results below first for word segmenta-tion and second for named entity recognition.4.1 Word Segmentation ResultsTo provide a basis for comparison, we computedbaseline and possible topline scores for each ofthe corpora.
The baseline was constructed by left-to-right maximal match implemented by Pythonscript, using the training corpus vocabulary.
Thetopline employed the same procedure, but insteadused the test vocabulary.
These results are shownin Tables 3 and 4.For the WS task, we computed the followingmeasures using the score(Sproat and Emerson,2003) program developed for the previous bake-offs: recall (R), precision (P), equally weightedF-measure (F = 2PR(P+R) ), the rate of out-of-vocabulary words (OOV rate) in the test cor-pus, the recall on OOV (Roov), and recall on in-vocabulary words (Riv).
In and out of vocabu-lary status are defined relative to the training cor-pus.
Following previous bakeoffs, we employ theCentral Limit Theorem for Bernoulli trials (Grin-stead and Snell, 1997) to compute 95% confidenceinterval as ?2?
(p(1?p)n ), assuming the binomialdistribution is appropriate.
For recall, Cr, we as-sume that recall represents the probability of cor-rect word identification.
Symmetrically, for pre-cision, we compute Cp, setting p to the precisionvalue.
One can determine if two systems may thenbe viewed as significantly different at a 95% con-fidence level by computing whether their confi-dence intervals overlap.Word segmentation results for all runs groupedby corpus and track appear in Tables 5-12; all ta-bles are sorted by F-score.4.2 Word Segmentation DiscussionAcross all corpora, the best F-score was achievedin the MSRA Open Track at 0.979.
Overall, aswould be expected, the best results on Open trackruns had higher F-scores than the best results forClosed Track runs on the same corpora.
Likewise,the OOV recall rates for the best Open Track sys-tems exceed those of the best Closed Track runson comparable corpora by exploiting outside in-formation.
Unfortunately, few sites submitted runsin both conditions making strong direct compar-isons difficult.Many systems strongly outperformed the base-line runs, though none achieved the topline.
Theclosest approach to the topline score was on theCITYU corpus, with the best performing runsachieving 99% of the topline F-score.It is also informative to observe the rather widevariation in scores across the test corpora.
Themaximum scores were achieved on the MSRA cor-pus closely followed by the CITYU corpus.
Thebest score achieved on the UPUC Open track con-dition, however, was lower than all scores but oneon the MSRA Open track.
However, a comparisonof the baseline, topline, and especially the OOVrates may shed some light on this disparity.
TheUPUC training corpus was only about one-thirdthe size of the MSRA corpus, and the OOV ratefor UPUC was more than double that of any ofthe other corpora, yielding a challenging task, es-pecially in the Closed track.
This high OOV ratemay also be attributed to a change in register, sincethe training data for UPUC had been drawn ex-clusively from the Chinese Treebank and the testdata also included data from other newswire andbroadcast news sources.
In contrast, the MSRAcorpus had both the highest baseline and highesttopline scores, possibly indicating an easier cor-pus in some sense.
The differences in topline alsosuggest a greater degree of variance in the UPUC,and in fact all other corpora, relative the MSRAcorpus.
These differences highlight the continuingchallenges of handling out-of-vocabulary wordsand performing segmentation across different reg-110Site Name Site Country CITYU CKIP MSRA UPUC CITYU LDC MSRAID WS WS WS WS NER NER NERNatural Language Processing Lab,Northeastern University of China 1 ZH C C C CLanguage Technologies Institute,Carnegie Mellon University 2 US O O O ONational Institute of Informationand Communications Technology, Japan 3 JP C C CBasis Technology Corp. 4 US C C C CPattern Recognition and IntelligentSystem Laboratory,Beijing Universityof Posts and Telecommunications 5 ZH C CHKUST, Human LanguageTechnology Center 6 HK O O OThe University of Tokyo 7 JP O O O OInstitute of Software,Chinese Academy of Sciences 8 ZH C C C C C C OCAlias-i, Inc. 9 US C C C C C CBeijing University of Postsand Telecommunications 10 ZH O O OFrance Telecom R&D Beijing 11 ZH C OC ONETEASE InformationTechnology (Beijing) Co., Ltd. 12 ZH O OAI Lab., Dept of InformationManagement, Huafan University,Taiwan.
13 TW OC OCNanjing University, China 14 ZH O OCIntelligent Agent Systems Lab (IASL),Academia Sinica 15 TW C C CSimon Fraser University 16 CA C C CTung Nan Institute of Technology 18 TW CInstitute of Information Science,Taiwan 19 TW C CMicrosoft Research Asia 20 ZH OC OC OCYahoo!
21 US C CCKIP, Academia Sinica, Taiwan 22 TW OKookmin University 23 KO C C C CShenyang Institute ofAeronautical Engineering 24 ZH OC OCInstitute for Infocomm Research,Singapore 26 SG C C C C C CNational Taiwan University 29 TW CITNLP, Harbin Institute ofTechnology, China 30 ZH OC ONational Central Universityat Taiwan 31 TW C CNational Laboratory on MachinePerception, Peking University, China 32 ZH OC OC OC OC OUniversity of Texas at Austin 34 US O O O OTable 2: Participating Sites by Corpus, Task, and TrackSource Recall Precision F-measure OOV Rate Roov RivCITYU 0.930 0.882 0.906 0.040 0.009 0.969CKIP 0.915 0.870 0.892 0.042 0.030 0.954MSRA 0.949 0.900 0.924 0.034 0.022 0.981UPUC 0.869 0.790 0.828 0.088 0.011 0.951Table 3: Baselines: WS: Maximum match with training vocabulary111Source Recall Precision F-measure OOV Rate Roov RivCITYU 0.982 0.985 0.984 0.040 0.993 0.981CKIP 0.980 0.987 0.983 0.042 0.997 0.979MSRA 0.991 0.993 0.992 0.034 0.999 0.991UPUC 0.961 0.976 0.968 0.088 0.989 0.958Table 4: Toplines: WS: Maximum match with testing vocabularySite RunID R Cr P Cp F Roov Riv15 d 0.973 ?0.000691 0.972 ?0.000703 0.972 0.787 0.98115 b 0.973 ?0.000691 0.972 ?0.000703 0.972 0.787 0.98120 0.972 ?0.000703 0.971 ?0.000715 0.971 0.792 0.97932 0.969 ?0.000739 0.970 ?0.000727 0.970 0.773 0.9781 a 0.971 ?0.000715 0.965 ?0.000783 0.968 0.719 0.98115 c 0.965 ?0.000783 0.967 ?0.000761 0.966 0.792 0.97215 a 0.966 ?0.000772 0.967 ?0.000761 0.966 0.786 0.97326 0.968 ?0.000750 0.961 ?0.000825 0.965 0.633 0.98311 0.962 ?0.000815 0.962 ?0.000815 0.962 0.722 0.97216 0.963 ?0.000805 0.958 ?0.000855 0.961 0.689 0.9749 0.966 ?0.000772 0.957 ?0.000865 0.961 0.555 0.9831 b 0.958 ?0.000855 0.963 ?0.000805 0.960 0.714 0.9688 0.952 ?0.000911 0.954 ?0.000893 0.953 0.747 0.96023 0.950 ?0.000929 0.949 ?0.000938 0.949 0.638 0.9634 b 0.845 ?0.001543 0.844 ?0.001547 0.844 0.632 0.8544 a 0.841 ?0.001559 0.844 ?0.001547 0.843 0.506 0.85513 1 0.589 ?0.002097 0.589 ?0.002097 0.589 0.022 0.613Table 5: CITYU: Word Segmentation: Closed TrackSite RunID R Cr P Cp F Roov Riv20 0.978 ?0.000625 0.977 ?0.000639 0.977 0.840 0.98432 0.979 ?0.000611 0.976 ?0.000652 0.977 0.813 0.98534 0.971 ?0.000715 0.967 ?0.000761 0.969 0.795 0.97822 0.970 ?0.000727 0.965 ?0.000783 0.967 0.761 0.9792 0.964 ?0.000794 0.964 ?0.000794 0.964 0.787 0.97113 2 0.544 ?0.002123 0.549 ?0.002121 0.547 0.194 0.55913 3 0.524 ?0.002129 0.503 ?0.002131 0.513 0.195 0.53813 1 0.497 ?0.002131 0.467 ?0.002127 0.481 0.057 0.516Table 6: CITYU: Word Segmentation: Open TrackSite RunID R Cr P Cp F Roov Riv20 0.961 ?0.001280 0.955 ?0.001371 0.958 0.702 0.97215 a 0.961 ?0.001280 0.953 ?0.001400 0.957 0.658 0.97415 b 0.961 ?0.001280 0.952 ?0.001414 0.957 0.656 0.97432 0.958 ?0.001327 0.948 ?0.001468 0.953 0.646 0.97226 0.958 ?0.001327 0.941 ?0.001558 0.949 0.554 0.9761 b 0.947 ?0.001482 0.943 ?0.001533 0.945 0.601 0.9621 a 0.949 ?0.001455 0.940 ?0.001571 0.944 0.694 0.9609 0.951 ?0.001428 0.935 ?0.001630 0.943 0.389 0.97623 0.937 ?0.001607 0.933 ?0.001654 0.935 0.547 0.9548 0.939 ?0.001583 0.929 ?0.001699 0.934 0.606 0.9544 a 0.836 ?0.002449 0.834 ?0.002461 0.835 0.521 0.8494 b 0.836 ?0.002449 0.828 ?0.002496 0.832 0.590 0.84713 1 0.747 ?0.002875 0.677 ?0.003093 0.710 0.036 0.778Table 7: CKIP: Word Segmentation: Closed Track112Site RunID R Cr P Cp F Roov Riv20 0.964 ?0.001232 0.955 ?0.001371 0.959 0.704 0.97534 0.959 ?0.001311 0.949 ?0.001455 0.954 0.672 0.97232 0.958 ?0.001327 0.948 ?0.001468 0.953 0.647 0.9722 a 0.953 ?0.001400 0.946 ?0.001495 0.949 0.679 0.9652 b 0.951 ?0.001428 0.944 ?0.001521 0.948 0.676 0.96413 2 0.724 ?0.002956 0.668 ?0.003115 0.695 0.161 0.74913 3 0.736 ?0.002915 0.653 ?0.003148 0.692 0.160 0.76113 1 0.654 ?0.003146 0.573 ?0.003271 0.611 0.057 0.680Table 8: CKIP: Word Segmentation: Open TrackSite RunID R Cr P Cp F Roov Riv32 0.964 ?0.001176 0.961 ?0.001222 0.963 0.612 0.97626 0.961 ?0.001222 0.953 ?0.001336 0.957 0.499 0.9779 0.959 ?0.001252 0.955 ?0.001309 0.957 0.494 0.9751 a 0.955 ?0.001309 0.956 ?0.001295 0.956 0.650 0.96615 d 0.953 ?0.001336 0.956 ?0.001295 0.955 0.574 0.96611 a 0.955 ?0.001309 0.953 ?0.001336 0.954 0.575 0.96915 b 0.952 ?0.001350 0.956 ?0.001295 0.954 0.575 0.96615 c 0.949 ?0.001389 0.957 ?0.001281 0.953 0.673 0.95915 a 0.949 ?0.001389 0.958 ?0.001266 0.953 0.672 0.95916 0.952 ?0.001350 0.954 ?0.001323 0.953 0.604 0.96411 b 0.950 ?0.001376 0.954 ?0.001323 0.952 0.602 0.9625 0.956 ?0.001295 0.947 ?0.001414 0.951 0.493 0.9721 b 0.946 ?0.001427 0.952 ?0.001350 0.949 0.568 0.95918 c 0.950 ?0.001376 0.930 ?0.001611 0.940 0.272 0.97430 a 0.963 ?0.001192 0.918 ?0.001732 0.940 0.175 0.99118 b 0.954 ?0.001323 0.921 ?0.001703 0.937 0.163 0.9818 0.933 ?0.001578 0.942 ?0.001476 0.937 0.640 0.94323 0.933 ?0.001578 0.939 ?0.001511 0.936 0.526 0.94824 0.923 ?0.001683 0.929 ?0.001621 0.926 0.554 0.93618 a 0.949 ?0.001389 0.897 ?0.001919 0.922 0.022 0.9824 a 0.830 ?0.002371 0.832 ?0.002360 0.831 0.473 0.8424 b 0.817 ?0.002441 0.821 ?0.002420 0.819 0.491 0.829Table 9: MSRA: Word Segmentation: Closed TrackSite RunID R Cr P Cp F Roov Riv11 a 0.980 ?0.000884 0.978 ?0.000926 0.979 0.839 0.98511 b 0.977 ?0.000946 0.976 ?0.000966 0.977 0.840 0.98214 0.975 ?0.000986 0.976 ?0.000966 0.975 0.811 0.98132 0.977 ?0.000946 0.971 ?0.001059 0.974 0.675 0.98810 0.970 ?0.001077 0.970 ?0.001077 0.970 0.804 0.97630 a 0.977 ?0.000946 0.960 ?0.001237 0.968 0.624 0.98934 0.959 ?0.001252 0.961 ?0.001222 0.960 0.711 0.9682 0.949 ?0.001389 0.954 ?0.001323 0.952 0.692 0.9587 0.953 ?0.001336 0.940 ?0.001499 0.947 0.503 0.96924 0.938 ?0.001522 0.946 ?0.001427 0.942 0.706 0.946Table 10: MSRA: Word Segmentation: Open Track113Site RunID R Cr P Cp F Roov Riv20 0.940 ?0.001207 0.926 ?0.001330 0.933 0.707 0.96332 0.936 ?0.001244 0.923 ?0.001355 0.930 0.683 0.9611 a 0.940 ?0.001207 0.914 ?0.001425 0.927 0.634 0.96926 a 0.936 ?0.001244 0.917 ?0.001402 0.926 0.617 0.96626 b 0.932 ?0.001279 0.910 ?0.001454 0.921 0.577 0.96616 0.929 ?0.001305 0.909 ?0.001462 0.919 0.628 0.9585 0.932 ?0.001279 0.904 ?0.001497 0.918 0.546 0.9691 b 0.922 ?0.001363 0.914 ?0.001425 0.918 0.637 0.9498 0.922 ?0.001363 0.912 ?0.001440 0.917 0.680 0.94531 1 0.917 ?0.001402 0.904 ?0.001497 0.910 0.676 0.9409 0.919 ?0.001387 0.895 ?0.001558 0.907 0.459 0.96423 0.915 ?0.001417 0.896 ?0.001551 0.905 0.565 0.94924 0.902 ?0.001511 0.887 ?0.001609 0.895 0.568 0.9344 a 0.831 ?0.001905 0.819 ?0.001957 0.825 0.487 0.8644 b 0.809 ?0.001998 0.827 ?0.001922 0.818 0.637 0.825Table 11: UPUC: Word Segmentation: Closed TrackSite RunID R Cr P Cp F Roov Riv34 0.949 ?0.001118 0.939 ?0.001216 0.944 0.768 0.9662 0.942 ?0.001188 0.928 ?0.001314 0.935 0.711 0.96420 0.940 ?0.001207 0.927 ?0.001322 0.933 0.741 0.9597 0.944 ?0.001169 0.922 ?0.001363 0.933 0.680 0.97012 0.933 ?0.001271 0.916 ?0.001410 0.924 0.656 0.95932 0.940 ?0.001207 0.907 ?0.001476 0.923 0.561 0.97624 0.928 ?0.001314 0.906 ?0.001483 0.917 0.660 0.95410 0.925 ?0.001339 0.897 ?0.001545 0.911 0.593 0.957Table 12: UPUC: Word Segmentation: Open Trackisters and writing styles.4.3 Named Entity ResultsWe employed a slightly modified version of theCoNLL 2002 scoring script to evaluate NER tasksubmissions.
For each submission, we computeoverall phrase precision (P), recall(R), and bal-anced F-measure (F), as well as F-measure foreach entity type (PER-F,ORG-F,LOC-F,GPE-F).For each corpus, we compute a baseline per-formance level as follows.
Based on the trainingdata, using a left-to-right pass over the test data,we assign a named entity tag to a span of charac-ters if it was tagged with a single unique NE tag(PER/LOC/ORG/GPE) in the training data.3 AllIn the case of overlapping spans, we tag the max-imal span.
These scores for all NER corpora arefound in Table 13.4.4 Named Entity DiscussionThough fewer sites participated in the NER task,performances overall were very strong, with only3If the span was a single character and appeared UN-tagged in the corpus, we exclude it.
Longer spans are re-tained for tagging even if they might appear both tagged anduntagged in the training corpus.two runs performing below baseline.
The best F-score overall on the MSRA Open Track reached0.912, with ten other scores for MSRA andCITYU Open Track above 0.85.
Only two sitessubmitted runs in both Open and Closed Trackconditions, and few Open Track runs were sub-mitted at all, again limiting comparability.
Forthe only corpus with substantial numbers of bothOpen and Closed Track runs, MSRA, the top threeruns outperformed all Closed Track runs.System scores and baselines were much higherfor the CITYU and MSRA corpora than for theLDC corpus.
This disparity can, in part, also be at-tributed to a substantially smaller training corpusfor the LDC than the other two collections.
Thepresence of an additional category, Geo-politicalentity, which is potentially confused for eitherlocation or organization also enhances the diffi-culty of this corpus.
Training requirements, vari-ation across corpora, and most extensive tag setswill continue to raise challenges for named entityrecognition.Named entity recognition results for all runsgrouped by corpus and track appear in Tables 14-19; all tables are sorted by F-score.4This result indicates a rescoring of the run below with all114Source P R F PER-F ORG-F LOC-F GPE-FCITY 0.611 0.467 0.529 0.587 0.516 0.503 N/ALDC 0.493 0.378 0.428 0.395 0.29 0.259 0.539MSRA 0.59 0.488 0.534 0.614 0.469 0.531 N/ATable 13: Baselines: NER: Maximal match with unique tag in training corpusSite RunID P R F ORG-F LOC-F PER-F3 0.9143 0.8676 0.8903 0.8046 0.9211 0.908719 ccrf 0.9201 0.8545 0.8861 0.8054 0.9251 0.887221 a 0.9266 0.8475 0.8853 0.7973 0.9232 0.893721 b 0.9242 0.8491 0.8850 0.7976 0.9236 0.892019 avdic 0.9079 0.8626 0.8847 0.7984 0.9233 0.89148 0.9276 0.8181 0.8694 0.7707 0.9114 0.876921 f 0.9188 0.8231 0.8683 0.7852 0.9105 0.865221 g 0.9164 0.8246 0.8681 0.7842 0.9114 0.86369 0.8690 0.8417 0.8551 0.7541 0.8861 0.884519 bme 0.8742 0.8307 0.8519 0.7667 0.9015 0.839526 0.8466 0.8061 0.8259 0.7467 0.8863 0.792731 1 0.9035 0.6973 0.7871 0.7703 0.8905 0.597429 0.7703 0.6447 0.7019 0.4948 0.7613 0.7531Table 14: CITYU: Named Entity Recognition: Closed TrackSite RunID P R F ORG-F LOC-F PER-F6 0.8692 0.7498 0.8051 0.6801 0.8604 0.8098Table 15: CITYU: Named Entity Recognition: Open TrackSite RunID P R F ORG-F LOC-F PER-F GPE-F3 0.8026 0.7265 0.7627 0.6585 0.3046 0.7884 0.82048 0.8143 0.5953 0.6878 0.5855 0.1705 0.6571 0.7727Table 16: LDC: Named Entity Recognition: Closed TrackSite RunID P R F ORG-F LOC-F PER-F GPE-F7 0.7616 0.6621 0.7084 0.5209 0.2857 0.7422 0.79306 GPE-LOC4 0.6720 0.6554 0.6636 0.4553 0.7078 0.74206 0.3058 0.2982 0.3019 0.4553 0.0370 0.7420 0.0Table 17: LDC: Named Entity Recognition: Open TrackSite RunID P R F ORG-F LOC-F PER-F14 0.8894 0.8420 0.8651 0.8310 0.8545 0.900921 a 0.9122 0.8171 0.8620 0.8196 0.9053 0.825721 b 0.8843 0.8288 0.8556 0.7698 0.9013 0.84953 0.8814 0.8234 0.8514 0.8150 0.9062 0.793821 f 0.8845 0.7931 0.8363 0.8071 0.9003 0.756821 g 0.8661 0.8032 0.8335 0.7742 0.8991 0.775319 avdic 0.8637 0.7767 0.8179 0.8138 0.8233 0.812619 dcrf 0.8623 0.7740 0.8158 0.8141 0.8207 0.80939 0.8188 0.8097 0.8142 0.7295 0.8529 0.819619 cnoword 0.8670 0.7554 0.8074 0.8100 0.8257 0.776419 bvoting 0.8583 0.7606 0.8065 0.8145 0.8133 0.789926 0.8105 0.7882 0.7992 0.7491 0.8385 0.769921 r 0.8748 0.7168 0.7880 0.7288 0.8604 0.71078 0.8164 0.3124 0.4519 0.4591 0.5084 0.3521Table 18: MSRA: Named Entity Recognition: Closed Track115Site RunID P R F ORG-F LOC-F PER-F10 0.9220 0.9018 0.9118 0.8590 0.9034 0.960414 0.9076 0.8922 0.8999 0.8397 0.9099 0.926111 b 0.8767 0.8753 0.8760 0.7611 0.8976 0.922511 a 0.8645 0.8399 0.8520 0.6945 0.8745 0.919932 0.8397 0.8184 0.8289 0.7261 0.8804 0.82077 0.8468 0.7822 0.8132 0.6958 0.8552 0.82806 0.8195 0.6926 0.7507 0.6443 0.8291 0.695530 a 0.8697 0.6556 0.7476 0.5841 0.7029 0.89878 0.8320 0.6703 0.7424 0.5651 0.8000 0.756512 b 0.7083 0.5464 0.6169 0.4168 0.6154 0.717112 a 0.7395 0.5186 0.6096 0.4168 0.6154 0.7074Table 19: MSRA: Named Entity Recognition: Open Track5 Conclusions & Future DirectionsThe Third SIGHAN Chinese Language Process-ing Bakeoff successfully brought together a col-lection of 29 strong research groups to assess theprogress of research in two important tasks, wordsegmentation and named entity recognition, that inturn enable other important language processingtechnologies.
The individual group presentationsat the SIGHAN workshop detail the approachesthat yielded strong performance for both tasks.
Is-sues of out-of-vocabulary word handling, anno-tation consistency, character encoding and codemixing of Chinese and non-Chinese text all con-tinue to challenge system designers and bakeofforganizers alike.In future analyses, we hope to develop addi-tional analysis tools to better assess progress inthese fundamental tasks, in a more corpus inde-pendent fashion.
Microsoft Research Asia hasbeen pursuing work along these lines focusing onimprovements in F-score and OOV F-score rela-tive to more intrinsic corpus measures, such asbaselines and toplines.5 Such developments willguide the planning of future evaluations.Finally, while word segmentation and namedentity recognition are important in themselves, itis also important to assess the impact of improve-ments in these enabling technologies on broaderdownstream applications.
More tightly coupledexperiments that involve joint word segmentationand named entity recognition could provide in-sight.
Integration of WS and NER with a higherlevel task such as parsing, reference resolution, ormachine translation could allow the developmentof more refined, task-oriented metrics to evalu-GPE tags in the truth data mapped to LOC, since no GPE tagswere present in the results.5Personal communication, Mu Li, Microsoft ResearchAsia.ate WS and NER and focus attention on improve-ments to the fundamental techniques which en-hance performance on higher level tasks.AcknowledgementsWe gratefully acknowledge the generous assis-tance of the organizations and individuals listedbelow who provided the data for this bakeoff;without their support, it could not have takenplace:?
Chinese Knowledge Information ProcessingGroup, Academia Sinica, Taiwan: Keh-JiannChen, Henning Chiu?
City University of Hong Kong: Benjamin K.Tsou, Olivia Oi Yee Kwong?
Linguistic Data Consortium: StephanieStrassel?
Microsoft Research Asia: Mu Li?
University of Pennsylvania/University ofColorado, USA: Martha Palmer, NianwenXueWe also thank Hwee Tou Ng and Olivia Oi YeeKwong, the co-organizers of the fifth SIGHANworkshop, in conjunction with which this bakeofftakes place.
Olivia Kwong merits special thanksboth for her help in co-organizing this bakeoff andin coordinating publications.
Finally, we thank allthe participating sites who enabled the success ofthis bakeoff.ReferencesThomas Emerson.
2005.
The Second InternationalChinese Word Segmentation Bakeoff.
In Proceed-ings of the Fourth SIGHAN Workshop on ChineseLanguage Processing, Jeju Island, Republic of Ko-rea.116Charles M. Grinstead and J. Laurie Snell.
1997.
In-troduction to Probability.
American MathematicalSociety, Providence, RI.Erik F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 Shared Task: Language-IndependentNamed Entity Recognition.
In Proceedings of the6th Conference on Natural Language Learning 2002(CoNLL-2002).Richard Sproat and Thomas Emerson.
2003.
The FirstInternational Chinese Word Segmentation Bakeoff.In Proceedings of the Second SIGHAN Workshop onChinese Language Processing, Sapporo, Japan.117
