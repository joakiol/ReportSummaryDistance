Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 57?65,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsThe semantic augmentation of a psycholinguistically-motivated syntacticformalismAsad Sayeed and Vera DembergComputational Linguistics and Phonetics / M2CI Cluster of ExcellenceSaarland University66123 Saarbru?cken, Germany{asayeed,vera}@coli.uni-saarland.deAbstractWe augment an existing TAG-based in-cremental syntactic formalism, PLTAG,with a semantic component designed tosupport the simultaneous modeling ef-fects of thematic fit as well as syntacticand semantic predictions.
PLTAG is apsycholinguistically-motivated formalismwhich extends the standard TAG opera-tions with a prediction and verificationmechanism and has experimental supportas a model of syntactic processing diffi-culty.
We focus on the problem of for-mally modelling semantic role predictionin the context of an incremental parseand describe a flexible neo-Davidsonianformalism and composition procedure toaccompany a PLTAG parse.
To thisend, we also provide a means of aug-menting the PLTAG lexicon with seman-tic annotation.
To illustrate this, we runthrough an experimentally-relevant modelcase, wherein the resolution of semanticrole ambiguities influences the resolutionof syntactic ambiguities and vice versa.1 IntroductionPLTAG (PsychoLinguistically-motivated TAG,Demberg and Keller, 2008; Demberg et al 2014)is a variant of Tree-Adjoining Grammar (TAG)which is designed to allow the construction ofTAG parsers that enforce strict incrementality andfull connectedness through (1) constraints on theorder of operations, (2) a new type of unlexical-ized tree, so-called prediction trees, and (3) a veri-fication mechanism that matches up and extendspredicted structures with later evidence.
Psy-cholinguistic evaluation has shown that PLTAGoperations can be used to predict data from eye-tracking experiments, lending this syntactic for-malism greater psycholinguistic support.Syntax, however, may not just be the skeleton ofa linguistic construction that bears semantic con-tent: there is some evidence that syntactic struc-ture and semantic plausibility interact with eachother.
In a strongly interactive view, we would ex-pect that semantic plausibility could directly affectthe syntactic expectations.
Consider the sentences:(1) a.
The woman slid the butter to the man.b.
The woman slid the man the butter.The ditransitive verb ?to slide?
provides threeroles for participants in the predicate: agent, pa-tient, and recipient.
In both cases, ?the woman?fills the agent role, ?the butter?
the patient, and?the man?
the recipient.
However, they do not gen-erally fill all roles equally well.
English-speakershave the intuition that ?the butter?
should neitherbe an agent nor a recipient under normal circum-stances.
Likewise, ?the man?
is not a typical pa-tient in this situation.
If there is a psycholinguis-tic effect of semantic plausibility, we would ex-pect that an incomplete sentence like ?The womanslid the butter?
would generate an expectation inthe listener of a PO construction (rather than DO)with preposition ?to?, as well as an expectation ofa noun phrase and an expectation that that nounphrase would belong to the class of entities thatare plausible recipients for entities that are slid.If this is the case, then there is not only a syntac-tic expectation at this point but a semantic expecta-tion that is in turn informed by the syntactic struc-ture and semantic content up to that point.
Con-structing a model that is formally rich, psycholin-guistically plausible, and empirically robust re-quires making design decisions about the specificrelationship between syntax and semantics and theoverall level of formal articulation on which thestatistical model rests.
For PLTAG, we are inter-ested in preserving as many of its syntactic charac-teristics as are necessary to model the phenomenathat it already does (Demberg and Keller, 2009).57In the rest of this paper, we therefore present asemantic augmentation of PLTAG that is based onneo-Davidsonian event semantics and is capableof supporting incrementality and prediction.2 Psycholinguistic backgroundDoes thematic fit dynamically influence the choiceof preferred syntactic structures, does it shape pre-dictions of upcoming semantic sorts, and can wemeasure this experimentally?A classic study (Altmann and Kamide, 1999)about the influence of thematic fit on predictionsshowed that listeners can predict the complementof a verb based on its selectional restrictions.
Par-ticipants heard sentences such as:(2) a.
The boy will eat the cake.b.
The boy will move the cake.while viewing images that depicted sets of rele-vant objects, in this example, a cake, a train set,a ball, and a model car.
Altmann and Kamide(1999) monitored participants?
eye-movementswhile they heard the sentences and found an in-creased number of looks to the cake during theword eat compared the control condition, i.e., dur-ing the word move (only the cake is edible, butall depicted objects are movable).
This indicatesthat selectional preference information providedby the verb is not only used as soon as it is avail-able (i.e., incremental processing takes place), butthis information also triggers the prediction of up-coming arguments of the verb.
Subsequent workhas demonstrated that this is not a simple associ-ation effect of eat and the edible item cake, butthat people assign syntactic roles rapidly based oncase marking and that missing obligatory thematicrole fillers are predicted; in a German visual worldstudy, Kamide et al(2003a) presented participantswith a scene containing a cabbage, a hare, a foxand a distractor object while they heard sentenceslike(3) a.
Der Hase frisst gleich den Kohl.
(The harenom will eat soon the cabbageacc.)b.
Den Hasen frisst gleich der Fuchs.?
(The hareacc will eat soon the foxnom.
)They found that, during the verb-adverb region,people looked more to the cabbage in the first con-dition and correctly anticipated the fox in the sec-ond condition.
This means that they were able tocorrectly anticipate the filler of the missing the-matic role.
Kamide et al(2003b) furthermoreshowed that role prediction is not only restrictedto the immediately-following grammatical object,but that goals as in The woman slid the butter tothe man are also anticipated.Thematic fit furthermore seems to interact withsyntactic structure.
Consider the sentences in (4),which are locally ambiguous with respect to amain clause interpretation or a reduced relativeclause.
(4) a.
The doctor sent for the patient arrived.b.
The flowers sent for the patient arrived.Comprehenders incur decreased processing diffi-culty in sentences like (4-b) compared to (4-a),due to flowers not being a good thematic fit forthe agent role of sending (Steedman, 2000).Taken together, the experimental evidence sug-gests that semantic information in the form ofthematic fit can influence the syntactic structuresmaintained by the comprehender and that peo-ple do generate anticipations not only based onthe syntactic requirements of a sentence, but alsoin terms of thematic roles.
While there is evi-dence that both syntactic and semantic process-ing is rapid and incremental, there remain, how-ever, some open questions on how closely syn-tactic and semantic processing are integrated witheach other.
The architecture suggested here mod-els the parallel, highly incremental construction ofsyntactic and semantic structure, but leaves opento exploration the question of how quickly andstrongly they interact with each other.
Note thatwith the present architecture, thematic fit wouldonly be calculated for word pairs which stand ina possible syntactic relation.
The syntax thus ex-erts strong constraints on which plausibilities areconsidered.
Our example in section 6.2 illustrateshow even a tight form of direct interaction betweensyntax and semantics can be modelled.3 Relation to previous work on jointsyntactic-semantic modelsPrevious attempts have been made to combinethe likelihood of syntactic structure and seman-tic plausibility estimates into one model for pre-dicting human processing difficulty (Pado?
et al2009; Jurafsky, 2002).
Pado?
et al(2009) pre-dict increased difficulty when the preferred syn-tactic analysis is incompatible with the analysisthat would have the best thematic fit.
They inte-grate syntactic and semantic models as a weightedcombination of plausibility scores.
The syntactic58and semantic models are computed to some extentindependently of one another, and then the resultis adjusted by a set of functions that take into ac-count conflicts between the models.
In relation tothe approach proposed here, it is also importantto note that the semantic components in (Pado?
etal., 2009; Jurafsky, 2002) are limited to semanticrole information, while the architecture proposedin this paper can build complete semantic expres-sions for a sentence.
Furthermore, these modelsdo not model the prediction and verification pro-cess (in particular, they do not make any seman-tic role predictions of upcoming input) which hasbeen observed in human language processing.Mitchell et al(2010) propose an integratedmeasure of syntactic and semantic surprisal as amodel of processing difficulty, and show that thesemantic component improves modelling resultsover a syntax-only model.
However, the syntacticand semantic surprisal components are only veryloosely integrated with one another, as the seman-tic model is a distributional bag-of-words modelwhich does not take syntax into account.Finally, the syntactic model underlying (Pado?
etal., 2009; Mitchell et al 2010) is an incrementaltop-down PCFG parser (Roark, 2001), which dueto its parsing strategy fails to predict human pro-cessing difficulty that arises in certain cases, suchas for center embedding (Thompson et al 1991;Resnik, 1992).
Using the PLTAG parsing model isthus more psycholinguistically adequate.3.1 Towards a broad-coverage integration ofsyntax and semanticsThe current paper does not propose a new modelof sentence processing difficulty, but rather ex-plores the formal architecture and mechanism nec-essary to enable the future implementation of anintegrated syntactic-semantic model.
A syntax-informed semantic surprisal component imple-mented using distributional semantics could usethe semantic expressions generated during thePLTAG semantics construction to determine whatwords (in which relationships to the current word)from the previous context to condition on for cal-culating semantic surprisal.4 PLTAG syntaxPLTAG uses the standard operations of TAG: sub-stitution and adjunction.
The order in which theyare applied during a parse is constrained by in-crementality.
This also implies that, in additionto the standard operations, there are reverse Upversions of these operations where the prefix treeis substituted or adjoined into a new elementarytree (see figure 4).
In order to achieve strict incre-mentality and full connectedness at the same timewhile still using linguistically motivated elemen-tary trees, PLTAG has an additional type of (usu-ally) unlexicalized elementary tree called predic-tion trees.
Each node in a prediction tree is markedwith upper and/or lower indices kk to indicate itspredictive status.
Examples for prediction treesare given at the right hand side of figure 5b.
Theavailability of prediction trees enable a sentencestarting with ?The thief quickly?
to integrate boththe NP (?The thief?)
and the ADVP (?quickly?
)into the derivation even though neither type of el-ementary tree can be substituted or adjoined to theother?the system predicts an S tree to which bothcan be attached, but no specific verb head.
Pre-diction markers can be removed from nodes viathe verification operation, which makes sure thatpredicted structure is matched against actually ob-served evidence from the input string.
For the ex-ample above, the verb ran in ?The thief quicklyran?
verifies the predicted verb structure.
In fig-ures 5c through 5e, we also provide an example ofprediction and verification as part of the demon-stration of our semantic framework.
Other foun-dational work on PLTAG (Demberg-Winterfors,2010) contains more detailed description.5 Neo-Davidsonian semanticsDavidsonian semantics organizes the representa-tion of predicates around existentially-quantifiedevent variables (e).
Sentences are therefore treatedas descriptions of these events, leading to a lessrecursive representation where predicates are notdeeply embedded inside one another.
Highlyrecursive representations can be incrementality-unfriendly, potentially requiring complex infer-ence rules to ?undo?
recursive structures if rele-vant information arrives later in the sentence.Neo-Davidsonian semantics (Parsons, 1990;Hunter, 2009) is an extension of Davidsoniansemantics wherein the semantic roles are alsoseparated out into their own first-order predi-cates, rather than being fixed arguments of themain predicate of the verb.
This enables a sin-gle verb predicate to correspond to multiple pos-sible arrangements of role predicates, also an59incrementality-friendly characteristic1.
The Neo-Davidsonian representation allows us separate thesemantic prediction of a role from its syntactic ful-fillment, permitting the type of flexible frameworkwe are proposing in this paper.We adopt a neo-Davidsonian approach to se-mantics by a formalism that bears similarity to ex-isting frameworks such as (R)MRS (Robust Min-imal Recursion Semantics) (Copestake, 2007).However, this paper is intended to explore whatarchitecture is minimally required to augment thePLTAG syntactic framework, so we do not adoptthese existing frameworks wholesale.
Our ex-amples such as figures 4, 5d, and several othersdemonstrate how this looks in practice.6 Semantics for PLTAG6.1 Semantic augmentation for the lexiconConstructing the lexicon for a semantically aug-mented PLTAG uses a process based on the onefor ?purely syntactic?
PLTAG.
The PLTAG lex-icon is extracted automatically from the PLTAGtreebank, which has been derived from the PennTreebank using heuristics for binarizing flat struc-tures as well as additional noun phrase annotations(Vadas and Curran, 2007), PropBank (Palmer etal., 2003), and a slightly modified version ofthe head percolation table of Magerman (1994).PLTAG trees in the treebank are annotated withsyntactic headedness information as well as infor-mation that allows one to distinguish argumentsand modifiers.Given the PLTAG treebank, we extract thecanonical lexicon using well-established ap-proaches from the LTAG literature (in particular(Xia et al 2000): we traverse the converted treefrom each leaf up towards the root, as long as theparental node is the head child of its parent.
If asubtree is not the head child of its parent, we ex-tract it as an elementary tree and proceed in thisway for each word of the converted tree.
Given theargument/modifier distinction, we then create sub-stitution nodes in the parent tree for arguments ora root and foot node in the child tree for modifiers.Prediction trees are extracted automatically by cal-culating the minimal amount of structure neededto connect each word into a structure including allprevious words of the sentence2.
The parts of this1Consider the optionality of the agent role in passive sen-tences, where the ?by-phrase?
may or may not appear.2The reader is referred to (Demberg-Winterfors, 2010;S{?e&?
= e}NP?
{Q1x1ARG0(e, x1)}VP{e}Vlikes{Like(e)}NP?
{Q2x2ARG1(e, x2)}NP{?e}NP*{Q1x1ARG0(e, x1)&?
= x1&?
= Q1}VP{e}Vincluding{Include(e)}NP?
{Q2x2ARG1(e, x2)}Figure 1: Verbal elementary trees extracted fromexample sentence Pete likes sugary drinks includ-ing alcoholic ones.minimally-needed connecting syntactic structurewhich belong to heads to the right of the currentword are stored in the lexicon as prediction trees,c.f.
right hand side of figure 5b.Since Propbank is used in the construction pro-cess of the PLTAG treebank, we can straightfor-wardly display the semantic role annotation on thetree and the extracted lexicon, with the exceptionthat we display role annotations for PPs on theirNP child.
For arguments, annotations are retainedon the substitution node in the parental tree, whilefor modifiers, the role annotation is displayed onthe foot node of the auxiliary tree, as shown for theverbal trees extracted from the sentence Pete likessugary drinks including alcoholic ones in Figure1.
PropBank assigns two roles to the NP nodeabove sugary drinks (it is the ARG1 of likes andthe ARG0 of including), but we can correctly teaseapart these annotations in the lexical extractionprocess using the syntactic annotation and argu-ment/modifier distinction.Using the same procedure, prediction trees areannotated with semantic roles.
It can then happenthat one form of a prediction tree is annotated withdifferent syntactic roles, hence introducing someadditional ambiguity into the lexicon.
For exam-ple, the NP substitution node in subject position ofthe prediction tree rooted in Sk in figure 5b couldbe an ARG0 for some verbs which can verify thistree and an ARG1 for others.PLTAG elementary trees can contain one ormore lexemes, where the first lexeme is the el-ementary tree?s main anchor, and all further lex-emes are predicted.
In earlier PLTAG extractions,elementary trees with several lexemes were usedfor particle verbs like show up and some hand-coded constructions in which the first part is pre-dictive of the second part, such as either .
.
.
or orboth .
.
.
and.
Here we extend this set of trees withDemberg et al 2014) for full details of the PLTAG conver-sion and syntactic part of the lexicon extraction process.60more than one lexeme to verbs with subcatego-rized PPs, as shown, for example, in the secondlexicon entry of slid in figure 5a.
Note the differ-ence to the lexicon entry of optional PPs in figure5b as in on Sunday.
Furthermore,?
All elementary trees which have a role anno-tation in PropBank also have a correspond-ing annotation ?e on their root node thatrepresents the existentially-quantified neo-Davidsonian event variable for that predicate,see fig.
1.?
The event variables and entity variables on anelementary tree are available for binding onthe path from the anchor3 of the elementarytree to the root node.?
Every role annotation on a node is in the formof a predicate ARGn(e, x), where e is theevent variable, and x is an entity variable towhich the role is conferred.?
Every role annotation is prefixed with a vari-able binding Qx, where Q is a higher-ordervariable that represents an unknown quanti-fier.
This ensures that all variables are boundif a role appears before its filler.?
Every elementary tree for an open-class wordhas a head with corresponding predicate.
Forexample, ?butter?
has a predicate Butter(x).?
Prediction trees for open lexical classes (suchas NPs) have a head with a (x) predicate.?
Every nominal elementary tree has a Qx atthe root node so that the entity variable thatis the argument to the predicate on the headis bound.
The Qx is on the root node so thatour semantic processing procedure for substi-tutions and adjunctions (described in the nextsection) can unify the entity variable x withvariables on higher trees.For PPs, we obtain role annotations from Prop-Bank and NomBank.
Other closed-class syntactictypes such as pronouns have appropriately-selected quantifier constants and predicates(e.g.
?someone?
would be represented as?xPerson(x)&?
= ?&?
= x, see next paragraphfor the use of question marks).
Determiners aremerely annotated with a quantifier ?constant?symbol and no variables or predicates.Then we require a type of additional annota-tion to which we refer as a ?variable assignmentstatement?, which we use in our syntactic com-3Lowest node on the path to where the anchor would bein a prediction tree which does not have a lexical anchor.bination process.
These statements are written?
= v, where v is either a quantifier variable(Q) or constant (e.g.
?)
or an entity variable (x).These statements represent the possibility that anincoming tree might have a variable v that couldhave the same binding as one already in the pre-fix tree.
Variable assignment statements occur onroot nodes or foot nodes, except where there is adescendent DT subsitution node, which receivesan additional ?
= Q statement.
The type of vari-able assignment statement (event, entity or quan-tifier) depends on the root node type (entity typelike NP or N vs. event type like S or VP), as shownin figure 1.
The next section describes the use ofthese statements in semantic parsing.
Note thatvariable assignment statements need not be rep-resented explicitly in an implementation, as reas-signing variables can be done via references orother data structures.
We use them as a represen-tational and illustrative convenience here.6.2 Semantic parsing procedureWe integrate semantics into the overall process ofPLTAG parsing by the rules in figures 2 and 3.
Inaddition, we provide a more procedural descrip-tion here.
At the highest level, a step in an incre-mental parse follows this pattern:1.
On scanning a new word or doing a predic-tion step, the PLTAG statistical model selectsa tree from the lexicon, an operation (substi-tion, adjunction, verification), and a positionin the prefix tree at which to insert the tree (ornone, if this is the first word).2.
All the nodes of the incoming tree are vis-ited by the visit operation, and their semanticcontent is appended as conjuncts to the out-put semantic expression.3.
The operation of attaching the new tree intothe derived tree is performed (pltagOp):(a) Variable assignment statements areemitted and appended to the semanticoutput expression according to therules in figure 3, as well as to thesemantic expression at the syntacticnode at which the integration occurs.For verification, the Verify rule has tobe applied to all nodes that are verified.
(b) The syntactic integration of merging thenodes at the substitution or adjunctionsite is performed.
The rules in 3 alsomake sure that the semantic expressions61D : {?}
TPltagSteppltagOp(D,T ) : {?&visit(T )}D : {?
}ResolveD : resolveEqns(?
)Figure 2: Overall rules for trees (T ) and derivations (D) and overall semantic expressions (?).
PltagStepapplies when a new tree is chosen to be integrated with the prefix tree.N1 ?
: {?1, Q1,?2} N2 ?
: {?3, ?
= Q2,?4} D : {?
}QuantEquateD[N1 7?
nodeMerge(N1 : {?1, Q1,?2}, N2 : {?3,?4})] : {?&Q1 = Q2}N1 ?
: {?1, x1,?2} N2 ?
: {?3, ?
= x2,?4} D : {?
}VarEquateD[N1 7?
nodeMerge(N1 : {?1, x1,?2}, N2 : {?3,?4})] : {?&x1 = x2}N1pp : {?1, 1,?2} N2 : {?3} anchor(N2):{?4, Pred(x),?5} D : {?
}VerifyD[N1 7?
nodeMerge(N1 : {?1, 1,?2}, N2 : {?3})] : {?& 1 = Pred}Figure 3: Rules for combining nodes.
The nodes are attached during the derivation via the nodeMergeoperation, with N1 being the node above (?
), and N2 being the node below (?).
These hold for substi-tution and adjunction (for both canonical and prediction trees).
The underlying intuition is that the (?
)node will contain the variable equation, and the (?)
node will contain the mention of a variable to beequated.
The Verify rule equates the variable with the predicate of the verification tree.
The equationis appended to the output expression ?.
Q2 can also be ?
or another quantifier.
VarEquate also applies toevent variables.
The ?n notation represents the prefixes and suffixes of the semantic expressions relativeto the mentioned variable or statement.
The rules delete the variable assignment statement from the nodeby concatenating ?3 and ?4.from both nodes involved in the integra-tion are included in the semantic expres-sion of the merged node.4.
Optionally, a Resolve step is applied, whicheliminates variable assignment statements byreplacing variable mentions with their mostconcrete realization.Regarding variable assignments at the integra-tion of two trees, the value for quantifier vari-ables can be a constant in the form of a quanti-fier.
Entity variables can be equated with otherentity variables, and entity constants (e.g., propernames) are a relatively simple extension to therules4.
Verification variables can only be equatedwith a constant?a predicate name.We present an example of the processing ofa substitution step in figure 4.
The S tree forsleeps with an open NP substitution node is inthe process of having the NP ?someone?
substi-tuted into it using the substUp operation.
So wehave already done step 1 of our parsing procedure.Step 2 is visit, such that the semantic expressionof the NP is appended to the output expression4A noun phrase like ?Peter?
will have the associated se-mantic expression peter&?
= peter and will require an ad-ditional inference rule to remove the quantifier when it isadjoined or substituted to a node carrying a role.
In otherwords, substituting peter into QxARG1(e, x) should resultin ARG1(e, peter).
An analogous rule for constant verifi-cation that allows Qx (x) to be verified as peter is alsorequired.?.
For step 3, the variable assignment statementsare then processed by application of QuantEquateand VarEquate.
Finally in step 4, the expression issimplified with Resolve.The Resolve operation.
From an implementa-tion perspective, resolving variable assignmentstatements does not really need a separate oper-ation, as references can be maintained such thatthe assignment is automatically performed with-out any explicit substitution in the manner of aProlog inference engine?s resolution procedure.The same holds for the variable assignment state-ments.
However, we include explicit mention ofthis mechanism for ease of expression of the se-mantic operations as well as to illustrate some de-gree of convergence with existing formalisms suchas (R)MRS, which also has a mechanism to assertrelationships between variables post hoc.There is only one condition under which ap-plication of Resolve can fail, which is if there ismore than one assignment statement connectingthe same variable to different constants.The Resolve rule is defined to be able to applyto the entire output expression.
When should itapply?
It is defined such that it can be applied atany time; its actual execution will be controlledby the parsing algorithm, e.g., after each parsingoperation or at the end of the parse.There are remaining matters of quantifier scope62NP{?x1Person(x1)&?
= x1&?
= ?}PROsomeonesubstUp??????
S{Ee}NP?
{Q0x0ARG0(e, x0)}VPsleeps(Syntactic view)?
= ?eQ0x0ARG0(e, x0)(Before substitution starts)?
= ?eQ0x0ARG0(e, x0)&?x1Person(x1)&?
= x1&?
= ?
(Result of visit)?
= ?eQ0x0ARG0(e, x0)&?x1Person(x1)&?
= x1&?
= ?&Q0 = ?&x0 = x1(Result of QuantEquate and VarEquate)?
= ?e?x0ARG0(e, x0)&Person(x0)(Result of Resolve)Figure 4: An example incremental step from thesemantic perspective.and semantic well-formedness that must be han-dled post hoc at every step.
For example, univer-sal quantifiers require a distinction to be made be-tween the restrictor of the quantified variable andthe nuclear scope.
It is possible within a neo-Davidsonian representation to perform such rep-resentational adjustments easily, as shown by Say-eed and Demberg (2012).Example Now that we have described the pro-cedure, we provide an example of how this se-mantic augmentation of PLTAG can represent rolelabeling and prediction inside the syntactic pars-ing system.
We perform a relevant segment of theparse of example (1-a), ?The woman slid the but-ter to the man.?
In this sentence, we expect that theparser will already know the expected role of theNP ?the man?
before it actually receives it.
That is,it will know in advance that there is an upcomingNP to be predicted such that it is compatible with arecipient (ARG2) role, and this knowledge will berepresented in the incremental output expression.The minimum lexicon required for our exampleis contained in figures 5a and 5b.
For our illustra-tion, we only include the ditransitive alternation of?slide?.
Both versions of slide contain all the roleson NP nodes.
This parse involves only the predic-tion of noun phrases, so we only have an NP pre-diction tree.
We presume for the sake of simplicitythat the determiner ?the?
represents the existentialquantifier ?.Our parse begins in figure 5c with ?The womanslid?, since these are the same in both cases, andit proceeds up to figure 5e with the sentence ?Thewoman slid the butter to the man?.
We Resolvethe assignments at every step for brevity in the ex-amples, and we also apply it to the nodes.
By fig-ure 5d, the parser already knows that the ARG2 of?slide?
is what is sought.
Finally, by figure 5e, theappropriate NP is expected by prediction.7 Discussion and conclusionsWe demonstrated how syntactic prediction andthematic roles can interact in our framework, butwe did so with a simple example of prediction:a single noun phrase.
Our framework is, how-ever, able to accomodate more complex interac-tions.
In particular, we want to draw attentionto an example which can not be modelled byother formalisms which are not fully connectedlike PLTAG.
Consider sentences beginning with?The victim/criminal was violently.
.
.
?.
Does thesemantic association between ?victim?
vs. ?crimi-nal?
and ?violently?
change the likelihoods of thesemantic roles that can be assigned to the subjectNP?
Does it make an active or a passive voiceverb more likely after ?violently??
These are thekinds of possible syntactic-semantic interactionsfor which one will need a flexible but robust for-malism such as we have described in this paper:the prediction mechanism allows dependents tojointly affect the expectation of a head even beforethe head has been encountered.
Note that theseinteractions can also go beyond thematic roles.In this paper, we have presented a procedureto augment a treebank-extracted PLTAG lexiconwith semantic annotations based in a flexible neo-Davidsonian theory of events.
Then we haveprovided the way to combine these representa-tions during incremental parsing in a manner fullysynchronized with the existing PLTAG syntacticoperations.
We demonstrated that we can rep-resent thematic role prediction in a case that isknown to be relevant to an on-going stream of psy-cholinguistic research.
Ongoing and future workincludes the development of a joint syntactic-semantic statistical model for PLTAG and experi-mental validation of predictions made by our se-mantic augmentation.
We are also consideringhigher-order semantic issues such as quantifierscope underspecification in the context of our for-malism (Koller et al 2003).63S{?e?
= e}NP?
{Q0x0ARG0(e, x0)}VP{e}Vslid{Slid(e)}NP?
{Q2x2ARG2(e, x2)}NP?
{Q1x1ARG1(e, x1)}S{?e?
= e}NP?
{Q0x0ARG0(e, x0)}VP{e}Vslid{Slid(e)}NP?
{Q1x1ARG1(e, x1)}PPTOktokkNP?
{Q2x2ARG2(e, x2)}(a) Lexicon: ditransitive alternation of slid.NP{Qx?
= Q&?
= x}DT?{?
= Q}Nwoman | man | butter{Woman(x)|Man(x)|Butter(x)}DT{?}theTOtoVPVP*{?e?
= e}PPPonNP?
{ARGM-TEMP(e, x)}Sk{?e?
= e}NPk ?
{Q1x1ARG0(x1)}VPkk{ (e)}NPk{Qx?
= Q&?
= x}DTk ?{?
= Q}Nkk{ (x)}(b) Lexicalized trees and prediction trees.S{?e?
= e}NP?
{?x0ARG0(e, x0)}DT{?
}theNwoman{Woman(x0)}VP{e}Vslid{Slid(e)}NP1{Q2x2ARG2(e, x2)}DT1 ?{?
= Q2}N11{ (x2)}NP?
{Q1x1ARG1(e, x1)}S{?e?
= e}NP?
{?x0ARG0(e, x0)}DT{?
}theNwoman{Woman(x0)}VP{e}Vslid{Slid(e)}NP1{Q1x1ARG1(e, x1)}DT1 ?{?
= Q1}N11{ (x1)}PPTOktokkNP?
{Q2x2ARG2(e, x2)}?e?
= e&?x0ARG0(e, x0)&Woman(x0)&Slid(e) ?e?
= e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)&Q2x2ARG2(e, x2)&?
= Q2& (x2)&Q1x1ARG1(e, x1) &Q1x1ARG1(e, x1)&?
= Q1& (x1)&Q2x2ARG2(e, x2)(c) Parse of ?The woman slid?
with respect to the ditransitive alternation, with the syntactic prediction of an NP.
Two possibilitiesstill remain.
The semantics are identical except for the role of the predicted nominal predicate.
The ?
= e variable assignmentstatement persists through the derivation, representing the possibility that this sentence is embedded in another.S{?e?
= e}NP?
{?x0ARG0(e, x0)}DT{?
}theNwoman{Woman(x0)}VP{e}Vslid{Slid(e)}NP{?x1ARG1(e, x1)}DT{?}theNbutter{Butter(x1)}PPTOktokkNP?
{Q2x2ARG2(e, x2)}?e?
= e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)&?x1ARG1(e, x1)&Butter(x1)&Qx2ARG2(e, x2)(d) Parse of ?The woman slid the butter.
.
.
?.
The arrival of?the butter?
greatly reduces the likelihood of the recipientrole (ARG2) being the one filled at this point, effectivelyabolishing the first parse.S{?e?
= e}NP?
{?x0 .
.
.}DT{?
}theNwoman{Woman(x0)}VP{e}Vslid{Slid(e)}NP{?x1 .
.
.}DT{?
}theNbutter{Butter(x1)}PPTOtoNP2{Q2x2 .
.
.
}DT2 ?{?
= Q2}N22{ (x2)}?e?
= e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)&?x1ARG1(e, x1)&Butter(x1)&Qx2ARG2(e, x2)&?
= Q2& (x2)(e) Parse of ?The woman slid the butter to.
.
.
?.
to is verifiedand the last NP is expanded via prediction.
This gives usthe last predicted predicate in the semantic expression.
Itshares its variable with the ARG2 role, thus thematicallyrestricting its possible verifications.Figure 5: Excerpt of our example parse.64ReferencesGerry Altmann and Yuki Kamide.
1999.
Incremen-tal interpretation at verbs: Restricting the domain ofsubsequent reference.
Cognition, 73(3):247?264.Ann Copestake.
2007.
Semantic composition with (ro-bust) minimal recursion semantics.
In Proc.
of theWorkshop on Deep Linguistic Processing.Vera Demberg and Frank Keller.
2008.
A psycholin-guistically motivated version of tag.
In Proceedingsof the 9th International Workshop on Tree Adjoin-ing Grammars and Related Formalisms.
Tu?bingen,pages 25?32.Vera Demberg and Frank Keller.
2009.
A computa-tional model of prediction in human parsing: Uni-fying locality and surprisal effects.
In Proceedingsof the 29th meeting of the Cognitive Science Society(CogSci-09).Vera Demberg, Frank Keller, and Alexander Koller.2014.
Parsing with psycholinguistically motivatedtree-adjoining grammar.
Computational Linguistics,40(1).Vera Demberg-Winterfors.
2010.
A Broad-CoverageModel of Prediction in Human Sentence Processing.Ph.D.
thesis, University of Edinburgh.Tim Hunter.
2009.
Deriving syntactic properties of ar-guments and adjuncts from neo-davidsonian seman-tics.
In Proc.
of MOL 2009, Los Angeles, CA, USA.Srini Narayanan Daniel Jurafsky.
2002.
A bayesianmodel predicts human parse preference and readingtimes in sentence processing.
In Advances in Neu-ral Information Processing Systems 14: Proceed-ings of the 2001 Neural Information Processing Sys-tems (NIPS) Conference, volume 1, page 59.
TheMIT Press.Yuki Kamide, Gerry Altmann, and Sarah L Haywood.2003a.
The time-course of prediction in incremen-tal sentence processing: Evidence from anticipatoryeye movements.
Journal of Memory and Language,49(1):133?156.Yuki Kamide, Christoph Scheepers, and Gerry TMAltmann.
2003b.
Integration of syntactic and se-mantic information in predictive processing: Cross-linguistic evidence from german and english.
Jour-nal of Psycholinguistic Research, 32(1):37?55.Alexander Koller, Joachim Niehren, and Stefan Thater.2003.
Bridging the gap between underspecifica-tion formalisms: Hole semantics as dominance con-straints.
In Proc.
of EACL 2003, pages 367?374.David M Magerman.
1994.
Natural language pars-ing as statistical pattern recognition.
Ph.D. thesis,Stanford University.Jeff Mitchell, Mirella Lapata, Vera Demberg, andFrank Keller.
2010.
Syntactic and semantic factorsin processing difficulty: An integrated measure.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 196?206.
Association for Computational Linguistics.Ulrike Pado?, Matthew W Crocker, and Frank Keller.2009.
A probabilistic model of semantic plausi-bility in sentence processing.
Cognitive Science,33(5):794?838.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2003.The proposition bank: An annotated corpus of se-mantic roles.
Computational Linguistics, 31(1):71?106.T.
Parsons.
1990.
Events in the semantics of English.MIT Press, Cambridge, MA, USA.Philip Resnik.
1992.
Left-corner parsing and psycho-logical plausibility.
In In The Proceedings of the fif-teenth International Conference on ComputationalLinguistics, COLING-92, pages 191?197.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational linguistics,27(2):249?276.Asad Sayeed and Vera Demberg.
2012.
Incremen-tal neo-davidsonian semantic construction for tag.In 11th Workshop on Tree-Adjoining Grammars andRelated Formalisms (TAG+11).Mark Steedman.
2000.
The syntactic process.
MITPress.Henry S. Thompson, Mike Dixon, and John Lamping.1991.
Compose-reduce parsing.
In Proceedings ofthe 29th annual meeting on Association for Compu-tational Linguistics, pages 87?97, Berkeley, Califor-nia.David Vadas and James Curran.
2007.
Adding nounphrase structure to the Penn Treebank.
In Proceed-ings of the 45th Annual Meeting of the Associa-tion of Computational Linguistics, pages 240?247,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Fei Xia, Martha Palmer, and Aravind Joshi.
2000.
Auniform method of grammar extraction and its appli-cations.
In Proceedings of the Joint SIGDAT Con-ference on Empirical Methods in Natural LanguageProcessing and Very Large Corpora, pages 53?62.65
