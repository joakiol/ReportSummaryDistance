Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 214?224,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAspect Level Sentiment Classification with Deep Memory NetworkDuyu Tang, Bing Qin?, Ting LiuHarbin Institute of Technology, Harbin, China{dytang, qinb, tliu}@ir.hit.edu.cnAbstractWe introduce a deep memory network foraspect level sentiment classification.
Un-like feature-based SVM and sequential neuralmodels such as LSTM, this approach explic-itly captures the importance of each contextword when inferring the sentiment polarity ofan aspect.
Such importance degree and tex-t representation are calculated with multiplecomputational layers, each of which is a neu-ral attention model over an external memory.Experiments on laptop and restaurant datasetsdemonstrate that our approach performs com-parable to state-of-art feature based SVM sys-tem, and substantially better than LSTM andattention-based LSTM architectures.
On bothdatasets we show that multiple computationallayers could improve the performance.
More-over, our approach is also fast.
The deep mem-ory network with 9 layers is 15 times fasterthan LSTM with a CPU implementation.1 IntroductionAspect level sentiment classification is a fundamen-tal task in the field of sentiment analysis (Pang andLee, 2008; Liu, 2012; Pontiki et al, 2014).
Givena sentence and an aspect occurring in the sentence,this task aims at inferring the sentiment polarity (e.g.positive, negative, neutral) of the aspect.
For ex-ample, in sentence ?great food but the service wasdreadful!
?, the sentiment polarity of aspect ?food?is positive while the polarity of aspect ?service?
is?
Corresponding author.negative.
Researchers typically use machine learn-ing algorithms and build sentiment classifier in a su-pervised manner.
Representative approaches in liter-ature include feature based Support Vector Machine(Kiritchenko et al, 2014; Wagner et al, 2014) andneural network models (Dong et al, 2014; Lakkara-ju et al, 2014; Vo and Zhang, 2015; Nguyen andShirai, 2015; Tang et al, 2015a).
Neural models areof growing interest for their capacity to learn textrepresentation from data without careful engineer-ing of features, and to capture semantic relations be-tween aspect and context words in a more scalableway than feature based SVM.Despite these advantages, conventional neuralmodels like long short-term memory (LSTM) (Tanget al, 2015a) capture context information in an im-plicit way, and are incapable of explicitly exhibitingimportant context clues of an aspect.
We believe thatonly some subset of context words are needed to in-fer the sentiment towards an aspect.
For example,in sentence ?great food but the service was dread-ful!
?, ?dreadful?
is an important clue for the aspect?service?
but ?great?
is not needed.
Standard LST-M works in a sequential way and manipulates eachcontext word with the same operation, so that it can-not explicitly reveal the importance of each contextword.
A desirable solution should be capable of ex-plicitly capturing the importance of context wordsand using that information to build up features forthe sentence after given an aspect word.
Further-more, a human asked to do this task will selectivelyfocus on parts of the contexts, and acquire informa-tion where it is needed to build up an internal repre-sentation towards an aspect in his/her mind.214In pursuit of this goal, we develop deep memo-ry network for aspect level sentiment classification,which is inspired by the recent success of compu-tational models with attention mechanism and ex-plicit memory (Graves et al, 2014; Bahdanau et al,2015; Sukhbaatar et al, 2015).
Our approach isdata-driven, computationally efficient and does notrely on syntactic parser or sentiment lexicon.
Theapproach consists of multiple computational layerswith shared parameters.
Each layer is a content- andlocation- based attention model, which first learn-s the importance/weight of each context word andthen utilizes this information to calculate continu-ous text representation.
The text representation inthe last layer is regarded as the feature for sentimentclassification.
As every component is differentiable,the entire model could be efficiently trained end-to-end with gradient descent, where the loss function isthe cross-entropy error of sentiment classification.We apply the proposed approach to laptop andrestaurant datasets from SemEval 2014 (Pontiki etal., 2014).
Experimental results show that our ap-proach performs comparable to a top system usingfeature-based SVM (Kiritchenko et al, 2014).
Onboth datasets, our approach outperforms both LST-M and attention-based LSTM models (Tang et al,2015a) in terms of classification accuracy and run-ning speed.
Lastly, we show that using multiplecomputational layers over external memory couldachieve improved performance.2 Background: Memory NetworkOur approach is inspired by the recent success ofmemory network in question answering (Weston etal., 2014; Sukhbaatar et al, 2015).
We describe thebackground on memory network in this part.Memory network is a general machine learningframework introduced by Weston et al (2014).
It-s central idea is inference with a long-term memo-ry component, which could be read, written to, andjointly learned with the goal of using it for predic-tion.
Formally, a memory network consists of amemory m and four components I , G, O and R,where m is an array of objects such as an array ofvectors.
Among these four components, I convert-s input to internal feature representation, G updatesold memories with new input, O generates an out-put representation given a new input and the currentmemory state, R outputs a response based on theoutput representation.Let us take question answering as an example toexplain the work flow of memory network.
Givena list of sentences and a question, the task aims tofind evidences from these sentences and generate ananswer, e.g.
a word.
During inference, I componentreads one sentence si at a time and encodes it into avector representation.
Then G component updates apiece of memory mi based on current sentence rep-resentation.
After all sentences are processed, weget a memory matrix m which stores the semanticsof these sentences, each row representing a sentence.Given a question q, memory network encodes it intovector representation eq, and thenO component useseq to select question related evidences from memo-ry m and generates an output vector o.
Finally, Rcomponent takes o as the input and outputs the finalresponse.
It is worth noting that O component couldconsist of one or more computational layers (hop-s).
The intuition of utilizing multiple hops is thatmore abstractive evidences could be found based onpreviously extracted evidences.
Sukhbaatar et al(2015) demonstrate that multiple hops could uncov-er more abstractive evidences than single hop, andcould yield improved results on question answeringand language modeling.3 Deep Memory Network for Aspect LevelSentiment ClassificationIn this section, we describe the deep memory net-work approach for aspect level sentiment classifica-tion.
We first give the task definition.
Afterwards,we describe an overview of the approach before p-resenting the content- and location- based attentionmodels in each computational layer.
Lastly, we de-scribe the use of this approach for aspect level senti-ment classification.3.1 Task Definition and NotationGiven a sentence s = {w1, w2, ..., wi, ...wn} con-sisting of n words and an aspect word wi 1 occur-ring in sentence s, aspect level sentiment classifica-tion aims at determining the sentiment polarity of1In practice, an aspect might be a multi word expressionsuch as ?battery life?.
For simplicity we still consider aspectas a single word in this definition.215sentence s towards the aspect wi.
For example, thesentiment polarity of sentence ?great food but theservice was dreadful!?
towards aspect ?food?
is pos-itive, while the polarity towards aspect ?service?
isnegative.
When dealing with a text corpus, we mapeach word into a low dimensional, continuous andreal-valued vector, also known as word embedding(Mikolov et al, 2013; Pennington et al, 2014).
Allthe word vectors are stacked in a word embeddingmatrix L ?
Rd?|V |, where d is the dimension ofword vector and |V | is vocabulary size.
The wordembedding of wi is notated as ei ?
Rd?1, which is acolumn in the embedding matrix L.3.2 An Overview of the ApproachWe present an overview of the deep memory net-work for aspect level sentiment classification.Given a sentence s = {w1, w2, ..., wi, ...wn} andthe aspect word wi, we map each word into its em-bedding vector.
These word vectors are separatedinto two parts, aspect representation and context rep-resentation.
If aspect is a single word like ?food?
or?service?, aspect representation is the embedding ofaspect word.
For the case where aspect is multi wordexpression like ?battery life?, aspect representationis an average of its constituting word vectors (Sun etal., 2015).
To simplify the interpretation, we consid-er aspect as a single word wi.
Context word vectors{e1, e2 ... ei?1, ei+1 ... en} are stacked and regardedas the external memory m ?
Rd?
(n?1), where n isthe sentence length.An illustration of our approach is given in Figure1, which is inspired by the use of memory networkin question answering (Sukhbaatar et al, 2015).
Ourapproach consists of multiple computational layers(hops), each of which contains an attention layer anda linear layer.
In the first computational layer (hop1), we regard aspect vector as the input to adaptivelyselect important evidences from memory m throughattention layer.
The output of attention layer and thelinear transformation of aspect vector2 are summedand the result is considered as the input of next layer(hop 2).
In a similar way, we stack multiple hop-s and run these steps multiple times, so that moreabstractive evidences could be selected from the ex-2In preliminary experiments, we tried directly using aspectvector without a linear transformation, and found that adding alinear layer works slightly better.LinearAttention?wiLinearAttention?LinearAttention?aspect wordhop 1hop 2hop 3?1 , ?2 ?
??
?1 , ??
, ?
?+1 ?
??
?1 , ?
?context words context wordssentence:word embeddingsoftmax?1?2 ???3?
?1 ????????1LinearTanhLinear?
?2 ????????2LinearTanhLinear???
?????????LinearTanhLinear??softmax?
?Figure 1: An illustration of our deep memory network withthree computational layers (hops) for aspect level sentimen-t classification.ternal memory m. The output vector in last hop isconsidered as the representation of sentence with re-gard to the aspect, and is further used as the featurefor aspect level sentiment classification.It is helpful to note that the parameters of attentionand linear layers are shared in different hops.
There-fore, the model with one layer and the model withnine layers have the same number of parameters.3.3 Content AttentionWe describe our attention model in this part.
Thebasic idea of attention mechanism is that it assign-s a weight/importance to each lower position whencomputing an upper level representation (Bahdanauet al, 2015).
In this work, we use attention modelto compute the representation of a sentence with re-gard to an aspect.
The intuition is that context wordsdo not contribute equally to the semantic meaning ofa sentence.
Furthermore, the importance of a wordshould be different if we focus on different aspect.Let us again take the example of ?great food but theservice was dreadful!?.
The context word ?great?is more important than ?dreadful?
for aspect ?food?.On the contrary, ?dreadful?
is more important than?great?
for aspect ?service?.Taking an external memory m ?
Rd?k and anaspect vector vaspect ?
Rd?1 as input, the attentionmodel outputs a continuous vector vec ?
Rd?1.
Theoutput vector is computed as a weighted sum of each216piece of memory in m, namelyvec =k?i=1?imi (1)where k is the memory size, ?i ?
[0, 1] is the weightof mi and ?i ?i = 1.
We implement a neuralnetwork based attention model.
For each piece ofmemory mi, we use a feed forward neural networkto compute its semantic relatedness with the aspect.The scoring function is calculated as follows, whereWatt ?
R1?2d and batt ?
R1?1.gi = tanh(Watt[mi; vaspect] + batt) (2)After obtaining {g1, g2, ... gk}, we feed them to asoftmax function to calculate the final importancescores {?1, ?2, ...
?k}.
?i =exp(gi)?kj=1 exp(gj)(3)We believe that such an attention model has twoadvantages.
One advantage is that this model couldadaptively assign an importance score to each pieceof memory mi according to its semantic relatednesswith the aspect.
Another advantage is that this at-tention model is differentiable, so that it could beeasily trained together with other components in anend-to-end fashion.3.4 Location AttentionWe have described our neural attention frameworkand a content-based model in previous subsection.However, the model mentioned above ignores the lo-cation information between context word and aspec-t.
Such location information is helpful for an atten-tion model because intuitively a context word closerto the aspect should be more important than a fartherone.
In this work, we define the location of a contextword as its absolute distance with the aspect in theoriginal sentence sequence3.
On this basis, we studyfour strategies to encode the location information inthe attention model.
The details are described below.3The location of a context word could also be measured byits distance to the aspect along a syntactic path.
We leave thisas a future work as we prefer to developing a purely data-drivenapproach without using external parsing results.?
Model 1.
Following Sukhbaatar et al (2015),we calculate the memory vector mi withmi = ei  vi (4)where  means element-wise multiplication andvi ?
Rd?1 is a location vector for word wi.
Everyelement in vi is calculated as follows,vki = (1?
li/n)?
(k/d)(1?
2?
li/n) (5)where n is sentence length, k is the hop number andli is the location of wi.?
Model 2.
This is a simplified version of Model1, using the same location vector vi for wi in differ-ent hops.
Location vector vi is calculated as follows.vi = 1?
li/n (6)?
Model 3.
We regard location vector vi as a pa-rameter and compute a piece of memory with vectoraddition, namelymi = ei + vi (7)All the position vectors are stacked in a positionembedding matrix, which is jointly learned with gra-dient descent.?
Model 4.
Location vectors are also regarded asparameters.
Different from Model 3, location rep-resentations are regarded as neural gates to controlhow many percent of word semantics is written intothe memory.
We feed location vector vi to a sigmoidfunction ?, and calculatemi with element-wise mul-tiplication:mi = ei  ?
(vi) (8)3.5 The Need for Multiple HopsIt is widely accepted that computational models thatare composed of multiple processing layers have theability to learn representations of data with multiplelevels of abstraction (LeCun et al, 2015).
In thiswork, the attention layer in one layer is essentiallya weighted average compositional function, whichis not powerful enough to handle the sophisticatedcomputationality like negation, intensification andcontrary in language.
Multiple computational lay-ers allow the deep memory network to learn repre-sentations of text with multiple levels of abstraction.Each layer/hop retrieves important context words,217and transforms the representation at previous levelinto a representation at a higher, slightly more ab-stract level.
With the composition of enough suchtransformations, very complex functions of sentencerepresentation towards an aspect can be learned.3.6 Aspect Level Sentiment ClassificationWe regard the output vector in last hop as the fea-ture, and feed it to a softmax layer for aspect levelsentiment classification.
The model is trained in asupervised manner by minimizing the cross entropyerror of sentiment classification, whose loss func-tion is given below, where T means all training in-stances, C is the collection of sentiment categories,(s, a) means a sentence-aspect pair.loss = ??
(s,a)?T?c?CP gc (s, a) ?
log(Pc(s, a)) (9)Pc(s, a) is the probability of predicting (s, a) as cat-egory c produced by our system.
P gc (s, a) is 1 or0, indicating whether the correct answer is c. Weuse back propagation to calculate the gradients ofall the parameters, and update them with stochasticgradient descent.
We clamp the word embeddingswith 300-dimensional Glove vectors (Pennington etal., 2014), which is trained from web data and thevocabulary size is 1.9M4.
We randomize other pa-rameters with uniform distribution U(?0.01, 0.01),and set the learning rate as 0.01.4 ExperimentWe describe experimental settings and report empir-ical results in this section.4.1 Experimental SettingWe conduct experiments on two datasets from Se-mEval 2014 (Pontiki et al, 2014), one from laptopdomain and another from restaurant domain.
Statis-tics of the datasets are given in Table 1.
It is worthnoting that the original dataset contains the fourthcategory - conflict, which means that a sentence ex-presses both positive and negative opinion towardsan aspect.
We remove conflict category as the num-ber of instances is very tiny, incorporating which4Available at: http://nlp.stanford.edu/projects/glove/.will make the dataset extremely unbalanced.
Evalu-ation metric is classification accuracy.Dataset Pos.
Neg.
Neu.Laptop-Train 994 870 464Laptop-Test 341 128 169Restaurant-Train 2164 807 637Restaurant-Test 728 196 196Table 1: Statistics of the datasets.4.2 Comparison to Other MethodsWe compare with the following baseline methods onboth datasets.
(1) Majority is a basic baseline method, whichassigns the majority sentiment label in training setto each instance in the test set.
(2) Feature-based SVM performs state-of-the-arton aspect level sentiment classification.
We comparewith a top system using ngram features, parse fea-tures and lexicon features (Kiritchenko et al, 2014).
(3) We compare with three LSTM models (Tanget al, 2015a)).
In LSTM, a LSTM based recurrentmodel is applied from the start to the end of a sen-tence, and the last hidden vector is used as the sen-tence representation.
TDLSTM extends LSTM bytaking into account of the aspect, and uses two LST-M networks, a forward one and a backward one, to-wards the aspect.
TDLSTM+ATT extends TDLST-M by incorporating an attention mechanism (Bah-danau et al, 2015) over the hidden vectors.
We usethe same Glove word vectors for fair comparison.
(4) We also implement ContextAVG, a simplisticversion of our approach.
Context word vectors areaveraged and the result is added to the aspect vector.The output is fed to a softmax function.Experimental results are given in Table 2.
Ourapproach using only content attention is abbreviat-ed to MemNet (k), where k is the number of hops.We can find that feature-based SVM is an extremelystrong performer and substantially outperforms oth-er baseline methods, which demonstrates the impor-tance of a powerful feature representation for aspectlevel sentiment classification.
Among three recur-rent models, TDLSTM performs better than LSTM,which indicates that taking into account of the as-pect information is helpful.
This is reasonable as thesentiment polarity of a sentence towards different as-218Laptop RestaurantMajority 53.45 65.00Feature+SVM 72.10 80.89LSTM 66.45 74.28TDLSTM 68.13 75.63TDLSTM+ATT 66.24 74.31ContextAVG 61.22 71.33MemNet (1) 67.66 76.10MemNet (2) 71.14 78.61MemNet (3) 71.74 79.06MemNet (4) 72.21 79.87MemNet (5) 71.89 80.14MemNet (6) 72.21 80.05MemNet (7) 72.37 80.32MemNet (8) 72.05 80.14MemNet (9) 72.21 80.95Table 2: Classification accuracy of different methods on laptopand restaurant datasets.
Best scores in each group are in bold.pects (e.g.
?food?
and ?service?)
might be different.It is somewhat disappointing that incorporating at-tention model over TDLSTM does not bring any im-provement.
We consider that each hidden vector ofTDLSTM encodes the semantics of word sequenceuntil the current position.
Therefore, the model ofTDLSTM+ATT actually selects such mixed seman-tics of word sequence, which is weird and not an in-tuitive way to selectively focus on parts of contexts.Different from TDLSTM+ATT, the proposed mem-ory network approach removes the recurrent calcula-tor over word sequence and directly apply attentionmechanism on context word representations.We can also find that the performance of Contex-tAVG is very poor, which means that assigning thesame weight/importance to all the context words isnot an effective way.
Among all our models fromsingle hop to nine hops, we can observe that usingmore computational layers could generally lead tobetter performance, especially when the number ofhops is less than six.
The best performances areachieved when the model contains seven and ninehops, respectively.
On both datasets, the proposedapproach could obtain comparable accuracy com-pared to the state-of-art feature-based SVM system.4.3 Runtime AnalysisWe study the runtime of recurrent neural models andthe proposed deep memory network approach withdifferent hops.
We implement all these approachesbased on the same neural network infrastructure, usethe same 300-dimensional Glove word vectors, andrun them on the same CPU server.Method Time costLSTM 417TDLSTM 490TDLSTM + ATT 520MemNet (1) 3MemNet (2) 7MemNet (3) 9MemNet (4) 15MemNet (5) 20MemNet (6) 24MemNet (7) 26MemNet (8) 27MemNet (9) 29Table 3: Runtime (seconds) of each training epoch on therestaurant dataset.The training time of each iteration on the restau-rant dataset is given in Table 3.
We can find thatLSTM based recurrent models are indeed compu-tationally expensive, which is caused by the com-plex operations in each LSTM unit along the wordsequence.
Instead, the memory network approachis simpler and evidently faster because it does notneed recurrent calculators of sequence length.
Ourapproach with nine hops is almost 15 times fasterthan the basic LSTM model.4.4 Effects of Location AttentionAs described in Section 3.4, we explore four strate-gies to integrate location information into the atten-tion model.
We incorporate each of them separate-ly into the basic content-based attention model.
Itis helpful to restate that the difference between fourlocation-based attention models lies in the usage oflocation vectors for context words.
In Model 1 andModel 2, the values of location vectors are fixed andcalculated in a heuristic way.
In Model 3 and Model4, location vectors are also regarded as the parame-ters and jointly learned along with other parametersin the deep memory network.219(a) Aspect: service, Answer: -1, Prediction: -1hop 1 hop 2 hop 3 hop 4 hop 5great 0.20 0.15 0.14 0.13 0.23food 0.11 0.07 0.08 0.12 0.06but 0.20 0.10 0.10 0.12 0.13the 0.03 0.07 0.08 0.12 0.06was 0.08 0.07 0.08 0.12 0.06dreadful 0.20 0.45 0.45 0.28 0.40!
0.19 0.08 0.08 0.12 0.07(b) Aspect: food, Answer: +1, Prediction: -1hop 1 hop 2 hop 3 hop 4 hop 5great 0.22 0.12 0.14 0.12 0.20but 0.21 0.11 0.10 0.11 0.12the 0.03 0.11 0.08 0.11 0.06service 0.11 0.11 0.08 0.11 0.06was 0.04 0.11 0.08 0.11 0.06dreadful 0.22 0.32 0.45 0.32 0.43!
0.16 0.11 0.08 0.11 0.07Table 4: Examples of attention weights in different hops for aspect level sentiment classification.
The model only uses contentattention.
The hop columns show the weights of context words in each hop, indicated by values and gray color.
This example showsthe results of sentence ?great food but the service was dreadful!?
with ?food?
and ?service?
as the aspects.
(a) Aspect: service, Answer: -1, Prediction: -1hop 1 hop 2 hop 3 hop 4 hop 5great 0.08 0.10 0.10 0.09 0.09food 0.08 0.07 0.07 0.07 0.07but 0.10 0.15 0.16 0.13 0.11the 0.07 0.07 0.07 0.07 0.07was 0.07 0.07 0.07 0.07 0.07dreadful 0.52 0.48 0.48 0.50 0.52!
0.07 0.07 0.07 0.07 0.07(b) Aspect: food, Answer: +1, Prediction: +1hop 1 hop 2 hop 3 hop 4 hop 5great 0.31 0.26 0.32 0.28 0.32but 0.14 0.18 0.15 0.18 0.15the 0.08 0.05 0.08 0.05 0.07service 0.09 0.09 0.09 0.08 0.09was 0.09 0.08 0.09 0.08 0.08dreadful 0.18 0.21 0.18 0.22 0.19!
0.11 0.12 0.10 0.11 0.10Table 5: Examples of attention weights in different hops for aspect level sentiment classification.
The model also takes into accountof the location information (Model 2).
This example is as same as the one we use in Table 4.1 2 3 4 5 6 7 8 90.720.730.740.750.760.770.780.790.80.81Number of hopsAccuracyContent+ Location 1+ Location 2+ Location 3+ Location 4Figure 2: Classification accuracy of different attention modelson the restaurant dataset.Figure 2 shows the classification accuracy of eachattention model on the restaurant dataset.
We canfind that using multiple computational layers couldconsistently improve the classification accuracy inall these models.
All these models perform compa-rably when the number of hops is larger than five.Among these four location-based models, we pre-fer Model 2 as it is intuitive and has less compu-tation cost without loss of accuracy.
We also findthat Model 4 is very sensitive to the choice of neuralgate.
Its classification accuracy decreases by almost5 percentage when the sigmoid operation over loca-tion vector is removed.4.5 Visualize Attention ModelsWe visualize the attention weight of each contextword to get a better understanding of the deep mem-ory network approach.
The results of context-basedmodel and location-based model (Model 2) are giv-en in Table 4 and Table 5, respectively.From Table 4(a), we can find that in the first hopthe context words ?great?, ?but?
and ?dreadful?
con-tribute equally to the aspect ?service?.
While afterthe second hop, the weight of ?dreadful?
increasesand finally the model correctly predict the polaritytowards ?service?
as negative.
This case shows theeffects of multiple hops.
However, in Table 4(b),the content-based model also gives a larger weightto ?dreadful?
when the target we focus on is ?food?.As a result, the model incorrectly predicts the po-larity towards ?food?
as negative.
This phenomenonmight be caused by the neglect of location informa-tion.
From Table 5(b), we can find that the weightof ?great?
is increased when the location of contextword is considered.
Accordingly, Model 2 predict-220s the correct sentiment label towards ?food?.
Webelieve that location-enhanced model captures bothcontent and location information.
For instance, inTable 5(a) the closest context words of the aspect?service?
are ?the?
and ?was?, while ?dreadful?
hasthe largest weight.4.6 Error AnalysisWe carry out an error analysis of our location en-hanced model (Model 2) on the restaurant dataset,and find that most of the errors could be sum-marized as follows.
The first factor is non-compositional sentiment expression.
This modelregards single context word as the basic computa-tional unit and cannot handle this situation.
Anexample is ?dessert was also to die for!
?, wherethe aspect is underlined.
The sentiment expres-sion is ?die for?, whose meaning could not becomposed from its constituents ?die?
and ?for?.The second factor is complex aspect expressionconsisting of many words, such as ?ask for theround corner table next to the large window.?
Thismodel represents an aspect expression by averag-ing its constituting word vectors, which could notwell handle this situation.
The third factor is senti-mental relation between context words such as nega-tion, comparison and condition.
An example is ?butdinner here is never disappointing, even if the pricesare a bit over the top?.
We believe that this is causedby the weakness of weighted average composition-al function in each hop.
There are also cases whencomparative opinions are expressed such as ?i ?vehad better japanese food at a mall food court?.5 Related WorkThis work is connected to three research areas in nat-ural language processing.
We briefly describe relat-ed studies in each area.5.1 Aspect Level Sentiment ClassificationAspect level sentiment classification is a fine-grained classification task in sentiment analysis,which aims at identifying the sentiment polarity ofa sentence expressed towards an aspect (Pontiki etal., 2014).
Most existing works use machine learn-ing algorithms, and build sentiment classifier fromsentences with manually annotated polarity label-s. One of the most successful approaches in liter-ature is feature based SVM.
Experts could designeffective feature templates and make use of externalresources like parser and sentiment lexicons (Kir-itchenko et al, 2014; Wagner et al, 2014).
In re-cent years, neural network approaches (Dong et al,2014; Lakkaraju et al, 2014; Nguyen and Shirai,2015; Tang et al, 2015a) are of growing attention fortheir capacity to learn powerful text representationfrom data.
However, these neural models (e.g.
L-STM) are computationally expensive, and could notexplicitly reveal the importance of context evidenceswith regard to an aspect.
Instead, we develop simpleand fast approach that explicitly encodes the con-text importance towards a given aspect.
It is worthnoting that the task we focus on differs from fine-grained opinion extraction, which assigns each worda tag (e.g.
B,I,O) to indicate whether it is an aspec-t/sentiment word (Choi and Cardie, 2010; Irsoy andCardie, 2014; Liu et al, 2015).
The aspect word inthis work is given as a part of the input.5.2 Compositionality in Vector SpaceIn NLP community, compositionality means thatthe meaning of a composed expression (e.g.
aphrase/sentence/document) comes from the mean-ings of its constituents (Frege, 1892).
Mitchell andLapata (2010) exploits a variety of addition andmultiplication functions to calculate phrase vector.Yessenalina and Cardie (2011) use matrix multipli-cation as compositional function to compute vec-tors for longer phrases.
To compute sentence rep-resentation, researchers develop denoising autoen-coder (Glorot et al, 2011), convolutional neural net-work (Kalchbrenner et al, 2014; Kim, 2014; Yinand Schu?tze, 2015), sequence based recurrent neu-ral models (Sutskever et al, 2014; Kiros et al, 2015;Li et al, 2015b) and tree-structured neural network-s (Socher et al, 2013; Tai et al, 2015; Zhu et al,2015).
Several recent studies calculate continuousrepresentation for documents with neural networks(Le and Mikolov, 2014; Bhatia et al, 2015; Li et al,2015a; Tang et al, 2015b; Yang et al, 2016).5.3 Attention and Memory NetworksRecently, there is a resurgence in computationalmodels with attention mechanism and explicit mem-ory to learn representations of texts (Graves et al,2014; Weston et al, 2014; Sukhbaatar et al, 2015;221Bahdanau et al, 2015).
In this line of research,memory is encoded as a continuous representationand operations on memory (e.g.
reading and writ-ing) are typically implemented with neural network-s.
Attention mechanism could be viewed as a com-positional function, where lower level representa-tions are regarded as the memory, and the func-tion is to choose ?where to look?
by assigninga weight/importance to each lower position whencomputing an upper level representation.
Such at-tention based approaches have achieved promisingperformances on a variety of NLP tasks (Luong etal., 2015; Kumar et al, 2015; Rush et al, 2015).6 ConclusionWe develop deep memory networks that capture im-portances of context words for aspect level senti-ment classification.
Compared with recurrent neu-ral models like LSTM, this approach is simplerand faster.
Empirical results on two datasets veri-fy that the proposed approach performs comparableto state-of-the-art feature based SVM system, andsubstantively better than LSTM architectures.
Weimplement different attention strategies and showthat leveraging both content and location informa-tion could learn better context weight and text rep-resentation.
We also demonstrate that using multi-ple computational layers in memory network couldobtain improved performance.
Our potential futureplans are incorporating sentence structure like pars-ing results into the deep memory network.AcknowledgmentsWe would especially want to thank Xiaodan Zhu forrunning their system on our setup.
We greatly thankYaming Sun for tremendously helpful discussions.We also thank the anonymous reviewers for theirvaluable comments.
This work was supported by theNational High Technology Development 863 Pro-gram of China (No.
2015AA015407), National Nat-ural Science Foundation of China (No.
61632011and No.61273321).ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
International Confer-ence on Learning Representations (ICLR).Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein.2015.
Better document-level sentiment analysis fromrst discourse parsing.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 2212?2218.Yejin Choi and Claire Cardie.
2010.
Hierarchical se-quential learning for extracting opinions and their at-tributes.
In Proceedings of the ACL 2010 ConferenceShort Papers, pages 269?274.
Association for Compu-tational Linguistics.Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, MingZhou, and Ke Xu.
2014.
Adaptive recursive neuralnetwork for target-dependent twitter sentiment clas-sification.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics,pages 49?54.Gottlob Frege.
1892.
On sense and reference.
Ludlow(1997), pages 563?584.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Proceed-ings of the 28th International Conference on MachineLearning (ICML-11), pages 513?520.Alex Graves, Greg Wayne, and Ivo Danihelka.
2014.Neural turing machines.
arXiv preprint arX-iv:1410.5401.Ozan Irsoy and Claire Cardie.
2014.
Opinion miningwith deep recurrent neural networks.
In Proceedingsof the 2014 Conference on Empirical Methods in Nat-ural Language Processing, pages 720?728.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics, pages 655?665.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1746?1751.Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, andSaif Mohammad.
2014.
Nrc-canada-2014: Detectingaspects and sentiment in customer reviews.
In Pro-ceedings of the 8th International Workshop on Seman-tic Evaluation (SemEval 2014), pages 437?442.Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,Richard Zemel, Raquel Urtasun, Antonio Torralba,and Sanja Fidler.
2015.
Skip-thought vectors.
InAdvances in Neural Information Processing Systems,pages 3276?3284.Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury,Robert English, Brian Pierce, Peter Ondruska, Ishaan222Gulrajani, and Richard Socher.
2015.
Ask me any-thing: Dynamic memory networks for natural lan-guage processing.
arXiv preprint arXiv:1506.07285.Himabindu Lakkaraju, Richard Socher, and Chris Man-ning.
2014.
Aspect specific sentiment analysis usinghierarchical deep learning.
In NIPS Workshop on DeepLearning and Representation Learning.Quoc V. Le and Tomas Mikolov.
2014.
Distributed rep-resentations of sentences and documents.
In Proceed-ings of The 31nd International Conference on MachineLearning, pages 1188?1196.Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.2015.
Deep learning.
Nature, 521(7553):436?444.Jiwei Li, Thang Luong, and Dan Jurafsky.
2015a.
A hi-erarchical neural autoencoder for paragraphs and doc-uments.
In Proceedings of the 53rd Annual Meeting ofthe Association for Computational Linguistics, pages1106?1115.Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard Hov-y.
2015b.
When are tree structures necessary fordeep learning of representations?
In Proceedings ofthe 2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 2304?2314.Pengfei Liu, Shafiq Joty, and Helen Meng.
2015.
Fine-grained opinion mining with recurrent neural network-s and word embeddings.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 1433?1443.Bing Liu.
2012.
Sentiment analysis and opinion mining.Synthesis Lectures on Human Language Technologies,5(1):1?167.Thang Luong, Hieu Pham, and Christopher D. Manning.2015.
Effective approaches to attention-based neuralmachine translation.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1412?1421.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corra-do, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems,pages 3111?3119.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Thien Hai Nguyen and Kiyoaki Shirai.
2015.
Phrasernn:Phrase recursive neural network for aspect-based sen-timent analysis.
In Proceedings of the 2015 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 2509?2514.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for word rep-resentation.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing,pages 1532?1543.Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-ris Papageorgiou, Ion Androutsopoulos, and SureshManandhar.
2014.
Semeval-2014 task 4: Aspec-t based sentiment analysis.
In Proceedings of the 8thInternational Workshop on Semantic Evaluation (Se-mEval 2014), pages 27?35.Alexander M. Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 379?389.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1631?1642.Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, andRob Fergus.
2015.
End-to-end memory networks.
InAdvances in Neural Information Processing Systems,pages 2431?2439.Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, ZhenzhouJi, and Xiaolong Wang.
2015.
Modeling mention,context and entity with neural networks for entity dis-ambiguation.
Proceedings of the Twenty-Fourth Inter-national Joint Conference on Artificial Intelligence (I-JCAI 2015), pages 1333?1339.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural networks.In Advances in neural information processing systems,pages 3104?3112.Kai Sheng Tai, Richard Socher, and Christopher D. Man-ning.
2015.
Improved semantic representations fromtree-structured long short-term memory networks.
InProceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics, pages 1556?1566.Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Li-u.
2015a.
Target-Dependent Sentiment Classificationwith Long Short Term Memory.
ArXiv preprint arX-iv:1512.01100.Duyu Tang, Bing Qin, and Ting Liu.
2015b.
Documentmodeling with gated recurrent neural network for sen-timent classification.
Proceedings of the 2015 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1422?1432.Duy-Tin Vo and Yue Zhang.
2015.
Target-dependenttwitter sentiment classification with rich automatic223features.
In Proceedings of the Twenty-Fourth Inter-national Joint Conference on Artificial Intelligence (I-JCAI 2015), pages 1347?1353.Joachim Wagner, Piyush Arora, Santiago Cortes, UtsabBarman, Dasha Bogdanova, Jennifer Foster, and Lami-a Tounsi.
2014.
Dcu: Aspect-based polarity classifi-cation for semeval task 4.
In Proceedings of the 8thInternational Workshop on Semantic Evaluation (Se-mEval 2014), pages 223?229.Jason Weston, Sumit Chopra, and Antoine Bordes.
2014.Memory networks.
arXiv preprint arXiv:1410.3916.Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, AlexSmola, and Eduard Hovy.
2016.
Hierarchical atten-tion networks for document classification.
In Proceed-ings of the 2016 Conference of the North AmericanChapter of the Association for Computational Linguis-tics.Ainur Yessenalina and Claire Cardie.
2011.
Compo-sitional matrix-space models for sentiment analysis.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages 172?182.Wenpeng Yin and Hinrich Schu?tze.
2015.
Multichan-nel variable-size convolution for sentence classifica-tion.
In Proceedings of the Nineteenth Conferenceon Computational Natural Language Learning, pages204?214.Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015.Long short-term memory over tree structures.
In Pro-ceedings of The 32nd International Conference on Ma-chine Learning, pages 1604?1612.224
