Learning Discourse Relations with Active Data SelectionTadashi Nomoto*National Inst i tute of Japanese Literature1-16-10 Yutaka ShinagawaTokyo 142-8585 Japannomoto@nij 1. ac.
jpYu j i  MatsumotoNara Inst i tute of Science and Technology8916-5 Takayama Ikoma Nara630-0101 Japanmatsu@is, aist-nara, ac.
jpAbstractThe paper presents a new approach to identi-fying discourse relations, which makes use of aparticular sampling method called committee-based sampling (CBS).
In the committee-basedsampling, multiple learning models are gener-ated to measure the utility of an input examplein classification; if it is judged as not useful,then the example will be ignored.
The methodhas the effect of reducing the amount of datarequired for training.
In the paper, we extendCBS for decision tree classifiers.
With an addi-tional extension called error feedback, it is foundthat the method achieves an increased accuracyas well as a substantial reduction in the amountof data for training classifiers.1 Int roduct ionThe success of corpus-based approaches to dis-course ultimately depends on whether one isable to acquire a large volume of data annotatedfor discourse-level information.
However, to ac-quire merely a few hundred texts annotated fordiscourse information is often impossible due tothe enormity of the haman labor required.This paper presents a novel method for reduc-ing the amount of data for training a decisiontree classifier, while not compromising the accu-racy.
While there has been some work explor-ing the use of machine leaning techniques fordiscourse and dialogue (Marcu, 1997; Samuel etal., 1998), to our knowledge, no computationalresearch on discourse or dialogue so far has ad-dressed the problem of reducing or minimizingthe amount of data for training a learning algo-rithm.
* The work reported here was conducted while the firstauthor was with Advanced Research Lab., Hitachi Ltd,2520 Hatoyama Saitama 350-0395 Japan.A particular method proposed here is builton the committee-based sampling, initially pro-posed for probabilistic lassifiers by Dagan andEngelson (1995), where an example is selectedfrom the corpus according to its utility in im-proving statistics.
We extend the method fordecision tree classifiers using a statistical tech-nique called bootstrapping (Cohen, 1995).
Withan additional extension, which we call error.feedback, it is found that the method achievesan increased accuracy as well as a significantreduction of training data.
The method pro-posed here should be of use in domains otherthan discourse, where a decision tree strategy isfound applicable.2 Tagging a corpus wi th  discourserelat ionsIn tagging a corpus, we adopted Ichikawa(1990)'s scheme for organizing discourse rela-tions (Table 1).
The advantage of Ichikawa(1990)'s cheme is that it directly associates dis-course relations with explicit surface cues (eg.sentential connectives), so it is possible for thecoder to determine a discourse relation by figur-ing a most natural cue that goes with a sentencehe/she is working on.
Another feature is that,unlike Rhetorical Structure Theory (Mann andThompson, 1987), the scheme assumes a dis-course relation to be a local one, which is de-fined strictly over two consecutive sentences.
1We expected that these features would make atagging task less laborious for a human coderthan it would be with RST.
Further, our earlierstudy indicated a very low agreement rate with1This does not mean to say that all of the discourserelations are local.
There could be some relations thatinvolve sentences separated far apart.
However we didnot consider non-local relations, as our preliminary studyfound that they are rarely agreed upon by coders.158Table 1: Ichikawa (1990)'s taxonomy of discourse relations.
The first column indicates major classesand the second subclasses.
The third column lists some examples associated with each subclass.Note that the EXPANDING subclass has no examples in it.
This is because no explicit cue is usedto mark the relationship.LOGICALSEQUENCEELABORATIONCONSEQUENTIALANTITHESISADDITIVECONTRASTINITIATIONAPPOSITIVECOMPLEMENTARYEXPANDINGdakara therefore, shitagatte thusshikashi but, daga butsoshite and, tsuigi-ni nextipp6 in contrast, soretomo rtokorode to change the subject, sonouchi in the meantimetatoeba .for example, y6suruni in other wordsnazenara because, chinamini incidentallyNONERST (~ = 0.43; three coders); especially for acasual coder, RST turned out to be a quite dif-ficult guideline to follow.In Ichikawa (1990), discourse relations are or-ganized into three major classes: the first classincludes logical (or strongly semantic) relation-ships where one sentence is a logical conse-quence or contradiction of another; the secondclass consists of sequential relationships wheretwo semantically independent sentences are jux-taposed; the third class includes elaboration-type relationships where one of the sentencesis semantically subordinate to the other.In constructing a tagged corpus, we askedcoders not to identify abstract discourse rela-tions such as LOGICAL, SEQUENCE and ELAB-ORATION, but to choose from a list of pre-determined connective xpressions.
We ex-pected that the coder would be able to iden-tify a discourse relation with far less effort whenworking with eXplicit cues than when workingwith abstract Concepts of discourse relations.Moreover, since 93% of sentences consideredfor labeling in the corpus did not contain pre-determined relation cues, the annotation taskwas in effect one of guessing a possible connec-tive cue that m'ay go with a sentence.
The ad-vantage of using explicit cues to identify dis-course relations is that even if one has little orno background in linguistics, he or she may beable to assign a discourse relation to a sentenceby just asking him/herself whether the associ-ated cue fits well with the sentence.
In addition,in order to make the usage of cues clear and un-ambiguous, the annotation instruction carrieda set of examples for each of the cues.
Fur-ther, we developed an emacs-based software aidwhich guides the coder to work through a cor-pus and also is capable of prohibiting the coderfrom making moves inconsistent with the tag-ging instruction.As it turned out, however, Ichikawa's cheme,using subclass relation types, did not improveagreement (~ = 0.33, three coders).
So, wemodified the relation taxonomy so that it con-tains just two major classes, SEQUENCE andELABORATION, (LOGICAL relationships beingsubsumed under the SEQUENCE class) and as-sumed that a lexical cue marks a major class towhich it belongs.
The modification successfullyraised the ~ score to 0.70.
Collapsing LOGICALand SEQUENCE classes may be justified by not-ing that both types of relationships have to dowith relating two semantically independent sen-tences, a property not shared by relations of theelaboration type.3 Learning with Active DataSelection3.1 Committee-based SamplingIn the committee-based sampling method (CBS,henceforth) (Dagan and Engelson, 1995; Engel-son and Dagan, 1996), a training example is se-lected from a corpus according to its usefulness;a preferred example is one whose addition tothe training corpus improves the current esti-mate of a model parameter which is relevantto classification and also affects a large propor-tion of examples.
CBS tries to identify such anexample by randomly generating multiple mod-els (committee members) based on posterior dis-159tributions of model parameters and measuringhow much the member models disagree in clas-sifying the example.
The rationale for this is:disagreement among models over the class ofan example would suggest hat the example af-fects some parameters sensitive to classification,and furthermore stimates of affected parame-ters are far from their true values.
Since modelsare generated randomly from posterior distribu-tions of model parameters, their disagreementon an example's class implies a large variancein estimates of parameters, which in turn indi-cates that the statistics of parameters involvedare insufficient and hence its inclusion in thetraining corpus (so as to improve the statisticsof relevant parameters).For each example it encounters, CBS goesthrough the following steps to decide whetherto select the example for labeling.1.
Draw k models (committee members) ran-domly from the probability distributionP(M \] S) of models M given the statisticsS of a training corpus.2.
Classify an input example by each ofthe committee members and measure howmuch they disagree on classification.3.
Make a biased random decision as towhether or not to select the examplefor labeling.
This would make a highlydisagreed-upon example more likely to beselected.As an illustration of how this might work,consider a problem of tagging words withparts of speech, using a Hidden Markov Model(HMM).
A (bigram) HMM tagger is typicallygiven as:nT(Wl .
.
.
Wn) = argmax ~ P(wi I ti)P(ti+l I ti)t~ .
.
.~ ~__~where w l .
.
.wn  is a sequence of input words,and t l .
.
.
tn  is a sequence of tags.
For a sequenceof input words wl .
.
.wn ,  a sequence of corre-sponding tags T(wl .
.
.wn)  is one that maxi-mizes the probability of reaching tn from tl viati (1 < i < n) and generating Wl.
.
.wn alongwith it.
Probabilities P(wi I ti) and P(ti+l I ti)are called model parameters of an HMM tag-ger.
In Dagan and Engelson (1995), P(M I S)is given as the posterior multinomial distribu-tion P(a l  = a l , .
.
.
, an  = an J S), where aiis a model parameter and ai represents oneof the possible values.
P(a l  = a l , .
.
.
, an  =an I S) represents the proportion of the timesthat each parameter oq takes a/, given thestatistics S derived from a corpus.
(Note that~ P(ai = ai I S) = 1.)
For instance, considera task of randomly drawing a word with replace-ment from a corpus consisting of 100 differentwords (wl , .
.
.
,  Wl00).
After 10 trials, you mighthave outcomes like wl = 3, w2 = 1, .
.
.
,  w55 =2,.
.
.
,w71 = 3,. .
.
,w76 = 1,. .
.
,wl00 = 0: i.e.,Wl was drawn three times, w2 was drawn once,w55 was drawn twice, etc.
If you try another 10times, you might get different results.
A multi-nomial distribution tells you how likely you geta particular sequence of word occurrences.
Da-gan and Engelson (1995)'s idea is to assumethe distribution P(a l  = a l , .
.
.
, an  = an I S)as a set of binomial distributions, each corre-sponding to one of its parameters.
An arbitraryHMM model is then constructed by randomlydrawing a value ai from a binomial distribu-tion for a parameter ai, which is approximatedby a normal distribution.
Given k such models(committee members) from the multinomial dis-tribution, we ask each of them to classify aninput example.
We decide whether to selectthe example for labeling based on how muchthe committee members disagree in classifyingthat example.
Dagan and Engelson (1995) in-troduces the notion of vote entropy to quantifydisagreements among members.
Though onecould use the kappa statistic (Siegel and Castel-lan, 1988) or other disagreement measures suchas the a statistic (Krippendorff, 1980) instead ofthe vote entropy, in our implementation f CBS,we decided to use the vote entropy, for the lackof reason to choose one statistic over another.A precise formulation of the vote entropy is asfollows:v(e, e) log V(c, e)V(e)  = - kCHere e is an input example and c denotes aclass.
V(c, e) is the number of votes for c. kis the number of committee members.
A se-lection function is given in probabilistic terms,160based on V(e).gPselect(e) = log k V(e)g here is called the entropy gain and is used todetermine the number of times an example isselected; a grea~ter g would increase the numberof examples elected for tagging.
Engelson andDagan (1996) investigated several plausible ap-proaches to the selection function but were un-able to find significant differences among them.At the beginning of the section, we mentionedsome properties of 'useful' examples.
A usefulexample is one which contributes to reducingvariance in parameter values and also affectsclassification.
By randomly generating multiplemodels and measuring a disagreement amongthem, one would be able to tell whether an ex-ample is useful in the sense above; if there werea large disagreement, then one would know thatthe example is relevant to classification and alsois associated with parameters with a large vari-ance and thus with insufficient statistics.In the following section, we investigate howwe might extend CBS for use in decision treeclassifiers.3.2 Decision Tree ClassifiersSince it is difficult, if not impossible, to expressthe model distribution of decision tree classi-fiers in terms of the multinomial distribution,we turn to the bootstrap sampling method toobtain P(M \[ S).
The bootstrap samplingmethod provides a way for artificially establish-ing a sampling distribution for a statistic, whenthe distribution is not known (Cohen, 1995).For us, a relevant statistic would be the poste-rior probability that a given decision tree mayoccur, given the training corpus.Bootstrap Sampling ProcedureRepeat i = 1. ,.
K times:1.
Draw a bootstrap seudosample S~ of sizeN from S by sampling with replacement asfollows:Repeat N times: select a member of S atrandom ai~d add it to S~.2.
Build a decision tree model M from S~.Add M to Ss.S is a small Set of samples drawn from thetagged corpus.
Repeating the procedure 100times would give 100 decision tree models, eachcorresponding to some S~ derived from the sam-ple set S. Note that the bootstrap rocedureallows a datum in the original sample to be se-lected more than once.Given a sampling distribution of decision treemodels, a committee can be formed by ran-domly selecting k models from Ss.
Of course,there are some other approaches to construct-ing a committee for decision tree classifiers (Di-etterich, 1998).
One such, known as random-ization, is to use a single decision tree and ran-domly choose a path at each attribute test.
Re-peating the process k times for each input ex-ample produces k models.3.2.1 FeaturesIn the following, we describe a set of featuresused to characterize a sentence.
As a conven-tion, we refer to a current sentence as 'B' andthe preceding sentence as 'A'.<LocSen> defines the location of a sentenceby:#s(x)# S ( Last..S entence)'#S(X) '  denotes an ordinal number indi-cating the position of a sentence X in atext, i.e., #S(kth_sentence) = k, (k >_ 0).
'Last_Sentence' refers to the last sentence in atext.
LocSen takes a continuous value between0 and 1.
A text-initial sentence takes 0, and atext-final sentence 1.<LocPar> is defined similarly to DistPar.
Itrecords information on the location of a para-graph in which a sentence X occurs.#Par(X)#Last.Paragraph'#Par(X)' denotes an ordinal number indicat-ing the position of a paragraph containing X.
'#Last_Paragraph' is the position of the lastparagraph in a text, represented by the ordinalnumber.<LocWithinPax> records information on thelocation of a sentence X within a paragraph inwhich it appears.#S(X) - #S(Par_\[nit_Sen)Length(Par(X))161'Par_Init_Sen' refers to the initial sentence of aparagraph in which X occurs, 'Length(Par(X))'denotes the number of sentences that occur inthat paragraph.
LocW:i.thinPar takes continu-ous values ranging from 0 to 1.
A paragraphinitial sentence would have 0 and a paragraphfinal sentence 1.<LenText> the length of a text, measured inJapanese characters.the length of A in Japanese char- <LenSenA>acters.<LenSenB>acters.the length of B in Japanese char-<Sire> encodes the lexical similarity betweenA and B, based on an information-retrievalmeasure known as t f .
idf (Salton and McGill,1983).
2 One important feature here is that wedefined similarity based on (Japanese) charac-ters rather than on words: in practice, we brokeup nominals from relevant sentences into simplealphabetical characters (including graphemes)and used them to measure similarity betweenthe sentences.
(Thus in our setup xi in foot-note 2 corresponds to one character, and notto one whole word.)
We did this to deal withabbreviations and rewordings, which we foundquite frequent in the corpus.<Cue> takes a discrete value 'y' or 'n'.
Thecue feature is intended to exploit surface cuesmost relevant for distinguishing between the SE-QUENCE and ELABORATION relations.
The fea-2For a word j in a sentence Si (j E Si), its weight wijis defined by:N w# = tf~j ?
log ~-df~ is the number of sentences in the text which havean occurrence of a word j. N is the total number ofsentences inthe text.
The tf.idf metric has the propertyof favoring high frequency words with local distribution.For a pair of sentences .,~ = (xl .... ) and Y = (yx,...),where x and y are words, we define the lexical similaritybetween X and Y by:t2 E w(xi)w(y~)S IM( .X ,Y )= t i=x tE Ei=1  i=1where w(xi) represents a t~idf weight assigned to theterm xi.
The measure is known as the Dice coefficient(Salton and McGill, 1983)ture takes 'y' if a sentence contains one or morecues relevant o distinguishing between the tworelation types.
We considered up to 5 wordn-grams found in the training corpus.
Out ofthese, those whose INFOx values are below aparticular threshold are included in the set ofcues .
3 And if a sentence contains one of thecues in the set, it is marked 'y', and 'n' other-wise.
The cutoff is determined in such a wayas to minimize INFOcue(T), where T is a setof sentences (represented with features) in thetraining corpus.
We had the total of 90 cue ex-pressions.
Note that using a single binary fea-ture for cues alleviates the data sparseness prob-lem; though some of the cues may have low fre-quencies, they will be aggregated to form a sin-gle cue category with a sufficient number of in-stances.
In the training corpus, which contained5221 sentences, 1914 sentences are marked 'y'and 3307 are marked 'n' with the cutoff at 0.85,which is found to minimize the entropy of thedistribution of relation types.
It is interesting tonote that the entropy strategy was able to pickup cues which could be linguistically motivated(Table 2).
In contrast o Samuel et al (1998),we did not consider relation cues reported inthe linguistics literature, since they would beuseless unless they contribute to reducing thecue entropy.
They may be linguistically 'right'cues, but their utility in the machine learningcontext is not known.<PrevRel> makes available information abouta relation type of the preceding sentence.
It hastwo values, ELA for the elaboration relation, andSEQ for the sequence relation.In the Japanese linguistics literature, there isa popular theory that sentence ndings are rel-evant for identifying semantic relations among3INFOx (T) measures the entropy of the distributionof classes in a set T with respect to a feature X.  Wedefine INFOx just as given in Quinlan (1993):xNFOx(T) = x xNFo(T,)i=1Ti represents a partit ion of T corresponding to one ofthe values for X. INFO(T) is defined as follows:kINFO(T) = ~ freq(Cj, T) freq(Cj, T)- ~ i .~\]  x log s \] T Ij= lfi'eq(C, T) is the number of cases from class C in a set Tof cases.162ITable 2: Some of the 'linguistically interesting' cues identified by the entropy strategy.mata on the other hand, dSjini at the same time, ippou in contrast, sarani inaddition, mo topic marker, ni-tsuite-wa regarding, tameda the reason is that,kekka as the result ga-nerai the goal is thatsentences.
Some of the sentence ndings are in-flectional categories of verbs such as PAST/NON-PAST, INTERROGATIVE, and also morpholog-ical categories :like nouns and particles (eg.question-markers).
Based on Ichikawa (1990),we defined six types of sentence-ending cuesand marked a sentence according to whether itcontains a part.icular type of cue.
Included inthe set are inflectional forms of the verb andthe verbal adjec~tive, PAST/NON-PAST, morpho-logical categories uch as COPULA, and NOUN,parentheses (quotation markers), and sentence-final particles such as -ka.
We use the follow-ing two attributes to encode information aboutsentence-ending cues.<EndCueh> records information about asentence-ending form of the preceding sentence.It takes a discrete value from 0 to 6, with0 indicating the absence in the sentence ofrelevant cues.<EadCueB> Sa~me as above except hat this fea-ture is concerned with a sentence-ending formof the current sentence, i.e.
the 'B' sentence.Finally, we have two classes, ELABORATIONand SEQUENCE.4 EvaluationTo evaluate our method, we carried out ex-periments, using a corpus of news articlesfrom a Japanese conomics daily (Nihon-Keizai-Shimbun-Sha, 1995).
The corpus had 477 arti-cles, randomly selected from issues that werepublished urilig the year.
Each sentence in thearticles was tagged with one of the discourse re-lations at the subclass level (i.e.
CONSEQUEN-TIAL, ANTITHESIS, etc.).
However, in evaluationexperiments, we translated a subclass relationinto a corresponding major class relation (SE-QUENCE/ELABORATION) for reasons discussedearlier.
Furthermore , we explicitly asked codersnot to tag a paragraph initial sentence for a dis-course relation, for we found that coders rarelyagree on their :classifications.
Paragraph-initialsentences were dropped ffrom the evaluation cor-pus.
This had left us with 5221 sentences, ofwhich 56% are labeled as SEQUENCE and 44%ELABORATION.To find out effects of the committee-basedsampling method (CBS), we ran the C4.5 (Re-lease 5) decision tree algorithm with CBSturned on and off (Quinlan, 1993) and measuredthe performance by the 10-fold cross validation,in which the corpus is divided evenly into 10blocks of data and 9 blocks are used for train-ing and the remaining one block is held out fortesting.
On each validation fold, CBS startswith a set of about 512 samples from the set oftraining blocks and sequentially examines am-ples from the rest of the training set for pos-sible labeling.
If a sample is selected, then adecision tree will be trained on the sample to-gether with the data acquired so far, and testedon the held-out data.
Performance scores (er-ror rates) are averaged over 10 folds to give asummary figure for a particular learning strat-egy.
Throughout he experiments, we assumethat k = 10 and g = 1, i.e., 10 committeemembers and the entropy gain of 1.
Figure 1shows the result of using CBS for a decision tree.Though the performance fluctuates erratically,we see a general tendency that the CBS methodfares better than a decision tree classifier alone.In fact differences between C4.5/CBS and C4.5alone proved statistically significant (t = 7.06,df = 90, p < .01).While there seems to be a tendency for per-formance to improve with an increase in theamount of training data, either with or withoutCBS, it is apparent that an increase in the train-ing data has non-linear effects on performance,which makes an interesting contrast with proba-bilistic classifiers like HMM, whose performanceimproves linearly as the training data grow.
Thereason has to do with the structural complex-ity of the decision tree model: it is possiblethat small changes in the INFO value lead to163Figure 1: Effects of CBS on the decision tree learning.
Each point in the scatterplots represents asummary figure, i.e.
the average of figures obtained for a given x in 10-fold cross validation trials.The x-axis represents he amount of training data, and the y-axis the error rate.
The error rate isthe proportion of the misclassified instances to the total number of instances.47.547,46.54645.54544.54443.5430@oo o O@~ o@ @ o o@ @@~ @ o@ O O O :@ @+ O @ 0 O"?+ $ O:O +o + o@ o o : o 0o +%+ + %>+ ~+ o+ + +++ +~ + ~+o ~ *++ + + + ++ ~ +?0 + ++.p + + ++ Oar  + ~+++ +++ + O@4- 4-+I I I200 400 600Training DataISTD C4.5CBS+C4.5 +O@oo@ o o @o oo o oo ?
o @ O-I-k++ ++ + +++ ++++?++I8OO01000a drastic restructuring of a decision tree.
In theface of this, we made a small change to the wayCBS works.
The idea, which we call a samplingwith error feedback, is to remove harmful exam-ples from the training data and only use thosewith positive effects on performance.
It forcesthe sampling mechanism to return to status quoante when it finds that an example selected e-grades performance.
More precisely, this wouldbe put as follows:f St U {e}, if E(CSU{e}) < E(C s~) S +l \[ St otherwiseSt is a training set at time t. C s denotes a clas-sifter built from the training set S. E(C s) is anerror rate of a classifier C s. Thus if there is anincrease or no reduction in the error rate afteradding an example to the training set, a clas-sifter goes back to the state before the change.As Figure 2 shows, the error feedback pro-duced a drastic reduction in the error rate.
At900, the committee-based method with the er-ror feedback reduced the error rate by as muchas 23%.
Figure 3 compares performance ofthree sampling methods, random sampling, thecommittee-based sampling with 100 bootstrapreplicates (i.e., K = 100) and that with 500bootstrap replicates.
In the random samplingmethod, a sample is selected randomly fromthe data and added to the training data.
Fig-ure 4 compares a random sampling approachwith CBS with 500 bootstrap replicates.
Bothused the error feedback mechanism.
Differ-ences, though they seem small, turned out tobe statistically significant (t = 4.51, df =90, p < .01), which demonstrates the signif-icance of C4.5/CBS approach.
Furthermore,Figure 5 demonstrates that the number of boot-strap replicates affects performance (t = 8.87,df = 90, p < .01).
CBS with 500 bootstrapsperforms consistently better than that with 100bootstrap replicates.
This might mean that inthe current setup, 100 replicates are not enoughto simulate the true distribution of P(M I S).Note that CBS with 500 replicates achieves theerror rate of 33.40 with only 1008 training sam-ples, which amount o one fourth of the trainingdata C4.5 alone required to reach 44.64.
While adirect comparison with other learning schemesin discourse such as a transformation method(Samuel et al, 1998) is not feasible, if Samuelet al (1998)'s approach is indeed comparable toC5.0, as discussed in Samuel et al (1998), thenthe present method might be able to reduce the164Figure 2: The committee-based method (with 100 bootstraps) with the error feedback as comparedagainst one without.
The error rate decreases rapidly with the growth of the training data (thelower scatterplot).
The upper scatterplot represents CBS with the error feedback disabled.4644'42uJ3836340o o C~+C4.5 o04200 CBS.~C4.5+EF + o o ~?
??
?oo~ o~:o  ~ ~o ooo o~%oo o o ~oo o o o ~ o~?o\+\\++~+%+++++++++++*+-H.+++++++'H'~'+~'++'~'+?++++'H ~'H' + ..H.+++.H..H.~.+ ......I I I I I I I I "J'H'4"l:+,~100 200 300 400 500 600 700 800 900Tra in ing  DataFigure 3: Comparing performance of three approaches, random sampling (RANDOM-EF), boot-strapped CBS with 100 replicates (CBS100-EF), and bootstrapped CBS with 500 replicates(CBS500-EF),: all with the error feedback on.42403 !363432i ,'RANDOM+EF oCBS100+EF +CBS500+EF E\]\[~?| I I I I I I I I100 200 300 400 500 600 700 800 900Tra in ing  Data1000amount of training data without hurting perfor-mance.5 Conc lus ionsWe presented a new approach for identifyingdiscourse relations, built upon the committee-based sampling method, in which useful ex-amples are selected for training and those notuseful are discarded.
Since the committee-based sampling method was originally devel-oped for probabilistic classifiers, we extendedthe method for a decision tree classifier, us-165Figure 4: Differences in performance ofRANDOM-EF (random sampling with the error feedback)and CBS500-EF (CBS with the error feedback, with 500 bootstrap replicates).4644424O38363432i i i i i i !random sampling+EF oCBS+BPSOO+EF ++oI I I I I I I I I100 200 300 400 5(X3 600 700 800 900Training Data1000Figure 5: Differences in performance of CBS500-EF and CBS100-EF.444O383634!
iCBSlOO+EF ?.CBSSOO+EF ++o +?
~ ~++~.
:v .
, , ,~32 I , I I I I I | I -0 1 O0 200 300 400 500 600 700 800 900Training Dataing a statistical technique called bootstrapping.The use of the method for learning discourserelations resulted in a drastic reduction in theamount of data required and also an increasedaccuracy.
Further, we found that the num-ber of bootstraps has substantial effects on per-formance; CBS with 500 bootstraps performedbetter than that with 100 bootstrapsReferencesPaul R. Cohen.
1995.
Empirical Methods in Ar-tificial Intelligence.
The MIT Press.Ido Dagan and Sean Engelson.
1995.Committee-based sampling for trainingprobabilistic lassifiers.
In Proceedings off In-ternational Conference on Machine Learning,pages 150-157, July.Thomas G. Dietterich.
1998.
An experimental166!comparison of three methods for constructingensembles of decision trees: Bagging, boost-ing, and randomization, submitted to Ma-chine Learning.Sean P. Engelson and Ido Dagan.
1996.
Mini-mizing manual annotation cost in supervisedtraining from ,corpora.
In Proceedings off the3~th Annual Meeting of the Association forComputational Linguistics, pages 319-326.ACL, June.
University of California,SantaCruz .Takashi Ichikawa.
1990.
Bunshddron-gaisetsu.KySiku-Shuppan, Tokyo.Klaus Krippendorff.
1980.
Content Analysis:An Introductiqn to Its Methodology, volume 5of The Sage COMMTEXT series.
The SagePublications, Inc.W.
C. Mann and S. A. Thompson.
1987.Rhetorical Structure Theory.
In L. Polyani,editor, The Structure of Discourse.
AblexPublishing Co:rp., Norwood, NJ.Daniel Marcu.
1997.
The Rhetorical Pars-ing of Natural Language Texts.
In Proceed-ings of the 35th Annual Meetings of the As-sociation for ,Computational Linguistics andthe 8th European Chapter of the Associationfor Computational Linguistics, pages 96-102,Madrid, Spain, July.Nihon-Keizai-Shimbun- Sha.
1995.
NihonKeizai Shimbun 95 hen CD-ROM ban.CD-ROM.
Nihon Keizai Shimbun, Inc.,Tokyo.J.
Ross Quinlani 1993.
C~.5: Programs for Ma-chine Learning.
Morgan Kanfmann.Gerald Salton and Michael J. McGill.
1983.Introduction to Modern Information Re-treival.
McGraw-Hill Computer Science Se-ries.
McGraw~Hill Publishing Co.Ken Samuel, Sandra Carberry, and K. Vijay-Shanker.
1998.
Diaglogue act tagging withtransformation-based l arning.
In Proceed-ings of the 36th Annual Meeting off the Asso-ciation of Computational Linguistics and the17th International Conference on Computa-tional Linguistics, pages 1150-1156, August10-14.
Montreal, Canada.Sidney Siegel and N. John CasteUan.
1988.Nonparametr~c Statistics for the BehavioralSciences.
McGraw-Hill, Second edition.167
