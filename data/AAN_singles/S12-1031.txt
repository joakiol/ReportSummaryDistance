First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228?236,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsThe Effects of Semantic Annotations on Precision Parse RankingAndrew MacKinlay?
?, Rebecca Dridan?
?, Diana McCarthy??
and Timothy Baldwin???
Dept.
of Computing and Information Systems, University of Melbourne, Australia?
NICTA Victoria Research Laboratories, University of Melbourne, Australia?
Department of Informatics, University of Oslo, Norway?
Computational Linguistics and Phonetics, Saarland University, Germanyamack@csse.unimelb.edu.au, rdridan@ifi.uio.no,diana@dianamccarthy.co.uk, tb@ldwin.netAbstractWe investigate the effects of adding semanticannotations including word sense hypernymsto the source text for use as an extra sourceof information in HPSG parse ranking for theEnglish Resource Grammar.
The semantic an-notations are coarse semantic categories or en-tries from a distributional thesaurus, assignedeither heuristically or by a pre-trained tagger.We test this using two test corpora in differentdomains with various sources of training data.The best reduces error rate in dependency F-score by 1% on average, while some methodsproduce substantial decreases in performance.1 IntroductionMost start-of-the-art natural language parsers (Char-niak, 2000; Clark and Curran, 2004; Collins, 1997)use lexicalised features for parse ranking.
These areimportant to achieve optimal parsing accuracy, andyet these are also the features which by their naturesuffer from data-sparseness problems in the trainingdata.
In the absence of reliable fine-grained statis-tics for a given token, various strategies are possible.There will often be statistics available for coarsercategories, such as the POS of the particular token.However, it is possible that these coarser represen-tations discard too much, missing out informationwhich could be valuable to the parse ranking.
Anintermediate level of representation could providevaluable additional information here.
For example,?This research was conducted while the second author wasa postdoctoral researcher within NICTA VRL.
?The third author is a visiting scholar on the ErasmusMundus Masters Program in ?Language and CommunicationTechnologies?
(LCT, 2007?0060).assume we wish to correctly attach the prepositionalphrases in the following examples:(1) I saw a tree with my telescope(2) I saw a tree with no leavesThe most obvious interpretation in each case has theprepositional phrase headed by with attaching in dif-ferent places: to the verb phrase in the first example,and to the noun tree in the second.
Such distinctionsare difficult for a parser to make when the trainingdata is sparse, but imagine we had seen examplessuch as the following in the training corpus:(3) Kim saw a eucalypt with his binoculars(4) Sandy observed a willow with plentiful foliageThere are few lexical items in common, but in eachcase the prepositional phrase attachment follows thesame pattern: in (3) it attaches to the verb, and in(4) to the noun.
A conventional lexicalised parserwould have no knowledge of the semantic similaritybetween eucalypt and tree, willow and tree, binoc-ulars and telescope, or foliage and leaves, so wouldnot be able to make any conclusions about the earlierexamples on the basis of this training data.
Howeverif the parse ranker has also been supplied with in-formation about synonyms or hypernyms of the lex-emes in the training data, it could possibly have gen-eralised, to learn that PPs containing nouns relatedto seeing instruments often modify verbs relating toobservation (in preference to nouns denoting inani-mate objects), while plant flora can often be modi-fied by PPs relating to appendages of plants such asleaves.
This is not necessarily applicable only to PPattachment, but may help in a range of other syntac-tic phenomena, such as distinguishing between com-plements and modifiers of verbs.228The synonyms or hypernyms could take the formof any grouping which relates word forms with se-mantic or syntactic commonality ?
such as a labelfrom the WordNet (Miller, 1995) hierarchy, a sub-categorisation frame (for verbs) or closely relatedterms from a distributional thesaurus (Lin, 1998).We present work here on using various levelsof semantic generalisation as an attempt to im-prove parse selection accuracy with the English Re-source Grammar (ERG: Flickinger (2000)), a preci-sion HPSG-based grammar of English.2 Related Work2.1 Parse Selection for Precision GrammarsThe focus of this work is on parsing using hand-crafted precision HPSG-based grammars, and inparticular the ERG.
While these grammars are care-fully crafted to avoid overgeneration, the ambiguityof natural languages means that there will unavoid-ably be multiple candidate parses licensed by thegrammar for any non-trivial sentence.
For the ERG,the number of parses postulated for a given sentencecan be anywhere from zero to tens of thousands.
Itis the job of the parse selection model to select thebest parse from all of these candidates as accuratelyas possible, for some definition of ?best?, as we dis-cuss in Section 3.2.Parse selection is usually performed by trainingdiscriminative parse selection models, which ?dis-criminate?
between the set of all candidate parses.A widely-used method to achieve this is outlinedin Velldal (2007).
We feed both correct and incor-rect parses licensed by the grammar to the TADMtoolkit (Malouf, 2002), and learn a maximum en-tropy model.
This method is used by Zhang et al(2007) and MacKinlay et al (2011) inter alia.
Oneimportant implementation detail is that rather thanexhaustively ranking all candidates out of possiblymany thousands of trees, Zhang et al (2007) showedthat it was possible to use ?selective unpacking?,which means that the exhaustive parse forest can berepresented compactly as a ?packed forest?, and thetop-ranked trees can be successively reconstructed,enabling faster parsing using less memory.2.2 Semantic Generalisation for parse rankingAbove, we outlined a number of reasons whysemantic generalisation of lexemes could enableparsers to make more efficient use of training data,and indeed, there has been some prior work investi-gating this possibility.
Agirre et al (2008) appliedtwo state-of-the-art treebank parsers to the sense-tagged subset of the Brown corpus version of thePenn Treebank (Marcus et al, 1993), and addedsense annotation to the training data to evaluate theirimpact on parse selection and specifically on PP-attachment.
The annotations they used were oraclesense annotations, automatic sense recognition andthe first sense heuristic, and it was this last methodwhich was the best performer in general.
The senseannotations were either the WordNet synset ID orthe coarse semantic file, which we explain in moredetail below, and replaced the original tokens inthe training data.
The largest improvement in pars-ing F-score was a 6.9% reduction in error rate forthe Bikel parser (Bikel, 2002), boosting the F-scorefrom 0.841 to 0.852, using the noun supersense only.More recently, Agirre et al (2011) largely repro-duced these results with a dependency parser.Fujita et al (2007) add sense information to im-prove parse ranking with JaCy (Siegel and Bender,2002), an HPSG-based grammar which uses simi-lar machinery to the ERG.
They use baseline syn-tactic features, and also add semantic features basedon dependency triples extracted from the semanticrepresentations of the sentence trees output by theparser.
The dataset they use has human-assignedsense tags from a Japanese lexical hierarchy, whichthey use as a source of annotations.
The dependencytriples are modified in each feature set by replacingelements of the semantic triples with correspondingsenses or hypernyms.
In the best-performing con-figuration, they use both syntactic and semantic fea-tures with multiple levels of the the semantic hier-archy from combined feature sets.
They achieve a5.6% improvement in exact match parsing accuracy.3 MethodologyWe performed experiments in HPSG parse rank-ing using the ERG, evaluating the impact on parseselection of semantic annotations such as coarsesense labels or synonyms from a distributional the-229WESCIENCE LOGONTotal Sentences 9632 9410Parseable Sentences 9249 8799Validated Sentences 7631 8550Train/Test Sentences 6149/1482 6823/1727Tokens/sentence 15.0 13.6Training Tokens 92.5k 92.8kTable 1: Corpora used in our experiments, with total sen-tences, how many of those can be parsed, how many ofthe parseable sentences have a single gold parse (and areused in these experiments), and average sentence lengthsaurus.
Our work here differs from the aforemen-tioned work of Fujita et al (2007) in a number ofways.
Firstly, we use purely syntactic parse selec-tion features based on the derivation tree of the sen-tence (see Section 3.4.3), rather than ranking usingdependency triples, meaning that our method is inprinciple able to be integrated into a parser more eas-ily, where the final set of dependencies would not beknown in advance.
Secondly, we do not use human-created sense annotations, instead relying on heuris-tics or trained sense-taggers, which is closer to thereality of real-world parsing tasks.3.1 CorporaFollowing MacKinlay et al (2011), we use two pri-mary training corpora.
First, we use the LOGONcorpus (Oepen et al, 2004), a collection of En-glish translations of Norwegian hiking texts.
TheLOGON corpus contains 8550 sentences with ex-actly one gold parse, which we partitioned ran-domly by sentence into 10 approximately equal sec-tions, reserving two sections as test data, and us-ing the remainder as our training corpus.
Thesesentences were randomly divided into training anddevelopment data.
Secondly, we use the We-Science (Ytrest?l et al, 2009) corpus, a collectionof Wikipedia articles related to computational lin-guistics.
The corpus contains 11558 sentences, fromwhich we randomly chose 9632, preserving the re-mainder for future work.
This left 7631 sentenceswith a single gold tree, which we divided into atraining set and a development set in the same way.The corpora are summarised in Table 1.With these corpora, we are able to investigate in-domain and cross-domain effects, by testing on adifferent corpus to the training corpus, so we canexamine whether sense-tagging alleviates the cross-domain performance penalty noted in MacKinlay etal.
(2011).
We can also use a subset of each trainingcorpus to simulate the common situation of sparsetraining data, so we can investigate whether sense-tagging enables the learner to make better use of alimited quantity of training data.3.2 EvaluationOur primary evaluation metric is Elementary De-pendency Match (Dridan and Oepen, 2011).
Thisconverts the semantic output of the ERG into a setof dependency-like triples, and scores these triplesusing precision, recall and F-score as is conven-tional for other dependency evaluation.
FollowingMacKinlay et al (2011), we use the EDMNA modeof evaluation, which provides a good level of com-parability while still reflecting most the semanticallysalient information from the grammar.Other work on the ERG and related grammars hastended to focus on exact tree match, but the granu-lar EDM metric is a better fit for our needs here ?among other reasons, it is more sensitive in termsof error rate reduction to changes in parse selectionmodels (MacKinlay et al, 2011).
Additionally, it isdesirable to be able to choose between two differentparses which do not match the gold standard exactlybut when one of the parses is a closer match than theother; this is not possible with exact match accuracy.3.3 Reranking for parse selectionThe features we are adding to the parse selectionprocedure could all in principle be applied by theparser during the selective unpacking stage, sincethey all depend on information which can be pre-computed.
However, we wish to avoid the need formultiple expensive parsing runs, and more impor-tantly the need to modify the relatively complex in-ternals of the parse ranking machinery in the PETparser (Callmeier, 2000).
So instead of performingthe parse ranking in conjunction with parsing, as isthe usual practice, we use a pre-parsed forest of thetop-500 trees for each corpus, and rerank the forestafterwards for each configuration shown.The pre-parsed forests use the same models whichwere used in treebanking.
Using reranking meansthat the set of candidate trees is held constant, which230means that parse selection models never get thechance to introduce a new tree which was not inthe original parse forest from which the gold treewas annotated, which may provide a very small per-formance boost (although when the parse selectionmodels are similar as is the case for most of the mod-els here, this effect is likely to be very small).3.4 Word Sense Annotations3.4.1 Using the WordNet HierarchyMost experiments we report on here make someuse of the WordNet sense inventory.
Obviously weneed to determine the best sense and correspondingWordNet synset for a given token.
We return to thisin Section 3.4.2, but for now assume that the sensedisambiguation is done.As we are concerned primarily with makingcommonalities between lemmas with different baseforms apparent to the parse selection model, the fine-grained synset ID will do relatively little to providea coarser identifier for the token ?
indeed, if twotokens with identical forms were assigned differentsynset IDs, we would be obscuring the similarity.1We can of course make use of the WordNet hier-archy, and use hypernyms from the hierarchy to tageach candidate token, but there are a large numberof ways this can be achieved, particularly when itis possibly to assign multiple labels per token as isthe case here (which we discuss in Section 3.4.3).We apply two relatively simple strategies.
We notedin Section 2.2 that Agirre et al (2008) found thatthe semantic file was useful.
This is the coarse lex-icographic category label, elsewhere denoted super-sense (Ciaramita and Altun, 2006), which is theterminology we use.
Nouns are divided into 26coarse categories such as ?animal?, ?quantity?
or?phenomenon?, and verbs into 15 categories such as?perception?
or ?consumption?.
In some configura-tions, denoted SS, we tag each open-class token withone of the supersense labels.Another configuration attempts to avoid makingassumptions about which level of the hierarchy willbe most useful for parse disambiguation, insteadleaving it the MaxEnt parse ranker to pick those la-bels from the hierarchy which are most useful.
Each1This could be useful for verbs since senses interact stronglysubcategorisation frames, but that is not our focus here.open class token is labelled with multiple synsets,starting with the assigned leaf synset and travellingas high as possible up the hierarchy, with no distinc-tion made between the different levels in the hier-archy.
Configurations using this are designated HP,for ?hypernym path?.3.4.2 Disambiguating sensesWe return now to the question of determinationof the synset for a given token.
One frequently-used and robust strategy is to lemmatise and POS-tag each token, and assign it the first-listed sensefrom WordNet (which may or may not be based onactual frequency counts).
We POS-tag using TnT(Brants, 2000) and lemmatise using WordNet?s na-tive lemmatiser.
This yields a leaf-level synset, mak-ing it suitable as a source of annotations for both SSand HP.
We denote this ?WNF?
for ?WordNet First?
(shown in parentheses after SS or HP).Secondly, to evaluate whether a more informedapproach to sense-tagging helps beyond the naiveWNF method, in the ?SST?
method, we use the out-puts of SuperSense Tagger (Ciaramita and Altun,2006), which is optimised for assigning the super-senses described above, and can outperform a WNF-style baseline on at least some datasets.
Since thisonly gives us coarse supersense labels, it can onlyprovide SS annotations, as we do not get the leafsynsets needed for HP.
The input we feed in is POS-tagged with TnT as above, for comparability withthe WNF method, and to ensure that it is compati-ble with the configuration in which the corpora wereparsed ?
specifically, the unknown-word handlinguses a version of the sentences tagged with TnT.
Weignore multi-token named entity outputs from Su-perSense Tagger, as these would introduce a con-founding factor in our experiments and also reducecomparability of the results with the WNF method.3.4.3 A distributional thesaurus methodA final configuration attempts to avoid the needfor curated resources such as WordNet, instead us-ing an automatically-constructed distributional the-saurus (Lin, 1998).
We use the thesaurus fromMcCarthy et al (2004), constructed along theselines using the grammatical relations from RASP(Briscoe and Carroll, 2002) applied to 90 millionswords of text from the British National Corpus.231root_fragnp_frg_chdn_bnp_caj-hdn_norm_clegal_a1"legal"n_pl_olrissue_n1"issues"Figure 1: ERG derivation tree for the phrase Legal issues[n_-_c_le "issues"][n_pl_olr n_-_c_le "issues"][aj-hdn_norm_c n_pl_olr n_-_c_le "issues"](a) Original features[n_-_c_le noun.cognition][n_pl_olr n_-_c_le noun.cognition][aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition](b) Additional features in leaf mode, which augment the originalfeatures[noun.cognition "issues"][n_pl_olr noun.cognition "issues"][aj-hdn_norm_c n_pl_olr noun.cognition "issues"](c) Additional features in leaf-parent (?P?)
mode, which augmentthe original featuresFigure 2: Examples of features extracted from for"issues" node in Figure 1 with grandparenting levelof 2 or lessTo apply the mapping, we POS-tag the text withTnT as usual, and for each noun, verb and adjec-tive we lemmatise the token (with WordNet again,falling back to the surface form if this fails), andlook up the corresponding entry in the thesaurus.
Ifthere is a match, we select the top five most simi-lar entries (or fewer if there are less than five), anduse these new entries to create additional features,as well as adding a feature for the lemma itself in allcases.
This method is denoted LDT for ?Lin Distri-butional Thesaurus?.
We note that many other meth-ods could be used to select these, such as differentnumbers of synonyms, or dynamically changing thenumber of synonyms based on a threshold againstthe top similarity score, but this is not something weevaluate in this preliminary investigation.Adding Word Sense to Parse Selection ModelsWe noted above that parse selection using themethodology established by Velldal (2007) useshuman-annotated incorrect and correct derivationtrees to train a maximum entropy parse selectionmodel.
More specifically, the model is trained usingfeatures extracted from the candidate HPSG deriva-tion trees, using the labels of each node (which arethe rule names from the grammar) and those of alimited number of ancestor nodes.As an example, we examine the noun phrase Le-gal issues from the WESCIENCE corpus, for whichthe correct ERG derivation tree is shown in Figure 1.Features are created by examining each node in thetree and at least its parent, with the feature name setto the concatenation of the node labels.
We also gen-erally make used of grandparenting features, wherewe examine earlier ancestors in the derivation tree.A grandparenting level of one means we would alsouse the label of the grandparent (i.e.
the parent?s par-ent) of the node, a level of two means we would addin the great-grandparent label, and so on.
Our exper-iments here use a maximum grandparenting level ofthree.
There is also an additional transformation ap-plied to the tree ?
the immediate parent of each leafis, which is usually a lexeme, is replaced with thecorresponding lexical type, which is a broader par-ent category from the type hierarchy of the grammar,although the details of this are not relevant here.For the node labelled "issues" in Figure 1 withgrandparenting levels from zero to two, we wouldextract the features as shown in Figure 2(a) (wherethe parent node issue_n1 has already been re-placed with its lexical type n_-c_le).In this work here, we create variants of these fea-tures.
A preprocessing script runs over the trainingor test data, and for each sentence lists variants ofeach token using standoff markup indexed by char-acter span, which are created from the set of addi-tional semantic tags assigned to each token by theword sense configuration (from those described inSection 3.4) which is currently in use.
These sets ofsemantic tags for a given word could be a single su-persense tag, as in SS, a set of synset IDs as in HPor a set of replacement lemmas in LDT.
In all cases,the set of semantic tags could also be empty ?
if ei-ther the word has a part of speech which we are not232Test Train SS (WNF) SSp(WNF)P/ R/ F P/ R/ F ?F P/ R/ F ?FLOGWESC (23k) 85.02/82.22/83.60 85.09/82.33/83.69 +0.09 84.81/82.20/83.48 ?0.11WESC (92k) 86.56/83.58/85.05 86.83/84.04/85.41 +0.36 87.03/83.96/85.47 +0.42LOG (23k) 88.60/87.23/87.91 88.72/87.20/87.95 +0.04 88.43/87.00/87.71 ?0.21LOG (92k) 91.74/90.15/90.94 91.82/90.07/90.94 ?0.00 91.90/90.13/91.01 +0.07WESCWESC (23k) 86.80/84.43/85.60 87.12/84.44/85.76 +0.16 87.18/84.50/85.82 +0.22WESC (92k) 89.34/86.81/88.06 89.54/86.76/88.13 +0.07 89.43/87.23/88.32 +0.26LOG (23k) 83.74/81.41/82.56 84.02/81.43/82.71 +0.15 84.10/81.67/82.86 +0.31LOG (92k) 85.98/82.93/84.43 86.02/82.69/84.32 ?0.11 85.89/82.76/84.30 ?0.13Table 2: Results for SS (WNF) (supersense from first WordNet sense), evaluated on 23k tokens (approx 1500sentences) of either WESCIENCE or LOGON, and trained on various sizes of in-domain and cross-domain trainingdata.
Subscript ?p?
indicates mappings were applied to leaf parents rather than leaves.Test Train SS (SST) SSp(SST)P/ R/ F P/ R/ F ?F P/ R/ F ?FLOGWESC (23k) 85.02/82.22/83.60 84.97/82.38/83.65 +0.06 85.32/82.66/83.97 +0.37WESC (92k) 86.56/83.58/85.05 87.05/84.47/85.74 +0.70 86.98/83.87/85.40 +0.35LOG (23k) 88.60/87.23/87.91 88.93/87.50/88.21 +0.29 88.84/87.40/88.11 +0.20LOG (92k) 91.74/90.15/90.94 91.67/90.02/90.83 ?0.10 91.47/89.96/90.71 ?0.23WESCWESC (23k) 86.80/84.43/85.60 86.88/84.29/85.56 ?0.04 87.32/84.48/85.88 +0.27WESC (92k) 89.34/86.81/88.06 89.53/86.54/88.01 ?0.05 89.50/86.56/88.00 ?0.05LOG (23k) 83.74/81.41/82.56 84.06/81.30/82.66 +0.10 83.96/81.64/82.78 +0.23LOG (92k) 85.98/82.93/84.43 86.13/82.96/84.51 +0.08 85.76/82.84/84.28 ?0.16Table 3: Results for SS (SST) (supersense from SuperSense Tagger)Test Train HPWNF HPp(WNF)P/ R/ F P/ R/ F ?F P/ R/ F ?FLOGWESC (23k) 85.02/82.22/83.60 84.56/82.03/83.28 ?0.32 84.74/82.20/83.45 ?0.15WESC (92k) 86.56/83.58/85.05 86.65/84.22/85.42 +0.37 86.41/83.65/85.01 ?0.04LOG (23k) 88.60/87.23/87.91 88.58/87.26/87.92 +0.00 88.58/87.35/87.96 +0.05LOG (92k) 91.74/90.15/90.94 91.68/90.19/90.93 ?0.01 91.66/89.85/90.75 ?0.19WESCWESC (23k) 86.80/84.43/85.60 86.89/84.19/85.52 ?0.08 87.18/84.43/85.78 +0.18WESC (92k) 89.34/86.81/88.06 89.74/86.96/88.33 +0.27 89.23/86.88/88.04 ?0.01LOG (23k) 83.74/81.41/82.56 83.87/81.20/82.51 ?0.04 83.47/81.00/82.22 ?0.33LOG (92k) 85.98/82.93/84.43 85.89/82.38/84.10 ?0.33 85.75/83.03/84.37 ?0.06Table 4: Results for HPWNF (hypernym path from first WordNet sense)Test Train LDTp(5)P/ R/ F P/ R/ F ?FLOGWESC (23k) 85.02/82.22/83.60 84.48/82.18/83.31 ?0.28WESC (92k) 86.56/83.58/85.05 86.36/84.14/85.23 +0.19LOG (23k) 88.60/87.23/87.91 88.28/86.99/87.63 ?0.28LOG (92k) 91.74/90.15/90.94 91.01/89.25/90.12 ?0.82WESCWESC (23k) 86.80/84.43/85.60 86.17/83.51/84.82 ?0.78WESC (92k) 89.34/86.81/88.06 88.31/85.61/86.94 ?1.12LOG (23k) 83.74/81.41/82.56 83.60/81.18/82.37 ?0.19LOG (92k) 85.98/82.93/84.43 85.74/82.96/84.33 ?0.11Table 5: Results for LDT (5) (Lin-style distributional thesaurus, expanding each term with the top-5 most similar)233attempting to tag semantically, or if our method hasno knowledge of the particular word.The mapping is applied at the point of feature ex-traction from the set of derivation trees ?
at modelconstruction time for the training set and at rerank-ing time for the development set.
If a given leaf to-ken has some set of corresponding semantic tags, weadd a set of variant features for each semantic tag,duplicated and modified from the matching ?core?features described above.
There are two ways thesemappings can be applied, since it is not immedi-ately apparent where the extra lexical generalisationwould be most useful.
The ?leaf?
variant applies tothe leaf node itself, so that in each feature involvingthe leaf node, add a variant where the leaf node sur-face string has been replaced with the new seman-tic tag.
The ?parent?
variant, which has a subscript?P?
(e.g.
SSp(WNF) ) applies the mapping to theimmediate parent of the leaf node, leaving the leafitself unchanged, but creating variant features withthe parent nodes replaced with the tag.For our example here, we assume that we havean SS mapping for Figure 2(a), and that this hasmapped the token for "issues" to the WordNetsupersense noun.cognition.
For the leaf vari-ant, the extra features that would be added (either forconsidering inclusion in the model, or for scoring asentence when reranking) are shown in Figure 2(b),while those for the parent variant are in Figure 2(c).3.4.4 Evaluating the contribution of senseannotationsWewish to evaluate whether adding sense annota-tions improve parser accuracy against the baseline oftraining a model in the conventional way using onlysyntactic features.
As noted above, we suspect thatthis semantic generalisation may help in cases whereappropriate training data is sparse ?
that is, wherethe training data is from a different domain or onlya small amount exists.
So to evaluate the variousmethods in these conditions, we train models fromsmall (23k token) training sets and large (96k token)training sets created from subsets of each corpus(WESCIENCE and LOGON).
For the baseline, wetrain these models without modification.
For eachof the various methods of adding semantic tags, wethen re-use each of these training sets to create newmodels after adding the appropriate additional fea-tures as described above, to evaluate whether theseadditional features improve parsing accuracy4 ResultsWe present an extensive summary of the results ob-tained using the various methods in Tables 2, 3, 4and 5.
In each case we show results for applyingto the leaf and to the parent.
Aggregating the re-sults for each method, the differences range betweensubstantially negative and modestly positive, with alarge number of fluctuations due to statistical noise.LDT is the least promising performer, with onlyone very modest improvement, and the largest de-creases in performance, of around 1%.
The HP-WNF and HPp(WNF) methods make changes ineither direction ?
on average, over all four train-ing/test combinations, there are very small dropsin F-score of 0.02% for HPWNF, and 0.06% forHPp(WNF), which indicates that neither of themethods is likely to be useful in reliably improvingparser performance.The SS methods are more promising.
SS (WNF)and SSp(WNF) methods yield an average im-provement of 0.10% each, while SS (SST) andSSp(SST) give average improvements of 0.12%and 0.13% respectively (representing an error ratereduction of around 1%).
Interestingly, the increasein tagging accuracy we might expect using Super-Sense Tagger only translates to a modest (and prob-ably not significant) increase in parser performance,possibly because the tagger is not optimised for thedomains in question.
Amongst the statistical noiseit is hard to discern overall trends; surprisingly, itseems that the size of the training corpus has rela-tively little to do with the success of adding these su-persense annotations, and that the corpus being froman unmatched domain doesn?t necessarily mean thatsense-tagging will improve accuracy either.
Theremay be a slight trend for sense annotations to bemore useful when WESCIENCE is the training cor-pus (either in the small or the large size).To gain a better insight into how the effectschange as the size of the training corpus changes forthe different domains, we created learning curves forthe best-performing method, SSp(SST) (althoughas noted above, all SS methods give similar levelsof improvement), shown in Figure 3.
Overall, these2340 20 40 60 80 100Training Tokens (thousands)0.760.780.800.820.840.860.880.90EDMNAF-scoreTrained on LOGON CorpusTest CorpusLOGON*LOGON* +SSWeScWeSc +SS(a) LOGON0 20 40 60 80 100Training Tokens (thousands)0.760.780.800.820.840.860.880.90EDMNAF-scoreTrained on WeScience CorpusTest CorpusLOGONLOGON +SSWeSc*WeSc* +SS(b) WESCIENCEFigure 3: EDMNA learning curves for SS (SST) (supersense from SuperSense Tagger).
?*?
denotes in-domaintraining corpus.graphs support the same conclusions as the tables?
the gains we see are very modest and there is aslight tendency for WESCIENCE models to benefitmore from the semantic generalisation, but no strongtendencies for this to work better for cross-domaintraining data or small training sets.5 ConclusionWe have presented an initial study evaluat-ing whether a fairly simple approach to usingautomatically-created coarse semantic annotationscan improve HPSG parse selection accuracy usingthe English Resource Grammar.
We have providedsome weak evidence that adding features based onsemantic annotations, and in particular word super-sense, can provide modest improvements in parseselection performance in terms of dependency F-score, with the best-performing method SSp(SST)providing an average reduction in error rate over 4training/test corpus combinations of 1%.
Other ap-proaches were less promising.
In all configurations,there were instances of F-score decreases, some-times substantial.It is somewhat surprising that we did not achievereliable performance gains which were seen in therelated work described above.
One possible expla-nation is that the model training parameters weresuboptimal for this data set since the characteris-tics of the data are somewhat different than with-out sense annotations.
The failure to improve some-what mirrors the results of Clark (2001), who was at-tempting to improve the parse ranking performanceof the unification-based based probabilistic parser ofCarroll and Briscoe (1996).
Clark (2001) used de-pendencies to rank parses, and WordNet-based tech-niques to generalise this model and learn selectionalpreferences, but failed to improve performance overthe structural (i.e.
non-dependency) ranking in theoriginal parser.
Additionally, perhaps the changeswe applied in this work to the parse ranking couldpossibly have been more effective with featuresbased on semantic dependences as used by Fujitaet al (2007), although we outlined reasons why wewished to avoid this approach.This work is preliminary and there is room formore exploration in this space.
There is scope formuch more feature engineering on the semantic an-notations, such as using different levels of the se-mantic hierarchy, or replacing the purely lexical fea-tures instead of augmenting them.
Additionally,more error analysis would reveal whether this ap-proach was more useful for avoiding certain kindsof parser errors (such as PP-attachment).AcknowledgementsNICTA is funded by the Australian Government asrepresented by the Department of Broadband, Com-munications and the Digital Economy and the Aus-tralian Research Council through the ICT Centre ofExcellence program.235ReferencesE.
Agirre, T. Baldwin, and D. Martinez.
2008.
Improv-ing parsing and PP attachment performance with senseinformation.
In Proceedings of ACL-08: HLT, pages317?325, Columbus, Ohio, June.Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, andJoakim Nivre.
2011.
Improving dependency parsingwith semantic classes.
In Proceedings of the 49th An-nual Meeting of the Association of Computational Lin-guistics, ACL-HLT 2011 Short Paper, Portland, Ore-gon?.D.
M. Bikel.
2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceed-ings of the second international conference on HumanLanguage Technology Research, pages 178?182, SanFrancisco, CA, USA.T.
Brants.
2000.
Tnt ?
a statistical part-of-speech tag-ger.
In Proceedings of the Sixth Conference on Ap-plied Natural Language Processing, pages 224?231,Seattle, Washington, USA, April.T.
Briscoe and J. Carroll.
2002.
Robust accurate statis-tical annotation of general text.
In Proceedings of the3rd International Conference on Language Resourcesand Evaluation, pages 1499?1504.U.
Callmeier.
2000.
Pet ?
a platform for experimenta-tion with efficient HPSG processing techniques.
Nat.Lang.
Eng., 6(1):99?107.J.
Carroll and E. Briscoe.
1996.
Apportioning devel-opment effort in a probabilistic lr pars- ing systemthrough evaluation.
In Proceedings of the SIGDATConference on Empirical Methods in Natural Lan-guage Processing, pages 92?100, Philadelphia, PA.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st North American chapter ofthe Association for Computational Linguistics confer-ence, pages 132?139.M.
Ciaramita and Y. Altun.
2006.
Broad-coverage sensedisambiguation and information extraction with a su-persense sequence tagger.
In Proceedings of the 2006Conference on Empirical Methods in Natural Lan-guage Processing, pages 594?602, Sydney, Australia,July.S.
Clark and J.R. Curran.
2004.
Parsing the WSJ us-ing CCG and log-linear models.
In Proceedings of the42nd Meeting of the ACL, pages 104?111.S.
Clark.
2001.
Class-based Statistical Models for Lex-ical Knowledge Acquisition.
Ph.D. thesis, Universityof Sussex.M.
Collins.
1997.
Three generative, lexicalised mod-els for statistical parsing.
In Proceedings of the 35thAnnual Meeting of the Association for ComputationalLinguistics, pages 16?23, Madrid, Spain, July.R.
Dridan and S. Oepen.
2011.
Parser evaluation us-ing elementary dependency matching.
In Proceedingsof the 12th International Conference on Parsing Tech-nologies, pages 225?230, Dublin, Ireland, October.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Nat.
Lang.
Eng., 6(1):15?28.S.
Fujita, F. Bond, S. Oepen, and T. Tanaka.
2007.
Ex-ploiting semantic information for HPSG parse selec-tion.
In ACL 2007 Workshop on Deep Linguistic Pro-cessing, pages 25?32, Prague, Czech Republic, June.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proceedings of the 17th internationalconference on Computational linguistics-Volume 2,pages 768?774.A.
MacKinlay, R. Dridan, D. Flickinger, and T. Baldwin.2011.
Cross-domain effects on parse selection for pre-cision grammars.
Research on Language & Computa-tion, 8(4):299?340.R.
Malouf.
2002.
A comparison of algorithms for maxi-mum entropy parameter estimation.
In Proceedings ofthe Sixth Conference on Natural Language Learning(CoNLL-2002), pages 49?55.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of english:the penn treebank.
Comput.
Linguist., 19(2):313?330.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004.Finding predominant word senses in untagged text.
InProceedings of the 42nd Annual Meeting on Associa-tion for Computational Linguistics, pages 279?es.G.A.
Miller.
1995.
WordNet: a lexical database for En-glish.
Communications of the ACM, 38(11):39?41.S.
Oepen, D. Flickinger, K. Toutanova, and C.D.
Man-ning.
2004.
LinGO Redwoods: A rich and dynamictreebank for HPSG.
Research on Language & Com-putation, 2(4):575?596.M.
Siegel and E.M. Bender.
2002.
Efficient deep pro-cessing of japanese.
In Proceedings of the 3rd work-shop on Asian language resources and internationalstandardization-Volume 12, pages 1?8.E.
Velldal.
2007.
Empirical Realization Ranking.
Ph.D.thesis, University of Oslo Department of Informatics.G.
Ytrest?l, D. Flickinger, and S. Oepen.
2009.
Ex-tracting and annotating Wikipedia sub-domains ?
to-wards a new eScience community resourc.
In Pro-ceedings of the Seventh International Workshop onTreebanks and Linguistic Theories, Groeningen, TheNetherlands, January.Y.
Zhang, S. Oepen, and J. Carroll.
2007.
Efficiency inunification-based n-best parsing.
In IWPT ?07: Pro-ceedings of the 10th International Conference on Pars-ing Technologies, pages 48?59, Morristown, NJ, USA.236
