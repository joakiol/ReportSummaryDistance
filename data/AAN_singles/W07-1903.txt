Proceedings of the Workshop on Embodied Language Processing, pages 17?24,Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational LinguisticsComputing Backchannel Distributions in Multi-Party ConversationsDirk HeylenHuman Media InteractionUniversity of Twenteheylen@cs.utwente.nlRieks op den AkkerHuman Media InteractionUniversity of Twenteinfrieks@cs.utwente.nlAbstractIn multi-party conversations it may not al-ways be obvious who is talking to whom.Backchannels may provide a partial answerto this question, possibly in combinationwith some other events, such as gaze behav-iors of the interlocutors.
We look at somepatterns in multi-party interaction relatingfeatures of backchannel behaviours to as-pects of the partipation framework.1 IntroductionIn this paper we present a summary of our investiga-tions into the distribution of back-channels and someother forms of feedback and assesments in argumen-tative multi-party discourse.
We are interested insuch expressions for several reasons.
First, the sheerutterance of a backchannel indicates the presence ofan auditor that indicates ?I am here, I am attending?.The fact that it is being uttered by an auditor indi-cates intrinsically that the auditor felt addressed insome way or another by the speaker.
For the anal-ysis of multi-party conversations, it is important toestablish who is talking to whom and backchannels,at least seem to give away the whom part.
Second,the exact form, the kind of vocalisation, the intona-tion and the context may further invest the utterancewith additional meanings, expressing various atti-tudes towards what has been said: skepticism, sur-prise, liking, agreement, and so on.
So, when welook at back-channels in the context of multi-partydialogues they may tell us something about the par-ticipation framework on the one hand (who was talk-ing to whom) and about the way utterances are beingassessed by their audience.The qualifier ?in some way or another?
with re-spect to feeling or being addressed is particularlyimportant in the context of multi-party dialogues(i.e.
dialogues with more than two persons present).Typically, an utterance by a speaker instantiates theperformance of a speech act with a particular il-locutionary and perlocutionary force.
The speechact involves a request for uptake.
However, as hasbeen pointed out several times (Goffman (Goffman,1981), Levinson (Levinson, 1988), Clark and Carl-son (Clark and Carlson, 1992), Schegloff (Schegloff,1988)), participants in a multi-party conversationcan have a different role or status and they can beaddressed in different ways.In this paper we report on some of our investiga-tions into the distribution of backchannels in mul-tiparty interactions (for instance in relation to otherphenomena such as gaze) and how this informationcan help us to uncover certain features of floor andstance taking automatically.We will first describe the corpus and the anno-tations.
Next we look at the annotations of utter-ances consisting of starting with ?yeah?
and try tosee whether we can classify these utterances as con-tinuers, i.e.
neutral with respect to stance taking(Schegloff, 1981), or as assessments.2 CorpusThe argumentative discourses that we are study-ing are part of the meeting corpus collected duringthe AMI project (McCowan et al, 2005).
From acomputational, technological perspective, the aims17of this research is directed at developing automaticprocedures that can help to provide answers to anyquery users may have about what goes on in themeetings.
The AMI corpus consists of meetings inwhich a group of four people discuss the design of anew remote control.
TThe kinds of queries that we would like our proce-dures to be able to answer are related to these moves:what suggestions have been made; what were the ar-guments given and how much animosity was thererelated to the decision.
In the AMI corpus, the meet-ing recordings have been annotated on many levels,allowing the use of machine learning techniques todevelop appropriate algorithms for answering suchquestions.
We focus on the dialogue act annotationscheme.
This contains three types of information.Information on the speech act, the relation betweenspeech acts and information on addressing.The dialogue act classes that are distinguished inour dialogue act annotation schema fall into the fol-lowing classes:?
Classes for things that are not really dialogueacts at all, but are present to account for some-thing in the transcription that doesn?t reallyconvey a speaker intention.
This includesbackchannels, stalls and fragments?
Classes for acts that are about information ex-change: inform and elicit inform.?
Classes for acts about some action that an indi-vidual or group might take: suggest, offer, elicitsuggest or offer.?
Classes for acts that are about commenting onthe previous discussion: assess, comment aboutunderstanding, elicit assessment, elicit com-ment about understanding?
Classes for acts whose primary purpose is tosmooth the social functioning of the group: be-positive, be-negative.?
A ?bucket?
type, OTHER, for acts that do con-vey a speaker intention, but where the intentiondoesn?t fit any of the other classes.For our studies into feedback in the AMI cor-pus, the dialogue acts labelled as backchannesl areclearly important.
They were defined in the annota-tion manual as follows.In backchannels, someone who has just beenlistening to a speaker says something in thebackground, without really stopping that speaker.[...]
Some typical backchannels are ?uhhuh?,?mm-hmm?, ?yeah?, ?yep?, ?ok?, ?ah?, ?huh?,?hmm?, ?mm?
and, for the Scottish speakers in thedata recorded in Edinburgh, ?aye?.
Backchannelscan also repeat or paraphrase part or all of whatthe main speaker has just said.The labels assess and comment-about-understanding are closely related.
They weredefined as follows.An ASSESS is any comment that expresses anevaluation, however tentative or incomplete, ofsomething that the group is discussing.
[...] Thereare many different kinds of assessment; they include,among other things, accepting an offer, express-ing agreement/disagreement or any opinion aboutsome information that?s been given, expressing un-certainty as to whether a suggestion is a good ideaor not, evaluating actions by members of the group,such as drawings.
[...] An ASSESS can be veryshort, like ?yeah?
and ?ok?.
It is important not toconfuse this type of act with the class BACKCHAN-NEL, where the speaker is merely expressing, in thebackground, that they are following the conversa-tion.C-A-U is for the very specific case of comment-ing on a previous dialogue act where the speaker in-dicates something about whether they heard or un-derstood what a previous speaker has said, withoutdoing anything more substantive.
In a C-A-U, thespeaker can indicate either that they did understand(or simply hear) what a previous speaker said, orthat they didn?t.The Backchannel class largely conforms to Yn-gve?s notion of backchannel and is used for thefunctions of contact (Yngve, 1970).
Assess is usedfor the attitudinal reactions, where the speaker ex-presses his stance towards what is said, either ac-ceptance or rejection.
Comments about understand-ing are used for explicit signals of understanding ornon-understanding.In addition to dialogue acts also relation betweendialogue acts are annotated.
Relations are anno-tated between two dialogue acts (a later source act18and an earlier target act) or between a dialogue act(the source of the relation) and some other action, inwhich case the target is not specified.
Relations area more general concept than adjacency pairs, likequestion-answer.
Relation have one of four types:positive, negative, partial and uncertain, indicatingthat the source expresses a positive, negative, par-tially positive or uncertain stance of the speaker to-wards the contents of the target of the related pair.For example: a ?yes?-answer to a question is an in-form act that is the source of a positive relation withthe question act, which is the target of the relation.A dialogue act that assesses some action that is not adialogue act, will be coded as the source of a relationthat has no (dialogue act as) target.A part of the scenario-based meetings (14 meet-ings) were annotated with addressee labels, i.e.
an-notators had to say who the speaker is talking to.The addressee tag is attached to the dialogue act.
Ifa speaker changes his addressee (for instance, fromgroup to a particular participant) during a turn theutterance should be split into two dialogue act seg-ments, even if the type of dialogue act is the samefor both segments.3 YeahIn this section we look at the distribution of yeah inthe AMI corpus.
?yeah?
utterances make up a sub-stantial part of the dialogue acts in the AMI meetingconversations (about 8%).
If we try to tell groupaddressed dialogue acts from individually addressedacts then ?yeah?
is the best cue phrase for the classof single addressed dialogue acts; cf.
(Stehouwer,2006).In order to get information about the stance thatparticipants take with respect towards the issue dis-cussed it is important to be able to tell utterances of?yeah?
as a mere backchannel, or a stall, from yeah-utterances that express agreement with the opinionof the speaker.
The latter will more often be classi-fied as assessments.
We first look at the way anno-tators used and confused the labels and then turn tosee in what way we can predict the assignments tothe class.3.1 Annotations of yeah utterancesOne important feature of the dialogue act annota-tion scheme is that the annotators had to decide whatthey consider to be the segments that constitute a di-alogue act.
Annotators differ in the way they seg-ment the transcribed speech of a speaker.
Where oneannotator splits ?Yeah.
Maybe pear yeah or some-thing like that.?
into two segments labeling ?yeah.
?as a backchannel and the rest as a suggest, an othermay not split it and consider the whole utterance asa suggest.In comparing how different annotators labeled?yeah?
occurrences, we compared the labels they as-signed to the segment that starts with the occurrenceof ?yeah?.The confusion matrix for 2 annotators of 213yeah-utterances, i.e.
utterances that start with?yeah?, is given below.
It shows that backchan-nel (38%), assess (37%) and inform (11%) are thelargest categories 1.
Each of the annotators has about80 items in the backchannel class.
In about 75% ofthe cases, annotators agree on the back-channel la-bel.
In either of the other cases a category deemeda backchannel is mostly categorized as assessmentby the other and vice versa.
For the assessments,annotators agree on about slightly more than halfof the cases (43 out of 79 and 43 out of 76).
Thedisagreements are, for both annotators split betweenthe backchannels, for the larger part, the inform cat-egory, as second largest, and the other category.The other category subsumes the following typesof dialogue acts: summing up for both annotators:be-positive(9), suggest(8), elicit-assess(3), elicit-inform(2), comment-about-understanding(2).
Thedialogue act type of these other labeled utterancesis mostly motivated by the utterances following?Yeah?.
Examples: ?Yeah , it?s a bit difficult?
islabeled as Be-positive.
?Yeah ?
Was it a nice way tocreate your remote control ??
is labeled as an Elicit-Assessment .Out of the 213 Yeah-utterances a number containsjust ?yeah?
without a continuation.
Below, the con-fusion matrix for the same two annotators, but nowfor only those cases that have text ?yeah?
only.
In1As the numbers for each of the classes by both annotatorsis about the same, we have permitted ourselves the license tothis sloppy way of presenting the percentages.19yeah 0 1 2 3 4 SUM0 59.0 2.0 17.0 0.0 2.0 80.01 0.0 9.0 4.0 2.0 2.0 17.02 21.0 3.0 43.0 7.0 5.0 79.03 2.0 0.0 7.0 13.0 4.0 26.04 1.0 0.0 5.0 0.0 5.0 11.0SUM 83.0 14.0 76.0 22.0 18.0 213.0Figure 1: Confusion matrix of two annotations ofall Yeah utterances.
labels: 0 = backchannel; 1 =fragment or stall; 2 = assess; 3 = inform; 4 = other.p0=0.61 (percentage agreement); kappa=0.44.yeah-only 0 1 2 SUM0 50.0 12.0 3.0 65.01 13.0 5.0 1.0 19.02 2.0 0.0 2.0 4.0SUM 65.0 17.0 6.0 88.0Figure 2: labels: 0 = bc 1 = assess 2 = other(subsuming: be-positive, fragment, comment-about-understanding).
p0=0.65; kappa=0.14the comparison only those segments were taken intoaccount that both annotators marked as a segmenti.e.
a dialogue act realized by the word ?Yeah?
only.2What do these patterns in the interpretation of?yeah?
expressions tell us about its semantics?
Itappears that there is a significant collection of occur-rences that annotators agree on as being backchan-nels.
For the classes of assessments and other therealso seem to be prototypical examples that are clearfor both annotators.
The confusions show that thereis a class of expressions that are either interpretedas backchannel or assess and a class whose expres-sions are interpreted as either assessments or someother label.
Annotators often disagree in segmenta-tion.
A segment of speech that only consist of theword ?yeah?
is considered to be either a backchan-nel or an assess, with very few exceptions.
There ismore confusion between annotators than agreementabout the potential assess acts.2The text segment covered by the dialogue act then contains?Yeah?, ?Yeah ?
?, ?Yeah ,?
or ?Yeah .
?.3.2 Predicting the class of a yeah utteranceWe derived a decision rule model for the assignmentof a dialogue act label to yeah utterances, basedon annotated meeting data.
For our exploration weused decision tree classifiers as they have the advan-tage over other classifiers that the rules can be inter-preted.The data we used consisted of 1122 yeah utter-ances from 15 meetings.
Because of the relative lowinter-annotator agreement, we took meetings thatwere all annotated by one and the same annotator,because we expect that it will find better rules forclassifying the utterances when the data is not toonoisy.There are 12786 dialogue act segments in the cor-pus.
The number of segments that start with ?yeah?is 1122, of which 861 are short utterances only con-taining the word ?yeah?.
Of the 1122 yeahs 493 di-alogue acts were annotated as related to a previousdialogue act.
319 out of the 861 short yeah utter-ances are related to a previous act.The distribution of the 1122 yeah utterances overdialogue act classes is: assess (407), stall (224),backchannel (348), inform (95) and other (48 ofwhich 25 comment-about-understanding).
These arethe class variables we used in the classification.
Themodel consists of five features.
We make use of thenotion of conversational state, being an ensembleof the speech activities of all participants.
Sincewe have four participants a state is a 4-tuple <a, b, c, d > where a is the dialogue act performed byparticipant A, etc.
A conversation is in a particularstate as long as no participant stops or starts speak-ing.
Thus, a state change occurs every time whensome participants starts speaking or stops speaking,in the sense that the dialogue act that he performshas finished.
The features that we use are:?
lex This feature has value 0 if the utterance con-sists of the word Yeah only.
Otherwise 1.?
continue Has value 1 when the producer of theutterance also speaks in the next conversationalstate.
Otherwise 0.
This feature models incipi-ent behavior of the backchanneler.?
samespeaker Has value 1 if the conversationalstate in which this utterance happens has the20Null 629.0Assess 81.0Inform 162.0Elicit-Comment-Understanding 2.0Elicit-Assessment 40.0Elicit-Inform 73.0Elicit-Offer-Or-Suggestion 2.0Suggest 114.0Comment-About-Understanding 13.0Offer 5.0Be-Positive 1.0Figure 3: Distribution of the types of dialogue actsthat yeah utterances are responses to.same speaker, but different from the backchan-neler, as the next state.
Otherwise 0.
This fea-ture indicates that there is another speaker thatcontinues speaking.?
overlap There is speaker overlap in the statewhere the utterance started.?
source This involves the relation labeling of theannotation scheme.
source refers to the dia-logue act type of the source of the relation ofthe dialogue act that is realized by the Yeah ut-terance.
If the yeah dialogue act is not relatedto some other act the value of this feature isnull.
The possible values for this feature are:null, assess, inform, suggest, elicitation (whichcovers all elicitations), and other.The distribution of source types of the 1122 yeahdialogue acts is shown in table 3.2.
The table showsthat 629 out of 1122 yeah utterances were not relatedto some other act.We first show the decision tree computed bythe J48-tree classifier as implemented in the weka-toolkit, if we do not use the source feature looks asfollows.
The tree shows that 392 utterances satisfythe properties: continued = 1 and short = 1.
Of these158 are misclassified as backchannel.1.
Continued ?
0(a) lex ?
0: bc(392.0/158.0)(b) lex > 0: as(56.0/24.0)2.
Continue ?
0(a) samespkr ?
0i.
overlap ?
0: st(105.0/27.0)ii.
overlap > 0A.
lex ?
0: st(76.0/30.0)B. lex > 0: bc(16.0/6.0)(b) samespkr > 0 : ass(477.0/233.0)In this case the J48 decision tree classifier has anaccuracy of 57%.
If we decide that every yeah utter-ance is a Backchannel, the most frequent class in ourdata, we would have an accuracy of 31%.
If we in-clude the source feature, so we know the type of dia-logue act that the yeah utterance is a response to, theaccuracy of the J48 classifier raises at 80%.
Figure3.2 shows the decision tree for this classifier.
The re-sults were obtained using ten-fold cross-validation.It is clear from these results that there is a strongrelation between the source type of a Yeah dialogueact and the way this Yeah dialogue act should beclassified: as a backchannel or as an assess.
Notethat since backchannels are never marked as targetof a relation, null as source value is a good indicatorfor the Yeah act to be a backchannel or a stall.We also tested the decision tree classifier on a testset that consists of 4453 dialogue acts of which 539are yeah-utterances (219 marked as related to somesource act).
Of these 219 are short utterances con-sisting only of the word ?Yeah?
(139 marked as re-lated).
The utterances in this test set were annotatedby other annotators than the annotator that annotatedthe training set.
The J48 classifier had an accuracyon the test set of 64%.
The classes which are con-fused most are those that are also confused most bythe human annotators: backchannels and stall, andassess and inform.
One cause of the performancedrop is that in the test corpus the distribution of classlabels differs substantially from that of the trainingset.
In the test set yeah utterances were very rarelylabelled as stall, whereas this was a frequent label(about 20%) in the training set.
The distribution ofyeah-utterance labels in the test set is: backchannels241, stalls 4, assessments 186, inform 66 and other42.When we merged the train and test meetings andtrained the J48 decision tree classifier, a 10 foldcross-validation test showed an accuracy of 75%.Classes that are confused most are again: backchan-nel and stall, and assessment and inform.21Figure 4: Decision tree for classification of yeah utterances when information about the source of the relateddialogue act is used.4 Measuring Speaker Gaze atBackchannelorsWhen thinking about the interaction betweenspeaker and backchannelor, it seems obvious, as wesaid before, that the person backchanneling feels ad-dressed by the speaker.
We were wondering whetherthe backchannel was not prompted by an invitationof a speaker, for example, by gazing at the listener.Gaze behavior of speaker and backchannelor isclassified by means of the following gaze targets, asequence of focus of attention labels that indicateswhere the actor is looking at during a period of time:1. the gaze targets of the speaker in the periodstarting some short time (DeltaT ime) beforethe start time of the backchannel act till the startof the backchannel act.2.
the gaze targets of the backchannelor in the pe-riod starting some short time (DeltaT ime) be-fore the start time of the backchannel act till thestart of the backchannel act.3.
the gaze targets of the speaker during thebackchannel act.4.
the gaze targets of the backchannelor during thebackchannel act.We set DeltaT ime at 1 sec, so we observed the gazebehavior of the speaker in the period from one sec-ond before the start of the backchannel act.
Usingthese gaze target sequences, we classified the gazebehavior of the actor as follows:0: the gaze before target sequence of the actordoes not contain any person1: the before gaze target sequence of the actordoes contain a person but not the other ac-tor involved: for the speaker this means thathe did not look at backchannelor before thebackchannel act started, for the backchannelorthis means that he did not look at the speakerbefore the start of the backchannel.2: the actor did look at the other person involvedbefore that backchannel act.22Figure 4 show a table with counts of these classesof events.
In the 13 meetings we counted 1085backchannel events.
There were 687 events with asingle speaker of a real dialogue act.
For this casesit is clear who the backchannelor was reacting on.This is the selected speaker.
The table shows speakerdata in rows and backchannel data in columns.
TheMaxDownTime is 1sec and the MinUpTime is2 sec.
The DeltaT ime for the gaze period is 1sec.From the table we can infer that:1.
The selected speaker looks at the backchan-nelor in the period before the backchannelor actstarts in 316 out of the 687 cases.2.
The backchannelor looks at the selectedspeaker in the period before the backchanneloract starts in 430 out of the 687 cases.3.
The selected speaker looks at someone elsethan the backchannelor in the period before thebackchannelor act starts in 209 out of the 687cases.4.
The backchannelor looks at someone else thanthe selected speaker in the period before thebackchannelor act starts in 54 out of the 687cases.5.
In 254 out of the 687 cases the speaker lookedat the backchannelor and the backchannelorlooked at the speaker.We may conclude that the speakers look more atthe backchannelor than at the other two persons to-gether (316 against 209).
The table also shows thatbackchannelors look far more at the selected speakerthan at the two others (430 against 54 instances).In order to compare gaze of speaker in backchan-nel events, we also computed for each of the13 meetings for each pair of participants (X,Y ):dagaze(X,Y ): how long X looks at Y in those timeframes that X is performing a dialogue act.dagaze(X,Y ) =?OT (gaze(X,Y ), da(X))?da(X)(1)where summation is over all real dialogue actsperformed by X ,OT (gaze(X,Y ), da(X)) is the overlap time of thesp|bc 0 1 2 T0 103 4 55 1621 46 42 121 2092 54 8 254 316T 203 54 430 687Figure 5: Gaze table of speaker and backchannelor.DeltaT ime = 1sec.
Total number of backchannelevents is 1085.
In the table only those 687 backchan-nel events with a single speaker are considered (ex-cluded are those instances where no speaker or morethan one speaker was performing a real dialogue actin the period with a MinUpTime of 2 sec and aMaxDownTime of 1 sec.).
Speaker data in rows;backchannelor data in columns.
The table showsfor example that in 121 cases the speaker lookedat someone but not the backchannelor, in the periodfrom 1 sec before the start of the backchannel act tillthe start of the backchannel act, while the backchan-nelor looked in that period at the speaker.two events: gaze(X,Y ): the time that X gazes at Y ,and da(X) the time that the dialogue act performedby X lasts.
The numbers are normalized over the to-tal duration of the dialogue acts during which gazebehavior was measured.Next we computed bcgaze(X,Y ): how long Xlooks at Y in those time frames that X performsa real dialogue act and the Y responds with abackchannel act.bcgaze(X,Y ) =?OT (gaze(X,Y ), dabc(X,Y ))?da(X,Y )(2)where dabc(X,Y ) is the time that X performsthe dialogue act that Y reacts on by a backchannel.Here normalization is with the sum of the lengthsof all dialogue acts performed by X that elicited abackchannel act by Y .Analysis of pairs of values gaze(X,Y ) andbcgaze(X,Y ) shows that in a situation where some-one performs a backchannel the speaker lookssignificantly more at the backchannelor than thespeaker looks at the same person in general whenthe speaker is performing a dialogue act (t = 8.66,df = 101, p < 0.0001).
The mean values are 0.3323and 0.16.3Perhaps we can use the information on gaze of theparticipants in the short period before the backchan-nel act as features for predicting who the backchan-nel actor is.
For the 687 data points of backchannelevents with a single speaker, we used gaze of partici-pants, the speaker and the duration of the backchan-nel act as features.
Using a decision tree classifierwe obtained an accuracy of 51% in predicting whowill perform a backchannel act (given that someonewill do that).
Note that there are three possible ac-tors (the speaker is given).
This score is 16% abovethe a priori likelihood of the most likely participant:A (36%).ConclusionIn this paper, we have explored some questionsabout the possible use and function of backchan-nels in multiparty interactions.
On the one handbackchannels can be informative about functions re-lated to floor and participation: who is talking towhom.
Obviously, a person producing a backchan-nel was responding to an utterance of speaker.For the semantic analysis of meeting data an im-portant question is whether he was just using thebackchannel as a continuer (a sign of attention) oras an assessment.
We also checked our intuitionthat backchannels in the kinds of meetings that weare looking at might often be invited by speakersthrough gaze.
Obviously, these investigations justscratch the service of how backchannels work inconversations and how we can use them to uncoverinformation from recorded conversations.ReferencesH.
H. Clark and T. B. Carlson.
1992.
Hearers and speechacts.
In Herbert H. Clark, editor, Arenas of LanguageUse, pages 205?247.
University of Chicago Press andCSLI.Erving Goffman.
1981.
Footing.
In Erving Goffman,editor, Forms of Talk, pages 124?159.
University ofPennsylvania Press, Philadelphia, PA.Stephen C. Levinson.
1988.
Putting linguistics on aproper footing: explorations in goffman?s concept of3For 13 meeting and 4 participants we would have 156 pairsof values.
We only used those 102 pairs of which both valuesare non-zero.participation.
In Paul Drew and Anthony Wootton, ed-itors, Erving Goffman.
Exploring the Interaction Or-der, pages 161?227.
Polity Press, Cambridge.I.
McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,V.
Karaiskos, M.Kronenthal, G. Lathoud, M. Lincoln,A.
Lisowska, W. Post, D. Reidsma, and P. Wellner.2005.
The ami meeting corpus.
In Measuring Be-haviour, Proceedings of 5th International Conferenceon Methods and Techniques in Behavioral Research.Emanuel A. Schegloff.
1981.
Discourse as an interac-tional achievement: some uses of ?uh huh?
and otherthings that come between sentences.
In Deborah Tan-nen, editor, Analyzing Discourse: Text and Talk, pages71?93.
Georgetown University Press, Washington.Emanuel A. Schegloff.
1988.
Goffman and the analysisof conversation.
In Paul Drew and Anthony Wootton,editors, Erving Goffman.
Exploring the Interaction Or-der, pages 89?135.
Polity Press, Cambridge.J.H.
Stehouwer.
2006.
Cue-phrase selection methodsfor textual classification problems.
Technical report,M.Sc.
Thesis, Twente University, Human Media Inter-action, Enschede, the Netherlands.V.H.
Yngve.
1970.
On getting a word in edgewise.
InPapers from the sixth regional meeting of the ChicagoLinguistic Society, pages 567?77, Chicago: ChicagoLinguistic Society.24
