Identifying Real or Fake Articles: Towards better Language ModelingSameer BadaskarSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh PA, United Statessbadaska@cs.cmu.eduSachin AgarwalSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh PA, United Statessachina@cs.cmu.eduShilpa AroraSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh PA, United Statesshilpaa@cs.cmu.eduAbstractThe problem of identifying good featuresfor improving conventional language mod-els like trigrams is presented as a classifica-tion task in this paper.
The idea is to usevarious syntactic and semantic features ex-tracted from a language for classifying be-tween real-world articles and articles gener-ated by sampling a trigram language model.In doing so, a good accuracy obtained on theclassification task implies that the extractedfeatures capture those aspects of the lan-guage that a trigram model may not.
Suchfeatures can be used to improve the exist-ing trigram language models.
We describethe results of our experiments on the classi-fication task performed on a Broadcast NewsCorpus and discuss their effects on languagemodeling in general.1 IntroductionStatistical Language Modeling techniques attemptto model language as a probability distributionof its components like words, phrases and topics.Language models find applications in classificationtasks like Speech Recognition, Handwriting Recog-nition and Text Categorization among others.
Con-ventional language models based on n-grams ap-proximate the probability distribution of a languageby computing probabilities of words conditioned onprevious n words as followsP (s) ?m?i=1p(wi|wi?n+1, .
.
.
, wi?1) (1)In most applications, lower order n-grams (such asbigram or trigram) are used but they are an unre-alistic approximation of the underlying language.Higher order n-grams are desirable but they presentproblems concerning data sparsity.
On the otherhand, low order n-grams are incapable of represent-ing other aspects of the language like the underlyingtopics, topical redundancy etc.
In order to build abetter language model, additional features have tobe augmented to the existing language model (e.g.a trigram model) which capture those aspects of thelanguage that the trigram model does not.
Now, oneway to test the goodness of a feature under consider-ation is to use it in a framework like an exponentialmodel (Rosenfeld, 1997; Cai et al, 2000) and notethe improvement in perplexity.
An alternative way(Eneva et al, 2001) is as follows: Let L be the lan-guage and L?
be an approximation of the languageobtained by sampling the trigram language model.Also, let X be a piece of text obtained from either Lor L?.
Let y = h(f(X)) such that y = 1 if X ?
Land y = 0 if X ?
L?
where f(.)
is the computed fea-ture and h(.)
is the hypothesis function (a classifierlike AdaBoost, SVM etc).
If Pr[y = h(f(x))] isfound to be sufficiently high, it means that the fea-ture f(x) is able to distinguish effectively betweenthe actual language L and the approximate languageL?.
In other words, f(x) captures those features ofthe language that are complementary to the onescaptured by the trigram model and therefore f(x)is a good feature to augment the trigram languagemodel with.The formalism explained previously can be inter-preted as a classification task in-order to distinguish817between Real articles and Fake articles.
Articles ofdifferent lengths drawn at random from the Broad-cast News Corpus (BNC)1 are termed as Real arti-cles (from language L).
Articles generated by sam-pling the trigram model trained on the same corpusare termed as Fake articles (language L?).
These arti-cles together form the training data for the classifierto associate the features with the classification labels(real or fake) where the features are computed fromthe text.
The features that give high classification ac-curacy on the test set of articles are considered goodcandidates for adding to the trigram model.
Further-more, the confidence that the classifier attaches toa classification decision can be used to compute theperplexity.In this paper, a classification-task based formal-ism is used to investigate the goodness of some newfeatures for language modeling.
At the same timefeatures proposed in the previous literature on lan-guage modeling are also revisited (Cai et al, 2000)Section 2 discusses various syntactic and semanticfeatures used for the classification task, Section 3gives details about the experiments conducted andthe classification results obtained and finally, Sec-tion 4 concludes the paper by discussing the implica-tions of the classification results on language model-ing with pointers to improvements and future work.2 Feature EngineeringTo differentiate a real article from a fake one, theempirical, syntactic and semantic characteristics ofa given article are used to compute the features forthe classification task.
The various types of featuresthat were experimented are as follows:2.1 Empirical FeaturesEmpirical features are based on the statistical anal-ysis of both the real and fake articles.
They includethe count of uncommon pairs of words within an ar-ticle, the ratio of perplexity of trigram and quadgrammodels for a given article and the nature of the POStags that occur at the start and end of sentences in anarticle.1http://www.cs.cmu.edu/ roni/11761-s07/project/LM-train-100MW.txt.gzRatio of Perplexity of trigram and quad-grammodelsGiven an article, the ratio of its perplexity for a tri-gram model to a quad-gram model is computed.
Thetrigram and quad-gram models are both trained onthe same BNC corpus.
Both real and fake articleswould give a low perplexity score for the tri-grammodel but for the quad-gram model, real articleswould have significantly lower perplexity than thefake articles.
This implies that the ratio of trigramto quad-gram perplexities would be lower for a fakearticle than for a real article.
In other words, this ra-tio is similar to computing the likelihood ratio of anarticle w.r.t the trigram and quad-gram models.
Thehistogram in Figure 1 shows a good separation inthe distribution of values of this feature for the realand fake articles which indicates the effectiveness ofthis feature.
A quadgram language model is a betterapproximation of real text than a trigram model andby using this as a feature, we are able to demonstratethe usefulness of the classification task as a methodfor identifying good features for language modeling.In the subsequent sections, we investigate other fea-tures using this classification framework.Figure 1: Histogram for the ratio of perplexities withrespect to Trigram and Quadgram Language modelsover the training setCount of uncommon pairs of wordsContent words are the frequently occurring words inthe corpus excluding the stop-words.
All the wordsin corpus are ranked according to frequency of theiroccurrence and content words are defined to be thewords with rank between 150 and 6500.
A list ofcommon content word pairs (pairs of content words818atleast 5 words apart) is prepared from the real cor-pus by sorting the list of content word pairs by theirfrequency of occurrence and retaining those above acertain threshold.
For a given article, a list of contentword pairs is compared against this list and wordpairs not in this list form the set of uncommon wordpairs.A real article is expected to have lesser num-ber of uncommon content-word pairs than fake arti-cles.
When normalized by the total number of wordpairs, we get the probability of finding an uncom-mon content-word pair in an article.
This probabil-ity is greater for fake articles than the real articlesand we use this probability as a feature for the clas-sification task.Start and End POS TagsCertain POS tags are more probable than others toappear at the beginning or end of a real sentence.This characteristic of real text could be used as afeature to distinguish real articles from fake.
Thedistribution of POS tags of the first and last words ofthe sentences in an article is used as a feature.
Ourexperiments show that this feature had very little ef-fect in the overall contribution to the classificationaccuracy over the development set.2.2 Syntactic FeaturesThese features are derived from the parse struc-ture of the sentence.
It is hypothesized that realsentences tend to be grammatical while the samemay not be the case for fake sentences.
An objec-tive measure of the grammaticality of a sentencecan be obtained by running it through a statisti-cal parser.
The log-likelihood score returned bythe parser can be used to judge the grammatical-ity of a sentence and thus determine whether itis fake or real.
The Charniak Parser (Charniak,2001; Charniak, 2005) was used for assessing thegrammaticality of the articles under test.
Givenan article containing sentences S1, S2, .
.
.
, SN withlengths L1, L2, .
.
.
, LN , we compute the parser log-likelihood scores P (S1), P (S2), .
.
.
, P (SN ).
Theoverall grammaticality score for an article is givenbyPGram =?Ni=1 LiP (Si)?Ni=1 Li(2)The grammaticality score was normalized using theaverage and standard deviation over the entire train-ing set.
This feature gave small improvement interms of classification accuracy.
There may be sev-eral reasons for this: (1) Our training data consistedof spoken transcripts from a broadcast news corpuswhereas the Charniak Parser was trained on a differ-ent domain (Wall Street Journal) and (2) The parserwas trained on mixed case text where as the data weused was all upper case.2.3 Semantic FeaturesReal articles contain sentences with correlated pairsof content-words and sentences that are correlatedwith each other.
An article with such sentence/wordcorrelations is said to be semantically coherent.
Ow-ing to the use of only the short term word history forcomputing the probability distribution of a language,a trigram model fails to model semantic coherenceand we exploit this fact for the classification task.Specifically, we intend to model both intra-sentenceand inter-sentence semantic coherence and use themas features for classification.Intra-sentence CoherenceTo model the intra-sentence word correlations, weuse Yule?s Q-statistic (Eneva et al, 2001).
The wordcorrelations are learned from the BNC corpus aswell as the fake corpus.
The coherence score foran article is defined as the sum of the correlationsbetween pairs of content words present in the arti-cle.
The coherence score for an article is normalizedby the total number of content-word pairs found inthe article.
Since the trigram and quad-gram lan-guage model can capture short distance coherenceswell, coherences between distant words can be usedto differentiate between real and fake articles.
TheYule Q-statistic is calculated for every pair of con-tent words, which are atleast 5 words apart within asentence, both in the real and fake corpus.The articles are scored according to content word-pair correlations learned from the real as well asfake corpus.
Each article is given two scores, onefor the word-pair correlations from real articles andother for the word-pair correlations from fake arti-cles.
For a real article, the real word-pair correla-tion score would be relatively higher compared tothe fake word-pair correlation score (and vice-versa819for a fake article).Modeling Topical Redundancy (Inter-sentenceCoherence)A characteristic of real articles is that they tend tobe cohesive in terms of the topic under discussion.For example, a news-article about a particular event(topic) would have several direct or indirect refer-ences to the event.
We interpret this as some sortof a redundancy in terms of the information con-tent which we term as Topical Redundancy.
Thefake articles would not exhibit such a redundancy.If a real article is transformed to another represen-tation space where some form of truncation is ap-plied, on transformation back to the original space,the amount of information-loss may not be signif-icant due to information redundancy.
However, ifthe same process is applied on a fake article, theinformation-loss would be significant when trans-formed back to the original space.
We intend to ex-ploit this fact for our classification task.Let DW?N be an article represented in the formof a matrix, where W is the article vocabulary and Nis the number of sentences in that article.
Every termof this matrix represents the frequency of occurrenceof a vocabulary word in a particular sentence.
Weconstruct a sentence-sentence matrix as follows:A = DTD (3)We now transform A into the Eigen-space using Sin-gular Value Decomposition (SVD) which givesA = USUT (4)Here, UN?N is the eigen-vector matrix and SN?Nis the diagonal eigen-value matrix.
If we retain onlythe top K eigen-values from S , we get the truncated(lossy) form S?K?K .
Thus the truncated form of Ai.e.
A?
isA?
= US?UT (5)We believe that the information loss ?
A?A?
?2will not be significant in the case of real articlessince the topical redundancy is captured in a verycompact manner by the eigen-representation.
How-ever, in the case of a fake article, the loss is con-siderable.
For a real article, the matrix would beless sparse than a fake article and so is the case forthe reconstructed matrix.
Therefore, the statistics -mean, median, minimum and maximum computedfrom the reconstructed matrix have higher values forreal articles than a fake articles.
We use these statis-tics as features for classifying the article.
Figure 2show the histograms of the statistics computed fromthe reconstructed matrix for the training set.
As canbe seen, there is a good separation between the twoclasses fake and real in all the cases.
Using thesefeatures increased the classification accuracy by asignificant amount as shown later.
From another per-spective, these features model the inter-sentence se-mantic coherence (Deerwester et al, 1990) within anarticle and this is consistent with our notion of topi-cal redundancy as explained previously.
The matrixpackage developed by NIST (Hicklin et al, 2005)was used for SVD.3 Experimental Results3.1 Data DistributionThe training data consisted of 1000 articles (500 realand 500 fake) obtained from Broadcast News Cor-pus (BNC) and the test set consisted of 200 articles(100 real and 100 fake).
Additionally, a develop-ment dataset consisting of 200 articles and havingthe same distribution as that of the test dataset wasused for tuning the parameters of the classifiers.
Toensure that the training and test data come from thesame article length distribution, the training data wasresampled to have the same percentage of articles ofa given length as in the test set.
The article lengthdistribution for both the training(resampled) and testdatasets is shown in Tables 1 and 2.3.2 ClassifierClassifiers like AdaBoost (Freund et al, 1999) andMax-Entropy (Rosenfeld, 1997) models were usedfor the classification task.The number of iterations for AdaBoost was esti-mated using 5-fold cross-validation.
Given a sub-set of features, Maxent classified 74.5% of the doc-uments correctly compared to 82% for AdaBoost.Therefore, Adaboost was chosen as the classifier forfurther experiments.820(a) Mean (b) Median(c) Minimum (d) MaximumFigure 2: Histograms of topical redundancy features computed over the training set.
In (b) , the medianvalues for the fake articles are close to zero and hence cannot be seen clearly.3.3 Results and DiscussionWe used two performance measures to evaluate ourmodel.
First is the accuracy which measures thenumber of articles correctly classified as real or fakeand the second measure is the log-probability thatthe model assigns to the classification decision i.e.
itmeasures the confidence the model has in its classi-fication.
Table 3 shows our experimental results onthe syntactic, semantic and empirical features.The combination of syntactic, semantic and em-pirical features gave an accuracy of 91.5% with anaverage log-likelihood of -0.22 on development dataset.
The accuracy on the test dataset was 87% withan average log-likelihood of -0.328.4 Conclusions and Future WorkIn this work, we have used a classification-taskbased formalism for evaluating various syntactic,semantic and empirical features with the objectiveof improving conventional language models.
Fea-tures that perform well in the task of classifyingreal and trigram-generated fake articles are usefulfor augmenting the trigram model.
Semantic fea-tures, such as topical redundancy, model long-rangedependencies which are not captured by a trigramlanguage model.
Therefore, the semantic featurescontribute significantly to the classification task ac-curacy.
Additionally, linguistic resources such asWordNet (WordNet, 1998) can be used to model821# Sentencesper article# RealArt.# FakeArt.% Total(Real &Fake)1 938 940 19.762 440 471 9.583 502 474 10.264 507 533 10.945 497 525 10.757 431 524 10.0510 475 479 10.0415 482 421 9.5020 421 446 9.12Table 1: Distribution of article lengths for trainingdataset.# Sentencesper article# RealArt.# FakeArt.% Total(Real &Fake)1 20 20 202 10 10 103 10 10 104 10 10 105 10 10 107 10 10 1010 10 10 1015 10 10 1020 10 10 10Table 2: Distribution of article lengths for testdataset.topical redundancy using synonyms and other inter-word dependencies.
The semantic features we ex-plored assume a single underlying topic for an arti-cle which may not be always true.
An article canbe a representation of different topics and we aim toexplore this direction in future.ReferencesCan Cai, Larry Wasserman and Roni Rosenfeld.2000.
Exponential language models, logistic regres-sion, and semantic coherence.
Proceedings of theNIST/DARPA Speech Transcription Workshop.Eugene Charniak.
2001.
Immediate-Head Parsing forLanguage Models.
Proceedings of 39th Annual Meet-ing of the ACL, 124-131.Feature Combination ClassificationAccuracyAvg.
LogLikelihoodSyntactic 60.5% -0.663Semantic 79.5% -0.510Empirical 83.0% -0.446Semantic + Syntactic 80.0% -0.553Semantic + Empirical 86.0% -0.410Semantic + Syntactic +Empirical91.5% -0.220Table 3: Performance of different features on thedevelopment-set.Eugene Charniak.
2005. ftp://ftp.cs.brown.edu/pub/ nl-parser/parser05Aug16.tar.gzThomas M. Cover and Joy A. Thomas.
1991.
Elementsof Information Theory.
John Wiley & Sons, New York.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journal ofJapanese Society for Artificial Intelligence, 41(6).Elena Eneva, Rose Hoberman and Lucian Lita.
2001.Learning within-sentence semantic coherence.
Pro-ceedings of the EMNLP 2001.Yoav Freund and Robert E. Schapire.
1999.
A short in-troduction to boosting Journal of Japanese Society forArtificial Intelligence, 14(5):771-780.Joe Hicklin, Cleve Moler, Peter Webb.
2005.http://math.nist.gov/javanumerics/jama/Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
MIT Press.Roni Rosenfeld.
1997.
A whole sentence maximum en-tropy language model.
In Proc.
of the IEEE Workshopon Automatic Speech Recognition and Understanding,1997.WordNet: An Electronic Lexical Database, ISBN-13:978-0-262-06197-1.822
