Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 978?986,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPGlobal Learning of Noun Phrase Anaphoricity in Coreference Resolu-tion via Label PropagationZHOU GuoDong      KONG FangJiangSu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and TechnologySoochow University.
Suzhou, China 215006Email:{gdzhou,kongfang}@suda.edu.cnAbstractKnowledge of noun phrase anaphoricity mightbe profitably exploited in coreference resolu-tion to bypass the resolution of non-anaphoricnoun phrases.
However, it is surprising to no-tice that recent attempts to incorporate auto-matically acquired anaphoricity informationinto coreference resolution have been some-what disappointing.
This paper employs aglobal learning method in determining theanaphoricity of noun phrases via a labelpropagation algorithm to improve learning-based coreference resolution.
In particular,two kinds of kernels, i.e.
the feature-basedRBF kernel and the convolution tree kernel,are employed to compute the anaphoricitysimilarity between two noun phrases.
Experi-ments on the ACE 2003 corpus demonstratethe effectiveness of our method in anaphoric-ity determination of noun phrases and its ap-plication in learning-based coreference resolu-tion.1 IntroductionCoreference resolution, the task of determiningwhich noun phrases (NPs) in a text refer to thesame real-world entity, has long been consideredan important and difficult problem in naturallanguage processing.
Identifying the linguisticconstraints on when two NPs can co-refer re-mains an active area of research in the commu-nity.
One significant constraint on coreference,the anaphoricity constraint, specifies that a non-anaphoric NP cannot be coreferent with any ofits preceding NPs in a given text.
Therefore, it isuseful to skip over these non-anaphoric NPsrather than attempt an unnecessary search for anantecedent for them, only to end up with inaccu-rate outcomes.
Although many existing machinelearning approaches to coreference resolutionhave performed reasonably well without explicitanaphoricity determination (e.g., Soon et al2001;Ng and Cardie 2002b; Strube and Muller 2003;Yang et al2003, 2008), anaphoricity determina-tion has been studied fairly extensively in theliterature, given the potential usefulness of NPanaphoricity in coreference resolution.
Onecommon approach involves the design of heuris-tic rules to identify specific types of non-anaphoric NPs, such as pleonastic pronouns (e.g.Paice and Husk 1987; Lappin and Leass 1994;Kennedy and Boguraev 1996; Denber 1998) andexistential definite descriptions (e.g., Vieira andPoesio 2000).
More recently, the problem hasbeen tackled using statistics-based (e.g., Beanand Riloff 1999; Bergsma et al2008) and learn-ing-based (e.g.
Evans 2001; Ng and Cardie2002a; Ng 2004; Yang et al2005; Denis andBalbridge 2007) methods.
Although there is em-pirical evidence (e.g.
Ng and Cardie 2002a,2004) that coreference resolution might be fur-ther improved with proper anaphoricity informa-tion, its contribution is still somewhat disap-pointing and lacks systematic evaluation.This paper employs a label propagation (LP)algorithm for global learning of NP anaphoricity.Given the labeled data and the unlabeled data,the LP algorithm first represents labeled andunlabeled instances as vertices in a connectedgraph, then propagates the label informationfrom any vertex to nearby vertices throughweighted edges and finally infers the labels ofunlabeled instances until a global stable stage isachieved.
Here, the labeled data in this paperinclude all the NPs in the training texts with theanaphoricity labeled and the unlabeled data in-clude all the NPs in a test text with the ana-phoricity unlabeled.
One major advantage of LP-based anaphoricity determination is that the ana-phoricity of all the NPs in a text can be deter-mined together in a global way.
Compared withprevious methods, the LP algorithm can effec-tively capture the natural clustering structure inboth the labeled and unlabeled data to smooththe labeling function.
In particular, two kinds of978kernels, i.e.
the feature-based RBF kernel andthe convolution tree kernel, are employed tocompute the anaphoricity similarity between twoNPs and weigh the edge between them.
Experi-ments on the ACE 2003 corpus show that ourLP-based anaphoricity determination signifi-cantly outperforms locally-optimized one, whichadopts a classifier (e.g.
SVM) to determine theanaphoricity of NPs in a text individually andsignificantly improves the performance of learn-ing-based coreference resolution.
It also showsthat, while feature-based anaphoricity determi-nation contributes much to pronoun resolution,its contribution on definite NP resolution can beignored.
In comparison, it shows that tree ker-nel-based anaphoricity resolution contributessignificantly to the resolution of both pronounsand definite NPs due to the inclusion of variouskinds of syntactic structured information.The rest of this paper is organized as follows.In Section 2, we review related work in ana-phoricity determination.
Then, the LP algorithmis introduced in Section 3 while Section 4 de-scribes different similarity measurements ex-plored in the LP algorithm.
Section 5 shows theexperimental results.
Finally, we conclude ourwork in Section 6.2 Related WorkGiven its potential usefulness in coreferenceresolution, anaphoricity determination has beenstudied fairly extensively in the literature andcan be classified into three categories: heuristicrule-based (e.g.
Paice and Husk 1987; Lappinand Leass 1994; Kennedy and Boguraev 1996;Denber 1998; Vieira and Poesio 2000), statis-tics-based (e.g., Bean and Riloff 1999; Cherryand Bergsma 2005; Bergsma et al2008) andlearning-based (e.g.
Evans 2001; Ng and Cardie2002a; Ng 2004; Yang et al2005; Denis andBalbridge 2007).For the heuristic rule-based approaches,Paice and Husk (1987), Lappin and Leass (1994),Kennedy and Boguraev (1996), Denber (1998),and Cherry and Bergsma (2005) looked for par-ticular constructions using certain trigger wordsto identify pleonastic pronouns while Vieira andPoesio (2000) recognized non-anaphoric definiteNPs through the use of syntactic cues and case-sensitive rules and found that nearly 50% ofdefinite NPs are non-anaphoric.
As a representa-tive, Lappin and Leass (1994), and Kennedy andBoguraev (1996) looked for modal adjectives(e.g.
?necessary?)
or cognitive verbs (e.g.
?It isthought that ?
?)
in a set of patterned construc-tions.For the statistics-based approaches, Beanand Riloff (1999) developed a statistics-basedmethod for automatically identifying existentialdefinite NPs which are non-anaphoric.
The intui-tion behind is that many definite NPs are notanaphoric since their meanings can be under-stood from general world knowledge.
Theyfound that existential NPs account for 63% of alldefinite NPs and 76% of them could be identi-fied by syntactic or lexical means.
Using 1600MUC-4 terrorism news documents as the train-ing data, they achieved 87% in precision and78% in recall at identifying non-anaphoric defi-nite NPs.
Cherry and Bergsma (2005) extendedthe work of Lappin and Leass (1994) for large-scale anaphoricity determination by additionallydetecting non-anaphoric instances of it usingMinipar?s pleonastic category Subj.
This is doneby both employing Minipar?s named entity rec-ognition to identify time expressions, such as ?itwas midnight?
?, and providing a number ofother linguistic patterns to match common non-anaphoric it cases, such as in expressions ?darnit?
and don?t overdo it?.
Bergsma et al(2008)proposed a distributional method in detectingnon-anaphoric pronouns by first extracting thesurrounding textual context of the pronoun, thengathering the distribution of words that occurredwithin that context from a large corpus and fi-nally learning to classify these distributions asrepresenting either anaphoric and non-anaphoricpronoun instances.
Experiments on  the ScienceNews corpus of It-Bank 1  in identifying non-anaphoric pronoun it show that their distribu-tional method achieved the performance of81.4%, 71.0% and 75.8 in precision, recall andF1-measure, respectively, compared with theperformance of 93.4%, 21.0% and 34.3 in preci-sion, recall and F1-measure, respectively usingthe rule-based approach as described in Lappinand Leass (1994), and  the performance of66.4%, 49.7% and 56.9 in precision, recall andF1-measure, respectively using the rule-basedapproach as described in Cherry and Bergsma(2005).Among the learning-based methods, Evans(2001) applied a machine learning approach onidentifying the non-anaphoricity of pronoun it.Ng and Cardie (2002a) employed various do-main-independent features in identifying ana-phoric NPs and showed how such information1 www.cs.ualberta.ca/~bergsma/ItBank/979can be incorporated into a coreference resolutionsystem.
Experiments show that their method im-proves the performance of coreference resolu-tion by 2.0 and 2.6 to 65.8 and 64.2 in F1-measure on the MUC-6 and MUC-7 corpora,respectively, due to much more gain in precisioncompared with the loss in recall.
Ng (2004) ex-amined the representation and optimization is-sues in computing and using anaphoricity infor-mation to improve learning-based coreferenceresolution systems.
He used an anaphoricityclassifier as a filter for coreference resolution.Evaluation on the ACE 2003 corpus shows that,compared with a baseline coreference resolutionsystem of no explicit anaphoricity determination,their method improves the performance by 2.8,2.2 and 4.5 to 54.5, 64.0 and 60.8 in F1-measure(due to the gain in precision) on the NWIRE,NPAPER and BNEWS domains, respectively,via careful determination of an anaphoricitythreshold with proper constraint-based represen-tation and global optimization.
However, he didnot look into the contribution of anaphoricitydetermination on coreference resolution of dif-ferent NP types, such as pronoun and definiteNPs.
Yang et al(2005) made use of non-anaphors to create a special class of training in-stances in the twin-candidate model (Yang et al2003) and thus equipped it with the non-anaphoricity determination capability.
Experi-ments show that the proposed method improvesthe performance by 2.9 and 1.6 to 67.3 and 67.2in F1-measure on the MUC-6 and MUC-7 cor-pora, respectively, due to much more gain inprecision compared with the loss in recall.
How-ever, surprisingly, their experiments also showthat eliminating non-anaphors using an ana-phoricity determination module in advanceharms the performance.
Denis and Balbridge(2007) employed an integer linear programming(ILP) formulation for coreference resolutionwhich models anaphoricity and coreference as ajoint task, such that each local model informs theother for final assignments.
Experiments on theNWIRE, NPAPER and BNEWS domains of theACE 2003 corpus shows that this joint ana-phoricity-coreference ILP formulation improvesthe F1-measure by 0.7-1.0 over the coreference-only ILP formulation.
However, their experi-ments assume true ACE mentions(i.e.
all theACE mentions are already known from the an-notated corpus).
Therefore, the actual effect ofthis joint anaphoricity-coreference ILP formula-tion on fully-automatic coreference resolution isstill unclear.3 Label PropagationIn the LP algorithm (Zhu and Ghahramani 2002),the natural clustering structure in data is repre-sented as a connected graph.
Given the labeleddata and unlabeled data, the LP algorithm firstrepresents labeled and unlabeled instances asvertices in a connected graph, then propagatesthe label information from any vertex to nearbyvertices through weighted edges and finally in-fers the labels of unlabeled instances until aglobal stable stage is achieved.
Figure 1 presentsthe label propagation algorithm.___________________________________________Assume:Y : the rn * labeling matrix, where ijy  representsthe probability of vertex )1( nixi K=  withlabel )1( rjr j K= ;LY : the top l  rows of0Y .
LY corresponds to thel  labeled instances;UY : the bottom u  rows of0Y .
UY corresponds tothe u  unlabeled instances;T : a nn *  matrix, with ijt  is the probabilityjumping from vertex ix to vertex jx ;BEGIN (the algorithm)Initialization:1)  Set the iteration index 0=t ;2)  Let 0Y  be the initial soft labels attached toeach vertex;3)  Let 0LY  be consistent with the labeling in thelabeled data, where 0ijy = the weight of thelabeled instance if ix  has the label jr  ;4)  Initialize 0UY ;REPEATPropagate the labels of any vertex to nearby ver-tices by tt YTY =+1 ;Clamp the labeled data, that is, replace 1+tLYwith 0LY ;UNTIL Y converges(e.g.
1+tLY  converges to0LY );Assign each unlabeled instance with a label: for)( nilxi ?p , find its label withjijymaxarg ;END (the algorithm)___________________________________________Figure 1: The LP algorithmHere, each vertex corresponds to an instance,and the edge between any two instances ix  andjx  is weighted by ijw  to measure their similar-ity.
In principle, larger edge weights allow labelsto travel through easier.
Thus the closer the in-stances are, the more likely they have similar980labels.
The algorithm first calculates the weightijw  using a kernel, then transforms itto ?==?=nkkjijij wwijpt1/)( , which meas-ures the probability of propagating a label frominstance jx to instance ix , and finally normal-izes ijt row by row using ?==nkikijij ttt1/  to maintainthe class probability interpretation of the label-ing matrix Y .During the label propagation process, the la-bel distribution of the labeled data is clamped ineach loop using their initial weights and acts likeforces to push out labels through the unlabeleddata.
With this push originating from the labeleddata, the label boundaries will be pushed fasteralong edges with larger weights and settle ingaps along those with lower weights.
Ideally, wecan expect that ijw  across different classesshould be as small as possible and ijw  within thesame class as big as possible.
In this way, labelpropagation tends to happen within the sameclass.
This algorithm has been shown to con-verge to a unique solution (Zhu and Ghahramani2002), which can be obtained without iterationin theory, and the initialization of YU0 (the unla-beled data) is not important since YU0 does notaffect its estimation.
However, proper initializa-tion of YU0 actually helps the algorithm convergemore rapidly in practice.
In this paper, each rowin YU0 , i.e.
the label distribution for each testinstance, is initialized to the weighted similarityof the test instance with the labeled instances.4 Kernel-based SimilarityThe key issue in label propagation is how tocompute the similarity ijw between two in-stances ix  and jx .
This paper examines twosimilarity measures: the feature-based RBF ker-nel and the convolution tree kernel.Feature Type Feature DescriptionIsPronoun 1 if current NP is a pronoun, else 0IsDefiniteNP 1 if current NP is a define NP, else 0IsDemonstrativeNP 1 if current NP is a demonstrative NP,  else 0IsArg0 1 if the semantic role of current NP is Arg0/agent, else 0IsArg0MainVerb 1 if current NP has the semantic role of Arg0/agent for themain predicate of the sentence, else 0IsArgs 0 if current NP has no semantic role, else 1IsSingularNP 1 if current NP is a singular noun, else 0Featuresrelated withcurrent NP itselfIsMaleFemalePronoun 1 if current NP is a male/female personal pronoun, else 0StringMatch 1 if there is a full string match between current NP and oneof other phrases in the context, else 0NameAlias 1 if current NP and one of other phrases in the context is aname alias or abbreviation of the other, else 0Appositive 1 if current NP and one of other phrases in the context arein an appositive structure, else 0NPNested 1 if current NP is nested in another NP, else 0NPNesting 1 if current NP nests another NP, else 0WordSenseAgreement 1 if current NP and one of other phrases in the context agreein the WordNet sense, else 0IsFirstNPinSentence 1 if current NP is the first NP of this sentence, else 0BackwardDistance The distance between current NP and  the nearest backwardclause, indicated by coordinating words (e.g.
that,which).Featuresrelated withthe local contextsurroundingcurrent NPForwardDistance The distance between the nearest forward clause, indicatedby coordinating words (e.g.
that, which), and current NP.Table 1: Features in anaphoricity determination of NPs.
Note: the semantic role-related features are derived froman in-house state-of-the-art semantic role labeling system.4.1 Feature-based KernelIn our feature-based RBF kernel to anaphoricitydetermination, an instance is represented by 17lexical, syntactic and semantic features, asshown in Table 1, which are specifically de-signed for distinguishing anaphoric and non-anaphoric NPs, according to common-senseknowledge and linguistic intuitions.
Since thelocal context surrounding an NP plays a criticalrole in discriminating whether an NP is ana-phoric or not, the features in Table 1 can be clas-sified into two categories: (a) current NP (i.e.
theNP in anaphoricity consideration) itself, e.g.981types and semantic roles of  current NP; (b) con-textual information, e.g.
whether current NP isnested in another NP, the distance between cur-rent NP and a clause structure, indicated by co-ordinating words (e.g.
that, this, which).4.2 Tree KernelGiven a NP in anaphoricity determination, aparse tree represents the local context surround-ing current NP in a structural way and thus con-tains much information in determining whethercurrent NP is anaphoric or not.
For example, thecommonly used knowledge for anaphoricity de-termination, such as the grammatical role of cur-rent NP or whether current NP is nested in otherNPs, can be directly captured by a parse treestructure.Given a parse tree and a NP in consideration,the problem is how to choose a proper parse treestructure to cover syntactic structured informa-tion well in the tree kernel computation.
Gener-ally, the more a parse tree structure includes, themore syntactic structured information would beprovided, at the expense of morenoisy/unnecessary information.
In this paper, welimit the window size to 5  chunks (either NPs ornon-NPs), including previous two chunks, cur-rent chunk (i.e.
current NP) and following twochunks, and prune out the substructures outsidethe window.
Figure 2 shows the full parse treefor the sentence ?Mary said the woman in theroom hit her too?, using the Charniak parser(Charniak 2001), and the chunk sequence de-rived from the parse tree using the Perl script2written by Sabine Buchholz from Tilburg Uni-versity.Here, we explore four parse tree structuresin NP anaphoricity determination: the commontree (CT), the shortest path-enclosed tree (SPT),the minimum tree (MT) and the dynamicallyextended tree (DET), motivated by Yang et al(2006) and Zhou et al(2008).
Following are theexamples of the four parse tree structures, corre-sponding to the full parse tree and the chunk se-quence, as shown in Figure 2, with the NP chunk?
(NP (DT the) (NN woman))?
in anaphoricitydetermination.Common Tree (CT)As shown in Figure 3(a), CT is the completesub-tree rooted by the nearest common ancestorof the first chunk ?
(NP (NNP Mary))?
and the2 http://ilk.kub.nl/~sabine/chunklink/last chunk ?
(NP (DT the) (NN room))?
of thefive-chunk window.Shortest Path-enclosed Tree (SPT)As shown in Figure 3(b), SPT is  the smallestcommon sub-tree enclosed by the shortest pathbetween the first chunk ?
(NP (NNP Mary))?
andthe last chunk ?
(NP (DT the) (NN room))?
of thefive-chunk window.
(a) the full parse tree(NP (NNP Mary)) (VP (VBD said)) (NP-E (DT the)(NN woman)) (PP (IN in)) (NP (DT the) (NN room))(VP (VBD hit)) (NP (PRP her)) (ADVP (RB too))(b) the chunk sequenceFigure 2: The full parse tree for the sentence ?Marysaid the woman in the room hit her too?, using theCharniak parser, and the corresponding chunk se-quence derived from it.
Here, the label ?E?
indicatesthe NP in consideration.
(a) CT: Common Tree(b) SPT: Shortest Path-enclosed Tree982(c) MT: Minimum Tree(d) DET: Dynamically Extended TreeFigure 3: Examples of parse tree structures.Minimum Tree (MT)As shown in Figure 3(c), MT only keeps the rootpath from the NP in anaphoricity determinationto the root node of SPT.Dynamically Extended Tree (DET),The intuitions behind DET are that the informa-tion related with antecedent candidates (all theantecedent candidates compatible3 with currentNP in anaphoricity consideration), predicates 4and right siblings plays a critical role in corefer-ence resolution.
Given a MT, this is done by:1)  Attaching all the compatible antecedent can-didates and their corresponding paths.
Asshown in Figure 3(d), ?Mary?
is attachedwhile ?the room?
is not since the former iscompatible with the NP ?the woman?
andthe latter is not compatible with the NP ?thewoman?.
In this way, possible coreferencebetween current NP and the compatible an-tecedent candidates can be included in theparse tree structure.
In some sense, this is anatural extension of the twin-candidate3 With matched number, person and gender agreements.4 For simplicity, only verbal predicates are considered inthis paper.
However, this can be extended to nominal predi-cates with automatic identification of nominal predicates.learning method proposed in Yang et al(2003), which explicitly models the compe-tition between two antecedent candidates.2)  For each node in MT, attaching the path fromthe node to the leaf node of the correspond-ing predicate, if it is predicate-headed, in thesense that such predicate-related informationis useful in identifying certain kinds of ex-pressions with non-anaphoric NPs, e.g.
thenon-anaphoric it in ?darn it?.
As shown inFigure 3(d), ?said?
and ?hit?
are attached.3)  Attaching the path to the head word of thefirst right sibling if the parent of current NPis a NP and current NP has one or more rightsiblings.
Normally, the NP in anaphoricityconsideration, NP-E, in the production of?NP->NP-E+PP?
introduces a new entityand thus non-anaphoric.4)  Pruning those nodes (except POS nodes)with the single in-arc and the single out-arcand with its syntactic phrase type same as itschild node.In this paper, the similarity between twoparse trees is measured using a convolution treekernel, which counts the number of commonsub-trees as the syntactic structure similaritybetween two parse trees.
For details, please referto Collins and Duffy (2001).5 ExperimentationWe have systematically evaluated the labelpropagation algorithm on global learning of NPanaphoricity determination on the ACE 2003corpus, and its application in coreference resolu-tion.5.1 Experimental SettingThe ACE 2003 corpus contains three domains:newswire (NWIRE), newspaper (NPAPER), andbroadcast news (BNEWS).
For each domain,there exist two data sets, training and devtest,which are used for training and testing respec-tively.As a baseline coreference resolution system,a  raw test text is first preprocessed automati-cally by a pipeline of NLP components, includ-ing sentence boundary detection, POS tagging,named entity recognition and phrase chunking,and then a training or test instance is formed bya anaphor and one of its antecedent candidates,similar to Soon et al(2001).
Among them,named entity recognition, part-of-speech taggingand noun phrase chunking apply the same Hid-den Markov Model (HMM)-based engine with983error-driven learning capability (Zhou and Su,2000 & 2002).
During training, for each anaphorencountered, a positive instance is created bypairing the anaphor and its closest antecedentwhile a set of negative instances is formed bypairing the anaphor with each of the non-coreferential candidates.
Based on the traininginstances, a binary classifier is generated using aparticular learning algorithm.
In this paper, weuse SVMLight developed by Joachims (1998).During resolution, an anaphor is first paired inturn with each preceding antecedent candidate toform a test instance, which is presented to aclassifier.
The classifier then returns a confi-dence value indicating the likelihood that thecandidate is the antecedent.
Finally, the candi-date with the highest confidence value is se-lected as the antecedent.
As a baseline, the NPswith mismatched number, person and genderagreements are filtered out.
On average, an ana-phor has ~7 antecedent candidates.
In particular,the test corpus is resolved in document-level, i.e.one document by one document.For anaphoricity determination, we reportthe performance in Acc+ and Acc-, which meas-ure the accuracies of identifying anaphoric NPsand non-anaphoric NPs, respectively.
Obviously,higher Acc+ means that more anaphoric NPswould be identified correctly, while higher Acc-means that more non-anaphoric NPs would befiltered out.
For coreference resolution, we re-port the performance in terms of recall, precision,and F1-measure using the commonly-usedmodel theoretic MUC scoring program (Vilainet al, 1995).
For separate scoring of differentNP types, a recognized reference is consideredcorrect if the reconized antecedent is in thecoreferential chain of the anaphor.
To seewhether an improvement is significant, we con-duct significance testing using paired t-test.
Inthis paper, ?>>>?, ?>>?
and ?>?
denote p-valuesof an improvement smaller than 0.01, in-between (0.01, 0,05] and bigger than 0.05,which mean significantly better, moderatelybetter and slightly better, respectively.5.2 Experimental ResultsTable 2 shows the performance of LP-based ana-phoricity determination using the feature-basedRBF kernel.
It shows that our method achievesthe accuracies of 74.8/84.4, 76.2/81.3 and71.8/81.7 on identifying anaphoric/non-anaphoric NPs in the NWIRE, NPAPER andBNEWS domains, respectively.
This suggeststhat our approach can effectively filter out about82% of non-anaphoric NPs.
However, it canonly keep about 74% of anaphoric NPs.
Table 2also shows the performance on different NPtypes.
Considering the effectiveness of ana-phoricity determination on indefinite NPs (dueto that most of anaphoric indefinite NPs are inan appositive structure and thus can be easilycaptured by the IsAppositive feature) and thatmost of errors in anaphoricity determination onproper nouns are caused by the named entityrecognition module in the preprocessing), it in-dicates the difficulty of anaphoricity determina-tion in filtering out non-anaphoric pronouns andidentifying anaphoric definite NPs.
As a com-parison, Table 2 also shows the performance oflocally-optimized anaphoricity determinationusing a classifier (SVM with the feature-basedRBF kernel, as adopted in this paper) to deter-mine the NPs in a text individually.
It shows thatthe LP-based method systematically outperforms(>>>) the SVM-based method.
This suggests theeffectiveness of the LP algorithm in global mod-eling of the natural clustering structure in ana-phoricity determination.Table 3 shows the performance of LP-basedanaphoricity determination using the convolu-tion tree kernel on different parse tree structures.It shows that while MT performed worst due toits simple structure, DET outperforms MT(>>>),SPT(>>>) and CT(>>>) on all the three domainsdue to fine inclusion of necessary structural in-formation, although inclusion of more informa-tion in both CT and SPT also improves the per-formance.
It again verifies that LP-based ana-phoricity determination outperforms (>>>)SVM-based one, using the tree kernel.
Table 4further indicates that all the three kinds of struc-tural information related with antecedent candi-dates, predicates and right siblings in DET con-tribute significantly (>>>).
In addition, Table 5shows the detailed performance of LP-basedanaphoricity determination on different anaphortypes using DET.
Compared with the feature-based RBF kernel as shown in Table 2, it showsthat the convolution tree kernel significantlyoutperforms (>>>) the feature-based RBF kernelin all the three domains, with much contributiondue to performance improvement on both pro-nouns and definite NPs, although the tree kernelperforms moderately worse than the feature-based RBF kernel due to the effectiveness ofanaphoricity determination on proper nouns andindefinite NPs using the IsNameAlias and IsAp-positive features respectively.984NWIRE NPAPER BNEWS AnaphorType Acc+(%)Acc-(%)Acc+(%)Acc-(%)Acc+(%)Acc-(%)Pronoun 88.7 56.2 90.2 58.6 87.4 57.8ProperNoun 72.5 85.2 74.6 80.5 70.6 78.8DefiniteNP 66.6 83.1 72.1 77.5 65.3 81.5InDefiniteNP 95.4 93.7 90.5 95.8 87.2 97.3Overall 74.8 84.4 76.2 81.3 71.8 81.7Overall(SVM) 71.3 80.2 73.5 79.1 68.4 78.6Table 2: The performance of LP-based anaphoric-ity determination using the feature-based RBF kernelParse Tree structureSchemeNWIRE( %)NPAPER( %)BNEWS( %)Acc+ 72.6 74.3 74.2 CT Acc- 82.1 80.2 72.3Acc+ 72.4 74.1 73.8 SPT Acc- 80.8 79.5 72.5Acc+ 71.4 70.5 66.9 MT Acc- 77.2 75.3 78.2Acc+ 79.2 81.2 76.5 DET Acc- 87.8 84.5 85.3Acc+ 76.5 78.9 74.3 DET(SVM)Acc- 82.3 81.6 83.2Table 3: The performance of LP-based anaphoric-ity determination using the convolution tree kernel ondifferent parse tree structuresPerformance Change NWIRE( %)NPAPER( %)BNEWS( %)Acc+ -4.0 -3.8 -4.3 - antecedentcandidates Acc- -5.2 -5.3 -4.5Acc+ -5.2 -4.8 -5.6 -predicate Acc- -4.3 -3.5 -4.9Acc+ -3.6 -4.1 -3.1 -first rightsibling Acc- -4.8 -5.2 -4.4Table 4: The contribution of structural informationin DETNWIRE NPAPER BNEWS AnaphorType Acc+(%)Acc-(%)Acc+(%)Acc-(%)Acc+(%)Acc-(%)Pronoun 90.1 75.6 90.7 79.2 89.2 77.5ProperNoun 71.4 83.5 72.8 78.1 68.3 77.2DefiniteNP 74.6 89.1 77.3 85.5 75.3 88.7InDefiniteNP 93.2 92.1 90.2 94.2 89.4 95.5Overall 79.2 87.8 81.2 84.5 76.5 85.3Table 5: The performance of LP-based anaphoric-ity determination using the tree kernel on DETFinally, we evaluate the effect of LP-basedanaphoricity determination on coreference reso-lution by including it as a preprocessing step to abaseline coreference resolution system withoutexplicit anaphoricity determination, which em-ploys the same set of features, as adopted in thesingle-candidate model of Yang et al(2003),using a SVM-based classifier and the feature-based RBF kernel.
It shows that anaphoricitydetermination with the feature-based RBF Ker-nel much improves (>>>) the performance ofcoreference resolution with most of the contribu-tion due to pronoun resolution while its contri-bution on definite NPs can be ignored.
It indi-cates the usefulness of anaphoricity determina-tion in filtering out non-anaphoric pronouns andthe difficulty in identifying anaphoric definiteNPs, using the feature-based RBF kernel.
It alsoshows that tree kernel-based anaphoricity deter-mination can not only improve (>>>) the per-formance on pronoun resolution but also im-prove (>>>) the performance on definite NPresolution due to the much better performance oftree kernel-based anaphoricity determination ondefinite NPs.
This suggests the necessity of ex-ploring structural information in identifyinganaphoric definite NPs.NWIRE NPAPER BNEWS SystemR% P% F R% P% F R% P% FPronoun 66.5 61.6 64.0 70.1 64.2 67.0 61.7 63.2 62.4DefiniteNP 26.9 80.3 40.2 34.5 62.4 44.4 30.5 71.4 42.9 BaseLine (No Anaphoricity)Overall 53.1 67.4 59.4 57.7 67.0 62.1 48.0 65.9 55.5Pronoun 64.1 67.9 66.0 67.3 72.4 69.8 59.5 75.7 66.6DefiniteNP 26.7 80.6 40.3 34.2 62.5 44.3 30.4 71.9 43.1 +Anaphoricity determination  with the feature-based RBF kernel Overall 50.6 75.4 60.7 54.4 77.1 63.8 45.9 76.9 57.4Pronoun 63.5 70.9 67.0 68 74.9 71.3 61.1 77.6 68.3DefiniteNP 28.5 82.4 42.1 36.2 65.3 46.1 32.3 73.1 44.2 +Anaphoricity determination with the convolution tree kernel Overall 51.6 77.2 61.8 55.2 78.6 65.2 47.5 80.3 59.6Table 6: Employment of anaphoricity determination in coreference resolution6 ConclusionThis paper systematically studies a global learn-ing method in identifying the anaphoricity ofnoun phrases via a label propagation algorithmand the application of an explicit anaphoricitydetermination module in improving learning-based coreference resolution.
In particular, twokinds of kernels, i.e.
the feature-based RBF ker-nel and the convolution tree kernel, are em-ployed to compute the anaphoricity similarity985between two NPs.
Evaluation on the ACE 2003corpus indicates that LP-based anaphoricity de-termination using both the kernels much im-proves the performance of coreference resolu-tion.
It also shows the usefulness of variousstructural information, related with antecedentcandidates, predicates and right siblings, in  treekernel-based anaphoricity determination and incoreference resolution of both pronouns anddefinite NPs.To our knowledge, this is the first system-atic exploration of both feature-based and treekernel methods in anaphoricity determinationand the application of an explicit anaphoricitydetermination module in learning coreferenceresolution.AcknowledgementThis research is supported by Project 60873150under the National Natural Science Foundation ofChina, project 2006AA01Z147 under the  ?863?National High-Tech Research and Development ofChina, project 200802850006 under the NationalResearch Foundation for the Doctoral Program ofHigher Education of China.ReferencesBean D. and Riloff E. (1999).
Corpus-based Identifi-cation of Non-Anaphoric Noun Phrases.ACL?1999:373-380.Bergsma S., Lin D.K.
and Goebel R. (2008).
Distri-butional Identification of Non-Referential Pro-nouns.
ACL?2008: 10-18.Charniak E. (2001).
Immediate-head Parsing for Lan-guage Models.
ACL?2001: 129-137.Cherry C. and Bergsma S. (2005).
An expectationmaximization approach to pronoun resolution.CoNLL?2005:88-95.Collins M. and Duffy N. (2001).
Convolution kernelsfor natural language.
NIPS?2001: 625-632.Denber M. (1998).
Automatic Resolution of Anaph-ora in English.
Technical Report, Eastman KodakCo.Denis P. and Baldridge J.
(2007).
Joint determinationof anaphoricity and coreference using integer pro-gramming.
NAACL-HLT?2007:236-243.Evans R. (2001).
Applying machine learning towardan automatic classification of it.
Literary and Lin-guistic Computing, 16(1):45.57.Joachims T. (1998).
Text Categorization with Sup-port Vector Machine: learning with many relevantfeatures.
ECML-1998: 137-142.Kennedy C. and Boguraev B.
(1996).
Anaphora foreveryone: pronominal anaphora resolution withouta parser.
COLING?1996: 113-118.Lappin S. and Leass H.J.
(1994).
An algorithm forpronominal anaphora resolution.
ComputationalLinguistics, 20(4):535.561.Ng V. and Cardie C. (2002a).
Identifying anaphoricand non-anaphoric noun phrases to improvecoreference resolution.
COLING?2002:730-736.Ng V. and Cardie C. (2002b).
Improving machinelearning approaches to coreference resolution.ACL?2002: 104-111Ng V. (2004).
Learning Noun Phrase Anaphoricity toImprove Conference Resolution: Issues in Repre-sentation and Optimization.
ACL?2004: 151-158Paice C.D.
and Husk G.D. (1987).
Towards the auto-matic recognition of anaphoric features in Englishtext: the impersonal pronoun it.
Computer Speechand Language, 2:109-132.Soon W.M., Ng H.T.
and Lim D. (2001).
A machinelearning approach to coreference resolution ofnoun phrase.
Computational Linguistics, 2001,27(4):521-544.Strube M. and Muller C. (2003).
A machine learningapproach to pronoun resolution in spoken dialogue.ACL?2003: 168-175Vieira R. and Poesio M. (2000).
An empiricallybased system for processing definite descriptions.Computational Linguistics, 27(4): 539?592.Vilain M., Burger J., Aberdeen J., Connolly D. andHirschman L. (1995).
A model theoretic corefer-ence scoring scheme.
MUC-6: 45?52.Yang X.F., Zhou G.D., Su J. and Chew C.L.
(2003).Coreference Resolution Using Competition Learn-ing Approach.
ACL?2003:177-184Yang X.F., Su J. and Tan C.L.
(2005).
A Twin-Candidate Model of Coreference Resolution withNon-Anaphor Identification Capability.IJCNLP?2005:719-730.Yang X.F., Su J. and Tan C.L.
(2006).
Kernel-basedpronoun resolution with structured syntacticknowledge.
COLING-ACL?2006: 41-48.Yang X.F., Su J., Lang J., Tan C.L., Liu T. and Li S.(2008).
An Entity-Mention Model for CoreferenceResolution with Inductive Logic Programming.ACL?2008: 843-851.Zhou G.D. and Su.
J.
(2000).
Error-driven HMM-based chunk tagger with context-dependent lexi-con.
EMNLP-VLC?2000: 71?79Zhou G.D. and Su J.
(2002).
Named Entity recogni-tion using a HMM-based chunk tagger.
InACL?2002:473?480.Zhou G.D., Kong F. and Zhu Q.M.
(2008).
Context-sensitive convolution tree kernel for pronounresolution .
IJCNLP?2008:25-31.Zhu X. and Ghahramani Z.
(2002).
Learning fromLabeled and Unlabeled Data with Label Propaga-tion.
CMU CALD Technical Report.CMU-CALD-02-107.986
