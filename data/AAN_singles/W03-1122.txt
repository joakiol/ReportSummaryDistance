Question-Answering Based on Virtually Integrated LexicalKnowledge BaseKey-Sun ChoiKAIST,KortermDaejeon305-701 Koreakschoi@cs.kaist.ac.krJae-Ho KimKAIST,KortermDaejeon305-701 Koreajjaeh@world.kaist.ac.krMasaruMiyazakiNHK STRLTokyo 157-8510Japanmiyazaki.m-fk@nhk.or.jpJun GotoNHK STRLHuman ScienceTokyo 157-8510Japangoto.j-fw@nhk.or.jpYeun-Bae KimNHK STRLHuman ScienceTokyo 157-8510Japankimu.y-go@nhk.or.jpAbstractThis paper proposes an algorithm for cau-sality inference based on a set of lexicalknowledge bases that contain informationabout such items as event role, is-a hier-archy, relevant relation, antonymy, andother features.
These lexical knowledgebases have mainly made use of lexicalfeatures and symbols in HowNet.
Severaltypes of questions are experimented totest the effectiveness of the algorithm hereproposed.
Particularly in this paper, thequestion form of ?why?
is dealt with toshow how causality inference works.1 IntroductionA virtually linked knowledge base is designed toutilize a pre-constructed knowledge base in a dy-namic mode when it is in actual use.An open-domain question answering architec-ture must consist of various components andprocesses (Pas?a, 2001) that include WordNet-like resources, part of speech tagging, parsing,named entity recognition, question processing,passage retrieval, answer extraction, and answerjustification.
Consider a question like the follow-ing: ?Why do doctors cure patients?
?The answer may be obtained by commonsenseknowledge as follows:1.
A patient suffered from adisease.2.
A doctor cures the disease.3.
The doctor cures at hospi-tal.4.
Doctor is an occupation.5.
So the doctor cures thepatient.These sentences are transformed into proposi-tional forms, as illustrated below:6. sufferFrom(patient,disease)7. cure(doctor,disease)8. cure(doctor,at-hospital)9. occupation(doctor)10. cure(doctor,patient)Linguistic knowledge bases like WordNet(Miller, 1995), EDR dictionary (Yokoi, 1995) andHowNet (Dong, 1999) have been used to interpretthese sentences.Moldovan et al (2002) generated lexical chainsfrom WordNet in order to trace these topically re-lated paths and thereby to search for causal expla-nations.
A conceptual word Cj inside of a glossunder a synset Ci is linked to the synset Cj.HowNet (Dong et al 1999) is a linguisticknowledge base that is designed to have the defini-tion of words and concepts as well as event roleand role-filling entities.
Commonsense knowledgelike naive physics is also built up through eventrole relation like the relation of sufferFrom requir-ing cure.HowNet is modularized into separate knowl-edge spaces for entity hierarchy, event hierarchy,antonymy, syntax, attributes, etc.
Relations be-tween various concepts (e.g., part-of, relevance,location) are defined implicitly in the definition ofeach concept.This paper will focus on building an algorithmthat allows for searching for some topical paths inorder to find causal explanations for questions like?Why do doctors cure patients??
or ?Why do pa-tients pay money??
as illustrated in Figure 1.patient doctor occupation money$cure *cure earn $earn#occupationconverseagent=patientpossession=moneytarget=?agent=?possession=moneysource=patiententitysynevent*pay $paygive take(1)(2) (3)(4)(5)(6)(7)(8)(9)Figure 1: A Snapshot of a virtually integratedknowledge base for the question: ?Why do patientspay money to doctors?
?In the following sections, issues on the virtual in-tegration of knowledge bases, their algorithms andexperimentations are presented.2 Underlined Knowledge Bases and Vir-tual IntegrationIn Figure 1, each marked numbering has the fol-lowing meaning:(1) Entity hierarchy: entity is the top node inthe hierarchy of entities.
(2) entity is the hypernym of patient, doctor,occupation, and money in the line (3).
(3) Concepts or word entries are listed in thisline.
All concepts and word entries repre-sent their definition by a list of conceptsand marked pointers.
(4) A concept (or word) in (3) features defini-tional relations to a list of concepts.
Forexample, a doctor definition is composedof two concepts and their marking point-ers: #occupation and *cure.
Pointers inHowNet represent relations between twoconcepts or word entries, e.g., ?#?
means?relevant?
and ?*?
does ?agent?.
(5) syn refers to the syntactic relation in thequestion ?Why do patients pay money todoctors??
(6) converse refers to the converse relation be-tween events, e.g., give and take.
(7) Event hierarchy: For example, the hy-pernym for pay is give and the hypernymof give is event.
(8) Event role: Now, event roles are partiallyfilled with entities, e.g., patient andmoney.
(9) Event role shift: The agent of give isequalized to the source of take.An overview of each component of the knowl-edge base is in Figure 2, where three word entrieswhy, patient, and money are in the dictionary.The four concept facets of entity, role, event, andconverse are described in this example, mainly aspart of linguistic knowledge.paygive takeagent=possession=target=agent=possession=source=Alter-possessionpatientdoctor occupation moneycure*cure earn $earn#occupationentitygive takeconverseeventearnhumanpaywhyrolequestioncausedictionary ConceptfacetsFigure 2: HowNet Architecture in Example.Some issues on ontology integration have beendiscussed from various points of view.
Pinto et al(1999) classified the notions of ontology integra-tion into three types: integration, merging anduse/application.
The term virtually integratedmeans the view of ontology-based use/application.This paper presents issues on and arguments forlinguistic knowledge base and commonsenseknowledge in (Lenat, Miller and Yokoi, 1995).One of the arguments was whether linguisticknowledge could be separated from commonsenseknowledge, but it was agreed that both types ofknowledge were essentially required for naturallanguage processing.This paper was motivated by the desire to makeinferences using a lexical knowledge base, thussuccessfully carrying out a kind of commonsensereasoning.3 Interpretation of Lexical KnowledgeConsider the following three sentences:11.
Doctors cure patients.12.
Doctors earn money.13.
Patients pay money.One major concern is finding connectabilityamong words and concepts.
As shown in Figure 2,the following facts are derived:14.
Doctor is relevant to oc-cupation.15.
Occupation allows you toearn money.Because there exists a converse relation be-tween give and take, their hyponyms earn and payalso fall under converse relation.
It is somethinglike the following social commonsense as shown inFigure 2: ?If someone X pays money to the other Y,Y earns money from X.?We humans now understand the reason for?why patients pay money.?
The answer is that?doctors cure patients as their occupation allowingthem to earn money.
?The following is a valid syllogism where Y isbeing instantiated to doctor:If ?X pays money to Y?
isequivalent to ?Y earns moneyfrom X?1, and ?a doctor earnsmoney from X?, then ?X paysmoney to the doctor?.Consider the next syllogism: If ?a doctorcures X?
and ?doctor is an occupa-tion?
and Axiom 1, then ?the doc-tor earns money from X?.Axiom 1 is needed to make such a syllogismthat ?If Y cures X and Y is an occu-pation, then Y earns money fromX.?
Then our challenge is to find out this Axiom1 from the lexical knowledge bases.
It is a com-monsense and thus there is a gap in the lexicalknowledge base.The following is a list of questions derivedfrom the three sentences of 11, 12 and 13 whichare designed to discover such axioms (or rules)from a set of lexical knowledge bases: ?Why do1 It is a converse relation.doctors cure patient?
?, ?Why do doctors earnmoney?
?, and ?Why do patients pay money to doc-tors?
?4 Connectability: Similarity MeasureConsider the query ?Why do doctors cure pa-tients??
Tracing Figure 2 back through Figure 1leads to obtaining logical forms from 6 through 10.The best connectable path is planned from the firstword of the question.For each pair of words, the function called"similar(*,*)" will be estimated to choose the nextbest tracing concepts (or words).
similar's mis-sions are summarized as (1) checking the connect-ability between two nodes2, (2) selecting the bestsense of the node,3 (3) selecting the best tracingcandidate node in the next step.
Finally, followingthe guidance by similar allows us to explain thequestion.4.1 Observation and Evidence of Topical Re-latednessLet's try to follow the steps 6-10 given in the logi-cal forms.
In the question ?Why do doctors curepatient??
that focuses on three words doctor, cure,and patient, we can trace some key words given inexample sentences as follows: patient ~ disease ~cure ~ doctor ~ occupation ~ earn ~ pay ~ pa-tient.What kind of lexical relations are relevant toeach pair of words or concepts?
Their observationcan be summarized as follows:A) The relation between patient ~ disease is arole relation of ?sufferFrom(patient, dis-ease)?.B) A sequence of cure ~ doctor ~ occupation~ earn lets us infer the relation amongcure ~ earn, which are closely linked bytheir relevance relation to occupation.Furthermore, earn and cure shares acommon subject of these two events.C) The sequence of earn ~ pay is the result ofa converse event relation between earnand pay.D) pay ~ patient: The agent of pay is a ge-neric human.
In other words, pay is a hy-2 A node means either concept or word.3 It is similar with word sense disambiguation.ponym for the act of human, one of whosehyponym is patient.Consider again the match between the tracingsequences of concepts and the knowledge base.Going into more details, notations with footnoteswill be given to each example.
At this point, wewill give names and formalization based on theobserved characteristics.A) Feature comparison: To find the role re-lation among patient ~ disease, search thedefinition of entities (referring to patientand disease) in ways that two entities sharethe same event concept (referring tocure):4patient ?
human?$cure ?
*sufferFrom.disease ?
medical?$cure ?
undesired.B) Interrelation: To find the event interrela-tion among cure ~ earn, two possiblepaths are presented as follows.?
First, inverse interrelation: Two event'srole entities can be found by searching all ofentities using *earn ~ *cure that share thesame subject, and using *earn ~ $curewhere the subject of earn is the object ofcure.?
Second, sister interrelation: The followinglogical form can be derived from Figure 2:5doctor ?
*cure ?
#occupation.occupation ?
earn.Because cure and occupation is in the defi-nition of doctor, a probable logical implica-tion can be derived as follows:6*cure ?
~#occupation.C) Converse/antonymy: earn and pay havetheir respective hypernyms take and give.There exists a converse relation betweenthese two hypernyms.4 According to HowNet convention, ?$?
represents patient,target, possession, or content of an event, and ?*?
representsagent, experiencer, or instrument.
???
means implies or hasfeatures.5 ?#?
means ?relevant?.6 ?~?
means ?very probable?.D) Inheritance: The relation among pay ~patient is represented as follows:7humanpatientacthumanactpaypp*?4.2 Rationale of ConnectabilityIn the former section, we summarized four charac-teristics8 of causality (relatedness)-based path find-ing: feature comparison, interrelation,converse/antonymy in their hypernym?s level, andinheritance.
Among search spaces available, it isnecessary to find out a measure of guiding the op-timal9 path tracing.We will call such a measure similar which willbe defined according to the four characteristics justmentioned.
Further details about the calculationformula will be presented again later.A) For ?feature comparison?, the measure fea-ture similar(X,Y) defines the notion ofsimilarity between the features in X and Y.B) There are two interrelations in the last sec-tion.?
For ?inverse interrelation?
', inverse simi-lar(X,Y) calculates how much similarity ex-ists between X?
and Y?
in a manner that X?= {Z | Z ?
?X}, where ?X is an abstractionof role-marked concepts like *X, $X, #X,etc.
Thus inverse similar(X,Y) = simi-lar(X?,Y?).?
For ?sister interrelation?, the measure sistersimilar(X,Y) means the following two situa-tions: First, X and Y are features to defineone concept (say, W).
Second, one of them,say, Y's definitional feature concepts (refer-ring to Z) are similar with X such that X andZ are similar if W ?
X ?Y and Y ?
Z.C) Converse or antonymy: The converse re-lation converse(X,Y) can be found by themeasure feature similar.
converse(X,Y) isformulated by X ?
?Y and Y ?
?X where?
= converse.7 ?
YX p ?
means ?Y is hypernym of X?.8 Their exhaustiveness should be discussed later.9 ?optimal?
will not be discussed.D) Using inheritance property in the concepthierarchy, relations between hypernym ofconcepts X and Y are inherited to X and Yin a way that X and Y is similar if thereexist X?
and Z such that 'XX p , Z ?
?X?,and ZY p  where ?
is a pointer or null.This inheritance tracing can be determinedby how much similar X and Y are in termsof their path upward based on the relationof hypernym.
We will define path similar.But tracing the path upward following hy-pernym links is to be described later ac-cording to the algorithm.A measure called similar will be defined basedon the discussion in this section.
Then an algorithmis introduced through this measure with an exam-ple.5 MeasuresIn the last section, we discussed four kinds of themeasure similar.?
path similar,?
feature similar,?
inverse similar,?
sister similar.For feature, inverse, and sister similar func-tions, path similar is used as a basis of calculation.They are different with respect to both their searchmethod and the depth of expanding features.
fea-ture similar finds similar features by using pathsimilar.
inverse similar(X,Y) searches for entriesthat contain X and Y as features and then use thepath similar.
In the same way, sister similar findssister concepts, expands them, and finally meas-ures using the path similar.Since path similar plays a key role in all thesesearch and measure processes, its role will be ex-plained in the next subsection.
Other measures areonly dealt with as part of the algorithm.5.1 Similarity Based on Hierarchy and Fea-tureThe mission of the measuring function simi-lar(X,Y) is to calculate their relevancy betweentwo concepts or words whether they are of typeentity, event, or of some other type.If X and Y belong to different types of knowl-edge plane (e.g., entity and event), it is hard tocompare their hypernym path upward to the topconcept.
However, if different types of conceptshave any relevance to (connect) causality, we willuse feature similar or inverse similar after find-ing the same type of concepts to calculate the pathsimilar.
Now we will explain the above by usingtwo pairs of concept type: entity-entity and entity-event, without loss of generality.First, pathsimilar(entity X, entity Y) is de-fined as follows:)()()()(2),(YpathXpathYpathXpathYXrpathsimila+++++?
?=where path+(X) is the ordered list of hypernym forX by descending order from the top concept.
Forexample,path+(doctor)= [entity...animate...human.doctor]path+(patient)= [entity...animate...human.patient]Because |path+(X)| counts the number of nodes onthe path, pathsimilar(doctor,patient) = 2?6/(7+7)=0.857.Second, pathsimilar(entity N, event V) is de-fined as follows:pathsimilar(N,V)= Max pathsimilar(N.feature,V)where N.feature means the feature list in the defi-nition of N. The following is an illustrative exam-ple for the definition:money ?
$earn,*buy,#sell, $setAside,it is equivalent to the following:money.feature=[$earn,*buy,#sell,$setAside].So pathsimilar(money,earn)=pathsimilar(earn,earn)=1.
According to this Max function, the selectionpriorities for the path can be specified.Third, pathsimilar(event V, entity N) is de-fined by inverse similar as follows: pathsimi-lar(V,N) = Max pathsimilar(V.inverse, N).
Forexample, pathsimilar(cure, doctor) = Max path-similar(cure.inverse, doctor) = Max pathsimi-lar({doctor, medical worker, medicine, patient},doctor).Fourth, pathsimilar(event X, event Y) sharesthe same formula with pathsimilar(entity X, en-tity Y) shown before.
But, we can give anotherinverse pathsimilar(event X, event Y) = Maxpathsimilar(X.inverse, Y.inverse).5.2 Logical Implication and Expansion DepthAll of the relations in Figure 2 are translated intological form (see below).
As shown in ?Interpreta-tion as Abduction?
(Hobbs et al 1988), ?abductiveinference is inference to the best explanation?.These relations showed ?the interpretation of a textis the minimal explanation of why the text wouldbe true?
based on the abductive inference.
By thesame token, ?the interpretation of a question is theminimal explanation of why the question would betrue?
based on a set of lexical knowledge bases.Before proceeding to our algorithm, an examplewill be applied to abductive inference briefly as aset of logical forms as well as a diagram in Figure3.16.
doctor ?
human, #occupation,*cure, medical.17.
medicine ?
*cure.18.
disease ?
$cure.19.
cure ?
medical,{agent,patient,content}.20. medical ?
#cure.21.
converse(pay,earn) ?agent=source,target=agent.22.
patient ?
human,$cure.23.
occupation ?
affairs, earn.24.
cause(cure,sufferFrom) ?patient=experiencer,content=content.25.
possibleConsequence(cure,beRecovered) ?patient=experiencer,content=stateIni.While pursuing the path tracing enabling mini-mal explanation, now we are going to proposea connectability measure similar such as?weighted abduction?
(Hobbs et al 1988).
As?likelihood estimation?
is useful to consider a?bounded conditioning?
(Russell & Norvig, 1995)in a belief network, the ?expansion depth?
of simi-lar will be useful for the explanation path tracingfor the purpose of the minimal explanation of thequestion.commercial$earn*buy#sell$setAsidepatient pay moneywhyhuman*sufferFrom$cureagentcontentsourcepayer*moneyadvanced$doctorgivehypernymtakehypernymoccupation affirsearnhuman#occupation*curemedicalinverseconverseFigure 3: Virtual Linking for CausalityThe ?expansion depth level?
of similar has twokinds of utilities: one is to find the minimal expla-nation, and the other is to be dynamically adapt-able to the level of interaction.
This level ofsimilar is defined as a function simi-lar(Level)(X,Y) for X and Y, concepts or words inthe following manner:?
similar(0)=pathsimilar: they use only them-selves and their hypernym path from X andY.?
similar(1)=feature_similar: they use theirfeatures that are expanded one more thansimilar(0).?
similar(2)=inverse_similar?
similar(3)=sister_similar=inverse_similar?feature_similar.Depending on what level of similar is chosen,the search paths may be changed.
A snapshot up tosimilar(2) is given in Figure 4.Figure 4: Snapshot for similar(2).human* sufferFrom$curedoctorcure patient whyhuman#occupation*curemedicalagentpatientcontentmedicalmedicine*disease&medical#ne*se$l#6 Tracing Algorithms6.1 Algorithm CrossoverThe overall algorithm 10  flow depends on simi-lar(Level) as in the next program.Algorithm CrossoverFor Level=0...N until stoppingcondition is satisfied:Expand the traceby similar(Level)For example, when Level=1, the algorithm cross-over finds a very primitive answer to the question?Why do doctors cure patients??
We will expandother features of doctor except for cure becausecure has a syntactic relation between doctor andpatient.As shown in the logical forms (16~24) intro-duced in the previous section, this algorithm inLevel=1 can find the following concepts as a re-sult: medical, human, cure ($cure, *cure).When Level=2, the algorithm crossover willseek higher-order relations (like the hypothesis)from the concept (by inverse_similar), con-verse/antonymy relations (by feature_similar),and    event relations (if any, for use in knowingthe cause or consequence relation).
Consider againour example "Why do doctors cure patients?"
byusing the previous section's logical forms.
The re-sults are as follows:*cure = {doctor, medicine}$cure = {patient, disease}*sufferFrom = {patient}$sufferFrom = {disease}Its generated meaning may be ?If a doctor cures apatient, the patient is recovered from disease.Because patients suffer from diseases, doctors curethe patients.
Patients are recovered after gettingcured.
?6.2 Stopping ConditionStopping conditions for the algorithm crossoverare as follows:(1) Event roles are filled up.
(2) If no event is found in the feature defini-tion, increase similar level.10 This algorithm will be called ?crossover?.
(3) [weak stopping condition] When there isno event, one of the other features is com-monly shared between two concepts.
Forexample, medical is a common feature be-tween doctor and cure.6.3 Hypernym ClimbingIn section 4.2, inheritance was discussed for thepurpose of finding a relation among pay ~ patient.After trying to make Level=2 in section 5.2, wehave been motivated to find the interrelation be-tween hypernyms.
The algorithm crossover is up-dated.Algorithm Crossover+For Level=0..N until stoppingcondition is satisfied:Expand the traceby similar(Level)If Level >= 2, thenrepeat climb up hypernymuntil it matches withthe higher relation.6.4 Algorithm Crossover++Consider again the question "Why do patients paymoney to doctors?"
As shown in Figure 1, the besttrace is $cure ~ *cure ~ *earn ~ $pay.
It providesan explanation for the statement that ?patients arecured by doctors ~ doctors earn money ~ patientspay money to doctors?.
This minimal explanationis observed by switching over the role pointers ?whenever tracing is performed.
For example,$cure was switched over to *cure.
This extendedversion of algorithm is called Crossover++.7 EvaluationBy the algorithm Crossover?s, the behavior of?why?-type questions are investigated by extract-ing the answer paths as follows.Q: Why does patient pay money?Path: patient ~ $cure ~ doctor ~ #occupation ~$earn ~ moneyQ: Why does researcher read textbook?Path: researcher ~ #knowledge ~ #information ~readings ~ textbookPaths between two concepts can now be foundby simply checking the presence of a path amongthe concepts reached from an initial concept.
Table1 and Table 2 show examples of the number ofpaths as a function of path size.Reached concepts path size Sourceconcept 1 2 3cure 275 593 24854eat 268 605 24903study 276 358 23172food 532 650 18066human 6713 3686 51171money 328 1312 19827Table 1: Examples of destination concepts reachedstarting from one source conceptPaths number length Concept1 Concept2 1 2 3cure human 0 78 26pay money 0 7 3human money 0 3 7food human 0 0 28read write 0 4 6earn pay 0 0 7Table 2: The number of paths between pairs ofconcepts8 DiscussionHowNet (Dong et al 1999-2003) has already de-fined the words and concepts using the features ofconcepts.
Each event role is also defined under thenotion of feature.
On the other hand, WordNet(Miller, 1995) consists of synsets and their glosses.Moldovan et al (2002) showed a lexical chain touse words in glosses in order to trace the topicallyrelated paths.Their search boundary is restricted to theshapes: V, W, VW, and WW.
In this paper, cross-over* is shown to be flexible and search for a moreprobable explanation.9 ConclusionIn this paper, we have attempted to show how tolink pre-existing lexical knowledge bases to oneanother.
The major issue was to generate a path togive explanation paths for answering the ?why?-type question.
While observing the causality pathbehavior, we proposed the measure similar andalso the algorithm crossover.
It is compared withthe ?weighted abduction?
(Hobbs et al 1988) and?lexical chain?
(Moldovan et al 2002).With the ability to provide explanations de-pending on the level of the measure similar, ourproposed algorithm adapts itself to the user knowl-edge level and well as to the type of interactivequestions to enable more detailed level of ques-tion-answering.ReferencesZhen Dong and Q. Dong.
1999-2003.
Hownet,http://www.keenage.com/Jerry R. Hobbs, Mark Stickel, Douglas Appelt andPaul Martin.
1988.
Interpretation as Abduction,Proceedings of the Conference on 26th AnnualMeeting of the Assocation for Computational Lin-guistics.Doug Lenat, George Miller, and Toshio Yokoi.
1995.CYC, WordNet, and EDR: Critiques and Re-sponses, Communications of the ACM, 38(11):45-48.Bernardo Magnini and Manuela Speranza.
2002.Merging Global and Specialized Linguistic On-tologies, Proceedings of Ontolex 2002 (Workshopheld in conjunction with LREC-2002), Las Palmas.George Miller.
1995.
WordNet: a lexical database.Communications of the ACM, 38(11):39-41.Dan Moldovan and Adrian Novischi.
2002.
LexicalChains for Question Answering, Proceedings ofCOLING 2002, Taipei.Takanoa Ogino and Masahiro Kobayashi.
2000.
VerbPatterns extracted from EDR Concept Description,IPSJ SIGNotes Natural Language Abstract,No.138 ?
006:39-46.Alexandru Marius Pas?a.
2001.
High-Performance,Open-Domain Question Answering from LargeText Collections.
Ph.D Dissertation, SouthernMethodist University.H.
Sofia Pinto, Asunci?n G?mez-P?rez and Jo?o P.Martins.
1999.
Some Issues on Ontology Integra-tion, Proceedings of the IJCAI-99 workshop onOntologies and Problem-Solving Methods (KRR5),Stockholm.Stuart Russell and Peter Norvig.
1995.
ArtificialIntelligence: A Modern Approach.
Prentice-Hall.Toshio Yokoi.
1995.
The EDR Electronic Dictionary.Communications of the ACM, 38(11).
