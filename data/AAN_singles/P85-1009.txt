REVERSIBLE  AUTOMATA AND INDUCTION OF THE ENGL ISH AUXIL IARY  SYSTEMSamuel F. PilatoRobert C. BerwickMIT  Artificial Intelligence Laboratory545 Technology SquareCambridge, MA 02139, USAABSTRACTIn this paper we apply some recent work of Angluin(1982) to the induction of the English auxiliary verb system.In general, the induction of finite automata is computation-ally intractable.
However, Angluin shows that restrictedfinite automata, the It-reversible automata, can be learnedby el~cient (polynomial time) algorithms.
We present an ex-plicit computer model demonstrating that the English aux-iliary verb system can in fact be learned as a I-reversibleautomaton, and hence in a computationally feasible amountof time.
The entire system can be acquired by looking atonly half the possible auxiliary verb sequences, and the pat-tern of generalization seems compatible with what is knownabout human acquisition of auxiliaries.
We conclude thatcertain linguistic subsystems may well be learnable by in-ductive inference methods of this kind, and suggest an ex-tension to context-free languages.INTRODUCTIONFormal inductive inference methods have rarely been ap-plied to actual natural language systems.
Linguists gener-ally suppose that languages axe easy to learn because grarn-mars axe highly constrained; no ~gener,d purpose" inductiveinference methods are required.
This assumption has gener-ally led to fruitful insights on the nature of grammars.
Yetit remains to determine whether ~ll of a language is learnedin a granHnar-specilic manner.
In this paper we show howto successfully apply one computationally emcient inductiveinference algorithm to the acquisition of a domain of Englishsy'nca.x.
Our results suggest that particular language subsys-tems can be learned by general induction procedures, givencertain general constraints.The problem is that these methods are in general com-pntationally intractablc.
Even for regular languages induc-tion can be exponentially diiTicult (Gold, 1978).
This sug-gests that there may be general constraints on the designof ce~ain linguistic subsystems to make them easy to learnby general inductive inference methods.
We propose theconstraint of k-reversibilit V as one such restriction.
Thisconstraint guarantees polynomial time inference (Angluin,1982).
In the remainder of this paper, we also show, byan explicit computer model, that the English auxiliary verbsystem meets this constraint, and so is easily inferred from acorpus.
The theory gives one precise characterization of justwhcre we may expect general inductive inference methodsto be of v,~,lue in language acquisition.LEARNING K-REVERSIBLE  LANGUAGESFROM EXAMPLESThe question we address is, If a learner presumes thata natural language domain is systematic in some way, canthe learner intelligently infer the complete system from only--- subset of sample sentences?
Let us develop a:i exaaupleto formally describe what we mean by "systematic in someway," and how such a systematic domain allows the infer-ence of a complete system front examples.
If you were toldthat Mar~ bakes cakes, John bakes cakes, and Mar V eat~pies are legal strings m some language, you might guessthat John eats pies is also in that language.
Strings in thelanguage seem to follow a recognizable pattern, so you ex-pect other strings that follow the same pattern to be in thelanguage also.In this particular case, you axe presuming that the to-be-learned language is a zero-reversible regular language.Angluin (1982) has defined and explored the formal proper-ties of reversible regular languages.
We here translate someof her formal definitions into less technical terms.A regular language is any language that can be generatedfrom a formula called a regular expression.
For example thestrings mentioned above might have come from the languagethat the following regular expression generates:(MarylJohu) (bakes6eats) livery* delicious\] (cakeslpies)\]A complete natural anguage is too complex to be gen-erated by some concise regular expression, but some simplesubsets of a natural language can fit this kind of pattern.To formally define when a regular language is reversible,let us first define a prefix as any substring (possibly zero-70Table 1: Example of incremcntal k-reversible inference for several values of k.SEQUENCE OF NEW NEW STRINGS INFERRED:STRINGS PRESENTED k = 0 k = INONE Mary bakes cakesJohn bakes cakesMary eats piesMary bakes piesMary bakesNONEJohn eats piesJohn bakes piesMary eats cakesJohn eats cakesJohn bakesMary eatsJohn eatsMary bakes cakes cakesJohn bakes cakes cakesMary bakes pies cakes(MarylJohn){bakes!eats) (cakesipies)*NONENONENONEJohnbakes piesJohn bakesk=2NONENONENONENONENONElength) that can be found at the very beginning of some legalstring in a language, and a suffix as any substring (again,possibly zero-length) that can be found at the very end ofsome legal string in a language.
In our case the strings ~esequences of words, and the langamge is the set of all legalsentences in our simplified subset of English.
Also, in anylegal string say that the surtax that immediately follows aprefix is a tail for that prefix.
Then a regular languageis zero-reverstble if whenever two prefixes in the languagehave a tail in common, then the two prefixes have all tailsin common.In the above example prefixes Mary and John have thetail bakes cakes in common.
If we presume that the languagethese two strings come from is zero-reversible, then Ma~and John must have all tails in common.
In particular, thethird string shows that Mary has eats pies as a tail, so Johnmust also have eats pies as a tail.
Our current hypothesisafter having seen these three strings is that they come notfrom the three-string language xpressed by (Mar~tiJohn)bakes cakes i Mary eats p:es, which is not zero-reversible,but rather from the four-string language (MarytJohn) (bakescakes !
eats pies), which is zero-reversible.
Notice that wehave enlarged the corpus just enough to make the languagezero-reversible.A regular language is k-reversible, where k is a non-negative integer, if whenever two prefixes whose l~t  k tuordamatch have a tail in common, then the two prefixes have alltails in common.
A higher value of k gives a more conser-vative condition for inference.
For example, i/we presumethat the aforementioned strings come from a l-reversiblelanguage, then instead of presuming that whatever Marydoes John does, we would presume only that whatever Marybakes, John bakes.
In this case the third string fails to yieldany inference, but if we were later told that Mary bakespies is in the language, we could infer that John bakes piesis also in the language.
Further adding the sentence Marybakes would allow 1-reversible inference to also induce Johnbakes, resulting in the seven-string 1-reversible anguage x-pressed by ( Maryldohn) bakes Icakesipiesi l Mary eats pies.With these examples zero-reversible inference wouldhave generated ( MarylJohn) ( bakesieats) (cakesipies)* bynow, which overgeneralizes an optional direct object intozero or more direct objects.
On the other hand, two-reversible inference would have inferred no additional stringsyet.
For a particular language we hope to find a k that issmall enough to yield some inference but not so small thatwe overgeneralize and start inferring strings that are in factnot in the true language we are trying to learn.
Table 1summarizes our examples of k-reversible inference.AN INFERENCE ALGORITHMIn addition to formally characterizing k-reversible lan.guages, Angluin also developed an algorithm for inferringa k-reversible language from a finite set of positive exam-pies, an well an a method for discovering an appropriate kwhen negative xamples (strings known not to be in the lan-guage) are also presented.
She also presented an algorithmfor determining, iven some k-reversible regular language,a minimal set of examples from which the entire language7"1can be induced.
We have implemented these procedures ona computer in MACL ISP  and have applied them to all ofthe artificial anguages in Angluin's paper as well as to allof the natural anguage xamples in this paper.To describe the inference algorithm, we make use of thefact that every regular language can be associated with acorresponding deterministic finite-state automaton (DFA)which accepts or generates exactly that language.Given a sample of strings taken from the full corpus, wefirst generate a prefix-tree automaton which accepts or gen-erates exactly those strings and no others.
We now wantto infer additional strings so as to induce a/c-reversible an-guage, for some chosen /C.
Let us say that when acceptinga string, the last k symbols encountered before arriving ata state is a ~c-leader of that state.
Then to generalize thelanguage, we recursively merge any two states where any ofthe following is true:*Another state arcs to both states on the same word.
(This enforces determinism.
)oBoth states have a common k-leader and either-both states are accepting states or-both states arc to a common state on the sameword.When none of these conditions obtains any longer, the re-suiting DFA accepts or generates the smallest k-reversiblelanguage that includes the original sample of strings.
(Theterm ~reversible" is used because a ~c-reversible DFA is stilldeterministic with lookahead /C when its sets of initial andfinal states are swapped and Ml of its arcs are reversed.
)This procedure works incrementally.
Each new stringmay be added to the DFA in prefix-tree fashion and thestate-merging algorithm repeated.
The resulting languageinduced is independent of the order of presentation of sam-ple strings.If an appropriate /C is not known a pr/o~', but somenegative as well as positive examples are presented, thenone can try increasing values of k until the induced languagecontains none of the negative examples.Though the inference algorithm takes a sample and in-duces a/c-reversible language, it is quite helpful to use An-gluin's algorithm for going in the reverse direction: given ak- reversible language we can determine what minimal set ofshortest possible examples (a "characteristic" or "covering nsample) will be sufficient for inducing the language.
Thoughthe minimal number of examples is of course unique, the setof particular strings in the covering sample is not necesm~rilyImique.INFERENCE OF  THE ENGL ISH AUXIL IARYSYSTEMWe have chosen to test the English auxiliary system un-der /c-reversible inference because English verb sequencesare highly regular, yet they have some degree of complexityand admit to some exceptions.
We represent he Englishauxiliary system am a corpus of 92 variants of a declarativestatement in third person singular.
The variants cover allstandard legal permutations of tense, aspect, and voice, in-cluding do support and nine models.
We simply use thesurface forms, which are strings of words with no additionalinformation such as syntactic ategory or root-by-inflectionbreakdown.
For instance, the present, simple, active ex-ample is Judy glvez bread.
One modal, perfective, passivevariant is Judy would have been given bread.We have explored the/c-reversible properties of this nat-,iral language subsystem in two main steps.
First we deter-mined for what values of k the corpus is in fact k-reversible.
(Given a finite corpus, we could be sure the language is/c-reversible for all /C at or above some value.)
To do thiswe treated the full corpus as a set of sample strings andtried successively larger values of/C until finding one where/c-reversible inference applied to the corpus generates no ad-ditional strings.
We could then be sure that any /C of thatvalue or greater could be used to infer an accurate model ofthe English auxiliary system without overgeneralizing.After finding the range of values of/C to work with, wewere interested in determining which, if any, of those valuesof/C would yield some power to infer the full corpus froma proper subset of examples.
To do this we took the DFAwhich represents the full corpus and computed, for a trialk, a set of samp|e strings that would be minimally sufficientto induce the full corpus.
If any such values of k exist, thenwe can say that, in a nontrivial way, the English auxiliarysystem is learnable as a k-reversible language from exam-ples.We found that the English auxiliary system can be faith-fully modeled as a/c-reversible regular language for k >_ I.Only zero-reversible inference overgeneralizes the full corpusas well as the active and passive corpora treated as separatelanguages.
For the active corpus, zero-reversible inferencegroups the forms of do with the other modals.
The DFAs  forthe passive and full corpora also contain loops and therebygenerate infinite numbers of illegal variants.F:.gure I compares a correct DFA for the English auxil-iary system with an overgeneralized DFA.
Both are shown ina minimized, canonical form.
The top, correct, automatoncan be generated by either minimizing the prefix tree for thefull corpus or by minhnizing the result of/c-reversible infer-ence applied to any sufficiently characteristic set of samplesentences, for any /C _.> 1.
One can read off all 92 variants72(giveslg,,,ve)(do.,,Idid) ~ give(i, lw"')(huth,,a)Judy ~ ~ ~ ~  7 ~'~ 4 b~dbe J \ (~6",'i-~Igi',,:n)glvengiveTHE ENGL ISH AUXIL IARY SYSTEM(giv.tgave) f r  -'~J udy  _ f(i*!wastha.lhsd) (beeatbeln|)(do.sQdidImlylmi|bttmus?~/i,hallt.hou~"d3~,h,*.
(S't ~ ) (givingtgiven) ~ /~ve jZERO-REVERSIBLE  OVERGENERAL IZAT IONOF  THE ENGL ISH AUXIL IARY SYSTEMbreadFigure I: The top automaton generates the English auxiliary system.
Zero-reversible inferencemerges state 3 with state 2 and merges states 7 and 6 with state 5, resulting in the bottomovergeneralized version.73in the language by taking different paths from initial stateto final state.
The bottom, overgeneralized, automaton isgenerated by subjecting the top one to zero-reversible infer-euce ,Does treating the English auxiliary system as a I-or-more-reversible l,'mguage yield any inferential power?
TheEnglish auxiliary system as a l-reversible language can infact be inferred from a cover of only 48 examples out ofthe 92 variants in the corpus.
The active corpus treatedseparately requires 38 examples out of 46 and the passivecorpus requires 28 out of 46.
Treating the full corpus asa 2-reversible language requires 76 examples, and a 3 "~-reversible model cannot infer the corpus from any propersubset whatsoever.For l-reversible inference, 45 of the verb sequences oflength three or shorter will yield the remaining nine suchstrings and nonc longer.
Verb sequences of length fouror five can be divided into two patterns, <modal> havebeen 9iv(ing,,en) ,'wad .
.
.
be, en} bern9 given.
Adding any one(length-four) string from the first pattern will yield the re-maining 17 strings of that pattern.
Further adding twolength-four strings from the awkward second pattern willyield the remaining 18 strings of that pattern, nine of whichare of length five.
This completes the corpus.DISCUSSIONThe auxiliary system has often been regarded ,as an acidtest for a theory of langulage acquisition.
Given this, we areencouraged that it is in fact learnable via a computationallyeII.icient general method.
It is significant that at \[east inthis domain we have found a k (of l) that is low enough togenerate a good amount of inference from examples yet highenough to avoid overgeneralization.
Even more conservative2-reversibility generates a little inference.This inductive power derives from the systematic se-quential structure of the English auxiliary system.
In anidealized form (ignoring tense and inflections) the regularexpression\ [DO I \[<modal>\] \ [HAVE\ ]  \[nEll \[BEpassive\] G IVEgenerates all English verb sequence patterns in our corpus.Zero-reversible inference basically attempts to simplifyany partial, disjunctive permutation like (a'.b)z:ay into anexhaustive, combinatorial permutation like (ab)(z',y).
Sincethe active corpus (excluding BE.passive from the idealizedregular expression) in fact has such a simple form except forthe DO disjunction, zero-reversible inference productivelycompletes the three-place permutation but also destroys thedisjunction, by overgeneralizing what patterns can followboth DO ,'rod <modal>.
One-reversible inference requiresthat disjuncts share some final word to be mergeable, sothat DO cannot merge with any auxiliary triplet, yet thepermutation of < modal:, IIA VE by BE;  is still productive.Similar considerations obtain in the passive case, as well asfor the joint corpus.
Table 2 illustrates the trade-off in thiscase between inferential power and the proper handling ofexceptions.In complex environments, rather than reduce the infer-ential power by raising k one could instead embed this al-gorithm within a larger system.
For example, a more re-alistic model of processing English verb sequences wouldhave an external, more linguistically motivated mechanismforce the separate tre.atment of active versus passive forms.Then if, say on considerations of frequency of occurrence,do exceptions were externally handled and the infrequentTable 2: Incremental k-reversible inference of some English auxiliary verb sequences.SEQUENCE ()F NEW NEW .~TRIN(;S INFERRED:.~TRIN(;S P I IESENTED ilk = 0 ' k = !
k = 2?,mhl giw NONE NONEmay givedoes give, could have givenfmay have givencould have been giving, NONENONEmay have givendoes have given(ALREADY INFERRED)may have been givingdoes have been givingNONENONENONENONEmay have been givingNONENONENONENONENONENONE74... BE being ... cases were similarly excluded from the im-mature learner, then one could apply the more powerfulzero-reversible inference to the remaining active and passiveforms without overgeneralizing.
In such a case the activesystem can be induced from 18 examples out of 44 variantsand the passive system from 14 out of 22.
The entire activesystem is learnable once examples of each form of each verband each modal have been seen, plus one example to fix therelative order of have vs. be, and one example each to fixthe order of modal vs. have or be.Though a more complex model must ultimately repre-sent a domain like the English auxiliary system, the wayk-reversible inference in itself handles a complex territorysatisfies ome condition~ of psychological fidelity.
Especially.
'-cro-reversibility is a rather simple form of generalizationof sequential patterns with which we believe humans read-ily identify.
In general the longer, more complex cases canbe inferred from simpler cases.
Also, there is a reasonabledegree of play in the composition of the covering saanple,and the order of presentation does not affect the languagelearned.Children evidently never make mistakes on the relativeorder of auxiliaries, which is consistent with the reversibilitymodel, but they do mistakenly combine do with tensed verbforms (Pinker, 1984).
Given that the appearance of do indeclarative sentences is also fairly rare, one might preferthe aforementioned zero-reversible system that handles dosupport as an exception, rather than opt for a 1-reversibleinference which is flawless but a slower learner.The ... BE being ... cases are systematically relatedto the rest, but also have a natural boundary: 1-reversibleinference from simpler cases doesn't intrude into that ter-ritory, yet only a few such examples allow one to infer theremainder.
Very.
rare sequences like could have been be-ing given will be successfully acquired even if they axe notseen.
This seems consistent with human judgments thatsuch phrasing is awkward but apparently legal.k-Reversibility is essentially a model of simplicity, not ofcomplexity.
As such, it induces not linguistic structure butthe substitution classes that linguistic structures typicallywork with, building these by analogy from examples.
In thelinguistic structure for which k-reversibility is definedregular ~ammars ~ it functions to induce the closes thatfill "slots" in a regular expression, based on the similarityof tail sets.
Increasing the value of k is a way of requiringa higher degree of similarity before calling a match.
(SeeGonzalez and Thomason, 1978, for other approaches to k-tail inference that are not so efficient.
)The same principle can apply to the induction of substi-tution classes in other linguistic domains including morpho-logical, syntactic, and semantic systems.
For a particularlydirect example, consider the right-hand sides of context-freerewrite rules.
Any subset of such rules having the same left-hand side constitutes a regular language over the set of ter-minal a~d nonterminal symbols, and is therefore a candidatefor induction.
One might thus infer new rewrite rules fromthe pattern of existing ones, thereby not only concludingthat words are members of certain simple syntactic classes,but also simplifying a disjunctive set of rules into a moreconcise set that exhibits systematic properties.
Berwick'sLparsi/al system (1982) is ,an example of this kind of exten-sion.We believe that k-reversibility illustrates a psycholog-ically plausible pattern induction process for natural lan-guage learning that in its simplest form has an efficientcomputational gorithm associated with it.
The basic prin-ciple behind k-reversible inference shows some promise ,'~ aflexible tool within more complex models of language ac-quisition.
It is encouraging that, at lea.st in a simple case,computational linguistic models can suggest formal leaxn-ability constraints that ;tre natural enough to be useful inthe le,'trning , f  human languages.ACKNOWLEDGMENTSThis paper describes research done at the Artificial Intel-ligence Laboratory of the .Massachusetts Institute of Tech-nology.
Support for the laboratory's artificial intelligenceresearch is provided in part by the Advanced ResearchProjects Agency of the Department of Defense under theOffice of NavM Research Contract .N0001.t-80-C-0505.REFERENCESAngluin, D., "Inference of reversible laugalages," Journalof the A.~sociation /or Computing Machinery, 29(3), 741-765, 1982.Berwick, R., Locality Principles and the ..lcquisitton o/Syntactic Knowledge, PhD, MIT Department cf ElectricalEngineering ,and Computer Science, 1982.Gold, E., "Complexity of Automaton Identification fromGiven Data," Information and Control, 37, 1978.Gonzalez, R., ztnd Thmnason, M., Syntactic PatternRecognition, Reading, MA: Addison-Wesley, 1978.Pinker, S., Language 5earnability and Language Devel-opment, Cambridge: MA: Harvard University Press, 1984.75
