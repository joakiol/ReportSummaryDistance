Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 96?99,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPA Syllable-based Name Transliteration SystemXue Jiang 1, 21Institute of Software, ChineseAcademy of Science.Beijing China, 100190jiangxue1024@yahoo.com.cnLe Sun 1, Dakun Zhang 12School of Software Engineering,Huazhong University of Science andTechnology.
Wuhan China, 430074sunle@iscas.ac.cndakun04@iscas.ac.cnAbstractThis paper describes the name entity transli-teration system which we conducted for the?NEWS2009 Machine Transliteration SharedTask?
(Li et al2009).
We get the translitera-tion in Chinese from an English name withthree steps.
We syllabify the English nameinto a sequence of syllables by some rules,and generate the most probable Pinyin se-quence with the mapping model of Englishsyllables to Pinyin (EP model), then we con-vert the Pinyin sequence into a Chinese cha-racter sequence with the mapping model ofPinyin to characters (PC model).
And we getthe final Chinese character sequence.
Oursystem achieves an ACC of 0.498 and aMean F-score of 0.786 in the official evalua-tion result.1 IntroductionThe main subject of shared task is to translateEnglish names (source language) to Chinesenames (target language).
Firstly, we fix somerules and syllabify the English names into a se-quence of syllables by these rules, in the mean-while, we convert the Chinese names into Pinyinsequence.
Secondly, we construct an EP modelreferring to the method of phrase-based machinetranslation.
In the next, we construct a 2-gramlanguage model on characters and a chart reflect-ing the using frequency of each character withthe same pronunciation, both of which constitutethe PC model converting Pinyin sequence intocharacter sequence.
When a Pinyin is mapped toseveral different characters, we can use them tomake a choice.
In our experiment, we adopt thecorpus provided by NEWS2009 (Li et al2004)and the LDC Name Entity Lists 1 respectively toconduct two EP models, while the NEWS2009corpus for the PC model.
The experiment indi-cates that the larger a training corpus is, the moreprecise the transliteration is.2 Transliteration System DescriptionKnowing from the definition of transliteration,we must make the translating result maintain theoriginal pronunciation in source language.
Wefound that most English letters and letter compo-sitions?
pronunciation are relatively fixed, so wecan take a syllabification on an English name,therefore the syllable sequence can represent itspronunciation.
In Chinese, Pinyin is used torepresent a character?s pronunciation.
Based onthese analyses, we transliterate the English sylla-ble sequence into a Pinyin sequence, and thentranslate the Pinyin sequence into characters.We suppose that the probability of a translitera-tion from an English name to a Chinese name isdenoted by P(Ch|En), the probability of a transla-tion from an English syllable sequence to a Pi-nyin  sequence is denoted by P(Py|En), and theprobability of a translation from a Pinyin se-quence to a characters is denoted by P(Ch|Py),then we can get the formula:P(Ch|En) = P(Ch|Py) * P(Py|En)      (1)The character sequence in candidates havingthe max value of P(Ch|En) is the best translitera-tion(Wan and Verspoor, 1998).2.1 Syllabification of English NamesEnglish letters can be divided into vowel letters(VL) and consonant letters (CL).
Usually, in a1: Chinese <-> English Name Entity Lists v 1.0, LDC Cata-log No.
: LDC2005T3496word, a phonetic syllable can be constructed in astructure of CL+VL, CL+VL+CL, CL+VL+NL.To adapt for Chinese phonetic rule, we divide thecontinuous CLs into independent CLs(IC) anddivide structure of CL+VL+CL into CL+VL andan IC.
Take ?Ronald?
as an example, it can besyllabified into ?Ro/na/l/d?, ?Ro?
is CL+VL,?nal?
is CL+VL+CL, and is divided into CL+VLand IC.
?d?
is an independent CL(KUO et al2007).
Of course there are some English namesmore complex to be syllabified, so we defineseven rules for syllabification (JIANG et al2006):(1) Define English letter set as O, vowel set asV={a, e, i, o, u}, consonant set as C=O-V.(2) Replace all ?x?
in a name with ?ks?
beforesyllabification because it?s always pro-nounced as  ?ks?.
(3) The continuous VLs should be regarded asone VL.
(4) There are some special cases in rule (3),the continuous VLs like ?oi?, ?io?, ?eo?are pronounced as two syllables, so theyshould be cut into two parts, so ?Wilhoit?will be syllabifyd into ?wi/l/ho/i/t?.
(5) The continuous CLs should be cut intoseveral independent CLs.
If the last one isfollowed by some VLs, they will make upa syllable.
(6) Some continuous CLs are pronounced as asyllable, such as ?ck?, ?th?, these CLs willnot be syllabifyd and be regarded as a sin-gle CL, ?Jack?
is syllabifyd into ?Ja/ck?.
(7) There are some other composition with thestructure of VL+CL, such as ?ing?, ?er?,?an?
and so on.
If it?s a consonant behindthese compositions in the name, we cansyllabify it at the end of the composition,while if it?s a vowel behind them, weshould double write the last letter and syl-labify the word between the two same let-ters.After syllabicating English names, we convertcorresponding Chinese names into Pinyin.
Thereare a few characters with multiple pronunciationsin the training data, we find them out and ensureits pronunciation in a name manually.We record all of these syllables got from thetraining data set, if we meet a syllable out of vo-cabulary when transliterating an English name,we will find a similar one with the shortest edit-distance in the vocabulary to replace that.2.2 Mapping Model of English Syllables toPinyinsThe EP model consists of a phrase-based ma-chine translation model with a trigram languagemodel.Given an English name f, we want to find itsChinese translation e, which maximize the condi-tional probability )|Pr( fe , as shown below.
)|Pr(maxarg* fee e?
(2)Using Bayes rule, (1) can be decomposed intoa Translation Model )|Pr( ef  and a LanguageModel )Pr(e  (Brown et al 1993), which canboth be trained separately.
These models areusually regarded as features and combined withscaling factors to form a log-linear model (Ochand Ney 2002).
It can then be written as:?
??????
'11)],'(exp[)],(exp[)|()|Pr(1emmMmmmMmfehfehfepfe M???
(3)In our model, we use the following features:?
phrase translation probability )|( fep?
lexical weighting )|( felex?
inverse phrase translation probability)|( efp?
inverse lexical weighting )|( eflex?
phrase penalty (always exp(1) = 2.718)?
word penalty (target name length)?
target language model, trigramThe first five features can be seen as a wholephrase translation cost and used as one duringdecoding.In general, the translation process can be de-scribed as follows:(1).
Segmenting input English syllable se-quence f into J syllables Jf 1(2).
Translating each English syllablejfinto several Pinyinsjke(3).
Selecting the N-best wordsnee ...1 ,combined with reordering and LanguageModel and other features97(4).
Rescoring the translation word set withadditional features to find the best one.We use SRI toolkit to train our trigram lan-guage model with modified Kneser-Ney smooth-ing (Chen and Goodman 1998).
In the standardexperiment, we use training data set provided byNEWS2009 (Li et al2004) to train this languagemodel, in the nonstandard one, we use that andthe LDC Name Entity Lists to train this languagemodel.2.3 Mapping Model of Pinyins to ChineseCharactersSince the Chinese characters used in peoplenames are limited, most of the conversions fromPinyin to character are fixed.
But some Pinyinsstill have several corresponding characters, andwe should make a choice among these characters.To solve this problem, we conduct a PC modelconsisting a frequency chart which reflects theusing frequency of each character at differentpositions in the names and a 2-gram languagemodel with absolute discounting smoothing.A Chinese name is represented as C1C2?Cn?Ci (1?i?n) is a Chinese character.
C1 is atthe first position, we call it FW; C2 ?Cn-1 are inthe middle, we call them MW; Cn is at the lastposition, we call it LW.
Usually, each characterhas different frequencies at these three positions.In the training data set of NEWS2009, Pinyin?luo?
can be mapped to three characters: ???,??
?, and ??
?, each of them has different fre-quencies at different positions.FW MW LW?
0.677 0.647 0.501?
0.323 0.352 0.499?
0 0.001 0Table 1.
Different frequencies at different positionsFrom this table, we can see that at FW andMW position, ???
is more probable to be cho-sen than the others, but sometimes ???
or ??
?is the correct one.
In order to ensure characterswith lower frequency like ???
and ???
can bechosen firstly in a certain context, we conduct a2-gram language model.If a Pinyin can be mapped to several charac-ters, the condition probability (P(Chi|py)) indicat-ing that how possible a character should be cho-sen is determined by the weighted average of itsposition frequency (P(Chi|pos)) and its probabili-ty in the 2-gram language model (P(Chi|Chi-1)).P(Chi|py) = a*P(Chi|pos)+(1-a)*P(Chi|Chi-1)  (4)0 < a < 1.
In our experiments, we set a = 0.1.2.4 Experiments and ResultsWe carried out two experiments.
The differencebetween them is the training data for EP model.The standard experiment adopts corpus providedby NEWS2009, while the nonstandard oneadopts LDC Name Entity Lists.Corpora Name NumLDC2005T34 572213NEWS09_train_ench_31961 31961Table 2.
Corpora used for training the EP modelConsidering that an English name may betranslated to different Chinese names in differentcorpora, so we established a unique PC modelwith the training data set provided byNEWS2009 to avoid the model?s deviationcaused by different corpora.The experimenting data is the developmentdata set provided by NEWS2009 (Li et al2004),testing script is also provided by NEWS2009.First, we take a syllabification on testingnames.
Then we use the EP model to generate 5-best Pinyin sequences and their probabilities.For each Pinyin sequence, the PC model gives 3-best character sequences and their probabilities.In the end, we sort the results by probabilities ofcharacter sequences and corresponding Pinyinsequences.The evaluation results are shown below.Metrics Standard NonstandardACC 0.490677 0.502417Mean F-score 0.782039 0.784203MRR 0.606424 0.611214MAP_ref 0.490677 0.502417MAP_10 0.189290 0.189782MAP_sys 0.191476 0.192129Table 3.
Evaluation results of standard andnonstandard experimentsIt?s easy to see that nonstandard test is betterthan standard one on each metric.
A larger cor-pus does make a contribution to a more accuratemodel.98For the official evaluation, we make two testson the testing data set provided by NEWS2009(Li et al2004).
The table 4 shows respectivelythe evaluation results of standard and nonstan-dard tests given by NEWS2009.Metrics Standard NonstandardACC 0.498 0.500Mean F-score 0.786 0.786MRR 0.603 0.607MAP_ref 0.498 0.500MAP_10 0.187 0.189MAP_sys 0.189 0.191Table 4.
Official evaluation results of standard andnonstandard tests3 ConclusionWe construct a name entity transliteration systembased on syllable.
This system syllabifies Eng-lish names by rules, then translates the syllablesto Pinyin and Chinese characters by statisticsmodel.
We found that a larger corpus may im-prove the transliteration.
Besides, we can dosomething else to improve that.
We need to fixmore complex rules for syllabification.
If we canget the name user?s gender from some features ofthe name itself, then translate the male and fe-male names on different Chinese character sets,the results may be more precise.AcknowledgmentsThis work was supported by the NationalScience Foundation of China (60736044,60773027), as well as 863 Hi-Tech Research andDevelopment Program of China (2006AA010108-5, 2008AA01Z145).We also thank Haizhou Li, Min Zhang andJian Su for providing the English-Chinese data.ReferenceFranz Josef Och and Hermann Ney.
2002.
?Discri-minative Training and Maximum Entropy Modelsfor Statistical Machine Translation?.
In  Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL).Haizhou Li, A Kumaran, Min Zhang, Vladimir Per-vouchine, "Whitepaper of NEWS 2009 MachineTransliteration Shared Task".
In Proceedings ofACL-IJCNLP 2009 Named Entities Workshop(NEWS 2009), Singapore, 2009Haizhou Li, Min Zhang, Jian Su.
2004.
?A jointsource channel model for machine transliteration?,In Proceedings of the 42nd ACL, 2004Jiang Long, Zhou Ming, and Chien Lee-feng.
2006.?Named Entity Translation with Web Mining andTransliteration?.
Journal of Chinese InformationProcessing, 21(1):1629--1634.Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang.2007.
?A Phonetic Similarity Model for AutomaticExtraction of Transliteration Pairs?.
ACM Trans.Asian Language Information Processing, 6(2), Sep-tember 2007.Peter F. Brown, Stephen A. Della Pietra, et al 1993.?The Mathematics of Statistical Machine Transla-tion: Parameter Estimation?.
Computational Lin-guistics 19(2): 263-311.Stanley F. Chen and Joshua Goodman.
1998.
?Anempirical study of smoothing techniques for lan-guage modeling?.
Technical Report TR-10-98, Har-vard University.Stephen Wan and Cornelia Maria Verspoor.
1998.?Automatic English-Chinese name transliterationfor development of multilingual resources?.
In Pro-ceedings of the 17th international conference onComputational linguistics, 2: 1352 ?
1356.99
