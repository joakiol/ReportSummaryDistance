Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 132?135,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsEstimating probability of correctness for ASR N-Best listsJason D. Williams and Suhrid BalakrishnanAT&T Labs - Research, Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932, USA{jdw,suhrid}@research.att.comAbstractFor a spoken dialog system to make gooduse of a speech recognition N-Best list, it isessential to know how much trust to placein each entry.
This paper presents a methodfor assigning a probability of correctness toeach of the items on the N-Best list, and tothe hypothesis that the correct answer is noton the list.
We find that both multinomial lo-gistic regression and support vector machinemodels yields meaningful, useful probabili-ties across different tasks and operating con-ditions.1 IntroductionFor spoken dialog systems, speech recognition er-rors are common, and so identifying and reducingdialog understanding errors is an important problem.One source of potentially useful information is theN-Best list output by the automatic speech recog-nition (ASR) engine.
The N-Best list contains Nranked hypotheses for the user?s speech, where thetop entry is the engine?s best hypothesis.
When thetop entry is incorrect, the correct entry is often con-tained lower down in the N-Best list.
For a dialogsystem to make use of the N-Best list, it is useful toestimate the probability of correctness for each en-try, and the probability that the correct entry is noton the list.
This paper describes a way of assigningthese probabilities.2 Background and related workTo begin, we formalize the problem.
The user takesa communicative action u, saying a phrase such as?Coffee shops in Madison New Jersey?.
Using a lan-guage model g, the speech recognition engine pro-cesses this audio and outputs an ordered list of Nhypotheses for u, u?
= {u?1, .
.
.
u?N}, N ?
2.
Tothe N-Best list we add the entry u?
?, where u = u?
?indicates that u does not appear on the N-Best list.The ASR engine also generates a set of K recog-nition features f = [f1, .
.
.
, fK ].
These featuresmight include properties of the lattice, word confu-sion network, garbage model, etc.
The aim of thispaper is to estimate a model which accurately as-signs the N + 1 probabilities P (u = u?n|u?, f) forn ?
{?, 1, .
.
.
, N} given u?
and f .
The model alsodepends on the language model g, but we don?t in-clude this conditioning in our notation for clarity.In estimating these probabilities, we are mostconcerned with the estimates being well-calibrated.This means that the probability estimates we ob-tain for events should accurately represent the em-pirically observed proportions of those events.
Forexample, if 100 1-best recognitions are assigned aprobability of 60%, then approximately 60 of those100 should in fact be the correct result.Recent work proposed a generative model of theN-Best list, P (u?, f |u) (Williams, 2008).
The mainmotivation for computing a generative model isthat it is a component of the update equation usedby several statistical approaches to spoken dialog(Williams and Young, 2007).
However, the diffi-culty with a generative model is that it must estimatea joint probability over all the features, f ; thus, mak-ing use of many features becomes problematic.
Asa result, discriminative approaches often yield bet-ter results.
In our work, we propose a discrimina-tive approach and focus on estimating the probabil-ities conditioned on the features.
Additionally, un-der some further fairly mild assumptions, by apply-ing Bayes Rule our model can be shown equivalentto the generative model required in the dialog stateupdate.
This is a desirable property because dialogsystems using this re-statement have been shown towork in practice (Young et al, 2009).Much past work has assigned meaningful proba-132bilities to the top ASR hypothesis; the novelty hereis assigning probabilities to all the entries on the list.Also, our task is different to N-Best list re-ranking,which seeks to move more promising entries towardthe top of the list.
Here we trust the ordering pro-vided by the ASR engine, and only seek to assignmeaningful probabilities to the elements.3 ModelOur task is to estimate P (u = u?n|u?, f) for n ?
{?, 1, .
.
.
, N}.
Ideally we could view each elementon the N-Best list as its own class and train an(N+1)-class regression model.
However this is dif-ficult for two reasons.
First, the number of classes isvariable: ASR results can have different N-Best listlengths for different utterances.
Second, we foundthat the distribution of items on the N-Best list hasa very long tail, so it would be difficult to obtainenough data to accurately estimate late position classprobabilities.As a result, we model the probability P in twostages: first, we train a (discriminative) model Pa toassign probabilities to just three classes: u = u?
?,u = u?1, and u ?
u?2+, where u?2+ = {u?2, .
.
.
, u?N}.In the second stage, we use a separate probabilitymodel Pb to distribute mass over the items in u?2+:P (u?n = u|u?, f) = (1)????
?Pa(u = u?1|f) if n = 1,Pa(u ?
u?2+|f)Pb(u = u?n|f) if n > 1,Pa(u = u?
?|f) if n = ?To model Pa, multinomial logistic regression(MLR) is a natural choice as it yields a well-calibrated estimator for multi-class problems.
Stan-dard MLR can over-fit when there are many featuresin comparison to the number of training examples;to address this we use ridge regularized MLR in ourexperiments below (Genkin et al, 2005).An alternative to MLR is support vector machines(SVMs).
SVMs are typically formulated includingregularization; however, their output scores are gen-erally not interpretable as probabilities.
Thus for Pa,we use an extension which re-scales SVM scores toyield well-calibrated probabilities (Platt, 1999).Our second stage model Pb, distributes massover the items in the tail of the N-best list (n ?0%20%40%60%80%100%0% 20% 40% 60% 80% 100%CumulativeprobabilityFractional position in N-Best list  (n/N) of correct entryN-Best lists with N < 100 entriesN-Best lists with N >= 100 entriesAll N-Best listsModel (Beta distribution)Figure 1: Empirical cumulative distribution of cor-rect recognitions for N-Best lists, and the Beta dis-tribution model for Pb on 1, 000 business search ut-terances (Corpus 1 training set, from Section 4.
){2, .
.
.
, N}).
In our exploratory analysis of N-Bestlists, we noticed a trend that facilitates modeling thisdistribution.
We observed that the distribution of thefraction of the correction position n/N was rela-tively invariant to N .
For example, for both short(N < 100) and long (N ?
100) lists, the proba-bility that the answer was in the top half of the listwas very similar (see Figure 1).
Thus, we chose acontinuous distribution in terms of the fractional po-sition n/N as the underlying distribution in our sec-ond stage model.
Given the domain of the fractionalposition [0, 1], we chose a Beta distribution.
Our fi-nal second stage model is then an appropriately dis-cretized version of the underlying Beta, namely, Pb:Pb(u = u?n|f) = Pb(u = u?n|N) =Pbeta(n?
1N ?
1;?, ?)
?
Pbeta(n?
2N ?
1;?, ?
)where Pbeta(x;?, ?)
is the standard Beta cumula-tive distribution function parametrized by ?
and ?.Figure 1 shows an illustration.
In summary, ourmethod requires training the three-class regressionmodel Pa, and estimating the Beta distribution pa-rameters ?
and ?.4 Data and experimentsWe tested the method by applying it to three cor-pora of utterances from dialog systems in the busi-ness search domain.
All utterances were from133Corpus WCN SVM MLR1 -0.714 -0.697 -0.7032 -0.251 -0.264 -0.2223 -0.636 -0.605 -0.581Table 1: Mean log-likelihoods on the portion of thetest set with the correct answer on the N-Best list.None of the MLR nor SVM results differ signifi-cantly from the WCN baseline at p < 0.02.2users with real information needs.
Corpus 1 con-tained 2, 000 high-quality-audio utterances spokenby customers using the Speak4It application, abusiness search application which operates on mo-bile devices, supporting queries containing a listingname and optionally a location.1 Corpus 2 and 3contained telephone-quality-audio utterances from14, 000 calls to AT&T?s ?411?
business directorylisting service.
Corpus 2 contained locations (re-sponses to ?Say a city and state?
); corpus 3 con-tained listing names (responses to ?OK what list-ing??).
Corpus 1 was split in half for training andtesting; corpora 2 and 3 were split into 10, 000 train-ing and 4, 000 testing utterances.We performed recognition using the Watsonspeech recognition engine (Goffin et al, 2005), intwo configurations.
Configuration A uses a sta-tistical language model trained to recognize busi-ness listings and optionally locations, and acous-tic models for high-quality audio.
Configuration Buses a rule-based language model consisting of allcity/state pairs in the USA, and acoustic models fortelephone-quality audio.
Configuration A was ap-plied to corpora 1 and 3, and Configuration B wasapplied to corpus 2.
This experimental design is in-tended to test our method on both rule-based andstatistical language models, as well as matched andmis-matched acoustic and language model condi-tions.We used the following recognition features in f :f1 is the posterior probability from the best paththrough the word confusion network, f2 is the num-ber of segments in the word confusion network,f3 is the length of the N-Best list, f4 is the aver-age per-frame difference in likelihood between the1http://speak4it.com22-tailed Wilcoxon Signed-Rank Test; 10-way partitioning.Corpus WCN SVM MLR1 -1.12 -0.882 -0.8902 -0.821 -0.753 -0.7343 -1.00 -0.820 -0.824Table 2: Mean log-likelihoods on the complete testset.
All MLR and SVM results are significantly bet-ter than the WCN baseline (p < 0.0054).2highest-likelihood lattice path and a garbage model,and f5 is the average per-frame difference in likeli-hood between the highest-likelihood lattice path andthe maximum likelihood of that frame on any paththrough the lattice.
Features are standardized to therange [?1, 1] and MLR and SVM hyperparameterswere fit by cross-validation on the training set.
The?
and ?
parameters were fit by maximum likelihoodon the training set.We used the BMR toolkit for regularized multi-nomial logistic regression (Genkin et al, 2005), andthe LIB-SVM toolkit for calibrated SVMs (Changand Lin, 2001).We first measure average log-likelihood the mod-els assign to the test sets.
As a baseline, we use theposterior probability estimated by the word confu-sion network (WCN), which has been used in pastwork for estimating likelihood of N-Best list entries(Young et al, 2009).
However, the WCN does notassign probability to the u = u??
case ?
indeed, thisis a limitation of using WCN posteriors.
So we re-ported two sets of results.
In Table 1, we report theaverage log-likelihood given that the correct resultis on the N-Best list (higher values, i.e., closer tozero are better).
This table includes only the itemsin the test set for which the correct result appearedon the N-Best list (that is, excluding the u = u??cases).
This table compares our models to WCNson the task for which the WCN is designed.
On thistask, the MLR and SVM methods are competitivewith WCNs, but not significantly better.In Table 2, we report average log-likelihood forthe entire test set.
Here the WCNs use a fixedprior for the u = u??
case, estimated on the trainingsets (u = u??
class is always assigned 0.284; otherclasses are assigned 1 ?
0.284 = 0.716 times theWCN posterior).
This table compares our modelsto WCNs on the task for which our model is de-signed.
Here, the MLR and SVM models yielded1340204060801001201401601800%10%20%30%40%50%60%70%80%90%100%0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95Number of entriesFractioncorrectRegression-assigned probabilityPerfect calibration (left axis)MLR Calibration (left axis)Number of entries (right axis)Figure 2: Calibration and histogram of probabilitiesassigned by MLR on corpus 1 (test set).significantly better results than the WCN baseline.We next investigated the calibration properties ofthe models.
Results for the MLR model on theu = u?1 class from corpus 1 test set are shown inFigure 2.
This illustrates that the MLR model is rel-atively well-calibrated and yields broadly distributedprobabilities.
Results for the SVM were similar, andare omitted for space.Finally we investigated whether the modelsyielded better accept/reject decisions than their in-dividual features.
Figure 3 shows the MLR modela receiver operating characteristic (ROC) curve forcorpus 1 test set for the u = u?1 class.
This con-firms that the MLR model produces more accurateaccept/reject decisions than the individual featuresalone.
Results for the SVM were similar.5 ConclusionsThis paper has presented a method for assigninguseful, meaningful probabilities to elements on anASR N-Best list.
Multinomial logistic regression(MLR) and support vector machines (SVMs) havebeen tested, and both produce significantly bettermodels than a word confusion network baseline, asmeasured by average log likelihood.
Further, themodels appear to be well-calibrated and yield a bet-ter indication of correctness than any of its input fea-tures individually.In dialog systems, we are often more interested inthe concepts than specific words, so in future work,we hope to assign probabilities to concepts.
In the0%10%20%30%40%50%0% 10% 20% 30% 40%TrueAcceptsFalse AcceptsMLR-assigned probabilityn=1 posterior from word confusion network (f )Average delta to best frame in lattice (f )Average delta to garbage model (f )154Figure 3: ROC curve for MLR and the 3 most infor-mative input features on corpus 1 (test set).meantime, we are applying the method to our dialogsystems, to verify their usefulness in practice.ReferencesCC Chang and CJ Lin, 2001.
LIBSVM: a library for sup-port vector machines.
http://www.csie.ntu.edu.tw/?cjlin/libsvm.A Genkin, DD Lewis, and D Madigan, 2005.BMR: Bayesian Multinomial Regression Soft-ware.
http://www.stat.rutgers.edu/?madigan/BMR/.V Goffin, C Allauzen, E Bocchieri, D Hakkani-Tur,A Ljolje, S Parthasarathy, M Rahim, G Riccardi, andM Saraclar.
2005.
The AT&T Watson speech recog-nizer.
In Proc ICASSP, Philadelphia.JC Platt.
1999.
Probabilistic outputs for support vectormachines and comparisons to regularized likelihoodmethods.
In Advances in Large Margin Classifiers,pages 61?74.
MIT Press.JD Williams and SJ Young.
2007.
Partially observableMarkov decision processes for spoken dialog systems.Computer Speech and Language, 21(2):393?422.JD Williams.
2008.
Exploiting the ASR N-best by track-ing multiple dialog state hypotheses.
In Proc ICSLP,Brisbane.SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,B Thomson, and K Yu.
2009.
The hidden informationstate model: a practical framework for POMDP-basedspoken dialogue management.
Computer Speech andLanguage.
To appear.135
