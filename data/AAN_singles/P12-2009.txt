Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 43?47,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Novel Burst-based Text Representation Modelfor Scalable Event DetectionWayne Xin Zhao?, Rishan Chen?, Kai Fan?, Hongfei Yan??
and Xiaoming Li??
?School of Electronics Engineering and Computer Science, Peking University, China?State Key Laboratory of Software, Beihang University, China{batmanfly,tsunamicrs,fankaicn,yhf1029}@gmail.com, lxm@pku.edu.cnAbstractMining retrospective events from text streamshas been an important research topic.
Classictext representation model (i.e., vector spacemodel) cannot model temporal aspects of doc-uments.
To address it, we proposed a novelburst-based text representation model, de-noted as BurstVSM.
BurstVSM correspondsdimensions to bursty features instead of terms,which can capture semantic and temporal in-formation.
Meanwhile, it significantly reducesthe number of non-zero entries in the repre-sentation.
We test it via scalable event de-tection, and experiments in a 10-year newsarchive show that our methods are both effec-tive and efficient.1 IntroductionMining retrospective events (Yang et al, 1998; Funget al, 2007; Allan et al, 2000) has been quite an im-portant research topic in text mining.
One standardway for that is to cluster news articles as events byfollowing a two-step approach (Yang et al, 1998):1) represent document as vectors and calculate simi-larities between documents; 2) run the clustering al-gorithm to obtain document clusters as events.1 Un-derlying text representation often plays a critical rolein this approach, especially for long text streams.
Inthis paper, our focus is to study how to representtemporal documents effectively for event detection.Classical text representation methods, i.e., VectorSpaceModel (VSM), have a few shortcomings whendealing with temporal documents.
The major one isthat it maps one dimension to one term, which com-pletely ignores temporal information, and thereforeVSM can never capture the evolving trends in textstreams.
See the example in Figure 1, D1 and D2?Corresponding author.1Post-processing may be also needed on the preliminarydocument clusters to refine the results.!"
!#$%&'()*+*,-./01#223 ,-./01#224Figure 1: A motivating example.
D1 and D2 are newsarticles about U.S. presidential election respectively inyears 2004 and 2008.may have a high similarity based on VSM due to thepresence of some general terms (e.g., ?election?)
re-lated to U.S. presidential election, although generalterms correspond to events in different periods (i.e.,November 2004 and November 2008).
Temporalinformation has to be taken into consideration forevent detection.
Another important issue is scala-bility, with the increasing of the number in the textstream, the size of the vocabulary, i.e., the numberof dimensions in VSM, can be very large, which re-quires a considerable amount of space for storageand time for downstream processing.To address these difficulties, in this paper, we pro-pose a burst based text representation method forscalable event detection.
The major novelty is to nat-urally incorporate temporal information into dimen-sions themselves instead of using external time de-caying functions (Yang et al, 1998).
We instantiatethis idea by using bursty features as basic representa-tion units of documents.
In this paper, bursty featurerefers to a sudden surge of the frequency of a singleterm in a text stream, and it is represented as the termitself together with the time interval during whichthe burst takes place.
For example, (Olympic,Aug-08-2008, Aug-24-2008)2 can be regardedas a bursty feature.
We also call the term in a bursty2Beijing 2008 Olympic Games43feature its bursty term.
In our model, each dimen-sion corresponds to a bursty feature, which containsboth temporal and semantic information.
Bursty fea-tures capture and reflect the evolving topic trends,which can be learnt by searching surge patterns instream data (Kleinberg, 2003).
Built on bursty fea-tures, our representation model can well adapt to textstreams with complex trends, and therefore providesa more reasonable temporal document representa-tion.
We further propose a split-cluster-merge algo-rithm to generate clusters as events.
This algorithmcan run a mutli-thread mode to speed up processing.Our contribution can be summarized as two as-pects: 1) we propose a novel burst-based text rep-resentation model, to our best knowledge, it is thefirst work which explicitly incorporates temporal in-formation into dimensions themselves; 2) we testthis representation model via scalable event detec-tion task on a very large news corpus, and extensiveexperiments show the proposed methods are both ef-fective and efficient.2 Burst-based Text RepresentationIn this section, we describe the proposed burst-basedtext representation model, denoted as BurstVSM.
InBurstVSM, each document is represented as onevector as in VSM, while the major novelty is that onedimension is mapped to one bursty feature insteadof one term.
In this paper, we define a bursty fea-ture f as a triplet (wf , tfs , tfe ), where w is the burstyterm and ts and te are the start and end timestampsof the bursty interval (period).
Before introductingBurstVSM, we first discuss how to identify burstyfeatures from text streams.2.1 Burst Detection AlgorithmWe follow the batch mode two-state automatonmethod from (Kleinberg, 2003) for bursty featuredetection.3 In this model, a stream of documentscontaining a term w are assumed to be generatedfrom a two-state automaton with a low frequencystate q0 and a high frequency state q1.
Each statehas its own emission rate (p0 and p1 respectively),and there is a probability for changing state.
If aninterval of high states appears in the optimal statesequence of some term, this term together with thisinterval is detected as a bursty feature.
To obtainall bursty features in text streams, we can performburst detection on each term in the vocabulary.
In-stead of using a fixed p0 and p1 in (Kleinberg, 2003),by following the moving average method (Vlachos3The news articles in one day is treated as a batch.et al, 2004) ,we parameterize p0 and p1 with thetime index for each batch, formally, we have p0(t)and p1(t) for the tth batch.
Given a term w, weuse a sliding window of length L to estimate p0(t)and p1(t) for the tth batch as follows: p0(t) =?j?WtNj,w?j?WtNjand p1(t) = p0(t) ?
s, where Nj,w andNj are w ?s document frequency and the total num-ber of documents in jth batch respectively.
s is ascaling factor lager than 1.0, indicating state q1 hasa faster rate, and it is empirically set as 1.5.
Wt is atime interval [max(t?L/2, 0), min(t+L/2, N)], andthe length of moving window L is set as 180 days.All the other parts remain the same as in (Kleinberg,2003).
Our detection method is denoted as TVBurst.2.2 Burst based text representation modelsWe apply TVBurst to all the terms in our vocabu-lary to identify a set of bursty features, denoted asB.
Given B, a document di(t) with timestamp t isrepresented as a vector of weights in bursty featuredimensions:di(t) = (di,1(t), di,2(t), ..., di,|B|(t)).We define the jth weight of di as followsdi,j =?tf-idfi,wBj , if t ?
[tBjs , tBje ] ,0, otherwise.When the timestamp of di is in the bursty inter-val of Bj and contains bursty term wBj , we set upthe weight using common used tf-idf method.
InBurstVSM, each dimension is mapped to one burstyfeature, and it considers both semantic and temporalinformation.
One dimension is active only when thedocument falls in the corresponding bursty interval.Usually, a document vector in BurstVSM has onlya few non-zero entries, which makes computation ofdocument similarities more efficient in large datasetscompared with traditional VSM.The most related work to ours is the boostVSMintroduced by (He et al, 2007b), it proposes toweight different term dimensions with correspond-ing bursty scores.
However, it is still based on termdimensions and fails to deal with terms with mul-tiple bursts.
Suppose that we are dealing with atext collection related with U.S. presidential elec-tions, Fig.
2 show sample dimensions for these threemethods.
In BurstVSM, one term with multiplebursts will be naturally mapped to different dimen-sions.
For example, two bursty features ( presiden-tial, Nov., 2004) and ( presidential, Nov., 2008 ) cor-respond to different dimensions in BurstVSM, while44Figure 2: One example for comparisons of different rep-resentation methods.
Terms in red box correspond tomultiple bursty periods.Table 1: Summary of different representation models.Here dimension reduction refers to the reduction of non-zero entries in representation vector.semantic temporal dimension trendinformation information reduction modelingVSM ?
?
?
badboostVSM ?
partially ?
moderateBurstVSM ?
?
?
goodVSM and boostVSM cannot capture such temporaldifferences.
Some methods try to design time de-caying functions (Yang et al, 1998), which decaythe similarity with the increasing of time gap be-tween two documents.
However, it requires effortsfor function selection and parameters tuning.
Wesummarize these discussions in Table 1.3 split-cluster-merge algorithm for eventdetectionIn this section, we discuss how to cluster documentsas events.
Since each document can be representedas a burst-based vector, we use cosine function tocompute document similarities.
Due to the large sizeof our news corpus, it is infeasible to cluster all thedocuments straightforward.
We develop a heuristicclustering algorithm for event detection, denoted assplit-cluster-merge, which includes three main steps,namely split, cluster and merge.
The idea is that wefirst split the dataset into small parts, then clusterthe documents of each part independently and finallymerge similar clusters from two consecutive parts.In our dataset, we find that most events last no morethan one month, so we split the dataset into parts bymonths.
After splitting, clustering can run in paral-lel for different parts (we useCLUTO4 as the cluster-ing tool), which significantly reduces total time cost.For merge, we merge clusters in consecutive monthswith an empirical threshold of 0.5.
The final clusters4www.cs.umn.edu/k?arypis/clutoare returned as identified events.4 Evaluation4.1 Experiment SetupWe used a subset of 68 millon deduplicatedtimestamped web pages generated from thisarchive (Huang et al, 2008).
Since our major focusis to detect events from news articles, we only keepthe web pages with keyword ?news?
in URL field.The final collection contains 11, 218, 581 articleswith total 1, 730, 984, 304 tokens ranging from 2000to 2009.
We run all the experiments on a 64-bit linuxserver with four Quad-Core AMD Opteron(tm) Pro-cessors and 64GB of RAM.
For split-cluster-mergealgorithm, we implement the cluster step in a multi-thread mode, so that different parts can be processedin parallel.4.2 Construction of test collectionWe manually construct the test collection for eventdetection.
To examine the effectiveness of event de-tection methods in different grains, we consider twotype of events in terms of the number of relevantdocuments, namely significant events and moder-ate events.
A significant event is required to haveat least 300 relevant docs, and a moderate event isrequired to have 10 ?
100 relevant docs.
14 grad-uate students are invited to generate the test collec-tion, starting with a list of 100 candidate seed eventsby referring to Xinhua News.5 For one target event,the judges first construct queries with temporal con-straints to retrieve candidate documents and thenjudge wether they are relevant or not.
Each doc-ument is assigned to three students, and we adoptthe majority-win strategy for the final judgment.
Fi-nally, by removing all candidate seed events whichneither belong to significant events nor moderateevents, we derive a test collection consisting of 24significant events and 40 moderate events.64.3 Evaluation metrics and baselinesSimilar to the evaluation in information retrieval ,given a target event, we evaluate the quality of thereturned ?relevant?
documents by systems.
We useaverage precision, average recall and mean averageprecision(MAP) as evaluation metrics.
A differenceis that we do not have queries, and the output of asystem is a set of document clusters.
So for a sys-tem, given an event in golden standard, we first se-lect the cluster (the system generates) which has the5http://news.xinhuanet.com/english6For access to the code and test collection, contact Xin Zhaovia batmanfly@gmail.com.45Table 2: Results of event detection.
Our proposed method is better than all the other baselines at confidence level 0.9.Signifcant Events Moderate EventsP R F MAP P R F MAPtimemines-?2(nouns) 0.52 0.2 0.29 0.11 0.22 0.27 0.24 0.09timemines-?2(NE) 0.61 0.18 0.28 0.08 0.27 0.25 0.26 0.13TVBurst+boostVSM 0.67 0.44 0.53 0.31 0.22 0.39 0.28 0.13swan+BurstVSM 0.74 0.56 0.64 0.48 0.39 0.54 0.45 0.38kleiberg+BurstVSM 0.68 0.63 0.65 0.52 0.35 0.53 0.42 0.36TVBurst+BurstVSM 0.78 0.69 0.73 0.63 0.4 0.61 0.48 0.39Table 3: Comparisons of average intra-class and inter-class similarity.Significant Events Moderate EventsMethods Intra Inter Intra InterTVBurst+boostVSM 0.234 0.132 0.295 0.007TVBurst+BurstVSM 0.328 0.014 0.480 0.004most relevant documents, then sort the documentsin the descending order of similarities with the clus-ter centroid and finally compute P, R ,F and MAP inthis cluster.
We perform Wilcoxon signed-rank testfor significance testing.We used the event detection method in (Swanand Allan, 2000) as baseline, denoted as timemines-?2.
As (Swan and Allan, 2000) suggested, wetried two versions: 1) using all nouns and 2) us-ing all named entities.
Recall that BurstVSM re-lies on bursty features as dimensions, we tested dif-ferent burst detection algorithms in our proposedBurstVSM model, including swan (Swan and Al-lan, 2000), kleinberg (Kleinberg, 2003) and our pro-posed TVBurst algorithm.4.4 Experiment resultsPreliminary results.
In Table 2, we can see that 1)BurstVSM with any of these three burst detection al-gorithms is significantly better than timemines-?2,suggesting our event detection method is very ef-fective; 2) TVBurst with BurstVSM gives the bestperformance, which suggests using moving averagebase probability will improve the performance ofburst detection.
We use TVBurst as the default burstdetection algorithm in later experiments.Then we compare the performance of differ-ent text representation models for event detection,namely BurstVSM and boostVSM (He et al, 2007b;He et al, 2007a).7 For different representation mod-els, we use split-cluster-merge as clustering algo-rithm.
Table 2 shows that BurstVSM is much ef-fecitve than boostVSM for event detection.
In fact,we empirically find boostVSM is appropriate for7We use the same parameter settings in the original paper.Table 4: Comparisons of observed runtime and storage.boostVSM BurstVSMAver.
# of non-zero entries per doc 149 14File size for storing vectors (gigabytes) 3.74 0.571Total # of merge 10,265,335 9,801,962Aver.
cluster cost per month (sec.)
355 55Total merge cost (sec.)
2,441 875Total time cost (sec.)
192,051 4,851clustering documents in a coarse grain (e.g., in topiclevel) but not for event detection.Intra-class and inter-class similarities.
In ourmethods, event detection is treated as documentclustering.
It is very important to study how similari-ties affect the performance of clustering.
To see whyour proposed representation methods are better thanboostVSM, we present the average intra-class simi-larity and inter-class similarity for different events inTable 3.8 We can see BurstVSM results in a largerintra-class similarity and a smaller inter-class simi-larity than boostVSM.Analysis of the space/time complexity.
We fur-ther analyze the space/time complexity of differentrepresentation models.
In Table 4.
We can see thatBurstVSM has much smaller space/time cost com-pared with boostVSM, and meanwhile it has a betterperformance for event detection (See Table 2).
Inburst-based representation, one document has fewernon-zero entries.Acknowledgement.
The core idea of this workis initialized and developped by Kai Fan.
Thiswork is partially supported by HGJ 2010 Grant2011ZX01042-001-001, NSFC Grant 61073082 and60933004.
Xin Zhao is supported by Google PhDFellowship (China).
We thank the insightful com-ments from Junjie Yao, Jing Liu and the anony-mous reviewers.
We have developped an online Chi-nese large-scale event search engine based on thiswork, visit http://sewm.pku.edu.cn/eventsearch formore details.8For each event in our golden standard, we have two clus-ters: relevant documents and non-relevant documents(withinthe event period).46ReferencesJames Allan, Victor Lavrenko, and Hubert Jin.
2000.First story detection in TDT is hard.
In Proceedingsof the ninth international conference on Informationand knowledge management.Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, andPhilip S. Yu.
2007.
Time-dependent event hierarchyconstruction.
In SIGKDD.Q.
He, K. Chang, and E. P. Lim.
2007a.
Using burstinessto improve clustering of topics in news streams.
InICDM.Qi He, Kuiyu Chang, Ee-Peng Lim, and Jun Zhang.2007b.
Bursty feature representation for clusteringtext streams.
In SDM.L.
Huang, L. Wang, and X. Li.
2008.
Achieving bothhigh precision and high recall in near-duplicate detec-tion.
In CIKM.J.
Kleinberg.
2003.
Bursty and hierarchical structure instreams.
Data Mining and Knowledge Discovery.Russell Swan and James Allan.
2000.
Automatic gener-ation of overview timelines.
In SIGIR.Michail Vlachos, Christopher Meek, Zografoula Vagena,and Dimitrios Gunopulos.
2004.
Identifying similari-ties, periodicities and bursts for online search queries.In SIGMOD.Yiming Yang, Tom Pierce, and Jaime Carbonell.
1998.A study of retrospective and on-line event detection.In SIGIR.47
