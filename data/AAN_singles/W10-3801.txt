Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 1?9,COLING 2010, Beijing, August 2010.Intersecting Hierarchical and Phrase-Based Models of Translation:Formal Aspects and AlgorithmsMarc Dymetman Nicola CanceddaXerox Research Centre Europe{marc.dymetman,nicola.cancedda}@xrce.xerox.comAbstractWe address the problem of construct-ing hybrid translation systems by inter-secting a Hiero-style hierarchical sys-tem with a phrase-based system andpresent formal techniques for doing so.We model the phrase-based componentby introducing a variant of weightedfinite-state automata, called ?-automata,provide a self-contained descriptionof a general algorithm for intersect-ing weighted synchronous context-freegrammars with finite-state automata, andextend these constructs to ?-automata.We end by briefly discussing complexityproperties of the presented algorithms.1 IntroductionPhrase-based (Och and Ney, 2004; Koehn etal., 2007) and Hierarchical (Hiero-style) (Chi-ang, 2007) models are two mainstream ap-proaches for building Statistical Machine Trans-lation systems, with different characteristics.While phrase-based systems allow a direct cap-ture of correspondences between surface-levellexical patterns, but at the cost of a simplistichandling of re-ordering, hierarchical systems arebetter able to constrain re-ordering, especiallyfor distant language pairs, but tend to producesparser rules and often lag behind phrase-basedsystems for less distant language pairs.
It mighttherefore make sense to capitalize on the com-plementary advantages of the two approaches bycombining them in some way.This paper attempts to lay out the formalprerequisites for doing so, by developing tech-niques for intersecting a hierarchical model anda phrase-based model.
In order to do so, one firstdifficulty has to be overcome: while hierarchicalsystems are based on the mathematically well-understood formalism of weighted synchronousCFG?s, phrase-based systems do not correspondto any classical formal model, although they areloosely connected to weighted finite state trans-ducers, but crucially go beyond these by allow-ing phrase re-orderings.One might try to address this issue by limitinga priori the amount of re-ordering, in the spiritof (Kumar and Byrne, 2005), which would allowto approximate a phrase-based model by a stan-dard transducer, but this would introduce furtherissues.
First, limiting the amount of reorder-ing in the phrase-based model runs contrary tothe underlying intuitions behind the intersection,namely that the hierarchical model should bemainly responsible for controlling re-ordering,and the phrase-based model mainly responsiblefor lexical choice.
Second, the transducer result-ing from the operation could be large.
Third,even if we could represent the phrase-basedmodel through a finite-state transducer, intersect-ing this transducer with the synchronous CFGwould actually be intractable in the general case,as we indicate later.We then take another route.
For a fixed sourcesentence x, we show how to construct an au-tomaton that represents all the (weighted) tar-get sentences that can be produced by apply-ing the phrase based model to x.
However, this??-automaton?
is non-standard in the sense thateach transition is decorated with a set of sourcesentence tokens and that the only valid paths are1those that do not traverse two sets containing thesame token (in other words, valid paths cannot?consume?
the same source token twice).The reason we are interested in ?-automatais the following.
First, it is known that inter-secting a synchronous grammar simultaneouslywith the source sentence x and a (standard) targetautomaton results in another synchronous gram-mar; we provide a self-contained description ofan algorithm for performing this intersection, inthe general weighted case, and where x is gener-alized to an arbitrary source automaton.
Second,we extend this algorithm to ?-automata.
Theresulting weighted synchronous grammar repre-sents, as in Hiero, the ?parse forest?
(or ?hy-pergraph?)
of all weighted derivations (that isof all translations) that can be built over x, butwhere the weights incorporate knowledge of thephrase-based component; it can therefore formthe basis of a variety of dynamic programmingor sampling algorithms (Chiang, 2007; Blunsomand Osborne, 2008), as is the case with standardHiero-type representations.
While in the worstcase the intersected grammar can contain an ex-ponential number of nonterminals, we argue thatsuch combinatorial explosion will not happen inpractice, and we also briefly indicate formal con-ditions under which it will not be allowed to hap-pen.2 Intersecting weighted synchronousCFG?s with weighted automataWe assume that the notions of weighted finite-state automaton [W-FSA] and weighted syn-chronous grammar [W-SCFG] are known (forshort descriptions see (Mohri et al, 1996) and(Chiang, 2006)), and we consider:1.
A W-SCFG G, with associated sourcegrammar Gs (resp.
target grammar Gt); theterminals of Gs (resp.
Gt) vary over thesource vocabulary Vs (resp.
target vocab-ulary Vt).2.
A W-FSA As over the source vocabularyVs, with initial state s# and final state s$.3.
A W-FSA At over the target vocabulary Vt,with initial state t# and final state t$.The grammar G defines a weighted synchronouslanguage LG over (Vs, Vt), the automaton As aweighted language Ls over Vs, and the automa-ton At a weighted language Lt over Vt. Wethen define the intersection language L?
betweenthese three languages as the synchronous lan-guage denoted L?
= Ls e LG e Lt over (Vs, Vt)such that, for any pair (x, y) of a source and atarget sentence, the weight L?
(x, y) is definedby L?
(x, y) ?
Ls(x) ?
LG(x, y) ?
Lt(y), whereLs(x), LG(x, y), Lt(y) are the weights associ-ated to each of the component languages.It is natural to ask whether there exists a syn-chronous grammar G?
generating the languageL?, which we will now show to be the case.1Our approach is inspired by the construction in(Bar-Hillel et al, 1961) for the intersection of aCFG and an FSA and the observation in (Lang,1994) relating this construction to parse forests,and also partially from (Satta, 2008), although,by contrast to that work, our construction, (i)is done simultaneously rather than as the se-quence of intersecting As with G, then the re-sulting grammar with At, (ii) handles weightedformalisms rather than non-weighted ones.We will describe the construction of G?
basedon an example, from which the general construc-tion follows easily.
Consider a W-SCFG gram-mar G for translating between French and En-glish, with initial nonterminal S, and containingamong others the following rule:N?
A manque a` B / B misses A : ?, (1)where the source and target right-hand sides areseparated by a slash symbol, and where ?
is anon-negative real weight (interpreted multiplica-tively) associated with the rule.Now let?s consider the following ?rulescheme?
:t0s0Nt3s4 ?
t2s0At3s1 s1manques2 s2 a`s3 t0s3Bt1s4 /t0s3Bt1s4 t1missest2 t2s0At3s1 (2)1We will actually only need the application of this resultto the case where As is a ?degenerate?
automaton describ-ing a single source sentence x, but the general constructionis not harder to do than this special case and the resultingformat for G?
is well-suited to our needs below.2This scheme consists in an ?indexed?
version ofthe original rule, where the bottom indices sicorrespond to states of As (?source states?
), andthe top indices ti to states of At (?target states?
).The nonterminals are associated with two sourceand two target indices, and for the same nonter-minal, these four indices have to match acrossthe source and the target RHS?s of the rule.
Asfor the original terminals, they are replaced by?indexed terminals?, where source (resp.
tar-get) terminals have two source (resp.
target) in-dices.
The source indices appear sequentiallyon the source RHS of the rule, in the patterns0, s1, s1, s2, s2 .
.
.
sm?1, sm, with the nonter-minal on the LHS receiving source indices s0and sm, and similarly the target indices appearsequentially on the target RHS of the rule, in thepattern t0, t1, t1, t2, t2 .
.
.
tn?1, tn, with the non-terminal on the LHS receiving target indices t0and tn.
To clarify, the operation of associatingindices to terminals and nonterminals can be de-composed into three steps:s0Ns4 ?
s0As1 s1manques2 s2 a` s3 s3Bs4 /B misses At0Nt3 ?
A manque a` B /t0Bt1 t1missest2 t2At3t0s0Nt3s4 ?
t2s0At3s1 s1manques2 s2 a` s3 t0s3Bt1s4 /t0s3Bt1s4 t1missest2 t2s0At3s1where the first two steps corresponds to handlingthe source and target indices separately, and thethird step then assembles the indices in order toget the same four indices on the two copies ofeach RHS nonterminal.
The rule scheme (2) nowgenerates a family of rules, each of which corre-sponds to an arbitrary instantiation of the sourceand target indices to states of the source and tar-get automata respectively.
With every such ruleinstantiation, we associate a weight ??
which isdefined as:??
?
?
?
?si s-termsi+1?As(si, s-term, si+1)?
?tj t-termtj+1?At(tj , t-term, tj+1), (3)where the first product is over the indexed sourceterminals sis-termsi+1 , the second productover the indexed target terminals tj t-termtj+1 ;?As(si, s-term, si+1) is the weight of the transi-tion (si, s-term, si+1) according to As, and sim-ilarly for ?At(tj , t-term, tj+1).
In these prod-ucts, it may happen that ?As(si, s-term, si+1) isnull (and similarly for At), and in such a case,the corresponding rule instantiation is consid-ered not to be realized.
Let us consider the multi-set of all the weighted rule instantiations for (1)computed in this way, and for each rule in thecollection, let us ?forget?
the indices associatedto the terminals.
In this way, we obtain a col-lection of weighted synchronous rules over thevocabularies Vs and Vt, but where each nonter-minal is now indexed by four states.2When we apply this procedure to all the rulesof the grammar G, we obtain a new weightedsynchronous CFG G?, with start symbol t#s#St$s$ ,for which we have the following Fact, of whichwe omit the proof for lack of space.Fact 1.
The synchronous language LG?
associ-ated with G?
is equal to L?
= Ls e LG e Lt.The grammar G?
that we have just constructeddoes fulfill the goal of representing the bilat-eral intersection that we were looking for, butit has a serious defect: most of its nontermi-nals are improductive, that is, can never pro-duce a bi-sentence.
If a rule refers to such animproductive nonterminal, it can be eliminatedfrom the grammar.
This is the analogue for aSCFG of the classical operation of reduction forCFG?s; while, conceptually, we could start fromG?
and perform the reduction by deleting themany rules containing improductive nontermi-nals, it is equivalent but much more efficient todo the reverse, namely to incrementally add theproductive nonterminals and rules of G?
startingfrom an initially empty set of rules, and by pro-ceeding bottom-up starting from the terminals.We do not detail this process, which is relatively2It is possible that the multiset obtained by this simpli-fying operation contains duplicates of certain rules (pos-sibly with different weights), due to the non-determinismof the automata: for instance, two sequences suchas?s1manques2 s2 a`s3 ?
and ?s1manques?2 s?2 a`s3 ?
become in-distinguishable after the operation.
Rather than producingmultiple instances of rules in this way, one can ?conflate?them together and add their weights.3straightforward.3A note on intersecting SCFGs with transduc-ers Another way to write Ls e LG e Lt is asthe intersection (Ls ?
Lt) ?
LG.
(Ls ?
Lt) canbe seen as a rational language (language gener-ated by a finite state transducer) of an especiallysimple form over Vs ?
Vt .
It is then naturalto ask whether our previous construction can begeneralized to the intersection of G with an arbi-trary finite-state transducer.
However, this is notthe case.
Deciding the emptiness problem forthe intersection between two finite state trans-ducers is already undecidable, by reduction toPost?s Correspondence problem (Berstel, 1979,p.
90) and we have extended the proof of this factto show that intersection between a synchronousCFG and a finite state transducer also has an un-decidable emptiness problem (the proof relies onthe fact that a finite state transducer can be sim-ulated by a synchronous grammar).
A fortiori,this intersection cannot be represented throughan (effectively constructed) synchronous CFG.3 Phrase-based models and ?-automata3.1 ?-automata: definitionLet Vs be a source vocabulary, Vt a target vocab-ulary.
Let x = x1, .
.
.
, xM be a fixed sequenceof words over a certain source vocabulary Vs.Let us denote by z a token in the sequence x,and by Z the set of the M tokens in x.
A ?-automaton over x has the general form of a stan-dard weighted automaton over the target vocabu-lary, but where the edges are also decorated withelements ofP(Z), the powerset ofZ (see Fig.
1).An edge in the ?-automaton between two statesq and q?
then carries a label of the form (?, ?
),where ?
?
P(Z) and ?
?
Vt (note that here wedo not allow ?
to be the empty string ).
A pathfrom the initial state of the automaton to its fi-nal state is defined to be valid iff each token of xappears in exactly one label of the path, but notnecessarily in the same order as in x.
As usual,the output associated with the path is the ordered3This bottom-up process is analogous to chart-parsing,but here we have decomposed the construction into firstbuilding a semantics-preserving grammar and then reduc-ing it, which we think is formally neater.sequence of target labels on that path, and theweight of the path is the product of the weightson its edges.
?-automata and phrase-based translationA mainstream phrase-based translation systemsuch as Moses (Koehn et al, 2007) can be ac-counted for in terms of ?-automata in the follow-ing way.
To simplify exposition, we assume thatthe language model used is a bigram model, butany n-gram model can be accommodated.
Then,given a source sentence x, decoding works by at-tempting to construct a sequence of phrase-pairsof the form (x?1, y?1), ..., (x?k, y?k) such that eachx?i corresponds to a contiguous subsequence oftokens of x, the x?i?s do not overlap and com-pletely cover x, but may appear in a differentorder than that of x; the output associated withthe sequence is simply the concatenation of allthe y?i?s in that sequence.4 The weight associ-ated with the sequence of phrase-pairs is thenthe product (when we work with probabilitiesrather than log-probabilities) of the weight ofeach (x?i+1, y?i+1) in the context of the previous(x?i, y?i), which consists in the product of severalelements: (i) the ?out-of-context?
weight of thephrase-pair (x?i+1, y?i+1) as determined by its fea-tures in the phrase table, (ii) the language modelprobability of finding y?i+1 following y?i,5 (iii)the contextual weight of (x?i+1, y?i+1) relative to(x?i, y?i) corresponding to the distorsion cost of?jumping?
from the token sequence x?i to the to-ken sequence x?i+1 when these two sequencesmay not be consecutive in x.6Such a model can be represented by a ?-automaton, where each phrase-pair (x?, y?)
?
for4We assume here that the phrase-pairs (x?i, y?i) are suchthat y?i is not the empty string (this constraint could be re-moved by an adaptation of the -removal operation (Mohri,2002) to ?-automata).5This is where the bigram assumption is relevant: fora trigram model, we may need to encode in the automatonnot only the immediately preceding phrase-pair, but alsothe previous one, and so on for higher-order models.
Analternative is to keep the n-gram language model outsidethe ?-automaton and intersect it later with the grammar G?obtained in section 4, possibly using approximation tech-niques such as cube-pruning (Chiang, 2007).6Any distorsion model ?
in particular ?lexicalized re-ordering?
?
that only depends on comparing two consec-utive phrase-pairs can be implemented in this way.4# hb ark$tcl ftcl1tcl2{ces}these{avocats, marrons}totally?lawyers?corrupt {cuits}finished{sont}are{avocats}avocadoes{sont}are{cuits}cooked{$}${marrons}brown{$}$# hb ark$tcl ftcl1tcl2thesetotallylawyerscorruptfinishedareavocadoesare cooked $brown$Figure 1: On the top: a ?-automaton with two valid paths shown.
Each box denotes a state corresponding to a phrasepair, while states internal to a phrase pair (such as tcl1 and tcl2) are not boxed.
Above each transition we have indicatedthe corresponding target word, and below it the corresponding set of source tokens.
We use a terminal symbol $ to denotethe end of sentence both on the source and on the target.
The solid path corresponds to the output these totally corruptlawyers are finished, the dotted path to the output these brown avocadoes are cooked.
Note that the source tokens are notnecessarily consumed in the order given by the source, and that, for example, there exists a valid path generating theseare totally corrupt lawyers finished and moving according to h ?
r ?
tcl1 ?
tcl2 ?
tcl ?
f ; Note, however, thatthis does not mean that if a biphrase such as (marrons avocats, avocado chestnuts) existed in the phrasetable, it would be applicable to the source sentence here: because the source words in this biphrase would not match theorder of the source tokens in the sentence, the biphrase would not be included in the ?-automaton at all.
On the bottom:The target W-FSA automaton At associated with the ?-automaton, where we are ignoring the source tokens (but keepingthe same weights).x?
a sequence of tokens in x and (x?, y?)
an entryin the global phrase table ?
is identified with astate of the automaton and where the fact that thephrase-pair (x?
?, y??)
= ((x1, ..., xk), (y1, ..., yl))follows (x?, y?)
in the decoding sequence is mod-eled by introducing l ?internal?
transitions withlabels (?, y1), (?, y2), ..., (?, yl), where ?
={x1, ..., xk}, and where the first transition con-nects the state (x?, y?)
to some unique ?internalstate?
q1, the second transition the state q1 tosome unique internal state q2, and the last tran-sition qk to the state (x?
?, y??
).7 Thus, a state(x?
?, y??)
essentially encodes the previous phrase-pair used during decoding, and it is easy to seethat it is possible to account for the differentweights associated with the phrase-based modelby weights associated to the transitions of the ?-automaton.87For simplicity, we have chosen to collect the set of allthe source tokens {x1, ..., xk} on the first transition, but wecould distribute it on the l transitions arbitrarily (but keep-ing the subsets disjoint) without changing the semantics ofwhat we do.
This is because once we have entered one ofthe l internal transitions, we will always have to traversethe remaining internal transitions and collect the full set ofsource tokens.8By creating states such as ((x?, y?
), (x?
?, y??))
that en-Example Let us consider the following Frenchsource sentence x: ces avocats marrons sontcuits (idiomatic expression for these totally cor-rupt lawyers are finished).
Let?s assume that thephrase table contains the following phrase pairs:h: (ces, these)a: (avocats, avocados)b: (marrons, brown)tcl: (avocats marrons,totally corrupt lawyers)r: (sont, are)k: (cuits, cooked)f: (cuits, finished).An illustration of the corresponding ?-automaton SA is shown at the top of Figure 1,with only a few transitions made explicit, andwith no weights shown.9code the two previous phrase-pairs used during decoding,it is possible in principle to account for a trigram languagemodel, and similarly for higher-order LMs.
This is simi-lar to implementing n-gram language models by automatawhose states encode the n?
1 words previously generated.9Only two (valid) paths are shown.
If we had shown thefull ?-automaton, then the graph would have been ?com-plete?
in the sense that for any two box states B,B?, wewould have shown a connection B ?
B?1... ?
B?k?1 ?B?, where the B?i are internal states, and k is the length ofthe target side of the biphrase B?.54 Intersecting a synchronous grammarwith a ?-automatonIntersection of a W-SCFG with a ?-automaton If SA is a ?-automaton overinput x, with each valid path in SA we asso-ciate a weight in the same way as we do fora weighted automaton.
For any target wordsequence in V ?t we can then associate the sumof the weights of all valid paths outputting thatsequence.
The weighted language LSA,x overVt obtained in this way is called the languageassociated with SA.
Let G be a W-SCFG overVs, Vt, and let us denote by LG,x the weightedlanguage over Vs, Vt corresponding to theintersection {x} e G e V ?t , where {x} denotesthe language giving weight 1 to x and weight 0to other sequences in V ?s , and V ?t denotes thelanguage giving weight 1 to all sequences in V ?t .Note that non-null bi-sentences in LG,x havetheir source projection equal to x and thereforeLG,x can be identified with a weighted languageover Vt.
The intersection of the languages LSA,xand LG,x is denoted by LSA,x e LG,x.Example Let us consider the following W-SCFG (where again, weights are not explicitlyshown, and where we use a terminal symbol $to denote the end of a sentence, a technicalityneeded for making the grammar compatible withthe SA automaton of Figure 1):S ?
NP VP $ / NP VP $NP ?
ces N A / these A NVP ?
sont A / are AA ?
marrons / brownA ?
marrons / totally corruptA ?
cuits / cookedA ?
cuits / finishedN ?
avocats / avocadoesN ?
avocats / lawyersIt is easy to see that, for instance, the sen-tences: these brown avocadoes are cooked $,these brown avocadoes are finished $, and thesetotally corrupt lawyers are finished $ all belongto the intersection LSA,x e LG,x, while the sen-tences these avocadoes brown are cooked $, to-tally corrupt lawyers are finished these $ belongonly to LSA,x.Building the intersection We now describehow to build a W-SCFG that represents the inter-section LSA,x eLG,x.
We base our explanationson the example just given.A Relaxation of the Intersection At thebottom of Figure 1, we show how we can as-sociate an automaton At with the ?-automatonSA: we simply ?forget?
the source-sides of thelabels carried by the transitions, and retain all theweights.
As before, note that we are only show-ing a subset of the transitions here.All valid paths for SAmap into valid paths forAt (with the same weights), but the reverse is nottrue because some validAt paths can correspondto traversals of SA that either consume severaltime the same source token or do not consume allsource tokens.
For instance, the sentence thesebrown avocadoes brown are $ belongs to thelanguage of At, but cannot be produced by SA.Let?s however consider the intersection {x} eG e At, where, with a slight abuse of notation,we have notated {x} the ?degenerate?
automatonrepresenting the sentence x, namely the automa-ton (with weights on all transitions equal to 1):?ces marronsavocats sont cuits $0 1 2 3 4 5 6This is a relaxation of the true intersection, butone that we can represent through a W-SCFG, aswe know from section 2.10This being noted, we now move to the con-struction of the full intersection.The full intersection We discussed in sec-tion 2 how to modify a synchronous grammarrule in order to produce the indexed rule scheme(2) in order to represent the bilateral intersectionof the grammar with two automata.
Let us redothat construction here, in the case of our example10Note that, in the case of our very simple example, anytarget string that belongs to this relaxed intersection (whichconsists of the eight sentences these {brown | totally cor-rupt} {avocadoes | lawyers} are {cooked | finished}) actu-ally belongs to the full intersection, as none of these sen-tences corresponds to a path in SA that violates the token-consumption constraint.
More generally, it may often bethe case in practice that the W-SCFG, by itself, providesenough ?control?
of the possible target sentences to pre-vent generation of sentences that would violate the token-consumption constraints, so that there may be little differ-ence in practice between performing the relaxed intersec-tion {x} e G e At and performing the full intersection{x} eG e LSA,x.6W-SCFG, of the target automaton represented onthe bottom of Figure 1, and of the source automa-ton {x}.The construction is then done in three steps:s0NPs3 ?
s0cess1 s1Ns2 s2As3 /these A Nt0NPt3 ?
ces N A /t0theset1 t1At2 t2Nt3t0s0NPt3s3 ?
s0cess1 t2s1Nt3s2 t1s2At2s3 /t0theset1 t1s2At2s3 t2s1Nt3s2In order to adapt that construction to the casewhere we want the intersection to be with a ?-automaton, what we need to do is to further spe-cialize the nonterminals.
Rather than specializ-ing a nonterminal X in the form tsXt?s?
, we spe-cialize it in the form: tsXt?,?s?
, where ?
representsa set of source tokens that correspond to ?collect-ing?
the source tokens in the ?-automaton alonga path connecting the states t and t?.11We then proceed to define a new rule schemeassociated to our rule, which is obtained as be-fore in three steps, as follows.s0NPs3 ?
s0cess1 s1Ns2 s2As3 /these A Nt0NPt3,?03 ?
ces N A /t0theset1,?01 t1At2,?12 t2Nt3,?23t0s0NPt3,?03s3 ?
s0cess1 t2s1Nt3,?23s2 t1s2At2,?12s3 /t0theset1,?01 t1s2At2,?12s3 t2s1Nt3,?23s2The only difference with our previous tech-nique is in the addition of the ?
?s to the top in-dices.
Let us focus on the second step of the an-notation process:t0NPt3,?03 ?
ces N A /t0theset1,?01 t1At2,?12 t2Nt3,?2311To avoid a possible confusion, it is important to noteright away that ?
is not necessarily related to the tokensappearing between the positions s and s?
in the source sen-tence (that is, between these states in the associated sourceautomaton), but is defined solely in terms of the source to-kens along the t, t?
path.
See example with ?persons?
and?people?
below.Conceptually, when instanciating this scheme,the ti?s may range over all possible states ofthe ?-automaton, and the ?ij over all subsets ofthe source tokens, but under the following con-straints: the RHS ?
?s (here ?01, ?12, ?23) mustbe disjoint and their union must be equal to the ?on the LHS (here ?03).
Additionally, a ?
associ-ated with a target terminal (as ?01 here) must beequal to the token set associated to the transitionthat this terminal realizes between ?-automatonstates (here, this means that ?01 must be equalto the token set {ces} associated with the transi-tion between t0, t1 labelled with ?these?).
If weperform all these instantiations, compute theirweights according to equation (3), and finally re-move the indices associated with terminals in therules (by adding the weights of the rules only dif-fering by the indices of terminals, as done previ-ously), we obtain a very large ?raw?
grammar,but one for which one can prove direct coun-terpart of Fact 1.
Let us call, as before G?
theraw W-SCFG obtained, its start symbol beingt#s#St$,?alls$ , with ?all the set of all source tokensin x.Fact 2.
The synchronous language LG?
associ-ated with G?
is equal to ({x}, LSA,x e LG,x).The grammar that is obtained this way, despitecorrectly representing the intersection, containsa lot of useless rules, this being due to the factthat many nonterminals can not produce any out-put.
The situation is wholly similar to the caseof section 2, and the same bottom-up techniquescan be used for activating nonterminals and rulesbottom-up.The algorithm is illustrated in Figure 2, wherewe have shown the result of the process of acti-vating in turn the nonterminals (abbreviated by)N1, A1, A2, NP1, VP1, S1.
As a consequenceof these activations, the original grammar ruleNP ?
ces N A /these A N (for instance)becomes instantiated as the rule:#0 NPtcl,{ces,avocats,marrons}3 ?0ces1 tcl21 Ntcl,?2 h2Atcl2,{avocats,marrons}3 /#theseh,{ces} h2Atcl2,{avocats,marrons}3 tcl21 Ntcl,?27# h r $tcl ftcl1tcl2{ces}these{avocats, marrons}totally?lawyers?corrupt {cuits}finished{sont}are{$}$ces marronsavocats sont cuits $S 1S 1N 1 A1 A2VP1NP1NP1 VP1A2A1N 10 1 2 3 4 5 62 ,1 22,{ , }2 3# ,{ , , }0 3,{ }4 5,{ , }3 5# ,{ , , , , ,$ $}0 61:1:1:2 :1:1:tcl tclh tcl avocats marronstcl ces avocats marronsr f cuitstcl f sont cuitsces avocats marrons sont cuitsN NA ANP NPA AVP VPS S?Figure 2: Building the intersection.
The bottom of the figure shows some active non-terminals associated with the sourcesequence, at the top these same non-terminals associated with a sequence of transitions in the ?-automaton, correspondingto the target sequence these totally corrupt lawyers are finished $.
To avoid cluttering the drawing, we have used theabbreviations shown on the right.
Note that while A1 only spans marrons in the bottom chart, it is actually decorated withthe source token set {avocats,marrons}: such a ?disconnect?
between the views that the W-SCFG and the ?-automatonhave of the source tokens is not ruled out.that is, after removal of the indices on terminals:#0 NPtcl,{ces,avocats,marrons}3 ?ces tcl21 Ntcl,?2 h2Atcl2,{avocats,marrons}3 /these h2Atcl2,{avocats,marrons}3 tcl21 Ntcl,?2Note that while the nonterminal tcl21 Ntcl,?2 byitself consumes no source token (it is associatedwith the empty token set), any actual use of thisnonterminal (in this specific rule or possibly insome other rule using it) does require travers-ing the internal node tcl2 and therefore all theinternal nodes ?belonging?
to the biphrase tcl(because otherwise the path from # to $ wouldbe disconnected); in particular this involves con-suming all the tokens on the source side of tcl,including ?avocats?.12Complexity considerations The bilateral in-tersection that we defined between a W-SCFG12In particular there is no risk that a derivation relativeto the intersected grammar generates a target containingtwo instances of ?lawyers?, one associated to the expansionof tcl21 Ntcl,?2 and consuming no source token, and anotherone associated with a different nonterminal and consumingthe source token ?avocats?
: this second instance would in-volve not traversing tcl1, which is impossible as soon astcl21 Ntcl,?2 is used.and two W-FSA?s in section 2 can be shown tobe of polynomial complexity in the sense that ittakes polynomial time and space relative to thesum of the sizes of the two automata and of thegrammar to construct the (reduced) intersectedgrammar G?, under the condition that the gram-mar right-hand sides have length bounded by aconstant.13The situation here is different, because theconstruction of the intersection can in princi-ple introduce nonterminals indexed not only bystates of the automata, but also by arbitrary sub-sets of source tokens, and this may lead in ex-treme cases to an exponential number of rules.Such problems however can only happen in sit-uations where, in a nonterminal tsXt?,?s?
, the set?
is allowed to contain tokens that are ?unre-lated?
to the token set {personnes} appearingbetween s and s?
in the source automaton.
An il-lustration of such a situation is given by the fol-lowing example.
Suppose that the source sen-13If this condition is removed, and for the simpler casewhere the source (resp.
target) automaton encodes a singlesentence x (resp.
y), (Satta and Peserico, 2005) have shownthat the problem of deciding whether (x, y) is recognizedby G is NP-hard relative to the sum of the sizes.
A conse-quence is then that the grammar G?
cannot be constructedin polynomial time unless P = NP .8tence contains the two tokens personnes andgens between positions i, i + 1 and j, j + 1 re-spectively, with i and j far from each other, thatthe phrase table contains the two phrase pairs(personnes, persons) and (gens, people), butthat the synchronous grammar only contains thetwo rules X ?
personnes/people and Y ?gens/persons, with these phrases and rules ex-hausting the possibilities for translating gensand personnes; Then the intersected grammarwill contain such nonterminals as tiXt?,{gens}i+1 andrjYr?,{personnes}j+1 , where in the first case the tokenset {gens} in the first nonterminal is unrelated tothe tokens appearing between i, i + 1, and simi-larly in the second case.Without experimentation on real cases, itis impossible to say whether such phenomenawould empirically lead to combinatorial explo-sion or whether the synchronous grammar wouldsufficiently constrain the phrase-base component(whose re-ordering capabilities are responsiblein fine for the potential NP-hardness of the trans-lation process) to avoid it.
Another possible ap-proach is to prevent a priori a possible combi-natorial explosion by adding formal constraintsto the intersection mechanism.
One such con-straint is the following: disallow introduction oftiXt?,?j when the symmetric difference between?
and the set of tokens between positions i andj in the source sentence has cardinality largerthan a small constant.
Such a constraint couldbe interpreted as keeping the SCFG and phrase-base components ?in sync?, and would be betteradapted to the spirit of our approach than limit-ing the amount of re-ordering permitted to thephrase-based component, which would contra-dict the reason for using a hierarchical compo-nent in the first place.5 ConclusionIntersecting hierarchical and phrase-based mod-els of translation could allow to capitalize oncomplementarities between the two approaches.Thus, one might train the hierarchical compo-nent on corpora represented at the part-of-speechlevel (or at a level where lexical units are ab-stracted into some kind of classes) while thephrase-based component would focus on transla-tion of lexical material.
The present paper doesnot have the ambition to demonstrate that suchan approach would improve translation perfor-mance, but only to provide some formal meansfor advancing towards that goal.ReferencesBar-Hillel, Y., M. Perles, and E. Shamir.
1961.
On for-mal properties of simple phrase structure grammars.Zeitschrift fu?r Phonetik, Sprachwissenschaft und Kom-municationsforschung, 14:143?172.Berstel, Jean.
1979.
Transductions and Context-Free Lan-guages.
Teubner, Stuttgart.Blunsom, P. and M. Osborne.
2008.
Probabilistic inferencefor machine translation.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 215?223.
Association for ComputationalLinguistics.
Slides downloaded.Chiang, David.
2006.
An introduction to synchronousgrammars.
www.isi.edu/?chiang/papers/synchtut.pdf, June.Chiang, David.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33:201?228.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In ACL.
The Associationfor Computer Linguistics.Kumar, Shankar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.
InProc.
HLT/EMNLP.Lang, Bernard.
1994.
Recognition can be harder than pars-ing.
Computational Intelligence, 10:486?494.Mohri, Mehryar, Fernando Pereira, and Michael Riley.1996.
Weighted automata in text and speech processing.In ECAI-96 Workshop on Extended Finite State Modelsof Language.Mohri, Mehryar.
2002.
Generic epsilon-removal and inputepsilon-normalization algorithms for weighted trans-ducers.
International Journal of Foundations of Com-puter Science, 13:129?143.Och, Franz Josef and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Comput.
Linguist., 30(4):417?449.Satta, Giorgio and Enoch Peserico.
2005.
Some compu-tational complexity results for synchronous context-freegrammars.
In HLT ?05: Proceedings of the conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, pages 803?810,Morristown, NJ, USA.
Association for ComputationalLinguistics.Satta, Giorgio.
2008.
Translation algorithms by means oflanguage intersection.
Submitted.
www.dei.unipd.it/?satta/publ/paper/inters.pdf.9
