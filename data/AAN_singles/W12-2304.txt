Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 26?34,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsA Regularized Compression Method To Unsupervised Word SegmentationRuey-Cheng Chen, Chiung-Min Tsai and Jieh HsiangNational Taiwan University1 Roosevelt Rd.
Sec.
4Taipei 106, Taiwanrueycheng@turing.csie.ntu.edu.twcmtsai@mail.lis.ntu.edu.twjhsiang@ntu.edu.twAbstractLanguages are constantly evolving throughtheir users due to the need to communicatemore efficiently.
Under this hypothesis, weformulate unsupervised word segmentation asa regularized compression process.
We re-duce this process to an optimization problem,and propose a greedy inclusion solution.
Pre-liminary test results on the Bernstein-Ratnercorpus and Bakeoff-2005 show that the ourmethod is comparable to the state-of-the-art interms of effectiveness and efficiency.1 IntroductionUnsupervised word segmentation has been a popularresearch subject due to its close connection to lan-guage acquisition.
It has attracted researchers fromdifferent communities, including linguistics, cogni-tive science, and machine learning, to investigatehow human beings develop and harness their lan-guages, and, more importantly, how knowledge isacquired.In this paper we propose a new formulation to theunsupervised word segmentation problem.
Our ideais based on the observation that language evolves be-cause of the need to reduce communication efforts.For instance, new terminologies, abbreviations, andslang that carry complex semantics which cannot beefficiently expressed in the original languages are in-vented so that concepts can be conveyed.
Such anevolution, we hypothesize, is limited to the extentwhere the evolved vocabulary exhibits similar com-plexity as the original one, in light of reducing theextra cost to pick up the new language.
This processis realized as an optimization problem called regu-larized compression, which gets this name from itsanalogy to text compression.The rest of the paper is organized as follows.We briefly summarize related work on unsupervisedword segmentation in Section 2.
In Section 3, we in-troduce the proposed formulation.
The iterative al-gorithm and other technical details for solving theoptimization problem are covered in Section 4.
InSection 5, we describe the evaluation procedure anddiscuss the experimental results.
Finally, we presentconcluding remarks in Section 6.2 Related WorkThe past few years have seen many nonparametricBayesian methods developed to model natural lan-guages.
Many such applications were applied toword segmentation and have collectively reshapedthe entire research field.
Two most notable exam-ples are hierarchical Bayesian models and the min-imum description length principle.
Our method fitsin the latter category since we use this principle tooptimize model parameters.Hierarchical Bayesian methods were first in-troduced to complement conventional probabilisticmethods to facilitate context-aware word generation.Goldwater et al (2006) used hierarchical Dirichletprocesses (HDP) to induce contextual word mod-els.
Their approach was a significant improvementover conventional probabilistic methods, and has in-spired further explorations into more advanced hi-erarchical modeling techniques.
Such examples in-clude the nested Pitman-Yor process (Mochihashi etal., 2009), a sophisticated installment for hierarchi-26cal modeling at both word and character levels, andadaptor grammars (Johnson and Goldwater, 2009), aframework that aligns HDP to probabilistic context-free grammars.The minimum description length (MDL) princi-ple, originally developed in the context of infor-mation theory, was adopted in Bayesian statisticsas a principled model selection method (Rissanen,1978).
Its connection to lexical acquisition wasfirst uncovered in behavioral studies, and early ap-plications focused mostly on applying MDL to in-duce word segmentation that results in compact lex-icons (Kit and Wilks, 1999; Yu, 2000; Argamon etal., 2004).
More recent approaches (Zhikov et al,2010; Hewlett and Cohen, 2011) used MDL in com-bination with existing algorithms, such as branch-ing entropy (Tanaka-Ishii, 2005; Jin and Ishii, 2006)and bootstrap voting experts (Hewlett and Cohen,2009), to determine the best segmentation parame-ters.
On various benchmarks, MDL-powered algo-rithms have achieved state-of-the-art performance,sometimes even surpassing that of the most sophis-ticated hierarchical modeling methods.3 Regularized Compression3.1 PreliminariesConsider that the unsegmented text consists ofK ut-terances and totally of N characters.
We denote thetext as a sequence of characters c = ?c1, .
.
.
, cN ?,as if conceptually concatenating all the K utter-ances into one string.
The positions of all the ut-terance boundaries in c are represented as a setU = {u0 = 0, u1, .
.
.
, uK}.
In other words, thek-th utterance (k = 1, .
.
.
,K) is stored as the sub-sequence ?cuk?1+1, .
.
.
, cuk?
in c.A segmented text is denoted as a sequence ofwords w = ?w1, w2, .
.
.
, wM ?
for some M < N .
Itrepresents the same piece of text as c does.
The wordsequence w is said to respect the utterance bound-aries U if any word in the sequence does not spanover two utterances.
Unique elements in a charac-ter or word sequence implicitly define an alphabetset (or lexicon).
Hereafter, we denote such alphabetsets for c and w as Ac and Aw, respectively.3.2 Effects of CompressionWord segmentation results from compressing a se-quence of characters.
By compression, we mean toreplace the occurrences for some k-characters sub-sequence ?c1, c2, .
.
.
, ck?
in the text with those for anew string w = c1c2 .
.
.
ck (word).
This procedurecan be generalized to include more subsequences tobe replaced, each with a different length.
The result-ing sequence is a mixture of characters and wordsintroduced during compression.
For clarity, we usethe term token sequence to refer to such a mixed se-quence of characters or words.Compression has a few effects to the token se-quence: (i) it increases the total number of tokens,(ii) it expands the alphabet set to include newly pro-duced tokens, (iii) it affects the entropy rate esti-mates.
Note that, by seeing a token sequence asa series of outcomes drawn from some underlyingstochastic process, we can estimate the entropy rateempirically.Items (i) and (ii) are natural consequences of com-pression.
The effort to describe the same pieceof information gets reduced at the expense of ex-panding the vocabulary, and sometimes even chang-ing the usage.
A real-life example for this is thatlanguage users invent new terminologies for effi-ciently conveying complex information.
Item (iii)describes something more subtle.
Observe that,when some n occurrences of a k-character subse-quence ?c1, c2, .
.
.
, ck?
get compressed, each char-acter ci loses n occurrences, and totally nk occur-rences move away from the subsequence; as a result,the newly created word w receives n occurrences.
Itis clear that compression has this side effect of re-distributing probability masses among the observa-tions (i.e., characters), thereby causing deviation toentropy rate estimates.3.3 FormulationThe choice of subsequences to be compressed is es-sential in the aforementioned process.
We hypothe-size that a good choice has the following two prop-erties: (i) higher frequency, and (ii) low deviation inentropy rate.We motivate these two properties as follows.First, high frequency subsequences are favorablehere since they are more likely to be character-level27collocations; compressing these subsequences re-sults in better compression rate.
Second, deviationin entropy rate is reflected in vocabulary complex-ity, and we believe that it directly translates to effortsthat language users pay to adapt to the new language.In this case, there seems no reason to believe that ei-ther increasing or decreasing vocabulary complexityis beneficial, since in two trivial ?bad choices?
thatone can easily imagine, i.e., the text being fully seg-mented or unsegmented, the entropy rates reach bothextremes.Motivated by these observations, we expect thatthe best word segmentation (i) achieves some prede-fined compression rate, and (ii) minimizes deviationin entropy rate.
This idea is realized as an optimiza-tion problem, called regularized compression.
Con-ceptually, this problem is defined as:minimize DV(c,w)subject to w respects U| |w||c| ?
?| ?
(1)where ?
denotes some expected compression ratioand  denotes the tolerance.
Note that DV(c,w) =|H?
(C) ?
H?
(W )| represents the deviation in en-tropy rate with respect to sequences c and w. Inthis definition, H?
(C) and H?
(W ) denote the em-pirical entropy rates for random variables C ?
Acand W ?
Aw, estimated on the corresponding se-quences c and w, respectively.4 Iterative Algorithm4.1 Ordered RulesetAcknowledging that exponentially many feasibleword sequences need to be checked, we propose analternative formulation in a restricted solution space.The idea is, instead of optimizing for segmentations,we search for segmentation generators, i.e., a set offunctions that generate segmentations from the in-put.
The generators we consider here is the orderedrulesets.An ordered ruleset R = ?r1, r2, .
.
.
, rk?
is a se-quence of translation rules, each of which takes thefollowing form:w ?
c1c2 .
.
.
cn,where the right-hand side (c1c2 .
.
.
cn) denotes then-token subsequence to be replaced, and the left-hand side (w) denotes the new token to be intro-duced.
Applying a translation rule r to a token se-quence has an effect of replacing all the occurrencesfor subsequence c1c2 .
.
.
cn with those for token w.Applying an ordered ruleset R to a token se-quence is equivalent to iteratively applying the trans-lation rules r1, r2, .
.
.
, rk in strict order.
Specifi-cally, consider that the initial token sequence is de-noted as c(0) and let the final result be denoted asc(k).
By iterative application, we mean to repeat thefollowing step for i = 1 .
.
.
k:Apply rule ri to c(i?1) and save the resultas c(i).4.2 Alternative FormulationThis notion of ordered rulesets allows one to explorethe search space efficiently using a greedy inclusionalgorithm.
The idea is to maintain a globally bestruleset B that covers the best translation rules wehave discovered so far, and then iteratively expandB by discovering new best rule and adding it to rule-set.
The procedure repeats several times until thecompression rate reaches some predefined ratio ?.
Ineach iteration, the best translation rule is determinedby solving a modified version of Equation (1), whichis written as follows:(In iteration i)minimize ?
|c(i)||c(i?1)| + DV(c(i?1), c(i))subject to r is a ruler(c(i?1)) = c(i)c(i) respects U(2)Note that the alternative formulation is largely agreedy version of Equation (1) except a few minorchanges.
First, the compression rate constraint be-comes the termination condition in the greedy in-clusion algorithm.
Second, we add an extra term|c(i)|/|c(i?1)| to the objective to encourage early in-clusion of frequent collocations.
The trade-off pa-rameter ?
is introduced in Equation (2) to scalarizeboth terms in the objective.A brief sketch of the algorithm is given in the fol-lowing paragraphs.1.
Let B be an empty ordered ruleset, and let c(0)be the original sequence of tokens.282.
Repeat the following steps for each i ?
N ,starting from i = 1, until the compression ratereaches some predefined threshold.
(a) Find a rule r that maximizes Equation (2)(b) Apply the rule r to form a new sequencec(i) from c(i?1).
(c) Add r to the end of B.3.
Output B and the final sequence.4.3 ImplementationAdditional care needs to be taken in implementingSteps 2a and 2b.
The simplest way to collect n-gramcounts for computing the objective in Equation (2) isto run multiple scans over the entire sequence.
Ourexperience suggests that using an indexing structurethat keeps track of token positions can be more ef-ficient.
This is especially important when updatingthe affected n-gram counts in each iteration.
Sincereplacing one occurrence for any subsequence af-fects only its surrounding n-grams, the total num-ber of such affected n-gram occurrences in one it-eration is linear in the number of occurrences forthe replaced subsequence.
Using an indexing struc-ture in this case has the advantage to reduce seektime.
Note that, however, the overall running timeremains in the same complexity class regardless ofthe deployment of an indexing structure.
The timecomplexity for this algorithm isO(TN), where T isthe number of iterations and N is the length of theinput sequence.Although it is theoretically appealing to create ann-gram search algorithm, in this preliminary studywe used a simple bigram-based implementation forefficiency.
We considered only bigrams in creat-ing translation rules, expecting that the discoveredbigrams can grow into trigrams or higher-order n-grams in the subsequent iterations.
To allow un-merged tokens (i.e., characters that was supposedto be in one n-gram but eventually left out due tobigram implementation) being merged into the dis-covered bigram, we also required that that one ofthe two participating tokens at the right-hand sideof any translation rule has to be an unmerged to-ken.
This has a side effect to exclude generation ofcollocation-based words1.
It can be an issue in cer-1Fictional examples include ?homework?
or ?cellphone?.tain standards; on the test corpora we used, this kindof problems is not obvious.Another constraint that we added to the imple-mentation is to limit the choice of bigrams to thosehas more frequency counts.
Generally, the numberof occurrence for any candidate bigram being con-sidered in the search space has to be greater or equalto some predefined threshold.
In practice, we foundlittle difference in performance for specifying anyinteger between 3 and 7 as the threshold; in this pa-per, we stick to 3.5 Evaluation5.1 SetupWe conducted a series of experiments to investi-gate the effectiveness of the proposed segmentationmethod under different language settings and seg-mentation standards.
In the first and the secondexperiments, we focus on drawing comparison be-tween our method and state-of-the-art approaches.The third experiment focuses on the influence ofdata size to segmentation accuracy.Segmentation performance is assessed using stan-dard metrics, such as precision, recall, and F-measure.
Generally, these measures are reportedonly at word level; in some cases where further anal-ysis is called for, we report boundary-level and type-level measures as well.
We used the evaluation scriptin the official HDP package to calculate these num-bers.The reference methods we considered in the com-parative study include the following:?
Hierarchical Dirichlet process, denoted as HDP(Goldwater et al, 2009);?
Nested Pitman-Yor process, denoted as NPY(Mochihashi et al, 2009);?
Adaptor grammars, denoted as AG (Johnsonand Goldwater, 2009);?
Branching entropy + MDL, denoted as Ent-MDL (Zhikov et al, 2010);?
Bootstrap voting experts + MDL, denoted asBVE-MDL (Hewlett and Cohen, 2011);?
Description length gain, denoted as DLG (Zhaoand Kit, 2008).29The proposed method is denoted as RC; it is alsodenoted as RC-MDL in a few cases where MDL isused for parameter estimation.5.2 Parameter EstimationThere are two free parameters ?
and ?
in our model.The parameter ?
specifies the degree to which wefavors high-frequency collocations when solvingEquation (2).
Experimentation suggests that ?
canbe sensitive when set too low2.
Practically, we rec-ommend optimizing ?
based on grid search on de-velopment data, or the MDL principle.
The formulafor calculating description length is not shown here;see Zhikov et al (2010), Hewlett and Cohen (2011),and Rissanen (1978) for details.The expected compression rate ?
determineswhen to stop the segmentor.
It is related to theexpected word length: When the compression rate|c|/|w| reaches ?
and the segmentor is about to stop,1/?
is the average word length in the segmentation.In this sense, it seems ?
is somehow connected to thelanguage of concern.
We expect that optimal valueslearned on one data set may thus generalize on theother sets of the same language.
Throughout the ex-periments, we estimated this value based on devel-opment data.5.3 Evaluation on Bernstein-Ratner CorpusWe conducted the first experiment on the Bernstein-Ratner corpus (Bernstein-Ratner, 1987), a standardbenchmark for English phonetic segmentation.
Weused the version derived by Michael Brent, whichis made available in the CHILDES database (Brentand Cartwright, 1996; MacWhinney and Snow,1990).
The corpus comprises 9,790 utterances,which amount to 95,809 words in total.
Its rel-atively small size allows experimentation with themost computational-intensive Bayesian models.Parameter estimation for the proposed method hasbeen a challenge due to the lack of appropriate de-velopment data.
We first obtained a rough estimatefor the compression rate ?
via human inspection intothe first 10 lines of the corpus (these 10 lines werelater excluded in evaluation) and used that estimateto set up the termination condition.
Since the first2Informally speaking, when ?
< H?(c).
The analysis is notcovered in this preliminary study.P R F TimeHDP 0.752 0.696 0.723 ?NPY, bigram 0.748 0.767 0.757 17 min.AG ?
?
0.890 ?Ent-MDL 0.763 0.745 0.754 2.6 sec.BVE-MDL 0.793 0.734 0.762 2.6 sec.RC-MDL 0.771 0.819 0.794 0.9 sec.Table 2: Performance evaluation on the Bernstein-Ratnercorpus.
The reported values for each method indicateword precision, recall, F-measure and running time, re-spectively.
The boldface value for each column indicatesthe top performer under the corresponding metric.10 lines are too small to reveal any useful segmenta-tion cues other than the word/token ration of interest,we considered this setting (?almost unsupervised?
)a reasonable compromise.
In this experiment, ?
isset to 0.37; the trade-off parameter ?
is set to 8.3,optimized using MDL principle in a two-pass gridsearch (the first pass over {1, 2, .
.
.
, 20} and the sec-ond over {8.0, 8.1, .
.
.
, 10.0}).A detailed performance result for the proposedmethod is described in Table 1.
A reference runfor HDP is included for comparison.
The pro-posed method achieved satisfactory result at wordand boundary levels.
Nevertheless, low type-levelnumbers (in contrast to those for HDP) together withhigh boundary recall suggested that we might haveexperienced over-segmentation.Table 2 covers the same result with less detailsin order to compare with other reference methods.All the reported measures for reference methodsare directly taken from the literature.
The resultshows that AG achieved the best performance in F-measure (other metrics are not reported), surpass-ing all the other methods by a large margin (10 per-cent).
Among the other methods, our method pairedwith MDL achieved comparable performance as theothers in precision; it does slightly better than theothers in recall (5 percent) and F-measure (2.5 per-cent).
Furthermore, our algorithm also seems to becompetitive in terms of computational efficiency.
Onthis benchmark it demanded only minimal memorylow as 4MB and finished the segmentation run in 0.9second, even less than the reported running time forboth MDL-based algorithms.30P R F BP BR BF TP TR TFHDP, Bernstein-Ratner 0.75 0.70 0.72 0.90 0.81 0.85 0.64 0.55 0.59RC-MDL, Bernstein-Ratner 0.77 0.82 0.79 0.85 0.92 0.89 0.57 0.48 0.50RC, CityU training 0.75 0.79 0.77 0.89 0.93 0.91 0.63 0.35 0.45RC, MSR training 0.73 0.82 0.77 0.86 0.96 0.91 0.70 0.26 0.38Table 1: Performance evaluation for the proposed method across different test corpora.
The first row indicates areference HDP run (Goldwater et al, 2009); the other rows represent the proposed method tested on different test cor-pora.
Columns indicates performance metrics, which correspond to precision, recall, and F-measure at word (P/R/F),boundary (BP/BR/BF), and type (TP/TR/TF) levels.Corpus Training (W/T) Test (W/T)AS 5.45M / 141K 122K / 19KPKU 1.1M / 55K 104K / 13KCityU 1.46M / 69K 41K / 9KMSR 2.37M / 88K 107K / 13KTable 3: A short summary about the subsets in theBakeoff-2005 dataset.
The size of each subset is given innumber of words (W) and number of unique word types(T).5.4 Evaluation on Bakeoff-2005 CorpusThe second benchmark that we adopted is theSIGHAN Bakeoff-2005 dataset (Emerson, 2005)for Chinese word segmentation.
The corpus hasfour separates subsets prepared by different researchgroups; it is among the largest word segmentationbenchmarks available.
Table 3 briefly summarizesthe statistics regarding this dataset.We decided to compare our algorithm with de-scription length gain (DLG), for that it seems to de-liver best segmentation accuracy among other un-supervised approaches ever reported on this bench-mark (Zhao and Kit, 2008).
Since the reportedvalues for DLG were obtained on another closeddataset Bakeoff-2006 (Levow, 2006), we followed asimilar experimental setup as suggested in the liter-ature (Mochihashi et al, 2009): We compared bothmethods only on the training sets for the commonsubsets CityU and MSR.
Note that this experimentalsetup departed slightly from that of Mochihashi et alin that all the comparisons were strictly made on thetraining sets.
The approach is more straightforwardthan the suggested sampling-based method.Other baseline methods that we considered in-clude HDP, Ent-MDL, and BVE-MDL, for theirrepresentativeness in segmentation performance andCityU MSRRC, r = 0.65 0.770 0.774DLG, ensemble 0.684 0.665Ent-MDL, nmax = 3 0.798 0.795Table 4: Performance evaluation on the common trainingsubsets in the Bakeoff-2005 and Bakeoff-2006 datasets.The reported values are token F-measure.
The boldfacevalue in each column indicates the top performer for thecorresponding set.ease of implementation.
The HDP implementationwe used is a modified version of the offical HDPpackage3; we patched the package to make it workwith Unicode-encoded Chinese characters.
For Ent-MDL and BVE-MDL, we used the software pack-age4 distributed by Hewlett and Cohen (2011).
Weestimated the parameters using the AS training setas the development data.
We set ?
to 6 based on agrid search.
The expected compression rate ?
thatwe learned from the development data is 0.65.In Table 1, we give a detailed listing of vari-ous performance measures for the proposed method.Segmentation performance seems moderate at bothword and boundary levels.
Nevertheless, high typeprecision and low type recall on both CityU andMSR training corpora signaled that our algorithmfailed to discover most word types.
This issue, wesuspect, was caused by exclusion of low-frequencycandidate bigrams, as discussed in Section 4.3.Table 4 summarizes the result for word segmen-tation conducted on the CityU and MSR subsets ofBakeoff-2005.
Due to practical computational lim-its, we were not able to run HDP and BVE-MDLon any complete subset.
The result shows that our3http://homepages.inf.ed.ac.uk/sgwater/4http://code.google.com/p/voting-experts310.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Data size in percentageF?measureFBFTFFigure 1: Performance evaluation for the proposedmethod on the CityU training set.CityU-1k MSR-1kRC, r = 0.65 0.505 0.492HDP, 10 sample average 0.591 0.623RC, r = 0.65/punc.
0.599 0.591Table 5: Performance evaluation on two random samplesfrom the common sets (CityU and MSR subsets) in theBakeoff-2005 and Bakeoff-2006 datasets.algorithm outperforms DLG by 8 to 10 percents inF-measure, while Ent-MDL still performs slightlybetter, achieving the top performance among all theexperimental runs on both subsets.To compare with HDP, we conducted another testrun on top of a random sample of 1,000 lines fromeach subset.
We chose 1,000 lines because HDP caneasily consume more than 4GB of main memory onany larger sample.
We adopted standard settings forHDP: ?0 = 3, 000, ?1 = 300, and pb = 0.2.
Ineach trial run, we ran the Gibbs sampler for 20,000iterations using simulated annealing (Goldwater etal., 2009).
We obtained 10 samples from the Gibbssampler and used the average performance in com-parison.
It took slightly more than 50 hours to col-lect one trial run on one subset.The evaluation result is summarized in Table 5.We ran our algorithm to the desired compressionratio r = 0.65 on this small sample.
The result0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Data size in percentageF?measureFBFTFFigure 2: Performance evaluation for the proposedmethod on the MSR training set.shows that the performance of regularized compres-sion is inferior to that of HDP by 9 to 13 percentsin F-measure for both sets.
To investigate why, welooked into the segmentation output.
We observedthat, in the regularized compression output, most ofthe punctuation marks were incorrectly aligned totheir neighboring words, owing to the short of fre-quency counts in this small sample.
The HDP, how-ever, does not seem to suffer from this issue.We devised a simple post-processing step, inwhich each punctuation mark was forced segmentedfrom the surrounding text.
Another outside test wasconducted to see how well the algorithm works us-ing heuristics derived from minimal domain knowl-edge.
The additional run is denoted as RC/punc.The result is shown in Table 5.
From the result,we found that the combined approach works slightlybetter than HDP in one corpus, but not in the other.5.5 Effects of Data SizeWe employed the third experiment to study the influ-ence of corpora size to segmentation accuracy.
Sincethe proposed method relies on empirical estimatesfor entropy rate to decide the word boundaries, wewere interested in learning about how it responds torelatively low and high volume input.This experiment was conducted on CityU and32MSR training sets.
On each corpus, we took the firstk% of data (in terms of utterances) and tested theproposed method against that subset; this test wasrepeated several times with different values for k. Inthis experiment, we chose the value for k from theset {2, 4, 6, 8, 10, 20, 30, .
.
.
, 90, 100}.
The perfor-mance is evaluated using word, boundary, and typeF-measures.Figures 1 and 2 show the experiment results.
Bothfigures revealed similar patterns for segmentationperformance at different volume levels.
Word F-measures for both corpora begin at roughly 0.52,climb up rapidly to 0.73 as the volume grows from2% to 20%, and finally settle on some value around0.77.
Boundary F-measures for both corpora show asimilar trend?a less steep increase before 20% from0.80 to 0.89 followed by a plateau at around 0.93.Here, the result seems to suggest that estimating to-ken entropy rate using less than 20% of data mightbe insufficient for this type of text corpora.
Further-more, since performance is saturated at such an earlystage, it seems feasible to split the entire dataset intoa number of folds (e.g., 5, in this case) and solveeach fold individually in parallel.
This techniquemay greatly enhance the run-time efficiency of thesegmentor.The patterns we observed for type F-measure tellsanother story.
On both corpora, type F-measures donot seem to improve as data volume increases.
OnCityU corpora, type F-measure gradually increasedfrom 0.42 to 0.48 and then slowly falling back to0.45.
On MSR corpora, type F-measure peaked at0.45 when receiving 10% of data; after that it starteddecreasing, going all the way down to 0.37, evenlower than the number 0.43 it received at the begin-ning.
Our guess is that, at some early point (20%),the proposed method started to under-segment thetext.
We suspect that there is some deep con-nection between performance saturation and under-segmentation, since from the result they both beginat roughly the same level.
Further investigation inthis respect is needed to give out definitive explana-tions.6 Concluding RemarksPreliminary experimental results suggest that theregularized compression method, even only withpartial evidence, seems as effective as the state-of-the-art methods in different language settings.
Whenpaired with MDL criteria, regularized compressionis comparable to hierarchical Bayesian methods andMDL-based algorithms in terms of segmentation ac-curacy and computational efficiency.
Furthermore,regularized compression is less memory-demandingthan the other approaches; thus, it scales more easilyto large corpora for carrying out certain tasks suchas segmenting historical texts written in ancient lan-guages, or preprocessing a large dataset for subse-quent manual annotation.We have identified a number of limitations of reg-ular compression.
First, the choice of candidate n-grams does not cover hapax legomena, i.e., wordsthat occur only once in the corpus.
At present, pre-cluding these low-frequency n-grams seems to bea necessary compromise due to our limited under-standing about the dynamics behind regular com-pression.
Second, regularized compression does notwork well with low volume data, since on smallerdataset the distribution of frequency counts is lessprecise.
Third, the algorithm may stop identifyingnew word types at some point.
We suspect that thisis related to the choice of n-gram, since in our im-plementation no two existing ?words?
can be aggre-gated into one.
These issues shall be addressed inour future work.AcknowledgmentsWe thank the anonymous reviewers for their valu-able comments.
The research efforts described inthis paper are supported under the National Tai-wan University Digital Archives Project (ProjectNo.
NSC-98-2631-H-002-005), which is sponsoredby National Science Council, Taiwan.ReferencesShlomo Argamon, Navot Akiva, Amihood Amir, andOren Kapah.
2004.
Efficient unsupervised recursiveword segmentation using minimum description length.In Proceedings of the 20th international conferenceon Computational Linguistics, COLING ?04, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Nan Bernstein-Ratner.
1987.
The phonology of parentchild speech.
Children?s language, 6:159?174.33Michael R. Brent and Timothy A. Cartwright.
1996.
Dis-tributional regularity and phonotactic constraints areuseful for segmentation.
In Cognition, pages 93?125.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHAN Workshop on Chinese LanguageProcessing, volume 133.
Jeju Island, Korea.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Contextual dependencies in unsupervisedword segmentation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, ACL-44, pages 673?680,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21?54, July.Daniel Hewlett and Paul Cohen.
2009.
Bootstrap votingexperts.
In Proceedings of the 21st international jontconference on Artifical intelligence, IJCAI?09, pages1071?1076, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Daniel Hewlett and Paul Cohen.
2011.
Fully unsuper-vised word segmentation with BVE and MDL.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 540?545, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Zhihui Jin and Kumiko T. Ishii.
2006.
Unsupervised seg-mentation of chinese text by use of branching entropy.In Proceedings of the COLING/ACL on Main confer-ence poster sessions, COLING-ACL ?06, pages 428?435, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, NAACL ?09, pages 317?325, Stroudsburg,PA, USA.
Association for Computational Linguistics.Chunyu Kit and Yorick Wilks.
1999.
Unsupervisedlearning of word boundary with description lengthgain.
In CoNLL-99, pages 1?6, Bergen, Norway.Gina-Anne Levow.
2006.
The third international chineselanguage processing bakeoff: Word segmentation andnamed entity recognition.
In Proceedings of the FifthSIGHAN Workshop on Chinese Language Processing,volume 117.
Sydney: July.Brian MacWhinney and Catherine Snow.
1990.
Thechild language data exchange system: an update.Journal of child language, 17(2):457?472, June.Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.2009.
Bayesian unsupervised word segmentation withnested Pitman-Yor language modeling.
In Proceed-ings of the Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International Joint Confer-ence on Natural Language Processing of the AFNLP:Volume 1 - Volume 1, ACL ?09, pages 100?108,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Jorma Rissanen.
1978.
Modeling by shortest data de-scription.
Automatica, 14(5):465?471, September.Kumiko Tanaka-Ishii.
2005.
Entropy as an indicator ofcontext boundaries: An experiment using a web searchengine.
In Robert Dale, Kam-Fai Wong, Jian Su,and Oi Kwong, editors, Natural Language Process-ing IJCNLP 2005, volume 3651 of Lecture Notes inComputer Science, chapter 9, pages 93?105.
SpringerBerlin / Heidelberg, Berlin, Heidelberg.Hua Yu.
2000.
Unsupervised word induction using MDLcriterion.
In Proceedings of the International Sympo-sium of Chinese Spoken Language Processing, Beijin,China.Hai Zhao and Chunyu Kit.
2008.
An empirical compar-ison of goodness measures for unsupervised chineseword segmentation with a unified framework.
In TheThird International Joint Conference on Natural Lan-guage Processing (IJCNLP-2008).Valentin Zhikov, Hiroya Takamura, and Manabu Oku-mura.
2010.
An efficient algorithm for unsupervisedword segmentation with branching entropy and MDL.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 832?842, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.34
