Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 97?104,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Alignment for Segmental-based Language UnderstandingSt?phane Huet and Fabrice Lef?vreUniversit?
d?Avignon, LIA-CERI, France{stephane.huet,fabrice.lefevre}@univ-avignon.frAbstractRecent years?
most efficient approaches forlanguage understanding are statistical.
Theseapproaches benefit from a segmental semanticannotation of corpora.
To reduce the produc-tion cost of such corpora, this paper proposesa method that is able to match first identifiedconcepts with word sequences in an unsuper-vised way.
This method based on automaticalignment is used by an understanding sys-tem based on conditional random fields andis evaluated on a spoken dialogue task usingeither manual or automatic transcripts.1 IntroductionOne of the very first step to build a spoken languageunderstanding (SLU) module for dialogue systemsis the extraction of literal concepts from word se-quences hypothesised by a speech recogniser.
Toaddress this issue of concept tagging, several tech-niques are available.
These techniques rely on mod-els, now classic, that can be either discriminantor generative.
Among these, we can cite: hiddenMarkov models, finite state transducers, maximalentropy Markov models, support vector machines,dynamic Bayesian networks (DBNs) or conditionalMarkov random fields (CRFs) (Lafferty et al, 2001).In (Hahn et al, 2011), it is shown that CRFs obtainthe best performance on a reference task (MEDIA) inFrench (Bonneau-Maynard et al, 2005), but also ontwo other comparable corpora in Italian and Polish.Besides, the comparison of the understanding resultsof manually vs automatically transcribed utteranceshas shown the robustness of CRFs.Among the approaches evaluated in (Hahn et al,2011) was a method using log-linear models compa-rable to those used in stochastic machine translation,which turned out to have lower performance thanCRF.
In this paper, we further exploit the idea of ap-plying automatic translation techniques to languageunderstanding but limiting ourselves to the objectiveof obtaining a segmental annotation of training data.In many former approaches literal interpretationwas limited to list lexical-concept relations; for in-stance this is the case of the PHOENIX system (Ward,1991) based on the detection of keywords.
Thesegmental approach allows a finer-grained analysisconsidering sentences as segment sequences duringinterpretation.
This characteristic enables the ap-proach to correctly connect the various levels ofsentence analysis (lexical, syntactic and semantic).However, in order to simplify its practical appli-cation, segments have been designed specificallyfor semantic annotation and do not integrate anyconstraint in their relation with the syntactic units(chunks, phrasal groups, etc.).
Not only it simpli-fies the annotation process itself but as the overallobjective is to use the interpretation module insidea spoken dialogue system, transcribed speech dataare noisy and generally bound the performance ofsyntactic analysers (due to highly spontaneous andungrammatical utterances from the users, combinedwith errors from the speech recognizer).Among other interesting proprieties, segmentalapproaches offer a convenient way to dissociate thedetection of a conceptual unit from the estimation ofits associated value.
The value corresponds to thenormalisation of the surface form.
For instance, if97the segment ?no later than eleven?
is associated withthe concept departure-time, its value is ?morn-ing?
; the same value is associated with the segments?between 8 and noon?
or ?in the morning?.
Thevalue estimation requires a link between conceptsand sentence words.
Then it becomes possible totreat the normalisation problem by means of regularexpressions or concept-dependent language models(allowing an integrated approach such as describedin (Lef?vre, 2007)).
In the case of global approaches(not segmental), value detection must be directlyincorporated in the conceptual units to identify, asin (Mairesse et al, 2009).
The additional level is areal burden and is only affordable when the numberof authorised values is low.Obviously a major drawback of the approach is itscost: associating concept tags with a dialogue tran-scription is already a tedious task and its complexityis largely increased by the requirement for a precisedelimitation of the support (lexical segment) corre-sponding to each tag.
The SLU evaluation campaignMEDIA has been the first opportunity to collect anddistribute a reasonably-sized corpus endowed withsegmental annotations.Anyhow the difficulty remains unchanged eachtime a corpus has to be collected for a new task.We propose in this study a new method that reducesthe effort required to build training data for segmen-tal annotation models.
Making the assumption thatthe concepts evoked in a sentence are automaticallydetected beforehand or provided by an expert, westudy how to associate them with their lexical sup-ports without prior knowledge.
A conceptual seg-mental annotation is obtained using alignment tech-niques designed to align multilingual parallel cor-pora in the machine translation domain.
This anno-tation can be considered as unsupervised since it isdone without a training corpus with links betweenword sequences and concepts.We present in the paper the necessary adaptationsfor the application of the alignment techniques inthis new context.
They have been kept to their mini-mal so as to maintain the highest level of generality,which in return benefits from the availability of ex-isting software tools.
Using a reference annotation,we evaluate the alignment quality from the unsuper-vised approach in two interesting situations depend-ing on whether the correct order of the concepts isknown or not.
Finally, the end-to-end evaluation ofthe approach is made by measuring the impact of thealignments on the CRF-based understanding system.After a brief recall of the conceptual decodingprinciples in Section 2, the principles of automaticalignment of parallel corpora are described in Sec-tion 3 along with the specificities due to the align-ment of semantic concepts.
Section 4 presents theexperiments and comments on the results, whileSection 5 concludes the paper.2 Segmental conceptual decodingIf literal interpretation can be seen as the transla-tion of natural language to the set of semantic tagsequences, then the methods and models of machinetranslation can be used.
Since the number of con-cepts is generally much lower than the vocabularysize, this particular type of translation can also beconsidered as a mere classification problem in whichthe conceptual constituents represent the class toidentify.
Interpretation can thus be performed bymethods and models of classification.Discriminant approaches model the conditionalprobability distribution of the semantic constituentsequence (or concepts) c1 .
.
.
cn considering a wordsequence w1 .
.
.
wT : P (cn1 |wT1 ).
In generative ap-proaches, the joint probability P (cn1 , wT1 ) is mod-elized instead and can be used to compute inferenceseither for prediction/decoding or parameter training.Generative models (such as hidden Markov mod-els) have been first introduced to address the under-standing problem with stochastic approaches (Levinand Pieraccini, 1995).
Recent variants offermore degrees of freedom in modeling (see for in-stance (He and Young, 2005) or (Lef?vre, 2007)).Since then log-linear models have clearly showntheir superiority for tasks of sequence tagging (Hahnet al, 2011).Several variants of log-linear models differ intheir conditional variable independence assumptionsand use different normalisation steps.
CRFs (Laf-ferty et al, 2001) represent linear chains of randomindependent variables, all conditioned over the en-tire sequence and the normalisation is global overthe sequence.Some generative approaches such as DBNs makeinferences in multi-level models (Lef?vre, 2007)98Figure 1: Example of an alignment of words with their conceptual units.and intrinsically take into account segmentation.For models unable to handle multi-level repre-sentations (as CRF), it is convenient to representsegments directly at the tag level.
For this purposethe BIO formalism can be used: B is added to tagsstarting a segment, I to tags inside a segment andO to out-of-domain tags (if these are not alreadyhandled through a specific NULL tag).
In the casedisplayed in Figure 1, the concept sequence be-comes: B-cmd-task I-cmd-task I-cmd-taskB-null I-null B-loc-town I-loc-townI-loc-town I-loc-town I-loc-townB-time-date I-time-date B-time-dateI-time-date I-time-date.3 Semantic concept alignmentAutomatic alignment is a major issue in machinetranslation.
For example, word-based alignmentsare used to generate phrase tables that are core com-ponents for many current statistical machine trans-lation systems (Koehn et al, 2007).
The alignmenttask aims at finding the mapping between words oftwo sentences in relation of translation.
It faces sev-eral difficulties:?
some source words are not associated with atranslated word;?
others are translated by several words;?
matched words may occur at different positionsin both sentences according to the syntacticrules of the considered languages.Several statistical models have been proposed toalign two sentences (Brown et al, 1993).
One oftheir main interests is their ability to be built in anunsupervised way from a parallel corpus aligned atthe sentence level, but not at the word level.
For-mally, from a sentence S = s1 .
.
.
sm expressed in asource language and its translation T = t1 .
.
.
tn ex-pressed in a target language, an IBM-style alignmentA = a1 .
.
.
am connects each source word to a tar-get word (aj ?
{1, ..., n}) or to the so-called NULLtoken which accounts for untranslated target words.IBM statistical models evaluate the translation of Sinto T from the computation of P (S,A|T ); the bestalignment A?
can be deduced from this criterion us-ing the Viterbi algorithm:A?
= argmaxAP (S,A|T ) .
(1)IBM models differ according to their complexitylevel.
IBM1 model makes the strong assumptionthat alignments are independent and can be evalu-ated only through the transfer probabilities P (si|tj).The HMM model, which is an improvement overIBM2, adds a new parameter P (aj |aj?1, n) that as-sumes a first-order dependency between alignmentvariables.
The next models (IBM3 to IBM5) aremainly based on two types of parameters:?
distortion, which measures how words of T arereordered with respect to the index of the wordsfrom S they are aligned with,?
fertility, which measures the usual number ofwords that are aligned with a target word tj .In order to improve alignments, IBM models areusually applied in both translation directions.
Thesetwo alignments are then symmetrized by combiningthem.
This last step is done via heuristic methods;a common approach is to start with the intersectionand then iteratively add links from the union (Och etal., 1999).If we have at our disposal a method that can findconcepts contained in an utterance, segmental anno-tation can be obtained by aligning words S = wT1with the found concepts T = cn1 (Fig.
1).
Con-cepts are ideally generated in the correct order withrespect to the word segments of the analysed utter-ance.
In a more pragmatic way, concepts are likelyto be produced as bag-of-concepts rather than or-dered sequences.99Statistical alignment methods used in machinetranslation are relevant in our context if we considerthat the target language is the concept language.There are nevertheless differences with genuine lan-guage translation.
First, each word is aligned to atmost one concept, while a concept is aligned withone word or more.
Consequently, it is expected thatword fertilities are one for the alignment of wordstoward concepts and concept fertilities are one ormore in the reverse direction.
Another consequenceis that NULL words are useless in our context.
Thesespecificities of the alignment process raise some dif-ficulties with regard to IBM models.
Indeed, ac-cording to the way probabilities are computed, thealignment of concepts toward words only allows oneword to be chosen per concept, which prevents thisdirection from having a sufficient number of linksbetween words and concepts.Another significant difference with translation isrelated to the translated token order.
While wordorder is not random in a natural language and fol-lows syntactic rules, it is not the case anymore whena word sequence have to be aligned with a bag-of-concepts.
HMM and IBM2 to IBM5 models haveparameters that assume that the index of a matchedsource word or the indices of the translations of theadjacent target words bear on the index of targetwords.
Therefore, the randomness of the conceptindices can disrupt performance obtained with thesemodels, contrary to IBM1.
As shown in the nextsection, it is appropriate to find ways to explicitlyre-order concept sequences than to let the distortionparameters handle the problem alone.4 Experiments and results4.1 Experimental setupThe evaluation of the introduced methods was car-ried out on the MEDIA corpus (Bonneau Maynard etal., 2008).
This corpus consists of human-machinedialogues collected with a wizard of Oz procedurein the domain of negotiation of tourist services.
Pro-duced for a realistic task, it is annotated with 145 se-mantic concepts and their values (more than 2k in to-tal for the enumerable cases).
The audio data are dis-tributed with their manual transcripts and automaticspeech recognition (ASR) hypotheses.
The corpusis divided into three parts: a training set (approxi-matively 12k utterances), a development set (1.2k)and a test set (3k).The experiments led on the alignment methodswere evaluated on the development corpus usingMGIZA++ (Gao and Vogel, 2008), a multi-threadversion of GIZA++ (Och and Ney, 2003) which alsoallows previously trained IBM alignments modelsto be applied on the development and test corpora.1The conceptual tagging process was evaluated on thetest corpus, using WAPITI (Lavergne et al, 2010)to train the CRF models.
Several setups have beentested:?
manual vs ASR transcriptions,?
inclusion (or not) of values during the errorcomputation.Several concept orderings (before automatic align-ment) have also been considered:?
a first ideal one, which takes reference conceptsequences as they are, aka sequential order;?
two more realistic variants that sort concepts ei-ther alphabetically or randomly, in order tosimulate bag-of-concepts.
Alphabetical orderis introduced solely to show that a particularorder (which is not related to the natural order)might misled the alignment process by intro-ducing undue regularities.To give a rough idea, these experiments requireda few minutes of computing time to train alignmentmodels of 12k utterances, a few hours to train CRFmodels (using 8 CPUs on our cluster of Xeon CPUs)and a few seconds to apply alignment and CRF mod-els in order to decode the test corpus.4.2 Experimental results for alignmentAlignment quality is estimated using the alignmenterror rate (AER), a metric often employed in ma-chine translation (Och and Ney, 2000).
If H standsfor hypothesis alignments andR for reference align-ments, AER is computed by the following relation:2AER = 1?2?
|H ?R||H|+ |R|.
(2)1With previousa, previoust, previousn, etc pa-rameters.2This equation is a simplification of the usually provided onebecause all alignments are considered as sure in our case.100In our context, this metrics is evaluated by repre-senting a link between source and target identities by(wi, cj), instead of the usual indices (i, j).
Indeed,alignments are then used to tag words.
Besides, con-cepts to align have positions that differ from the onesin the reference when they are reordered to simulatebags-of-concepts.As mentioned in the introduction, we resort towidely used tools for alignment in order to be as gen-eral as possible in our approach.
We do not modifythe algorithms and rely on their generality to dealwith specificities of the studied domain.
To trainiteratively the alignment models, we use the samepipeline as in MOSES, a widely used machine trans-lation system (Koehn et al, 2007):1.
5 iterations of IBM1,2.
5 iterations of HMM,3.
3 iterations of IBM3 then4.
3 iterations of IBM4.To measure the quality of the built models, themodel obtained at the last iteration of this chain isapplied on the development corpus.All the words of an utterance should normallybe associated with one concept, which makes theIBM models?
NULL word useless.
However, in theMEDIA corpus, a null semantic concept is associ-ated with words that do not correspond to a conceptrelevant for the tourist domain and may be omit-ted by counting on the probability with the NULLword included in the IBM models.
Two versionswere specifically created to test this hypothesis: onewith all the reference concept sequences and anotherwithout the null tags.
The results measured whentaking into account these tags (AER of 14.2 %) arefar better than the ones obtained when they are dis-carded (AER of 27.4 %), in the word ?
conceptalignment direction.3 We decided therefore to keepthe null in all the experiments.Table 1 presents the alignment results measuredon the development corpus according to the wayconcepts are reordered with respect to the referenceand according to the considered alignment direction.3For a fair comparison between both setups, the null con-cept was ignored in H and R for this series of experiments.The three first lines exhibit the results obtained withthe last IBM4 iteration.
As expected, the AER mea-sured with this model in the concept?
word direc-tion (second line), which can only associate at mostone word per concept, is clearly higher than the oneobtained in the opposite direction (first line).
Quitesurprisingly, an improvement in terms of AER (thirdline) over the best direction (first line) is observedusing the default MOSES heuristics (called grow-diag-final) that symmetrizes alignments obtained inboth directions.IBM1 models, contrary to other models, do nottake into account word index inside source and tar-get sentences, which makes them relevant to dealwith bag-of-concepts.
Therefore, we measured howAER varies when using models previously built inthe training chain.
The results obtained by applyingIBM1 and by symmetrizing alignments (last line),show finally that these simple models lead to lowerperformance than the one measured with IBM4 oreven HMM (last line), the concepts being orderedalphabetically or randomly (two last columns).The previous experiments have shown that align-ment is clearly of lower quality when algorithms arefaced with bags-of-concepts instead of well-orderedsequences.
In order to reduce this phenomenon, se-quences are reordered after a first alignmentA1 gen-erated by the symmetrized IBM4 model.
Two strate-gies have been considered to fix the new position ofeach concept ci.
The first one averages the indicesof the words wi that are aligned with ci according toA1:pos1(cj) =?is.t.
(i,j)?A1 iCard({(i, j) ?
A1}).
(3)The second one weights each word index with theirtransfer probabilities determined by IBM4:pos2(cj) =?is.t.
(i,j)?A1 i?
f(wi, cj)?is.t.
(i,j)?A1 f(wi, cj)(4)wheref(wi, cj) = ?P (cj |wi) + (1?
?
)P (wi|cj) (5)and ?
is a coefficient fixed on the development cor-pus.Training alignment models on the corpus re-ordered according to pos1 (Tab.
2, second column)101Sequential order Alphabetic order Random orderword?
concept IBM4 14.4 29.2 28.6concept?
word IBM4 40.9 51.6 49.0symmetrized IBM4 12.8 27.3 25.7symmetrized IBM1 33.2 33.2 33.1symmetrized HMM 14.8 29.9 28.7Table 1: AER (%) measured on the MEDIA development corpus with respect to the alignment model used and itsdirection.Initial 1st reordering iteration Last reordering iterationpos1 pos2 pos2Alphabetic order 27.3 22.2 21.0 19.4Random order 25.7 21.9 20.2 18.5Table 2: AER (%) measured on the MEDIA development corpus according to the strategy used to reorder concepts.or pos2 (third column) leads to a significant im-provement of the AER.
This reordering step can berepeated as long as performance goes on improving.By proceeding like this until step 3 for the alphabeticorder and until step 7 for the random order, values ofAER below 20 % (last column) are finally obtained.It is noteworthy that random reordering has betterresults than alphabetic reordering.
Indeed, HMM,IBM3 and IBM4 models have probabilities that aremore biased in this latter case, where the same se-quences occur more often although many are not inthe reference.4.3 Experimental results for spoken languageunderstandingIn order to measure how spoken language un-derstanding is disturbed by erroneous alignments,CRFs parameters are trained under two conditions:one where concept tagging is performed by an ex-pert and one where corpora are obtained using au-tomatic alignment.
The performance criterion usedto evaluate the understanding task is the concept er-ror rate (CER).
CER is computed in a similar wayas word error rate (WER) used in speech recogni-tion; it is obtained from the Levenshtein alignmentbetween both hypothesized and reference sequencesas the ratio of the sum of the concepts in the hy-pothesis substituted, inserted or omitted on the totalnumber of concepts in the manual reference anno-tation.
The null concept is not considered duringthe score computation.
The CER can also take intoaccount the normalized values in addition to the con-cept tags.Starting from a state-of-the-art system (Manualcolumn), degradations due to various alignment con-ditions are reported in Table 3.
It can be noted thatthe absolute increase in CER is at most 8.0 % (from17.6 to 25.6 with values) when models are trained onthe corpus aligned with IBM models; the orderinginformation brings it back to 3.7 % (17.6 to 21.3),and finally with automatic transcription the impactof the automatic alignments is smaller (resp.
5.8 %and 2.0 %).
As expected random order is preferableto alphabetic order (slight gain of 1 %).In Table 4, the random order alignments are usedbut this time the n-best lists of alignments are con-sidered and not only the 1-best hypotheses.
Insteadof training CRFs with only one version of the align-ment for a concept-word sequence pair, we filterout from the n-best lists the alignments having aprobability above a given threshold.
It can be ob-served that varying this confidence threshold allowsan improvement of the SLU performance (CER canbe reduced by 0.8 % for manual transcription and0.4 % for automatic transcription).
However, thisimprovement is not propagated to scores with val-ues (CER was reduced at best by 0.1 for manualtranscription and was increased for automatic tran-102Automatic alignmentsManual Sequential Alphabetic order Random orderManual transcription 13.9 (17.6) 17.7 (21.3) 22.6 (26.4) 22.0 (25.6)ASR transcription (wer 31 %) 24.7 (29.8) 27.1 (31.8) 31.5 (36.4) 30.6 (35.6)Table 3: CER (%) measured for concept decoding on the MEDIA test corpus with several alignment methods of thetraining data.
Inside parenthesis, CER for concepts and values.scription).
After closer inspection of the scoringalignments, an explanation for this setback is thatthe manually-designed rules used for value extrac-tion are perturbed by loose segmentation.
This isparticularly the case for the concept used to anno-tate co-references, which has confusions betweenthe values singular and plural (e.g.
?this?
is sin-gular and ?those?
plural).
This issue can be solvedby an ad hoc adaptation of the rules.
However, itwould infringe our objective of relying upon unsu-pervised approaches and minimizing human exper-tise.
Therefore, a better answer would be to resort toa probabilistic scheme also for value extraction (asproposed in (Lef?vre, 2007)).The optimal configuration (confidence thresholdof 0.3, 4th row of Table 4) is close to the baseline1-best system in terms of the number of trainingutterances.
We also tried a slightly different setupwhich adds the filtered alignments to the former cor-pus before CRF parameter training (i.e.
the 1-bestis not filtered in the n-best list).
In that case perfor-mance remains pretty stable with respect to the filter-ing process (CER is around 21.4 % for concepts and25.2 % for concept+value for thresholds between 0.1and 0.7).5 ConclusionIn this study an unsupervised approach is proposedto the problem of conceptual unit alignment for spo-ken language understanding.
We show that unsuper-vised statistical word alignment from the machinetranslation domain can be used in this context to as-sociate semantic concepts with word sequences.
Thequality of the derived alignment, already good in thegeneral case (< 20 % of errors on the word-conceptassociations), is improved by knowledge of the cor-rect unit order (< 15 %).
The impact of automaticalignments on the understanding performance is anabsolute increase of +8 % in terms of CER, but is re-duced to less than +4 % in the ordered case.
Whenautomatic transcripts are used, these gaps decreaseto +6 % and below +3 % respectively.
From theseresults we do believe that the cost vs performanceratio is in favour of the proposed method.AcknowledgementsThis work is partially supported by the ANR fundedproject PORT-MEDIA.4ReferencesH?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-ache, Anne Kuhn, and Djamel Mostefa.
2005.
Seman-tic annotation of the MEDIA corpus for spoken dialog.In Proceedings of Eurospeech, pages 3457?3460, Lis-boa, Portugal.H?l?ne Bonneau Maynard, Alexandre Denis, Fr?d?ricB?chet, Laurence Devillers, F. Lef?vre, MatthieuQuignard, Sophie Rosset, and Jeanne Villaneau.
2008.MEDIA : ?valuation de la compr?hension dans lessyst?mes de dialogue.
In L?
?valuation des technolo-gies de traitement de la langue, les campagnes Tech-nolangue, pages 209?232.
Herm?s, Lavoisier.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameter es-timation.
Computational Linguistics, 19(2):263?311.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engineer-ing, Testing, and Quality Assurance for Natural Lan-guage Processing, pages 49?57, Columbus, OH, USA.Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-rice Lef?vre, Patrick Lehen, Renato De Mori, Alessan-dro Moschitti, Hermann Ney, and Giuseppe Riccardi.2011.
Comparing stochastic approaches to spokenlanguage understanding in multiple languages.
IEEETransactions on Audio, Speech and Language Pro-cessing (TASLP), 19(6):1569?1583.4www.port-media.org103# train utterances Manual transcription ASR transcription(WER = 31 %)1-best 12795 22.0 (25.6) 30.6 (35.6)filtered 10-best (conf thres = 0.1) 18955 21.7 (25.8) 31.2 (36.9)filtered 10-best (conf thres = 0.2) 15322 21.3 (25.5) 30.7 (36.3)filtered 10-best (conf thres = 0.3) 13374 21.2 (25.7) 30.2 (36.0)filtered 10-best (conf thres = 0.5) 10963 21.4 (25.7) 30.6 (36.2)filtered 10-best (conf thres = 0.7) 9647 25.4 (29.1) 32.9 (38.2)Table 4: CER (%) measured for concept decoding on the MEDIA test corpus with filtered n-best lists of random orderalignments of the training data.
Inside parenthesis, CER for concepts and values.Yulan He and Steve Young.
2005.
Spoken languageunderstanding using the hidden vector state model.Speech Communication, 48(3?4):262?275.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of ACL, Companion Volume, pages 177?180,Prague, Czech Republic.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
InProceedings of ICML, pages 282?289, Williamstown,MA, USA.Thomas Lavergne, Olivier Capp?, and Fran?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings of ACL, pages 504?513, Uppsala, Sweden.Fabrice Lef?vre.
2007.
Dynamic bayesian networks anddiscriminative classifiers for multi-stage semantic in-terpretation.
In Proceedings of ICASSP, Honolulu,Hawai.Esther Levin and Roberto Pieraccini.
1995.
Concept-based spontaneous speech understanding system.
InProceedings of Eurospeech, pages 555?558, Madrid,Spain.Fran?ois Mairesse, Milica Ga?ic?, Filip Jurc?
?c?ek, SimonKeizer, Blaise Thomson, Kai Yu, and Steve Young.2009.
Spoken language understanding from unaligneddata using discriminative classification models.
InProceedings of ICASSP, Taipei, Taiwan.Franz Joseph Och and Hermann Ney.
2000.
A compari-son of alignment models for statistical machine trans-lation.
In Proceedings of Coling, volume 2, pages1086?1090, Saarbr?cken, Germany.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och, Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Proceedings of the Joint SIGDATConference on Empirical Methods in Natural Lan-guage Processing and Very Large Corpora, pages 20?28, College Park, MD, USA.Wayne Ward.
1991.
Understanding spontaneous speech:the Phoenix system.
In Proceedings of ICASSP, pages365?368, Toronto, Canada.104
