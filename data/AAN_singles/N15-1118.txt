Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1119?1129,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsInjecting Logical Background Knowledge intoEmbeddings for Relation ExtractionTim Rockt?aschelUniversity College LondonLondon, UKSameer SinghUniversity of WashingtonSeattle, WASebastian RiedelUniversity College LondonLondon, UKAbstractMatrix factorization approaches to relationextraction provide several attractive features:they support distant supervision, handle openschemas, and leverage unlabeled data.
Unfortu-nately, these methods share a shortcoming withall other distantly supervised approaches: theycannot learn to extract target relations with-out existing data in the knowledge base, andlikewise, these models are inaccurate for rela-tions with sparse data.
Rule-based extractors,on the other hand, can be easily extended tonovel relations and improved for existing butinaccurate relations, through first-order formu-lae that capture auxiliary domain knowledge.However, usually a large set of such formulaeis necessary to achieve generalization.In this paper, we introduce a paradigmfor learning low-dimensional embeddings ofentity-pairs and relations that combine the ad-vantages of matrix factorization with first-orderlogic domain knowledge.
We introduce simpleapproaches for estimating such embeddings,as well as a novel training algorithm to jointlyoptimize over factual and first-order logic in-formation.
Our results show that this methodis able to learn accurate extractors with little orno distant supervision alignments, while at thesame time generalizing to textual patterns thatdo not appear in the formulae.1 IntroductionRelation extraction, the task of identifying rela-tions between named entities, is a crucial compo-nent for information extraction.
A recent successfulapproach (Riedel et al, 2013) relies on two ideas:(a) unifying traditional canonical relations, such asthose of the Freebase schema, with OpenIE surfaceform patterns in a universal schema, and (b) complet-ing a knowledge base of such a schema using matrixfactorization.
This approach has several attractiveproperties.
First, for canonical relations it effectivelyperforms distant supervision (Bunescu and Mooney,2007; Mintz et al, 2009; Yao et al, 2011; Hoffmannet al, 2011; Surdeanu et al, 2012) and hence re-quires no textual annotations.
Second, in the spirit ofOpenIE, a universal schema can use textual patternsas novel relations and thus increases the coverageof traditional schemas (Riedel et al, 2013; Fan etal., 2014).
Third, matrix factorization learns betterembeddings for entity-pairs for which only surfaceform patterns are observed, and these can also leadto better extractions of canonical relations.Unfortunately, populating a universal schemaknowledge base using matrix factorization suffersfrom a problem all distantly-supervised techniquesshare: you can only reliably learn relations that ap-pear frequently enough in the knowledge base.
In par-ticular, for relations that do not appear in the knowl-edge base or for which no facts are known we cannotlearn a predictor at all.
One way to overcome thisproblem is to incorporate additional domain knowl-edge, either specified manually or bootstrapped fromauxiliary sources.
In fact, domain knowledge en-coded as simple logic formulae over patterns andrelations has been used in practice to directly specifyrelation extractors (Reiss et al, 2008; Chiticariu et al,2013; Akbik et al, 2014).
However, these extractorscan be brittle and obtain poor recall, since they areunable to generalize to textual patterns that are not1119found in given formulae.
Hence, there is a need forlearning extractors that are able to combine logicalknowledge with benefits of factorization techniquesto facilitate precise extractions and generalization tonovel relations.In this paper, we propose a paradigm for learninguniversal schema extractors by combining matrix fac-torization based relation extraction with additional in-formation in the form of first-order logic knowledge.Our contributions are threefold: (i) We introduce sim-ple baselines that enforce logic constraints throughdeterministic inference before and after matrix factor-ization (?3.1).
(ii) We propose a novel joint trainingalgorithm that learns vector embeddings of relationsand entity-pairs using both distant supervision andfirst-order logic formulae such that the factorizationcaptures these formulae (?3.2).
(iii) We present anempirical evaluation using automatically mined rulesthat demonstrates the benefits of incorporating logi-cal knowledge in relation extraction, in particular thatjoint factorization of distant and logic supervision isefficient, accurate, and robust to noise (?5).2 Matrix Factorization and LogicIn this section we provide background on matrix fac-torization for universal schema relation extraction,and describe its connections to first-order logic.2.1 NotationIn order to later unify observed facts and logical back-ground knowledge, we first represent given factualdata in terms of first-order logic.
We have a set Eof constants that refer to entities, and a set of predi-catesR that refer to relations between these entities.In the following we will focus on binary relationsin a universal schema that contains both structuredrelations from one (or more) knowledge bases, andsurface-form relations.
Further, with P ?
E ?
E wedenote the domain over entity-pairs of interest.In function-free first-order logic a term is de-fined as a constant or a variable, and the mostbasic form of a formula is an atom such asprofessorAt(x, y) that applies a predicate to apair of terms.
More complex formulae such as ?x, y :professorAt(x, y)?
employeeAt(x, y) canbe constructed by combining atoms with logical con-nectives (such as ?
and ?)
and quantifiers (?x, ?x).The simplest form of first-order formulae areground atoms: predicates applied to constants, suchas directorOf(NOLAN,INTERSTELLAR).
A pos-sible world is a set of ground atoms.
Ground lit-erals are either ground atoms or negated groundatoms such as ?bornIn(NOLAN,BERLIN), andcorrespond to positive or negative facts.
Trainingdata for distant supervision can now be viewed asa knowledge base of such ground literals.
Our goalis to extend the class of formulae from such facts torules such as the first-order formula above.2.2 Matrix Factorization with Ground AtomsGiven the notation presented above, matrix factor-ization can now be seen as a learning task in whichlow-dimensional embeddings are estimated for allconstant pairs in P and predicates (relations) in R,given a collection of ground atoms (facts) as supervi-sion.
We represent constant-pairs as rows and predi-cates as columns of a |P| ?
|R| binary matrix, andeach atom in the training data represents an observedcell in this matrix.
As introduced in Riedel et al(2013), we seek to find a low-rank factorization intoa |P|?k matrix of embeddings of constant-pairs anda k ?
|R| matrix of predicate embeddings such thatthey approximate the observed matrix.More precisely, let v(?
)denote the mapping fromconstant-pairs and predicates to their correspondingembedding.
That is, vrmis the embedding for predi-cate rm, and v(ei,ej)is the embedding for the pairof constants (ei, ej).
Let w be a possible world(i.e.
a set of ground atoms), and V be the set ofall entity-pair and relation embeddings.
Further, letpiei,ejm= ?(vrm?
vei,ej) where ?
is the sigmoid func-tion and vrm?
v(ei,ej)denotes the vector dot-productbetween the embeddings of relation rmand entity-pair (ei, ej).
We define the conditional probability ofa possible world w given embeddings V asp(w|V) =?rm(ei,ej)?wpiei,ejm?rm(ei,ej)/?w(1?
piei,ejm).The embeddings can be estimated by maximizing thelikelihood of a set of observed ground atoms with `2regularization (Collins et al, 2001), optimized usingstochastic gradient descent.
In summary, with atomicformulae (i.e.
factual knowledge) we learn entity-pairand relation embeddings that reconstruct known factsand are able to generalize to unknown facts.1120KB?x, y : co-founder-of(x, y)?
company/founders(y, x)?x, y : review-by(x, y)?
author/works written(y, x)?x, y : daughter-of(x, y)?
person/parents(x, y)|R||P|k|P||R|k|R||P|Evidence Sparse Training Matrix Low-rank Logic Embeddings Completed MatrixFactsFirst-orderFormulaeFigure 1: Injecting Logic into Matrix Factorization: Given a sparse binary matrix consisting of observed facts overentity-pairs P and predicates/relationsR, matrix factorization is used to learn k-dimensional relation and entity-pairembeddings that approximate the observed matrix.
In this paper we use additional first-order logic formulae overentities and relations to learn the embeddings such that the predictions (completed matrix) also satisfy these formulae.3 Injecting Logic Into FactorizationMatrix factorization is capable of learning complexdependencies between relations, but requires ob-served facts as training signal.
However, often weeither do not have this signal because the relations ofinterest do not have pre-existing facts, or this signalis noisy due to alignment errors or mismatches whenlinking knowledge base entities to mentions in text.To overcome this problem we investigate the useof first-order logic background knowledge (e.g.
im-plications) to aid relation extraction.
One option isto rely on a fully symbolic approach that exclusivelyuses first-order logic (Bos and Markert, 2005; Baaderet al, 2007; Bos, 2008).
In this case incorporatingadditional background knowledge is trivial.
However,it is difficult to generalize and deal with noise anduncertainty in language when relying only on manualrules.
In contrast, matrix factorization methods canovercome these shortcomings, but it is not clear howthey can be combined with logic formulae.In this section, we propose to inject formulae intothe embeddings of relations and entity-pairs, i.e., esti-mate the embeddings such that predictions based onthem conform to given logic formulae (see Figure 1for an overview).
We refer to such embeddings aslow-rank logic embeddings.
Akin to matrix factoriza-tion, inference of a fact at test time still amounts toan efficient dot product of the corresponding relationand entity-pair embeddings, and logical inference isnot needed.
We present two techniques for inject-ing logical background knowledge, pre-factorizationinference (?3.1) and joint optimization (?3.2), anddemonstrate in subsequent sections that they gen-eralize better than direct logical inference, even ifsuch inference is performed on the predictions of thematrix factorization model.3.1 Pre-Factorization InferenceBackground knowledge in form of first-order formu-lae can be seen as hints that can be used to generateadditional training data (Abu-Mostafa, 1990).
Forpre-factorization inference we first perform logicalinference on the training data and add inferred factsas additional training data.
For example, for a for-mula F = ?x, y : rs(x, y)?
rt(x, y), we add anadditional observed cell rt(x, y) for any (x, y) forwhich rs(x, y) is observed in the distant supervisiontraining data.
This is repeated until no further factscan be inferred.
Subsequently, we run matrix factor-ization on the extended set of observed cells.The intuition is that the additional training datagenerated by the formulae provide evidence of thelogical dependencies between relations to the matrixfactorization model, while at the same time allowingthe factorization to generalize to unobserved factsand to deal with ambiguity and noise in the data.
Nofurther logical inference is performed during or aftertraining of the factorization model as we expect thatthe learned embeddings encode the given formulae.3.2 Joint OptimizationOne drawback of pre-factorization inference is thatthe formulae are enforced only on observed atoms,1121i.e., first-order dependencies on predicted facts areignored.
Instead we would like to include a loss termfor the logical formulae directly in the matrix factor-ization objective, thus jointly optimizing embeddingsto reconstruct factual training data as well as obeyingto first-order logical background knowledge.3.2.1 Training ObjectiveHere we first present a learning objective that uni-fies ground atoms (facts) and logical backgroundknowledge by treating both as logic formulae (atomicor complex), and define a loss function over this gen-eral representation.
We then define the loss functionfor ground atoms and simple implications, along witha brief sketch of how the loss can be defined for arbi-trarily complex logic formulae.As introduced in ?2.1, let R be the set of allrelations/predicates and P be the set of all entity-pairs/constants.
Furthermore, let F be a training setof logic formulae F , and L a loss function.
The train-ing objective (omitting `2regularization on v(?
)forsimplicity) isminV?F?FL([F ]) (1)where V is the set of all relation and entity-pair em-beddings, and [F ] is the marginal probability p(w|V)that the formula F is true under the model.
In this pa-per we use the logistic loss: L([F ]) := ?
log([F ]).The objective thus prefers embeddings that assignformulae a high marginal probability.To optimize this function we need the marginalprobabilities [F ], and the gradients of the lossesL([F ]) for every F ?
F with respect to entity-pair and relation embeddings, i.e., ?L([F ])/?vrmand ?L([F ])/?v(ei,ej).
Below we discuss how thesequantities can be computed or approximated for arbi-trary first-order logic formulae, with details providedfor ground atoms and implications.Ground Atoms Due to the conditional indepen-dence of ground atoms in the distribution p(w|V),the marginal probability of a ground atom F =rm(ei, ej) is [F ] = piei,ejm= ?(vrm?
vei,ej).
Hencewhen only ground atoms (or literals) are used, objec-tive (1) reduces to the standard log-likelihood loss.The gradients of the loss for the entity-pair embed-ding v(ei,ej)and relation embedding vrmare?
[F ]/?v(ei,ej)= [F ](1?
[F ])vrm(2)?
[F ]/?vrm= [F ](1?
[F ])v(ei,ej)(3)?L([F ])/?v(ei,ej)= ?
[F ]?1?
[F ]/?v(ei,ej)(4)?L([F ])/?vrm= ?
[F ]?1?
[F ]/?vrm.
(5)First-order Logic Crucially, and in contrast to thelog-likelihood loss for matrix factorization, we can in-ject more expressive logic formulae than just groundatoms.
We briefly outline how to recursively computethe probability of the formula [F ] and the gradients ofthe loss L([F ]) for any first-order formula F .
Again,note that the probabilities of ground atoms in ourmodel are independent conditioned on embeddings.This means that for any two formulae A and B, themarginal probability of [A ?
B] can be computed as[A][B] (known as product t-norm), provided both for-mula concern non-overlapping sets of ground atoms.In combination with [?A] := 1 ?
[A] and the [ ]operator as defined for ground atoms earlier, we cancompute the probability of any propositional formularecursively, e.g.,[A ?
B] = [A] + [B]?
[A][B][A ?
B] = [A]([B]?
1) + 1[A ?
?B ?
C] = ([A](1?
[B]))([C]?
1) + 1.Note that for statements [F ] ?
{0, 1}, we directlyrecover logical semantics.
First-order formulae infinite domains can be embedded through explicitgrounding.
For universal quantification we can get[?x, y : F(x, y)] = [?x,yF(x, y)].
If we again as-sume non-overlapping ground atoms in each of thearguments of the conjunction, we can simplify this to?x,y[F(x, y)].
When arguments do overlap we canthink of this simplification as an approximation.Since [F(x, y)] is defined recursively, we canback-propagate the training signal through thestructure of [F ] to compute ?
[F(x, y)]/?vrmand?
[F(x, y)]/?vei,ejfor any nested formula.Implications A particularly useful family of for-mulae for relation extraction are universally quanti-fied first-order formula over a knowledge base suchas F = ?x, y : rs(x, y) ?
rt(x, y).
Assuminga finite domain, such a formula can be unrolledinto a conjunction of propositional statements ofthe form Fij= rs(ei, ej) ?
rt(ei, ej), one for1122each entity-pair (ei, ej) in the domain.
Specifically,[F ] =?
(ei,ej)?P[Fij], and therefore L([F ]) =?(ei,ej)?PL([Fij]).
The gradients are derived as:[Fij] = [rs(ei, ej)] ([rt(ei, ej)]?
1) + 1 (6)?L([Fij])?vrs= ?
[Fij]?1([rt(ei, ej)]?
1)?
[rs(ei, ej)]?vrs?L([Fij])?vrt= ?
[Fij]?1[rs(ei, ej)]?
[rt(ei, ej)]?vrt(7)?L([Fij])?vei,ej= ?
[Fij]?1([rt(ei, ej)]?
1)?
[rs(ei, ej)]?vei,ej?
[Fij]?1[rs(ei, ej)]?
[rt(ei, ej)]?vei,ej.
(8)Following such a derivation, one can obtain gradientsfor other first-order logic formulae as well.3.2.2 LearningWe learn the embeddings by minimizing Eq.
1with `2-regularization using AdaGrad (Duchi et al,2011).
Since we have no negative training facts, wefollow Riedel et al (2013) by sampling unobservedfacts that we assume to be false.
Specifically, in ev-ery epoch and for every true training fact rm(ei, ej)we sample an (ep, eq) such that rm(ep, eq) is unob-served.
Subsequently, we perform two kinds of up-dates: F = rm(ei, ej) and F = ?
rm(ep, eq).
Forevery non-atomic first-order formula in F we iterateover all entity-pairs for which at least one atom in theformula is observed (in addition to as many sampledentity-pairs for which none of the atoms have beenobserved) and add corresponding grounded propo-sitional formulae to the training objective.
At testtime, predicting a score for any unobserved state-ment rm(ei, ej) is done efficiently by calculating[rm(ei, ej)].
Note that this does not involve any ex-plicit logical inference, instead we expect that thepredictions from the learned embeddings already re-spect the provided formulae.4 Experimental SetupThere are two orthogonal question when evaluat-ing the effectiveness of low-rank logic embeddings:a) does injection of logic formulae into the embed-dings of entity-pairs and relations provide any bene-fits, and b) where do the background formulae comefrom?
The latter is a well-studied problem (Hipp etal., 2000; Schoenmackers et al, 2010; V?olker andNiepert, 2011).
In this paper we focus the evaluationon the ability of various approaches to benefit fromformulae that we directly extract from the trainingdata using a simple method.Distant Supervision Evaluation We follow theprocedure as used in Riedel et al (2013) for eval-uating knowledge base completion of Freebase (Bol-lacker et al, 2008) with textual data from the NY-Times corpus (Sandhaus, 2008).
The training matrixconsists of 4 111 columns, representing 151 Free-base relations and 3 960 textual patterns, 41 913 rows(entity-pairs) and 118 781 training facts of which7 293 belong to Freebase relations.
The entity-pairsare divided into train and test, and we hide all Free-base relations for the test pairs from training.
Our pri-mary evaluation measure is average and (weighted)mean average precision, MAP and wMAP respec-tively (see Riedel et al (2013) for details).Formulae Extraction and Annotation We use asimple technique for extracting formulae from thematrix factorization model.
We first run matrix fac-torization over the complete training data to learnaccurate relation and entity-pair embeddings.
Aftertraining, we iterate over all pairs of relations (rs, rt)where rtis a Freebase relation.
For every relation-pair we iterate over all training atoms rs(ei, ej), eval-uate the score [rs(ei, ej) ?
rt(ei, ej)] as describedin ?3.2.1, and calculate the average to arrive at ascore for the formula.
Finally, we rank all formulaeby their score and manually filter the top 100 for-mulae, which resulted in 36 annotated high-qualityformulae (see Table 1 for examples).
Note that ourformula extraction approach does not observe the re-lations for test entity-pairs.
All models used in ourexperiments have access to these formulae, exceptfor the matrix factorization baseline.Methods Our proposed methods for injecting logicinto relation embeddings are pre-factorization infer-ence (Pre; ?3.1) which performs regular matrix fac-torization after propagating the logic formulae in adeterministic manner, and joint optimization (Joint;?3.2) which maximizes an objective that combinesterms from factual and first-order logic knowledge.Additionally, we use the following three baselines.The matrix factorization (MF; ?2.2) model uses onlyground atoms to learn relation and entity-pair embed-1123Formula Score?x, y : #2-unit-of-#1(x, y)?
org/parent/child(x, y) 0.97?x, y : #2-city-of-#1(x, y)?
location/containedby(x, y) 0.97?x, y : #2-minister-#1(x, y)?
person/nationality(x, y) 0.97?x, y : #2-executive-#1(x, y)?
person/company(x, y) 0.96?x, y : #2-co-founder-of-#1(x, y)?
company/founders(y, x) 0.96Table 1: Sample Extracted Formulae: Top implica-tions of textual patterns to five different Freebase relations.These implications were extracted from the matrix factor-ization model and manually annotated.
The premises ofthese implications are dependency paths, but we present asimplified version to make them more readable.dings (i.e.
it has no access to any formulae).
Further-more, we consider pure logical inference (Inf).
Ourfinal approach, post-factorization inference (Post),first runs matrix factorization and then performs logi-cal inference on the known and predicted facts.
Post-inference is computationally expensive, since for allpremises of formulae we have to iterate over allrows (entity-pairs) in the matrix to assess whetherthe premise is true or not.Parameters For every matrix factorization basedmethod we use k = 100 as the dimension for the em-beddings, ?
= 0.01 as parameter of `2-regularizationand ?
= 0.1 as initial learning rate for AdaGrad,which we run for 200 epochs.Complexity Each AdaGrad update is defined overa single cell of the matrix, and thus training data canbe streamed one ground atom at a time.
For matrixfactorization, each AdaGrad epoch touches all theobserved atoms once, and as many sampled negativeatoms.
With given formulae, it additionally revisitsall the observed atoms that appear as an atom in theformula (and as many sampled negative atoms), andthus more general formulae will be more expensive.However the updates over atoms are performed inde-pendently and thus not all the data needs to be storedin memory.
All presented models take less than 15minutes to train on a 2.8 GHz Intel Core i7 machine.5 Results and DiscussionTo evaluate the utility of injecting logic formulaeinto embeddings, we present a comparison on a va-riety of benchmarks.
First, in ?5.1 we study thescenario of learning extractors for relations for whichwe do not have any Freebase alignments, evaluatinghow the approaches are able to generalize only fromRelation # MF Inf Post Pre Jointperson/company 102 0.07 0.03 0.15 0.31 0.35location/containedby 72 0.03 0.06 0.14 0.22 0.31author/works written 27 0.02 0.05 0.18 0.31 0.27person/nationality 25 0.01 0.19 0.09 0.15 0.19parent/child 19 0.01 0.01 0.48 0.66 0.75person/place of birth 18 0.01 0.43 0.40 0.56 0.59person/place of death 18 0.01 0.24 0.23 0.27 0.23neighborhood/neighborhood of 11 0.00 0.00 0.60 0.63 0.65person/parents 6 0.00 0.17 0.19 0.37 0.65company/founders 4 0.00 0.25 0.13 0.37 0.77film/directed by 2 0.00 0.50 0.50 0.36 0.51film/produced by 1 0.00 1.00 1.00 1.00 1.00MAP 0.01 0.23 0.34 0.43 0.52Weighted MAP 0.03 0.10 0.21 0.33 0.38Table 2: Zero-shot Relation Learning: Average and(weighted) mean average precisions with relations that donot appear in any of the annotated formulae omitted fromthe evaluation.
The difference between ?Pre?
and ?Joint?is significant according to the sign-test (p < 0.05).logic formulae and textual patterns.
In ?5.2 we thendescribe an experiment where the amount of Free-base alignments is varied in order to assess the effectof combining distant supervision and backgroundknowledge on the accuracy of predictions.
Althoughthe methods presented in this paper target relationswith insufficient alignments, we also provide a com-parison on the complete distant supervision datasetin ?5.3.
We conclude in ?5.4 with a brief analysis ofthe reasoning capacity of the learned embeddings.5.1 Zero-shot Relation LearningWe start with the scenario of learning extractors forrelations that do not appear in the knowledge baseschema, i.e., those that do not have any textual align-ments.
Such a scenario occurs in practice when a newrelation needs to be added to a knowledge base forwhich there are no facts that connect the new relationto existing relations or surface patterns.
For accurateextractions of such relations, the model needs to relyprimarily on background domain knowledge to iden-tify relevant textual alignments, but at the same timeit also needs to utilize correlations between textualpatterns for generalization.
To simulate this setup,we remove all alignments between all entity-pairsand Freebase relations from the distant supervisiondata, use the extracted logic formulae (?4) as back-ground knowledge, and evaluate on the ability of thedifferent methods to recover the lost alignments.Table 2 provides detailed results.
Unsurprisingly,11240 0.2 0.40.60  0.1  0.2  0.3  0.4  0.5wMAP Fraction of Freebase training factsMFJointPrePostInfFigure 2: Relations with Few Distant Labels:Weighted mean average precisions of the various methodsas the fraction of Freebase training facts is varied.
For0% Freebase training facts we get the zero-shot relationlearning results presented in Table 2.matrix factorization (MF) performs poorly since em-beddings cannot be learned for the Freebase relationswithout any observed cells.
Scores higher than zerofor matrix factorization are caused by random pre-dictions.
Logical inference (Inf) is limited by thenumber of known facts that appear as premise in oneof the implications.
Although post-factorization in-ference (Post) is able to achieve a large improvementover logical inference, explicitly injecting logic for-mulae into the embeddings (i.e.
learning low-ranklogic embeddings) using pre-factorization inference(Pre) or joint optimization (Joint) gives superior re-sults.
Last, we observe that joint optimization is ableto best combine logic formulae and textual patternsfor accurate, zero-shot learning of relation extractors.5.2 Relations with Few Distant LabelsIn this section we study the scenario of learning rela-tions that have only a few distant supervision align-ments, in particular, we observe the behavior of thevarious methods as the amount of distant supervi-sion is varied.
We run all methods on training datathat contains different fractions of Freebase trainingfacts (and therefore different degrees of relation/textpattern alignment), but keep all textual patterns inaddition to the set of extracted formulae.Figure 2 summarizes the results.
The performanceof pure logical inference does not depend on theamount of distant supervision data, since it does nottake advantage of the correlations in the data.
Ma-0.2 0.4 0.6 0.810  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Precision RecallJointMFMintz09Yao11Surdeanu12Riedel13-FFigure 3: Comparison on Complete Data: Aver-aged precision/recall curve demonstrating that the ?Joint?method outperforms existing factorization approaches(?MF?
and ?Riedel13-F?).
The formulae used by our ap-proach have been extracted only from the training data.trix factorization ignores logic formulae, and thusis the baseline performance when only using distantsupervision.
For the factorization based methods,only a small fraction (15%) of the training data isneeded to achieve around 0.50 wMAP performance,thus demonstrating that they are efficiently exploitingcorrelations and generalizing to unobserved facts.Pre-factorization inference, however, does not out-perform post-factorization inference, and is on parwith matrix factorization for most of the curve.
Thissuggests that it is not an effective way of injectinglogic into embeddings when ground facts are alsoavailable.
In contrast, joint optimization leads tolow-rank logic embeddings that outperform all othermethods in the 0 to 30% Freebase training data inter-val.
Beyond 30% there seem to be sufficient Freebasefacts for matrix factorization to encode these formu-lae, thus yielding diminishing returns.5.3 Comparison on Complete DataAlthough the focus of this paper is injection of logicalknowledge for relations without sufficient alignmentsto the knowledge base, we also present an evaluationon the complete distant supervision data as used byRiedel et al (2013).
Compared to the Riedel et al?s?F?
model, our matrix factorization implementation(?MF?)
achieves a lower wMAP (64% vs 68%) anda higher MAP (66% vs 64%).
We attribute this dif-ference to the different loss function (logistic loss inour case vs. ranking loss).
We show the PR curve1125in Figure 3, demonstrating that joint optimizationprovides benefits over the existing factorization anddistant supervision techniques even on the completedataset, and obtains 66% wMAP and 69% MAP.
Thisimprovement over the matrix factorization model canbe explained by reinforcement of high-quality anno-tated formulae via the joint model.5.4 Analysis of Asymmetry in the PredictionsSince the injected formulae are of the form ?x, y :rs(x, y)?
rt(x, y), it is worthwhile to study the ex-tent to which these rules are captured, and whichapproaches are in fact capturing the asymmetric na-ture of the implication.
To this end, we compute theprobabilities that the formulae and their inverse hold,averaged over all annotated formulae and cells.
Thedegree to which rs?
rtis captured is quite high forall models (0.94, 0.96, and 0.97 for matrix factoriza-tion, pre-factorization inference, and joint optimiza-tion respectively).
On the other hand, the probabilityof rt?
rsis also relatively high for matrix factoriza-tion and pre-factorization inference (0.81 and 0.83respectively), suggesting that these methods are pri-marily capturing symmetric similarity between rela-tions.
Joint optimization, however, produces muchmore asymmetric predictions (probability of rt?
rsis 0.49), demonstrating that it is appropriate for en-coding logic in the embeddings.6 Related WorkEmbeddings for Knowledge Base CompletionEmbedding predicates and constants (or pairs of con-stants) based on factual knowledge for knowledgebase completion has for instance been investigated byBordes et al (2011), Nickel et al (2012), Socher etal.
(2013), Riedel et al (2013) and Fan et al (2014).Our work goes further in that we learn embeddingsthat follow not only factual but also first-order logicknowledge, and the ideas presented in this paper canbe incorporated with any embedding-based methodthat uses a per-atom loss.Logical Inference A common alternative that di-rectly incorporates first-order logic knowledge is toperform logical inference (Bos and Markert, 2005;Baader et al, 2007; Bos, 2008), however such purelysymbolic approaches cannot deal with the uncertaintyinherent to natural language, and generalize poorly.Probabilistic Inference To ameliorate some ofthe drawbacks of symbolic logical inference, proba-bilistic logic based approaches have been proposed(Schoenmackers et al, 2008; Garrette et al, 2011;Beltagy et al, 2013; Beltagy et al, 2014).
Since log-ical connections between relations are modeled ex-plicitly, such approaches are generally hard to scale.Specifically, approaches based on Markov Logic Net-works (MLNs) (Richardson and Domingos, 2006)encode logical knowledge in dense, loopy graphicalmodels, making structure learning, parameter estima-tion, and inference hard for the scale of our data.
Incontrast, in our model the logical knowledge is cap-tured directly in the embeddings, leading to efficientinference.
Furthermore, as our model is based onmatrix factorization, we have a natural way to dealwith linguistic ambiguities and label errors.Weakly Supervised Learning Our work is also in-spired by weakly supervised approaches (Ganchev etal., 2010) that use structural constraints as a source ofindirect supervision, and have been used for severalNLP tasks (Chang et al, 2007; Mann and McCallum,2008; Druck et al, 2009; Singh et al, 2010).
Carlsonet al (2010) in particular is similar since they usecommon sense constraints to jointly train multipleinformation extractors.
In this work, however, we aretraining a matrix factorization model, and allowingfor arbitrarily complex logic formulae.Combining Symbolic and Distributed Represen-tations There have been a number of recent ap-proaches that combine distributed representationswith symbolic knowledge.
Grefenstette (2013) de-scribes an isomorphism between first-order logicand tensor calculus, using full-rank matrices to ex-actly memorize facts.
Based on this isomorphism,Rockt?aschel et al (2014) combine logic with ma-trix factorization for learning low-dimensional em-beddings that approximately satisfy given formulaeand generalize to unobserved facts on toy data.
Ourwork extends this workshop paper by proposing asimpler formalism without tensor-based logical con-nectives, presenting results on a real-world task, anddemonstrating the utility of this approach for learningrelations with few textual alignments.Chang et al (2014) use Freebase entity types ashard constraints in a tensor factorization objective foruniversal schema relation extraction.
In contrast, our1126approach is imposing soft constraints that are formu-lated as universally quantified first-order formula.de Lacalle and Lapata (2013) combine first-orderlogic knowledge with a topic model to improve sur-face pattern clustering for relation extraction.
Sincethese formulae only specify which relations can beclustered and which not, they do not capture the va-riety of dependencies embeddings can model, suchas asymmetry.
Lewis and Steedman (2013) use dis-tributed representations to cluster predicates beforelogical inference.
Again, this approach is not as pow-erful as factorizing the relations, as it makes symme-try assumptions for the predicates.Several studies have investigated the use of sym-bolic representations (such as dependency trees)to guide the composition of distributed representa-tions (Clark and Pulman, 2007; Mitchell and Lapata,2008; Coecke et al, 2010; Hermann and Blunsom,2013).
Instead we are using symbolic representa-tions (first-order logic) as prior domain knowledge todirectly learn better embeddings.Combining symbolic information with neural net-works has also been an active area of research.
Towelland Shavlik (1994) introduce Knowledge-Based Arti-ficial Neural Networks whose topology is isomorphicto a knowledge base of facts and inference formulae.There, facts are input units, intermediate conclusionshidden units, and final conclusions (inferred facts)output units.
Unlike our work, there is no latent rep-resentation of predicates and constants.
H?olldobleret al (1999) and Hitzler et al (2004) prove that forevery logic program theoretically there exists a recur-rent neural network that approximates the semanticsof that program.
Finally, Bowman (2014) recentlydemonstrated that a neural tensor network can accu-rately learn natural logic reasoning.7 ConclusionsInspired by the benefits of logical background knowl-edge that can lead to precise extractors, and of distantsupervision based matrix factorization that can utilizedependencies between textual patterns to generalize,in this paper we introduced a novel training paradigmfor learning embeddings that combine matrix factor-ization with logic formulae.
Along with a determin-istic approach to enforce the formulae a priori, wepropose a joint objective that rewards predictions thatsatisfy given logical knowledge, thus learning embed-dings that do not require logical inference at test time.Experiments show that the proposed approaches areable to learn extractors for relations with little to noobserved textual alignments, while at the same timebenefiting more common relations.
The source codeof the methods presented in this paper and the anno-tated formulae used for evaluation are available atgithub.com/uclmr/low-rank-logic.This research has thrown up many questions inneed of further investigation.
As opposed to our ap-proach that modifies both relation and entity-pairembeddings, further work needs to explore train-ing methods that only modify relation embeddingsin order to encode logical dependencies explicitly,and thus avoid memorization.
Although we ob-tain significant gains by using implications, our ap-proach facilitates the use of arbitrary formulae; itwould be worthwhile to pursue this direction by fol-lowing the steps outlined in ?3.2.1.
Furthermore,we are interested in combining relation extractionwith models that learn entity type representations(e.g.
tensor factorization or neural models) to al-low for expressive logical statements such as ?x, y :nationality(x, y)?
country(y).
Since suchcommon sense formulae are often not directly ob-served in distant supervision, they can go a long wayin fixing common extraction errors.
Finally, we willinvestigate methods to automatically mine common-sense knowledge for injection into embeddings fromadditional resources such as Probase (Wu et al, 2012)or directly from text using a semantic parser (Zettle-moyer and Collins, 2005).AcknowledgmentsThe authors want to thank Mathias Niepert forproposing pre-factorization inference as alternativeto joint optimization.
We thank Edward Grefenstette,Luke Zettlemoyer, and Guillaume Bouchard for com-ments on the manuscript, and the reviewers for veryhelpful feedback.
This work was supported in partby Microsoft Research through its PhD ScholarshipProgramme and in part by the TerraSwarm ResearchCenter, one of six centers supported by the STAR-net phase of the Focus Center Research Program(FCRP) a Semiconductor Research Corporation pro-gram sponsored by MARCO and DARPA.1127ReferencesYaser S Abu-Mostafa.
1990.
Learning from hints inneural networks.
Journal of complexity, 6(2):192?198.Alan Akbik, Thilo Michael, and Christoph Boden.
2014.Exploratory relation extraction in large text corpora.
InInternational Conference on Computational Linguistics(COLING), pages 2087?2096.Franz Baader, Bernhard Ganter, Baris Sertkaya, and UlrikeSattler.
2007.
Completing description logic knowledgebases using formal concept analysis.
In IJCAI, pages230?235.Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.
Mon-tague meets markov: Deep semantics with probabilis-tic logical form.
In 2nd Joint Conference on Lexicaland Computational Semantics: Proceeding of the MainConference and the Shared Task, Atlanta, pages 11?21.Islam Beltagy, Katrin Erk, and Raymond J. Mooney.
2014.Probabilistic soft logic for semantic textual similarity.In Proceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (ACL-14), Bal-timore, MD.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring humanknowledge.
In Proceedings of the 2008 ACM SIGMODinternational conference on Management of data, pages1247?1250.
ACM.Antoine Bordes, Jason Weston, Ronan Collobert, andYoshua Bengio.
2011.
Learning structured embed-dings of knowledge bases.
In AAAI.Johan Bos and Katja Markert.
2005.
Recognising textualentailment with logical inference.
In Proceedings of theconference on Human Language Technology and Em-pirical Methods in Natural Language Processing, pages628?635.
Association for Computational Linguistics.Johan Bos.
2008.
Wide-coverage semantic analysis withboxer.
In Johan Bos and Rodolfo Delmonte, editors,Semantics in Text Processing.
STEP 2008 ConferenceProceedings, Research in Computational Semantics,pages 277?286.
College Publications.Samuel R Bowman.
2014.
Can recursive neural tensornetworks learn logical reasoning?
In ICLR?14.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to Extract Relations from the Web using Mini-mal Supervision.
In Annual Meeting of the Associationfor Computational Linguistics (ACL), Prague, CzechRepublic, June.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka, Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In International conference on Web searchand data mining (WSDM).Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007.Guiding semi-supervision with constraint-driven learn-ing.
In Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 280?287.Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christo-pher Meek.
2014.
Typed tensor decomposition ofknowledge bases for relation extraction.Laura Chiticariu, Yunyao Li, and Frederick R. Reiss.2013.
Rule-based information extraction is dead!
longlive rule-based information extraction systems!
InEmpirical Methods in Natural Language Processing(EMNLP), pages 827?832.Stephen Clark and Stephen Pulman.
2007.
Combiningsymbolic and distributional models of meaning.
InAAAI Spring Symposium: Quantum Interaction, pages52?55.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a composi-tional distributional model of meaning.
CoRR,abs/1003.4394.Michael Collins, Sanjoy Dasgupta, and Robert E Schapire.2001.
A generalization of principal components anal-ysis to the exponential family.
In Advances in neuralinformation processing systems, pages 617?624.Oier Lopez de Lacalle and Mirella Lapata.
2013.
Unsu-pervised relation extraction with general domain knowl-edge.
In EMNLP, pages 415?425.Gregory Druck, Gideon Mann, and Andrew McCallum.2009.
Semi-supervised learning of dependency parsersusing generalized expectation criteria.
In Joint Confer-ence of the Annual Meeting of the ACL and the Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of Machine Learn-ing Research, 12:2121?2159.Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu,Thomas Fang Zheng, and Edward Y Chang.
2014.Distant supervision for relation extraction with matrixcompletion.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics,volume 1, pages 839?849.Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of Machine Learn-ing Research (JMLR), July.Dan Garrette, Katrin Erk, and Raymond Mooney.
2011.Integrating logical representations with probabilisticinformation using markov logic.
In Proceedings ofthe Ninth International Conference on ComputationalSemantics, pages 105?114.
Association for Computa-tional Linguistics.1128Edward Grefenstette.
2013.
Towards a formal distribu-tional semantics: Simulating logical calculi with ten-sors.
In Proceedings of the Second Joint Conferenceon Lexical and Computational Semantics, pages 1?10.Karl Moritz Hermann and Phil Blunsom.
2013.
Therole of syntax in vector space models of compositionalsemantics.
In Proc.
of ACL, pages 894?904.Jochen Hipp, Ulrich G?untzer, and GholamrezaNakhaeizadeh.
2000.
Algorithms for association rulemininga general survey and comparison.
ACM sigkddexplorations newsletter, 2(1):58?64.Pascal Hitzler, Steffen H?olldobler, and Anthony KarelSeda.
2004.
Logic programs and connectionist net-works.
Journal of Applied Logic, 2(3):245?272.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In Annual Meeting of the Associ-ation for Computational Linguistics (ACL).Steffen H?olldobler, Yvonne Kalinke, and Hans-Peter St?orr.1999.
Approximating the semantics of logic programsby recurrent neural networks.
Appl.
Intell., 11(1):45?58.Mike Lewis and Mark Steedman.
2013.
Combined distri-butional and logical semantics.
In Transactions of theAssociation for Computational Linguistics, volume 1,pages 179?192.Gideon S. Mann and Andrew McCallum.
2008.
General-ized expectation criteria for semi-supervised learningof conditional random fields.
In Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 870?878.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extractionwithout labeled data.
In Association for ComputationalLinguistics (ACL).Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proc.
of ACL,pages 236?244.Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel.2012.
Factorizing yago: scalable machine learning forlinked data.
In Proc.
of WWW, pages 271?280.Frederick Reiss, Sriram Raghavan, Rajasekar Krishna-murthy, Huaiyu Zhu, and Shivakumar Vaithyanathan.2008.
An algebraic approach to rule-based informa-tion extraction.
In International Conference on DataEngineering (ICDE), pages 933?942, April.Matthew Richardson and Pedro Domingos.
2006.
Markovlogic networks.
Machine Learning, 62(1-2):107?136.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M Marlin.
2013.
Relation extraction withmatrix factorization and universal schemas.
In Proceed-ings of NAACL-HLT, pages 74?84.Tim Rockt?aschel, Matko Bosnjak, Sameer Singh, andSebastian Riedel.
2014.
Low-Dimensional Embed-dings of Logic.
In ACL Workshop on Semantic Parsing(SP?14).Evan Sandhaus.
2008.
The New York Times annotatedcorpus.
Linguistic Data Consortium.Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld.2008.
Scaling textual inference to the web.
InEmpirical Methods in Natural Language Processing(EMNLP).Stefan Schoenmackers, Jesse Davis, Oren Etzioni, andDaniel Weld.
2010.
Learning first-order horn clausesfrom web text.
In Empirical Methods in Natural Lan-guage Processing (EMNLP).Sameer Singh, Dustin Hillard, and Chris Leggetter.
2010.Minimally-supervised extraction of entities from textadvertisements.
In North American Chapter of theAssociation for Computational Linguistics - HumanLanguage Technologies (NAACL HLT).Richard Socher, Danqi Chen, Christopher D. Manning,and Andrew Y. Ng.
2013.
Reasoning with neuraltensor networks for knowledge base completion.
InNIPS, pages 926?934.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, andChristopher D. Manning.
2012.
Multi-instance multi-label learning for relation extraction.
In EmpiricalMethods in Natural Language Processing (EMNLP).Geoffrey G Towell and Jude W Shavlik.
1994.Knowledge-based artificial neural networks.
Artificialintelligence, 70(1):119?165.Johanna V?olker and Mathias Niepert.
2011.
Statisticalschema induction.
In The Semantic Web: Research andApplications, pages 124?138.
Springer.Wentao Wu, Hongsong Li, Haixun Wang, and Kenny QZhu.
2012.
Probase: A probabilistic taxonomy fortext understanding.
In Proceedings of the 2012 ACMSIGMOD International Conference on Management ofData, pages 481?492.
ACM.Limin Yao, Aria Haghighi, Sebastian Riedel, and AndrewMcCallum.
2011.
Structured relation discovery usinggenerative models.
In Proceedings of the Conferenceon Empirical methods in natural language processing(EMNLP ?11), July.Luke S Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: Structured clas-sification with probabilistic categorial grammars.
InUncertainty in Artifical Intelligence (UAI).1129
