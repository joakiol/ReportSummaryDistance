Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 521?530,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsVariational Neural Machine TranslationBiao Zhang1,2, Deyi Xiong1?, Jinsong Su2, Hong Duan2 and Min Zhang1Provincial Key Laboratory for Computer Information Processing TechnologySoochow University, Suzhou, China 2150061Xiamen University, Xiamen, China 3610052zb@stu.xmu.edu.cn, {jssu,hduan}@xmu.edu.cn{dyxiong, minzhang}@suda.edu.cnAbstractModels of neural machine translation are of-ten from a discriminative family of encoder-decoders that learn a conditional distributionof a target sentence given a source sentence.In this paper, we propose a variational modelto learn this conditional distribution for neu-ral machine translation: a variational encoder-decoder model that can be trained end-to-end.Different from the vanilla encoder-decodermodel that generates target translations fromhidden representations of source sentencesalone, the variational model introduces a con-tinuous latent variable to explicitly model un-derlying semantics of source sentences and toguide the generation of target translations.
Inorder to perform efficient posterior inferenceand large-scale training, we build a neuralposterior approximator conditioned on boththe source and the target sides, and equip itwith a reparameterization technique to esti-mate the variational lower bound.
Experi-ments on both Chinese-English and English-German translation tasks show that the pro-posed variational neural machine translationachieves significant improvements over thevanilla neural machine translation baselines.1 IntroductionNeural machine translation (NMT) is an emergingtranslation paradigm that builds on a single andunified end-to-end neural network, instead of us-ing a variety of sub-models tuned in a long trainingpipeline.
It requires a much smaller memory than?Corresponding authorphrase- or syntax-based statistical machine transla-tion (SMT) that typically has a huge phrase/rule ta-ble.
Due to these advantages over traditional SMTsystem, NMT has recently attracted growing inter-ests from both deep learning and machine transla-tion community (Kalchbrenner and Blunsom, 2013;Cho et al, 2014; Sutskever et al, 2014; Bahdanau etal., 2014; Luong et al, 2015a; Luong et al, 2015b;Shen et al, 2015; Meng et al, 2015; Tu et al, 2016).Current NMT models mainly take a discrimi-native encoder-decoder framework, where a neu-ral encoder transforms source sentence x into dis-tributed representations, and a neural decoder gen-erates the corresponding target sentence y accordingto these representations1 (Cho et al, 2014; Sutskeveret al, 2014; Bahdanau et al, 2014).
Typically, theunderlying semantic representations of source andtarget sentences are learned in an implicit way inthis framework, which heavily relies on the atten-tion mechanism (Bahdanau et al, 2014) to iden-tify semantic alignments between source and targetwords.
Due to potential errors in these alignments,the attention-based context vector may be insuffi-cient to capture the entire meaning of a source sen-tence, hence resulting in undesirable translation phe-nomena (Tu et al, 2016).Unlike the vanilla encoder-decoder framework,we model underlying semantics of bilingual sen-tence pairs explicitly.
We assume that there existsa continuous latent variable z from this underlyingsemantic space.
And this variable, together with x,1In this paper, we use bold symbols to denote variables, andplain symbols to denote their values.
Without specific state-ment, all variables are multivariate.521amssymb amsmathx y?
?NzFigure 1: Illustration of VNMT as a directed graph.We use solid lines to denote the generative modelp?(z|x)p?
(y|z,x), and dashed lines to denote the varia-tional approximation q?
(z|x) to the intractable posteriorp(z|x,y).
Both variational parameters ?
and generativemodel parameters ?
are learned jointly.guides the translation process, i.e.
p(y|z,x).
Withthis assumption, the original conditional probabilityevolves into the following formulation:p(y|x) =?zp(y, z|x)dz =?zp(y|z,x)p(z|x)dz(1)This brings in the benefits that the latent variable zcan serve as a global semantic signal that is com-plementary to the attention-based context vector forgenerating good translations when the model learnsundesirable attentions.
However, although this la-tent variable enables us to explicitly model under-lying semantics of translation pairs, the incorpora-tion of it into the above probabilistic model has twochallenges: 1) the posterior inference in this modelis intractable; 2) large-scale training, which laysthe ground for the data-driven NMT, is accordinglyproblematic.In order to address these issues, we propose a vari-ational encoder-decoder model to neural machinetranslation (VNMT), motivated by the recent suc-cess of variational neural models (Rezende et al,2014; Kingma and Welling, 2014).
Figure 1 illus-trates the graphic representation of VNMT.
As deepneural networks are capable of learning highly non-linear functions, we employ them to fit the latent-variable-related distributions, i.e.
the prior and pos-terior, to make the inference tractable.
The former ismodeled to be conditioned on the source side alonep?
(z|x), because the source and target part of a sen-tence pair usually share the same semantics so thatthe source sentence should contain the prior infor-mation for inducing the underlying semantics.
Thelatter, instead, is approximated from all observedvariables q?
(z|x,y), i.e.
both the source and the tar-get sides.
In order to efficiently train parameters,we apply a reparameterization technique (Rezendeet al, 2014; Kingma and Welling, 2014) on the vari-ational lower bound.
This enables us to use standardstochastic gradient optimization for training the pro-posed model.
Specifically, there are three essentialcomponents in VNMT (The detailed architecture isillustrated in Figure 2):?
A variational neural encoder transformssource/target sentence into distributed repre-sentations, which is the same as the encoder ofNMT (Bahdanau et al, 2014) (see section 3.1).?
A variational neural inferer infers the repre-sentation of z according to the learned sourcerepresentations (i.e.
p?
(z|x)) together with thetarget ones (i.e.
q?
(z|x,y)), where the repa-rameterization technique is employed (see sec-tion 3.2).?
And a variational neural decoder integrates thelatent representation of z to guide the genera-tion of target sentence (i.e.
p(y|z,x)) togetherwith the attention mechanism (see section 3.3).Augmented with the posterior approximation andreparameterization, our VNMT can still be trainedend-to-end.
This makes our model not only effi-cient in translation, but also simple in implementa-tion.
To train our model, we employ the conven-tional maximum likelihood estimation.
Experimentson both Chinese-English and English-German trans-lation tasks show that VNMT achieves significantimprovements over several strong baselines.2 Background: Variational AutoencoderThis section briefly reviews the variational autoen-coder (VAE) (Kingma and Welling, 2014; Rezendeet al, 2014).
Given an observed variable x, VAE in-troduces a continuous latent variable z, and assumesthat x is generated from z, i.e.,p?
(x, z) = p?(x|z)p?
(z) (2)where ?
denotes the parameters of the model.
p?
(z)is the prior, e.g, a simple Gaussian distribution.p?
(x|z) is the conditional distribution that modelsthe generation procedure, typically estimated via adeep non-linear neural network.Similar to our model, the integration of z in Eq.
(2) imposes challenges on the posterior inference as522reparameterizationhzh?zh?elog ?2?hfs3s2s1s0y0 y1 y2 y3?
?2,1?2,2 ?2,3 ?2,4(a) Variational Neural Encoder(c) Variational Neural Decoder(b) Variational Neural Inferermean-pooling??h1??h1??h2??h2??h3?
?h3 ??h4?
?h4x4x3x2x1mean-poolingy1 y2 y3??h3??h3??h2??h2??h1?
?h1heFigure 2: Neural architecture of VNMT.
We use blue, gray and red color to indicate the encoder-related (x,y), under-lying semantic (z) and decoder-related (y) representation respectively.
The yellow lines show the flow of informationemployed for target word prediction.
The dashed red line highlights the incorporation of latent variable z into targetprediction.
f and e represent the source and target language respectively.well as large-scale learning.
To tackle these prob-lems, VAE adopts two techniques: neural approxi-mation and reparameterization.Neural Approximation employs deep neural net-works to approximate the posterior inference modelq?
(z|x), where ?
denotes the variational parame-ters.
For the posterior approximation, VAE regardsq?
(z|x) as a diagonal GaussianN (?, diag(?2)), andparameterizes its mean ?
and variance ?2 with deepneural networks.Reparameterization reparameterizes z as a func-tion of ?
and ?, rather than using the standardsampling method.
In practice, VAE leverages the?location-scale?
property of Gaussian distribution,and uses the following reparameterization:z?
= ?+ ?
 (3)where  is a standard Gaussian variable that playsa role of introducing noises, and  denotes anelement-wise product.With these two techniques, VAE tightly incor-porates both the generative model p?
(x|z) and theposterior inference model q?
(z|x) into an end-to-end neural network.
This facilitates its optimiza-tion since we can apply the standard backpropaga-tion to compute the gradient of the following varia-tional lower bound:LVAE(?, ?
;x) =?
KL(q?(z|x)||p?(z))+Eq?
(z|x)[log p?
(x|z)] ?
log p?
(x)(4)KL(Q||P ) is the Kullback-Leibler divergence be-tween Q and P .
Intuitively, VAE can be consideredas a regularized version of the standard autoencoder.It makes use of the latent variable z to capture thevariations  in the observed variable x.3 Variational Neural Machine TranslationDifferent from previous work, we introduce a latentvariable z to model the underlying semantic spaceas a global signal for translation.
Formally, giventhe definition in Eq.
(1) and Eq.
(4), the varia-tional lower bound of VNMT can be formulated asfollows:LVNMT(?, ?
;x,y) = ?KL(q?(z|x,y)||p?(z|x))+Eq?
(z|x,y)[log p?
(y|z,x)] (5)where p?
(z|x) is our prior model, q?
(z|x,y) is ourposterior approximator, and p?
(y|z,x) is the de-coder with the guidance from z.
Based on thisformulation, VNMT can be decomposed into threecomponents, each of which is modeled by a neu-ral network: a variational neural inferer that modelsp?
(z|x) and q?
(z|x,y) (see part (b) in Figure 2), avariational neural decoder that models p?
(y|z,x)(see part (c) in Figure 2), and a variational neuralencoder that provides distributed representations ofa source/target sentence for the above two modules(see part (a) in Figure 2).
Following the informationflow illustrated in Figure 2, we describe part (a), (b)and (c) successively.3.1 Variational Neural EncoderAs shown in Figure 2 (a), the variational neural en-coder aims at encoding an input sequence (w1, w2,523.
.
.
, wT ) into continuous vectors.
In this paper,we adopt the encoder architecture proposed by Bah-danau et al (2014), which is a bidirectional RNNwith a forward and backward RNN.
The forwardRNN reads the sequence from left to right whilethe backward RNN in the opposite direction (see theparallel arrows in Figure 2 (a)):?
?h i = RNN(?
?h i?1, Ewi)?
?h i = RNN(?
?h i+1, Ewi)(6)where Ewi ?
Rdw is the embedding for word wi,and?
?h i,?
?h i are hidden states generated in two direc-tions.
Following Bahdanau et al (2014), we employthe Gated Recurrent Unit (GRU) as our RNN unitdue to its capacity in capturing long-distance depen-dencies.We further concatenate each pair of hidden statesat each time step to build a set of annotation vec-tors (h1, h2, .
.
.
, hT ), hTi =[?
?h Ti ;?
?h Ti].
In thisway, each annotation vector hi encodes informationabout the i-th word with respect to all the other sur-rounding words in the sequence.
Therefore, theseannotation vectors are desirable for the followingmodeling.We use this encoder to represent both the sourcesentence {xi}Tfi=1 and the target sentence {yi}Tei=1(see the blue color in Figure 2).
Accordingly, ourencoder generates both the source annotation vec-tors {hi}Tfi=1 ?
R2df and the target annotation vec-tors {h?i}Tei=1 ?
R2de .
The source vectors flow intothe inferer and decoder while the target vectors theposterior approximator.3.2 Variational Neural InfererA major challenge of variational models is how tomodel the latent-variable-related distributions.
InVNMT, we employ neural networks to model boththe prior p?
(z|x) and the posterior q?
(z|x,y), andlet them subject to a multivariate Gaussian distri-bution with a diagonal covariance structure.2 Asshown in Figure 1, these two distributions mainlydiffer in their conditions.2The reasons of choosing Gaussian distribution are twofold:1) it is a natural choice for modeling continuous variables; 2) itbelongs to the family of ?location-scale?
distributions, which isrequired for the following reparameterization.3.2.1 Neural Posterior ApproximatorExactly modeling the true posterior p(z|x,y) ex-actly usually intractable.
Therefore, we adopt anapproximation method to simplify the posterior in-ference.
Conventional models typically employ themean-field approaches.
However, a major limitationof this approach is its inability to capture the trueposterior of z due to its oversimplification.
Follow-ing the spirit of VAE, we use neural networks forbetter approximation in this paper, and assume theapproximator has the following form:q?
(z|x,y) = N (z;?
(x,y), ?
(x,y)2I) (7)The mean ?
and s.d.
?
of the approximate poste-rior are the outputs of neural networks based on theobserved variables x and y as shown in Figure 2 (b).Starting from the variational neural encoder, wefirst obtain the source- and target-side representa-tion via a mean-pooling operation over the annota-tion vectors, i.e.
hf = 1Tf?Tfi hi, he = 1Te?Tei h?i.With these representations, we perform a non-lineartransformation that projects them onto our con-cerned latent semantic space:h?z = g(W (1)z [hf ;he] + b(1)z ) (8)where W (1)z ?
Rdz?2(df+de), b(1)z ?
Rdz is the pa-rameter matrix and bias term respectively, dz is thedimensionality of the latent space, and g(?)
is anelement-wise activation function, which we set to betanh(?)
throughout our experiments.In this latent space, we obtain the abovementionedGaussian parameters ?
and log ?2 through linear re-gression:?
= W?h?z + b?, log ?2 = W?h?z + b?
(9)where ?, log ?2 are both dz-dimension vectors.3.2.2 Neural Prior ModelDifferent from the posterior, we model (ratherthan approximate) the prior as follows:p?
(z|x) = N (z;??
(x), ??
(x)2I) (10)We treat the mean ??
and s.d.
??
of the prior as neuralfunctions of source sentence x alone.
This is soundand reasonable because bilingual sentences are se-mantically equivalent, suggesting that either y or x524is capable of inferring the underlying semantics ofsentence pairs, i.e., the representation of latent vari-able z.The neural model for the prior p?
(z|x) is thesame as that (i.e.
Eq (8) and (9)) for the posteriorq?
(z|x,y), except for the absence of he.
Besides,the parameters for the prior are independent of thosefor the posterior.To obtain a representation for latent variable z, weemploy the same technique as the Eq.
(3) and repa-rameterized it as hz = ?+ ?
, ?N (0, I).
Dur-ing decoding, however, due to the absence of targetsentence y, we set hz to be the mean of p?
(z|x), i.e.,??.
Intuitively, the reparameterization bridges thegap between the generation model p?
(y|z,x) andthe inference model q?(z|x,y).
In other words, itconnects these two neural networks.
This is impor-tant since it enables the stochastic gradient optimiza-tion via standard backpropagation.We further project the representation of latentvariable hz onto the target space for translation:h?e = g(W (2)z hz + b(2)z ) (11)where h?e ?
Rd?e .
The transformed h?e is then in-tegrated into our decoder.
Notice that because ofthe noise from , the representation h?e is not fixedfor the same source sentence and model parameters.This is crucial for VNMT to learn to avoid overfit-ting.3.3 Variational Neural DecoderGiven the source sentence x and the latent variablez, our decoder defines the probability over transla-tion y as a joint probability of ordered conditionals:p(y|z,x) =Te?j=1p(yj |y<j , z,x) (12)where p(yj |y<j ,z,x) = g?
(yj?1, sj?1, cj)The feed forward model g?(?)
(see the yellow arrowsin Figure 2) and context vector cj = ?i ?jihi (seethe ???
in Figure 2) are the same as (Bahdanau etal., 2014).
The difference between our decoder andBahdanau et al?s decoder (2014) lies in that in ad-dition to the context vector, our decoder integratesthe representation of the latent variable, i.e.
h?e, intothe computation of sj , which is denoted by the bolddashed red arrow in Figure 2 (c).Formally, the hidden state sj in our decoder is cal-culated by3sj = (1?
uj) sj?1 + uj  s?j ,s?j = tanh(WEyj + U [rj  sj?1] + Ccj + V h?e)uj = ?
(WuEyj + Uusj?1 + Cucj + Vuh?e)rj = ?
(WrEyj + Ursj?1 + Crcj + Vrh?e)Here, rj , uj , s?j denotes the reset gate, update gateand candidate activation in GRU respectively, andEyj ?
Rdw is the word embedding for target word.W, Wu, Wr ?
Rde?dw , U, Uu, Ur ?
Rde?de , C, Cu,Cr ?
Rde?2df , and V, Vu, Vr ?
Rde?d?e are parame-ter weights.
The initial hidden state s0 is initializedin the same way as Bahdanau et al (2014) (see thearrow to s0 in Figure 2).In our model, the latent variable can affect the rep-resentation of hidden state sj through the gate be-tween rj and uj .
This allows our model to access thesemantic information of z indirectly since the pre-diction of yj+1 depends on sj .
In addition, when themodel learns wrong attentions that lead to bad con-text vector cj , the semantic representation he?
canhelp to guide the translation process .3.4 Model TrainingWe use the Monte Carlo method to approximatethe expectation over the posterior in Eq.
(5), i.e.Eq?(z|x,y)[?]
' 1L?Ll=1 log p?
(y|x,h(l)z ), whereL isthe number of samples.
The joint training objectivefor a training instance (x,y) is defined as follows:L(?, ?)
' ?KL(q?(z|x,y)||p?
(z|x))+ 1LL?l=1Te?j=1log p?
(yj |y<j ,x,h(l)z ) (13)where h(l)z = ?+ ?
(l) and (l) ?
N (0, I)The first term is the KL divergence between twoGaussian distributions which can be computed anddifferentiated without estimation (see (Kingma andWelling, 2014) for details).
And the second termis the approximate expectation, which is also dif-ferentiable.
Suppose that L is 1 (which is used inour experiments), then our second term will be de-generated to the objective of conventional NMT.
In-tuitively, VNMT is exactly a regularized version of3We omit the bias term for clarity.525System MT05 MT02 MT03 MT04 MT06 MT08 AVGMoses 33.68 34.19 34.39 35.34 29.20 22.94 31.21GroundHog 31.38 33.32 32.59 35.05 29.80 22.82 30.72VNMT w/o KL 31.40 33.50 32.92 34.95 28.74 22.07 30.44VNMT 32.25 34.50++ 33.78++ 36.72?++ 30.92?++ 24.41?++ 32.07Table 1: BLEU scores on the NIST Chinese-English translation task.
AVG = average BLEU scores on test sets.
Wehighlight the best results in bold for each test set.
??/??
: significantly better than Moses (p < 0.05/p < 0.01); ?+/++?
:significantly better than GroundHog (p < 0.05/p < 0.01);NMT, where the introduced noise  increases its ro-bustness, and reduces overfitting.
We verify thispoint in our experiments.Since the objective function in Eq.
(13) is differ-entiable, we can optimize the model parameter ?
andvariational parameter ?
jointly using standard gradi-ent ascent techniques.4 Experiments4.1 SetupTo evaluate the effectiveness of the proposedVNMT, we conducted experiments on both Chinese-English and English-German translation tasks.
OurChinese-English training data4 consists of 2.9M sen-tence pairs, with 80.9M Chinese words and 86.4MEnglish words respectively.
We used the NISTMT05 dataset as the development set, and the NISTMT02/03/04/06/08 datasets as the test sets for theChinese-English task.
Our English-German train-ing data5 consists of 4.5M sentence pairs with 116MEnglish words and 110M German words6.
We usedthe newstest2013 (3000 sentences) as the develop-ment set, and the newstest2014 (2737 sentences)as the test set for English-German translation.
Weemployed the case-insensitive BLEU-4 (Papineni etal., 2002) metric to evaluate translation quality, andpaired bootstrap sampling (Koehn, 2004) for signif-icance test.We compared our model against two state-of-the-art SMT and NMT systems:?
Moses (Koehn et al, 2007): a phrase-basedSMT system.4This corpus consists of LDC2003E14, LDC2004T07,LDC2005T06, LDC2005T10 and LDC2004T08 (Hong KongHansards/Laws/News).5This corpus is from the WMT?14 training data (Jean et al,2015; Luong et al, 2015a)6The preprocessed data can be found and downloaded fromhttp://nlp.stanford.edu/projects/nmt/?
GroundHog (Bahdanau et al, 2014): anattention-based NMT system.Additionally, we also compared with a variant ofVNMT, which does not contain the KL part in theobjective (VNMT w/o KL).
This is achieved by set-ting hz to ?
?.For Moses, we adopted all the default settings ex-cept for the language model.
We trained a 4-gramlanguage model on the Xinhua section of the EnglishGigaword corpus (306M words) using the SRILM7toolkit with modified Kneser-Ney smoothing.
Im-portantly, we used all words in the vocabulary.For GroundHog, we set the maximum lengthof training sentences to be 50 words, and pre-served the most frequent 30K (Chinese-English) and50K (English-German) words as both the sourceand target vocabulary , covering approximately98.9%/99.2% and 97.3%/93.3% on the source andtarget side of the two parallel corpora respectively .All other words were represented by a specific to-ken ?UNK?.
Following Bahdanau et al (2014), weset dw = 620, df = 1000, de = 1000, and M = 80.All other settings are the same as the default config-uration (for RNNSearch).
During decoding, we usedthe beam-search algorithm, and set beam size to 10.For VNMT, we initialized its parameters with thetrained RNNSearch model.
The settings of ourmodel are the same as that of GroundHog, exceptfor some parameters specific to VNMT.
FollowingVAE, we set the sampling number L = 1.
Addi-tionally, we set d?e = dz = 2df = 2000 accordingto preliminary experiments.
We used the Adadeltaalgorithm for model training with ?
= 0.95.
Withregard to the source and target encoders, we sharedtheir recurrent parameters but not word embeddings.We implemented our VNMT based on Ground-Hog8.
Both NMT systems are trained on a Telsa K407http://www.speech.sri.com/projects/srilm/download.html8Our code is publicly available at526System MT05 MT02 MT03 MT04 MT06 MT08GroundHog 18.23 22.20 20.19 21.67 19.11 13.41VNMT 21.31 26.02 23.78 25.81 21.81 15.59Table 2: BLEU scores on the new dataset.
All improvements are significant at p < 0.01.System Architecture BLEUExisting end-to-end NMT systemsJean et al (2015) RNNSearch 16.46Jean et al (2015) RNNSearch + unk replace 18.97Jean et al (2015) RNNsearch + unk replace + large vocab 19.40Luong et al (2015a) LSTM with 4 layers + dropout + local att.
+ unk replace 20.90Our end-to-end NMT systemsthis workRNNSearch 16.40VNMT 17.13++VNMT + unk replace 19.58++Table 3: BLEU scores on the English-German translation task.5 15 25 35 45 55202326293235Sentence LengthBLEUScoresGroundHogOur VNMTFigure 3: BLEU scores on different groups of sourcesentences in terms of their length.GPU.
In one hour, GroundHog processes about 1100batches, while our VNMT processes 630 batches.4.2 Results on Chinese-English TranslationTable 1 summarizes the BLEU scores of differentsystems on the Chinese-English translation tasks.Clearly VNMT significantly improves translationquality in terms of BLEU on most cases, and ob-tains the best average results that gain 0.86 and 1.35BLEU points over Moses and GroundHog respec-tively.
Besides, without the KL objective, VNMTw/o KL obtains even worse results than GroundHog.These results indicate the following two points: 1)explicitly modeling underlying semantics by a latentvariable indeed benefits neural machine translation,and 2) the improvements of our model are not fromenlarging the network.https://github.com/DeepLearnXMU/VNMT.4.3 Results on Long SentencesWe further testify VNMT on long sentence transla-tion where the vanilla NMT usually suffers from at-tention failures (Tu et al, 2016; Bentivogli et al,2016).
We believe that the global latent variable canplay an important role on long sentence translation.Our first experiment is carried out on 6 disjointgroups according to the length of source sentences inour test sets.
Figure 3 shows the BLEU scores of twoneural models.
We find that the performance curveof our VNMT model always appears to be on top ofthat of GroundHog with a certain margin.
Specif-ically, on the final group with the longest sourcesentences, our VNMT obtains the biggest improve-ment (3.55 BLEU points).
Overall, these obviousimprovements on all groups in terms of the length ofsource sentences indicate that the global guidancefrom the latent variable benefits our VNMT model.Our second experiment is carried out on a syn-thetic dataset where each new source sentence isa concatenation of neighboring source sentences inthe original test sets.
As a result, the average lengthof source sentences in the new dataset (> 50) isalmost twice longer than the original one.
Trans-lation results is summarized in Table 2, where ourVNMT obtains significant improvements on all newtest sets.
This further demonstrates the advantage ofintroducing the latent variable.4.4 Results on English-German TranslationTable 3 shows the results on English-German trans-lation.
We also provide several existing NMT sys-527Source?????????????????
,????????????
,?????????????????????
,????????????????????????????????
,????????????
?Referencethe officials of the two countries have established the mechanism for continued dialogue downthe road, including a confirmed schedule and model of the talks.
this symbolizes the restartof the dialogue process between pakistan and india after an interruption of two years and haspaved a foundation for the two countries to sort out gradually all the questions hanging in theair, including the kashmir dispute.
it is also a realization of their precious sincerity for peace.Mosesofficials of the two countries set the agenda for future talks , and the pattern of a continuingdialogue mechanism .
this marks a break in the process of dialogue between pakistan and india, two years after the restart of the two countries including kashmir dispute to gradually solveall the outstanding issues have laid the foundation of the two sides showed great sincerity inpeace .GroundHogthe two countries have decided to set up a mechanism for conducting continuous dialogue onthe agenda and mode of the talks .
this indicates that the ongoing dialogue between the twocountries has laid the foundation for the gradual settlement of all outstanding issues includingthe dispute over kashmir .VNMTthe officials of the two countries set up a mechanism for holding a continuous dialogue onthe agenda and mode of the future talks, and this indicates that the ongoing dialogue betweenpakistan and india has laid a foundation for resolving all outstanding issues , including thekashmir disputes , and this serves as a valuable and sincere peace sincerity .Table 4: Translation examples of different systems.
We highlight important parts in red color.tems that use the same training, development andtesting data.
The results show that VNMT signifi-cantly outperforms GroundHog and achieves a sig-nificant gain of 0.73 BLEU points (p < 0.01).
Withunknown word replacement (Jean et al, 2015; Lu-ong et al, 2015a), VNMT reaches the performancelevel that is comparable to the previous state-of-the-art NMT results.4.5 Translation AnalysisTable 4 shows a translation example that helps un-derstand the advantage of VNMT over NMT .
Asthe source sentence in this example is long (morethan 40 words), the translation generated by Mosesis relatively messy and incomprehensible.
In con-trast, translations generated by neural models (bothGroundHog and VNMT) are much more fluent andcomprehensible.
However, there are essential differ-ences between GroundHog and our VNMT.
Specifi-cally, GroundHog does not translate the phrase ????
at the beginning of the source sentence.
Thetranslation of the clause ???????????????
at the end of the source sentence is com-pletely lost.
In contrast, our VNMT model does notmiss or mistake these fragments and can convey themeaning of entire source sentence to the target side.From these examples, we can find that althoughattention networks can help NMT trace back to rel-evant parts of source sentences for predicting tar-get translations, capturing the semantics of entiresentences still remains a big challenge for neuralmachine translation.
Since NMT implicitly modelsvariable-length source sentences with fixed-size hid-den vectors, some details of source sentences (e.g.,the red sequence of words in Table 4) may not beencoded in these vectors at all.
VNMT seems to beable to capture these details through a latent vari-able that explicitly model underlying semantics ofsource sentences.
The promising results suggest thatVNMT provides a new mechanism to deal with sen-tence semantics.5 Related Work5.1 Neural Machine TranslationNeural machine translation starts from the sequenceto sequence learning, where Sutskever et al (2014)employ two multilayered Long Short-Term Memory(LSTM) models that first encode a source sentenceinto a single vector and then decode the translationword by word until a special end token is gener-ated.
In order to deal with issues caused by encodingall source-side information into a fixed-length vec-tor, Bahdanau et al (2014) introduce attention-based528NMT that aims at automatically concentrating onrelevant source parts for predicting target words dur-ing decoding.
The incorporation of attention mech-anism allows NMT to cope better with long sen-tences, and makes it really comparable to or evensuperior to conventional SMT.Following the success of attentional NMT, a num-ber of approaches and models have been proposedfor NMT recently, which can be grouped into differ-ent categories according to their motivations: deal-ing with rare words or large vocabulary (Jean et al,2015; Luong et al, 2015b; Sennrich et al, 2015),learning better attentional structures (Luong et al,2015a), integrating SMT techniques (Cheng et al,2015; Shen et al, 2015; Feng et al, 2016; Tu et al,2016), memory network (Meng et al, 2015), etc.
Allthese models are designed within the discriminativeencoder-decoder framework, leaving the explicit ex-ploration of underlying semantics with a variationalmodel an open problem.5.2 Variational Neural ModelIn order to perform efficient inference and learn-ing in directed probabilistic models on large-scaledataset, Kingma and Welling (2014) as well asRezende et al (2014) introduce variational neuralnetworks.
Typically, these models utilize an neuralinference model to approximate the intractable pos-terior, and optimize model parameters jointly with areparameterized variational lower bound using thestandard stochastic gradient technique.
This ap-proach is of growing interest due to its success invarious tasks.Kingma et al (2014) revisit the approach to semi-supervised learning with generative models and fur-ther develop new models that allow effective gen-eralization from a small labeled dataset to a largeunlabeled dataset.
Chung et al (2015) incorporatelatent variables into the hidden state of a recurrentneural network, while Gregor et al (2015) combinea novel spatial attention mechanism that mimics thefoveation of human eyes, with a sequential varia-tional auto-encoding framework that allows the it-erative construction of complex images.
Very re-cently, Miao et al (2015) propose a generic varia-tional inference framework for generative and con-ditional models of text.The most related work is that of Bowman etal.
(2015), where they develop a variational autoen-coder for unsupervised generative language model-ing.
The major difference is that they focus on themonolingual language model, while we adapt thistechnique to bilingual translation.
Although varia-tional neural models have been widely used in NLPtasks and the variational decoding has been investi-gated for SMT (Li et al, 2009), the adaptation andutilization of variational neural model to neural ma-chine translation, to the best of our knowledge, hasnever been investigated before.6 Conclusion and Future WorkIn this paper, we have presented a variational modelfor neural machine translation that incorporates acontinuous latent variable to model the underlyingsemantics of sentence pairs.
We approximate theposterior distribution with neural networks and repa-rameterize the variational lower bound.
This en-ables our model to be an end-to-end neural networkthat can be optimized through the stochastic gradi-ent algorithms.
Comparing with the conventionalattention-based NMT, our model is better at trans-lating long sentences.
It also greatly benefits froma special regularization term brought with this la-tent variable.
Experiments on Chinese-English andEnglish-German translation tasks verified the effec-tiveness of our model.In the future, since the latent variable in ourmodel is at the sentence level, we want to exploremore fine-grained latent variables for neural ma-chine translation, such as the Recurrent Latent Vari-able Model (Chung et al, 2015).
We are also inter-ested in applying our model to other similar tasks.AcknowledgmentsThe authors were supported by National Nat-ural Science Foundation of China (Grant Nos61303082, 61672440, 61622209 and 61403269),Natural Science Foundation of Fujian Province(Grant No.
2016J05161), Natural Science Founda-tion of Jiangsu Province (Grant No.
BK20140355),and Research fund of the Provincial Key Laboratoryfor Computer Information Processing Technology inSoochow University (Grant No.
KJS1520).
We alsothank the anonymous reviewers for their insightfulcomments.529ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
In Proc.
of ICLR.L.
Bentivogli, A. Bisazza, M. Cettolo, and M. Federico.2016.
Neural versus Phrase-Based Machine Transla-tion Quality: a Case Study.
ArXiv e-prints, August.S.
R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Joze-fowicz, and S. Bengio.
2015.
Generating Sentencesfrom a Continuous Space.
ArXiv e-prints, November.Y.
Cheng, S. Shen, Z.
He, W. He, H. Wu, M. Sun, andY.
Liu.
2015.
Agreement-based Joint Training forBidirectional Attention-based Neural Machine Trans-lation.
ArXiv e-prints, December.Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase represen-tations using rnn encoder?decoder for statistical ma-chine translation.
In Proc.
of EMNLP, pages 1724?1734, October.Junyoung Chung, Kyle Kastner, Laurent Dinh, KratarthGoel, Aaron C. Courville, and Yoshua Bengio.
2015.A recurrent latent variable model for sequential data.In Proc.
of NIPS.S.
Feng, S. Liu, M. Li, and M. Zhou.
2016.
ImplicitDistortion and Fertility Models for Attention-basedEncoder-Decoder NMT Model.
ArXiv e-prints, Jan-uary.Karol Gregor, Ivo Danihelka, Alex Graves, and DaanWierstra.
2015.
DRAW: A recurrent neural networkfor image generation.
CoRR, abs/1502.04623.Se?bastien Jean, Kyunghyun Cho, Roland Memisevic, andYoshua Bengio.
2015.
On using very large target vo-cabulary for neural machine translation.
In Proc.
ofACL-IJCNLP, pages 1?10, July.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proc.
of EMNLP,pages 1700?1709, October.Diederik P Kingma and Max Welling.
2014.
Auto-Encoding Variational Bayes.
In Proc.
of ICLR.Diederik P. Kingma, Shakir Mohamed, Danilo JimenezRezende, and Max Welling.
2014.
Semi-supervisedlearning with deep generative models.
In Proc.
ofNIPS, pages 3581?3589.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.
ofACL, pages 177?180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
of EMNLP.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proc.
of ACL, pages 593?601, August.Thang Luong, Hieu Pham, and Christopher D. Manning.2015a.
Effective approaches to attention-based neuralmachine translation.
In Proc.
of EMNLP, pages 1412?1421, September.Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,and Wojciech Zaremba.
2015b.
Addressing the rareword problem in neural machine translation.
In Proc.of ACL-IJCNLP, pages 11?19, July.F.
Meng, Z. Lu, Z. Tu, H. Li, and Q. Liu.
2015.A Deep Memory-based Architecture for Sequence-to-Sequence Learning.
ArXiv e-prints, June.Y.
Miao, L. Yu, and P. Blunsom.
2015.
Neural Varia-tional Inference for Text Processing.
ArXiv e-prints,November.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
of ACL, pages311?318.Danilo Jimenez Rezende, Shakir Mohamed, and DaanWierstra.
2014.
Stochastic backpropagation and ap-proximate inference in deep generative models.
InProc.
of ICML, pages 1278?1286.R.
Sennrich, B. Haddow, and A. Birch.
2015.
Neu-ral Machine Translation of Rare Words with SubwordUnits.
ArXiv e-prints, August.S.
Shen, Y. Cheng, Z.
He, W. He, H. Wu, M. Sun, andY.
Liu.
2015.
Minimum Risk Training for Neural Ma-chine Translation.
ArXiv e-prints, December.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural networks.CoRR, abs/1409.3215.Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,and Hang Li.
2016.
Coverage-based neural machinetranslation.
CoRR, abs/1601.04811.530
