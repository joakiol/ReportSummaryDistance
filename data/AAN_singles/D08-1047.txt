Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 447?456,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA Discriminative Candidate Generator for String TransformationsNaoaki Okazaki?okazaki@is.s.u-tokyo.ac.jpYoshimasa Tsuruoka?yoshimasa.tsuruoka@manchester.ac.ukSophia Ananiadou?sophia.ananiadou@manchester.ac.ukJun?ichi Tsujii?
?tsujii@is.s.u-tokyo.ac.jp?Graduate School of InformationScience and TechnologyUniversity of Tokyo7-3-1 Hongo, Bunkyo-kuTokyo 113-8656, Japan?School of Computer Science,University of ManchesterNational Centre for Text Mining (NaCTeM)Manchester Interdisciplinary Biocentre131 Princess Street, Manchester M1 7DN, UKAbstractString transformation, which maps a sourcestring s into its desirable form t?, is relatedto various applications including stemming,lemmatization, and spelling correction.
Theessential and important step for string trans-formation is to generate candidates to whichthe given string s is likely to be transformed.This paper presents a discriminative approachfor generating candidate strings.
We use sub-string substitution rules as features and scorethem using an L1-regularized logistic regres-sion model.
We also propose a procedure togenerate negative instances that affect the de-cision boundary of the model.
The advantageof this approach is that candidate strings canbe enumerated by an efficient algorithm be-cause the processes of string transformationare tractable in the model.
We demonstratethe remarkable performance of the proposedmethod in normalizing inflected words andspelling variations.1 IntroductionString transformation maps a source string s into itsdestination string t?.
In the broad sense, string trans-formation can include labeling tasks such as part-of-speech tagging and shallow parsing (Brill, 1995).However, this study addresses string transformationin its narrow sense, in which a part of a source stringis rewritten with a substring.
Typical applications ofthis task include stemming, lemmatization, spellingcorrection (Brill and Moore, 2000; Wilbur et al,2006; Carlson and Fette, 2007), OCR error correc-tion (Kolak and Resnik, 2002), approximate stringmatching (Navarro, 2001), and duplicate record de-tection (Bilenko and Mooney, 2003).Recent studies have formalized the task in the dis-criminative framework (Ahmad and Kondrak, 2005;Li et al, 2006; Chen et al, 2007),t?
= argmaxt?gen(s)P (t|s).
(1)Here, the candidate generator gen(s) enumeratescandidates of destination (correct) strings, and thescorer P (t|s) denotes the conditional probability ofthe string t for the given s. The scorer was modeledby a noisy-channel model (Shannon, 1948; Brill andMoore, 2000; Ahmad and Kondrak, 2005) and max-imum entropy framework (Berger et al, 1996; Li etal., 2006; Chen et al, 2007).The candidate generator gen(s) also affects theaccuracy of the string transformation.
Previous stud-ies of spelling correction mostly defined gen(s),gen(s) = {t | dist(s, t) < ?}.
(2)Here, the function dist(s, t) denotes the weightedLevenshtein distance (Levenshtein, 1966) betweenstrings s and t. Furthermore, the threshold ?
requiresthe distance between the source string s and a can-didate string t to be less than ?.The choice of dist(s, t) and ?
involves a tradeoffbetween the precision, recall, and training/taggingspeed of the scorer.
A less restrictive design of thesefactors broadens the search space, but it also in-creases the number of confusing candidates, amountof feature space, and computational cost for thescorer.
Moreover, the choice is highly dependent onthe target task.
It might be sufficient for a spelling447correction program to gather candidates from knownwords, but a stemmer must handle unseen words ap-propriately.
The number of candidates can be hugewhen we consider transformations from and to un-seen strings.This paper addresses these challenges by explor-ing the discriminative training of candidate genera-tors.
More specifically, we build a binary classifierthat, when given a source string s, decides whethera candidate t should be included in the candidate setor not.
This approach appears straightforward, but itmust resolve two practical issues.
First, the task ofthe classifier is not only to make a binary decisionfor the two strings s and t, but also to enumerate aset of positive strings for the string s,gen(s) = {t | predict(s, t) = 1}.
(3)In other words, an efficient algorithm is necessaryto find a set of strings with which the classifierpredict(s, t) yields positive labels for the string s.Another issue arises when we prepare a trainingset.
A discriminative model requires a training setin which each instance (pair of strings) is annotatedwith a positive or negative label.
Even though someexisting resources (e.g., inflection table and querylog) are available for positive instances, such re-sources rarely contain negative instances.
Therefore,we must generate negative instances that are effec-tive for discriminative training.To address the first issue, we design features thatexpress transformations from a source string s to itsdestination string t. Feature selection and weight-ing are performed using an L1-regularized logisticregression model, which can find a sparse solutionto the classification model.
We also present an al-gorithm that utilizes the feature weights to enumer-ate candidates of destination strings efficiently.
Wedeal with the second issue by generating negativeinstances from unlabeled instances.
We describe aprocedure to choose negative instances that affectthe decision boundary of the classifier.This paper is organized as follows.
Section 2 for-malizes the task of the candidate generator as a bi-nary classification modeled by logistic regression.Features for the classifier are designed using therules of substring substitution.
Therefore, we canobtain, efficiently, candidates of destination stringsand negative instances for training.
Section 3 re-ports the remarkable performance of the proposedmethod in various applications including lemmati-zation, spelling normalization, and noun derivation.We briefly review previous work in Section 4, andconclude this paper in Section 5.2 Candidate generator2.1 Candidate classification modelIn this section, we first introduce a binary classifierthat yields a label y ?
{0, 1} indicating whether acandidate t should be included in the candidate set(1) or not (0), given a source string s. We expressthe conditional probability P (y|s, t) using a logisticregression model,P (1|s, t) =11 + exp (?
?TF (s, t)), (4)P (0|s, t) = 1?
P (1|s, t).
(5)In these equations, F = {f1, ..., fK} denotes a vec-tor of the Boolean feature functions; K is the num-ber of feature functions; and ?
= {?1, ..., ?K}presents a weight vector of the feature functions.We obtain the following decision rule to choosethe most probable label y?
for a given pair ?s, t?,y?
= argmaxy?
{0,1}P (y|s, t) ={1(?TF (s, t) > 0)0 (otherwise).
(6)Finally, given a source string s, the generator func-tion gen(s) is defined to collect all strings to whichthe classifier assigns positive labels:gen(s) = {t | P (1|s, t) > P (0|s, t)}= {t | ?TF (s, t) > 0}.
(7)2.2 Substitution rules as featuresThe binary classifier can include any arbitrary fea-ture.
This is exemplified by the Levenshtein dis-tance and distributional similarity (Lee, 1999) be-tween two strings s and t. These features can im-prove the classification accuracy, but it is unrealisticto compute these features for every possible string,as in equation 7.
For that reason, we specificallyexamine substitution rules, with which the process448^oestrogen$^estrogen$^anaemia$^anemia$^studies$^study$('o', ''), ('^o', '^'), ('oe', 'e'),('^oe', '^e'), ('^oes', '^es'), ...('a', ''), ('na', 'n'), ('ae', 'e'),('ana', 'an'), ('nae', 'ne'), ('aem', 'em'),...('ies', 'y'), ('dies', 'dy'), ('ies$', 'y$'),('udies', 'udy'), ('dies$', 'dy$'), ...S:t:S:t:S:t:(1)(2)(3)Figure 1: Generating substitution rules.of transforming a source string s into its destinationform t is tractable.In this study, we assume that every string has aprefix ???
and postfix ?$?, which indicate the headand tail of a string.
A substitution rule r = (?, ?
)replaces every occurrence of the substring ?
in asource string into the substring ?.
Assuming that astring s can be transformed into another string twitha single substitution operation, substitution rules ex-press the different portion between strings s and t.Equation 8 defines a binary feature function witha substitution rule between two strings s and t,fk(s, t) ={1 (rule rk can convert s into t)0 (otherwise).
(8)We allow multiple substitution rules for a given pairof strings.
For instance, substitution rules (?a?,??
), (?na?, ?n?
), (?ae?, ?e?
), (?nae?, ?ne?
), etc.form feature functions that yield 1 for strings s =??anaemia$?
and t = ??anemia$?.
Equation6 produces a decision based on the sum of featureweights, or scores of substitution rules, representingthe different portions between s and t.Substitution rules for the given two strings s andt are obtained as follows.
Let l denote the longestcommon prefix between strings s and t, and r thelongest common postfix.
We define cs as the sub-string in s that is not covered by the longest commonprefix l and postfix r, and define ct for t analogously.In other words, strings s and t are divided into threeregions, lcsr and lctr, respectively.
For strings s =??anaemia$?
and t = ??anemia$?
in Figure 1(2), we obtain cs = ?a?
and ct = ??
because l =??an?
and r = ?emia$?.Because substrings cs and ct express differentportions between strings s and t, we obtain the mini-mum substitution rule (cs, ct), which can convert thestring s into t by replacing substrings cs in s withct; the minimum substitution rule for the same ex-ample is (?a?, ??).
However, replacing letters ?a?in ??anaemia$?
into empty letters does not pro-duce the correct string ??anemia$?
but ?
?nemi$?.Furthermore, the rule might be inappropriate for ex-pressing string transformation because it always re-moves the letter ?a?
from every string.Therefore, we also obtain expanded substitutionrules, which insert postfixes of l to the head of min-imum substitution rules, and/or append prefixes ofr to the rules.
For example, we find an expandedsubstitution rule (?na?, ?n?
), by inserting a postfixof l = ??an?
to the head of the minimum substitu-tion rule (?a?, ??
); similarly, we obtain an expandedsubstitution rule (?ae?, ?e?
), by appending a prefixof r = ?emia$?
to the tail of the rule (?a?, ??
).Figure 1 displays examples of substitution rules(the right side) for three pairs of strings (the leftside).
Letters in blue, green, and red respectivelyrepresent the longest common prefixes, longest com-mon postfixes, and different portions.
In this study,we expand substitution rules such that the number ofletters in rules is does not pass a threshold ?1.2.3 Parameter estimationGiven a training set that consists of N instances,D =((s(1), t(1), y(1)), ..., (s(N), t(N), y(N))), weoptimize the feature weights in the logistic regres-sion model by maximizing the log-likelihood of theconditional probability distribution,L?
=N?i=1logP (y(i)|s(i), t(i)).
(9)The partial derivative of the log-likelihood with re-spect to a feature weight ?k is given as equation 10,?L??
?k=N?i=1{y(i) ?
P (1|s(i), t(i))}fk(s(i), t(i)).
(10)The maximum likelihood estimation (MLE) isknown to suffer from overfitting the training set.
The1The number of letters for a substitution rule r = (?, ?)
isdefined as the sum of the quantities of letters in ?
and ?, i.e.,|?|+ |?|.
We determined the threshold ?
= 12 experimentally.449common approach for addressing this issue is to usethe maximum a posteriori (MAP) estimation, intro-ducing a regularization term of the feature weights?, i.e., a penalty on large feature weights.
In addi-tion, the generation algorithm of substitution rulesmight produce inappropriate rules that transform astring incorrectly, or overly specific rules that areused scarcely.
Removing unnecessary substitutionrules not only speeds up the classifier but also thealgorithm for candidate generation, as presented inSection 2.4.In recent years, L1 regularization has received in-creasing attention because it produces a sparse so-lution of feature weights in which numerous fea-ture weights are zero (Tibshirani, 1996; Ng, 2004).Therefore, we regularize the log-likelihood with theL1 norm of the weight vector ?
and define the finalform the objective function to be minimized asE?
= ?L?
+|?|?.
(11)Here, ?
is a parameter to control the effect of L1regularization; the smaller the value we set to ?,the more features the MAP estimation assigns zeroweights to: it removes a number of features from themodel.
Equation 11 is minimized using the Orthant-Wise Limited-memory Quasi-Newton (OW-LQN)method (Andrew and Gao, 2007) because the secondterm of equation 11 is not differentiable at ?k = 0.2.4 Candidate generationThe advantage of our feature design is that we canenumerate strings to which the classifier is likely toassign positive labels.
We start by observing the nec-essary condition for t in equation 7,?TF (s, t) > 0?
?k : fk(s, t) = 1 ?
?k > 0.
(12)The classifier might assign a positive label to stringss and t when at least one feature function whoseweight is positive can transform s to t.Let R+ be a set of substitution rules to whichMAP estimation has assigned positive featureweights.
Because each feature corresponds to a sub-stitution rule, we can obtain gen(s) for a given strings by application of every substitution rule r ?
R+,gen(s) = {r(s) | r ?
R+ ?
?TF (s, r(s)) > 0}.
(13)Input: s = (s1, ..., sl): an input string s (series of letters)Input: D: a trie dictionary containing positive featuresOutput: T : gen(s)T = {};1U = {};2foreach i ?
(1, ..., |s|) do3F ?
D.prefix search(s, i);4foreach f ?
F do5if f /?
U then6t?
f .apply(s);7if classify(s, t) = 1 then8add t to T ;9end10add f to U ;11end12end13end14return T ;15Algorithm 1: A pseudo-code for gen(s).Here, r(s) presents the string to which the substitu-tion rule r transforms the source string s. We cancompute gen(s) with a small computational cost ifthe MAP estimation with L1 regularization reducesthe number of active features.Algorithm 1 represents a pseudo-code for obtain-ing gen(s).
To search for positive substitution rulesefficiently, the code stores a set of rules in a triestructure.
In line 4, the code obtains a set of positivesubstitution rules F that can rewrite substrings start-ing at offset #i in the source string s. For each rulef ?
F , we obtain a candidate string t by applicationof the substitution rule f to the source string s (line7).
The candidate string t is qualified to be includedin gen(s) when the classifier assigns a positive labelto strings s and t (lines 8 and 9).
Lines 6 and 11 pre-vent the algorithm from repeating evaluation of thesame substitution rule.2.5 Generating negative instancesThe parameter estimation requires a training set Din which each instance (pair of strings) is annotatedwith a positive or negative label.
Negative instances(counter examples) are essential for penalizing in-appropriate substitution rules, e.g.
(?a?, ??).
Eventhough some existing resources (e.g.
verb inflectiontable) are available for positive instances, such re-sources rarely contain negative instances.A common approach for handling this situationis to assume that every pair of strings in a resource450Input: D+ = [(s1, t1), ..., (sl, tl)]: positive instancesInput: V : a suffix array of all strings (vocabulary)Output: D?
: negative instancesOutput: R: substitution rules (features)D?
= [];1R = {};2foreach d ?
D+ do3foreach r ?
features(d) do4add r to R;5end6end7foreach r ?
R do8S ?
V .search(r.src);9foreach s ?
S do10t?
r.apply(s);11if (s, t) /?
D+ then12if t ?
V then13append (s, t) to D?
;14end15end16end17end18return D?, R;19Algorithm 2: Generating negative instances.is a negative instance; however, negative instancesamount to ca.
V (V ?
1)/2, where V represents thetotal number of strings.
Moreover, substitution rulesexpressing negative instances are innumerable andsparse because the different portions are peculiar toindividual negative instances.
For instance, the min-imum substitution rule for unrelated words anaemiaand around is (?naemia?, ?round?
), but the rulecannot be too specific to generalize the conditionsfor other negative instances.In this study, we generate negative instances sothat they can penalize inappropriate rules and settlethe decision boundary of the classifier.
This strat-egy is summarized as follows.
We consider everypair of strings as candidates for negative instances.We obtain substitution rules for the pair using thesame algorithm as that described in Section 2.2 if astring pair is not included in the dictionary (i.e., notin positive instances).
The pair is used as a nega-tive instance only when any substitution rule gener-ated from the pair also exists in the substitution rulesgenerated from positive instances.Algorithm 2 presents the pseudo-code that imple-ments the strategy for generating negative instancesefficiently.
First, we presume that we have positiveinstances D+ = [(s1, t1), ..., (sl, tl)] and unlabeledTable Description # EntriesLRSPL Spelling variants 90,323LRNOM Nominalizations (derivations) 14,029LRAGR Agreement and inflection 910,854LRWD Word index (vocabulary) 850,236Table 1: Excerpt of tables in the SPECIALIST Lexicon.Data set # + # - # RulesOrthography 15,830 33,296 11,098Derivation 12,988 85,928 5,688Inflection 113,215 124,747 32,278Table 2: Characteristics of datasets.strings V .
For example, positive instance D+ repre-sent orthographic variants, and unlabeled strings Vinclude all possible words (vocabulary).
We insertthe vocabulary into a suffix array, which is used tolocate every occurrence of substrings in V .The algorithm first generates substitution rules Ronly from positive instances D+ (lines 3 to 7).
Foreach substitution rule r ?
R, we enumerate knownstrings S that contain the source substring r.src (line9).
We apply the substitution rule to each string s ?S and obtain its destination string t (line 11).
If thepair of strings ?s, t?
is not included in D+ (line 12),and if the destination string t is known (line 13), thesubstitution rule r might associate incorrect stringss and t, which do not exist in D+.
Therefore, weinsert the pair to the negative set D?
(line 14).3 Evaluation3.1 ExperimentsWe evaluated the candidate generator using threedifferent tasks: normalization of orthographic vari-ants, noun derivation, and lemmatization.
Thedatasets for these tasks were obtained from theUMLS SPECIALIST Lexicon2, a large lexicon thatincludes both commonly occurring English wordsand biomedical vocabulary.
Table 1 displays the listof tables in the SPECIALIST Lexicon that were usedin our experiments.
We prepared three datasets, Or-thography, Derivation, and Inflection.The Orthography dataset includes spelling vari-ants (e.g., color and colour) in the LRSPL table.
We2UMLS SPECIALIST Lexicon:http://specialist.nlm.nih.gov/451chose entries as positive instances in which spellingvariants are caused by (case-insensitive) alphanu-meric changes3.
The Derivation dataset was built di-rectly from the LRNOM table, which includes nounderivations such as abandon ?
abandonment.
TheLRAGR table includes base forms and their inflec-tional variants of nouns (singular and plural forms),verbs (infinitive, third singular, past, past participleforms, etc), and adjectives/adverbs (positive, com-parative, and superlative forms).
For the Inflectiondataset, we extracted the entries in which inflec-tional forms differ from their base forms4, e.g., study?
studies.For each dataset, we applied the algorithm de-scribed in Section 2.5 to generate substitution rulesand negative instances.
Table 2 shows the number ofpositive instances (# +), negative instances (# -), andsubstitution rules (# Rules).
We evaluated the per-formance of the proposed method in two differentgoals of the tasks: classification (Section 3.2) andnormalization (Section 3.3).3.2 Experiment 1: Candidate classificationIn this experiment, we measured the performanceof the classification task in which pairs of stringswere assigned with positive or negative labels.We trained and evaluated the proposed methodby performing ten-fold cross validation on eachdataset5.
Eight baseline systems were preparedfor comparison: Levenshtein distance (LD), nor-malized Levenshtein distance (NLD), Dice coef-ficient on letter bigrams (DICE) (Adamson andBoreham, 1974), Longest Common Substring Ra-tio (LCSR) (Melamed, 1999), Longest CommonPrefix Ratio (PREFIX) (Kondrak, 2005), Porter?sstemmer (Porter, 1980), Morpha (Minnen et al,2001), and CST?s lemmatiser (Dalianis and Jonge-3LRSPL table includes trivial spelling variants that can behandled using simple character/string operations.
For example,the table contains spelling variants related to case sensitivity(e.g., deg and Deg) and symbols (e.g., Feb and Feb.).4LRAGR table also provides agreement information evenwhen word forms do not change.
For example, the table con-tains an entry indicating that the first-singular present form ofthe verb study is study, which might be readily apparent to En-glish speakers.5We determined the regularization parameter ?
= 5 experi-mentally.
Refer to Figure 2 for the performance change.jan, 2006)6.The five systems LD, NLD, DICE, LCSR, andPREFIX employ corresponding metrics of stringdistance or similarity.
Each system assigns a posi-tive label to a given pair of strings ?s, t?
if the dis-tance/similarity of strings s and t is smaller/largerthan the threshold ?
(refer to equation 2 for distancemetrics).
The threshold of each system was chosenso that the system achieves the best F1 score.The remaining three systems assign a positive la-bel only if the system transforms the strings s andt into the identical string.
For example, a pair oftwo words studies and study is classified as positiveby Porter?s stemmer, which yields the identical stemstudi for these words.
We trained CST?s lemmatiserfor each dataset to obtain flex patterns that are usedfor normalizing word inflections.To examine the performance of the L1-regularized logistic regression as a discriminativemodel, we also built two classifiers based on theSupport Vector Machine (SVM).
These SVMclassifiers were implemented by the SVMperf 7 ona linear kernel8.
An SVM classifier employs thesame feature set (substitution rules) as the proposedmethod so that we can directly compare the L1-regularized logistic regression and the linear-kernelSVM.
Another SVM classifier incorporates the fivestring metrics; this system can be considered as ourreproduction of the discriminative string similarityproposed by Bergsma and Kondrak (2007).Table 3 reports the precision (P), recall (R), andF1 score (F1) based on the number of correct de-cisions for positive instances.
The proposed methodoutperformed the baseline systems, achieving 0.919,0.888, and 0.984 of F1 scores, respectively.
Porter?sstemmer worked on the Inflection set, but not onthe Orthography set, which is beyond the scope ofthe stemming algorithms.
CST?s lemmatizer suf-fered from low recall on the Inflection set becauseit removed suffixes of base forms, e.g., (cloning,clone) ?
(clone, clo).
Morpha and CST?s lemma-6We used CST?s lemmatiser version 2.13:http://www.cst.dk/online/lemmatiser/uk/index.html7SVM for Multivariate Performance Measures (SVMperf ):http://svmlight.joachims.org/svm_perf.html8We determined the parameter C = 500 experimentally; itcontrols the tradeoff between training error and margin.452System Orthography Derivation InflectionP R F1 P R F1 P R F1Levenshtein distance (?
= 1) .319 .871 .467 .004 .006 .005 .484 .679 .565Levenshtein distance .323 .999 .488 .131 1.00 .232 .479 .988 .646Normalized Levenshtein distance .441 .847 .580 .133 .990 .235 .598 .770 .673Dice coefficient (letter bigram) .401 .918 .558 .137 .984 .240 .476 1.00 .645LCSR .322 1.00 .487 .156 .841 .263 .476 1.00 .645PREFIX .418 .927 .576 .140 .943 .244 .476 1.00 .645Porter stemmer (Porter, 1980) .084 .074 .079 .197 .846 .320 .926 .839 .881Morpha (Minnen et al, 2001) .009 .007 .008 .012 .022 .016 .979 .836 .902CST?s lemmatiser (Dalianis et al 2006) .119 .008 .016 .383 .682 .491 .821 .176 .290Proposed method .941 .898 .919 .896 .880 .888 .985 .986 .984Substitution rules trained with SVM .943 .890 .916 .894 .886 .890 .980 .987 .983+ LD, NLD, DICE, LCSR, PREFIX .946 .906 .926 .894 .886 .890 .980 .987 .983Table 3: Performance of candidate classificationRank Src Dst Weight Examples1 uss us 9.81 focussing2 aev ev 9.56 mediaeval3 aen en 9.53 ozaena4 iae$ ae$ 9.44 gadoviae5 nni ni 9.16 prorennin6 nne ne 8.84 connexus7 our or 8.54 colour8 aea ea 8.31 paean9 aeu eu 8.22 stomodaeum10 ooll ool 7.79 woollenTable 4: Feature weights for the Orthography settizer were not designed for orthographic variants andnoun derivations.Levenshtein distance (?
= 1) did not work forthe Derivation set because noun derivations oftenappend two or more letters (e.g., happy ?
happi-ness).
No string similarity/distance metrics yieldedsatisfactory results.
Some metrics obtained the bestF1 scores with extreme thresholds only to classifyevery instance as positive.
These results imply thedifficulty of the string metrics for the tasks.The L1-regularized logistic regression was com-parable to the SVM with linear kernel in this exper-iment.
However, the presented model presents theadvantage that it can reduce the number of activefeatures (features with non-zero weights assigned);the L1 regularization can remove 74%, 48%, and82% of substitution rules in each dataset.
Theperformance improvements by incorporating stringmetrics as features were very subtle (less than 0.7%).What is worse, the distance/similarity metrics do notspecifically derive destination strings to which theclassifier is likely to assign positive labels.
There-fore, we can no longer use the efficient algorithmas a candidate generator (in Section 2.4) with thesefeatures.Table 4 demonstrates the ability of our approachto obtain effective features; the table shows the top10 features with high weights assigned for the Or-thography data.
An interesting aspect of the pro-posed method is that the process of the orthographicvariants is interpretable through the feature weights.Figure 2 shows plots of the F1 scores (y-axis) forthe Inflection data when we change the number ofactive features (x-axis) by controlling the regular-ization parameter ?
from 0.001 to 100.
The largerthe value we set for ?, the better the classifier per-forms, generally, with more active features.
In ex-treme cases, the number of active features drops to97 with ?
= 0.01; nonetheless, the classifier stillachieves 0.961 of the F1 score.
The result suggeststhat a small set of substitution rules can accommo-date most cases of inflectional variations.3.3 Experiment 2: String transformationThe second experiment examined the performanceof the string normalization tasks formalized in equa-tion 1.
In this task, a system was given a string s andwas required to yield either its transformed form t?
(s 6= t?)
or the string s itself when the transforma-tion is unnecessary for s. The conditional probabil-ity distribution (scorer) in equation 1 was modeled453System Orthography Derivation Inflection XTAG morph 1.5P R F1 P R F1 P R F1 P R F1Morpha .078 .012 .021 .233 .016 .029 .435 .682 .531 .830 .587 .688CST?s lemmatiser .135 .160 .146 .378 .732 .499 .367 .762 .495 .584 .589 .587Proposed method .859 .823 .841 .979 .981 .980 .973 .979 .976 .837 .816 .827Table 5: Performance of string transformation0.960.9650.970.9750.980.9850.990 1000 2000 3000 4000 5000 6000 7000F1 scoreNumber of active features (with non-zero weights)Spelling variationFigure 2: Number of active features and performance.by the maximum entropy framework.
Features forthe maximum entropy model consist of: substitutionrules between strings s and t, letter bigrams and tri-grams in s, and letter bigrams and trigrams in t.We prepared four datasets, Orthography, Deriva-tion, Inflection, and XTAG morphology.
Eachdataset is a list of string pairs ?s, t?
that indicatethe transformation of the string s into t. A sourcestring s is identical to its destination string t whenstring s should not be changed.
These instancescorrespond to the case where string s has alreadybeen lemmatized.
For each string pair (s, t) in LR-SPL9, LRNOM, and LRAGR tables, we generatedtwo instances ?s, t?
and ?t, t?.
Consequently, a sys-tem is expected to leave the string t unchanged.
Wealso used XTAG morphology10 to perform a cross-domain evaluation of the lemmatizer trained on theInflection dataset11.
The entries in XTAG morphol-9We define that s precedes t in dictionary order.10XTAG morphology database 1.5:ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph-1.5.tar.gz11We found that XTAG morphology contains numerous in-ogy that also appear in the Inflection dataset were39,130 out of 317,322 (12.3 %).
We evaluatedthe proposed method and CST?s lemmatizer by per-forming ten-fold cross validation.Table 5 reports the performance based on thenumber of correct transformations.
The proposedmethod again outperformed the baseline systemswith a wide margin.
It is noteworthy that the pro-posed method can accommodate morphological in-flections in the XTAG morphology corpus with nomanual tuning or adaptation.Although we introduced no assumptions abouttarget tasks (e.g.
a known vocabulary), the aver-age number of positive substitution rules relevantto source strings was as small as 23.9 (in XTAGmorphology data).
Therefore, the candidate gen-erator performed 23.9 substitution operations for agiven string.
It applied the decision rules (equa-tion 7) 21.3 times, and generated 1.67 candidatestrings per source string.
The experimental resultsdescribed herein demonstrated that the candidategenerator was modeled successfully by the discrim-inative framework.4 Related workThe task of string transformation has a long historyin natural language processing and information re-trieval.
As described in Section 1, this task is re-lated closely to various applications.
Therefore, wespecifically examine several prior studies that arerelevant to this paper in terms of technical aspects.Some researchers have reported the effectivenessof the discriminative framework of string similarity.MaCallum et al (2005) proposed a method to trainthe costs of edit operations using Conditional Ran-dom Fields (CRFs).
Bergsma and Kondrak (2007)correct comparative and superlative adjectives, e.g., unpopular?
unpopularer ?
unpopularest and refundable ?
refundabler?
refundablest.
Therefore, we removed inflection entries forcomparative and superlative adjectives from the dataset.454presented an alignment-based discriminative stringsimilarity.
They extracted features from substringpairs that are consistent to a character-based align-ment of two strings.
Aramaki et al (2008) also usedfeatures that express the different segments of thetwo strings.
However, these studies are not suited fora candidate generator because the processes of stringtransformations are intractable in their discrimina-tive models.Dalianis and Jongejan (2006) presented a lem-matiser based on suffix rules.
Although they pro-posed a method to obtain suffix rules from a trainingdata, the method did not use counter-examples (neg-atives) for reducing incorrect string transformations.Tsuruoka et al (2008) proposed a scoring methodfor discovering a list of normalization rules for dic-tionary look-ups.
However, their objective was totransform given strings, so that strings (e.g., studiesand study) referring to the same concept in the dic-tionary are mapped into the same string (e.g., stud);in contrast, this study maps strings into their destina-tion strings that were specified by the training data.5 ConclusionWe have presented a discriminative approach forgenerating candidates for string transformation.Unlike conventional spelling-correction tasks, thisstudy did not assume a fixed set of destinationstrings (e.g.
correct words), but could even generateunseen candidate strings.
We used anL1-regularizedlogistic regression model with substring-substitutionfeatures so that candidate strings for a given stringcan be enumerated using the efficient algorithm.
Theresults of experiments described herein showed re-markable improvements and usefulness of the pro-posed approach in three tasks: normalization of or-thographic variants, noun derivation, and lemmati-zation.The method presented in this paper allows onlyone region of change in string transformation.
Anatural extension of this study is to handle mul-tiple regions of changes for morphologically richlanguages (e.g.
German) and to handle changesat the phrase/term level (e.g., ?estrogen receptor?and ?receptor of oestrogen?).
Another directionwould be to incorporate the methodologies for semi-supervised machine learning to accommodate situa-tions in which positive instances and/or unlabeledstrings are insufficient.AcknowledgmentsThis work was partially supported by Grants-in-Aidfor Scientific Research on Priority Areas (MEXT,Japan), and for Solution-Oriented Research for Sci-ence and Technology (JST, Japan).ReferencesGeorge W. Adamson and Jillian Boreham.
1974.
Theuse of an association measure based on character struc-ture to identify semantically related pairs of words anddocument titles.
Information Storage and Retrieval,10(7-8):253?260.Farooq Ahmad and Grzegorz Kondrak.
2005.
Learninga spelling error model from search query logs.
In Pro-ceedings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing (HLT-EMNLP 2005), pages 955?962.Galen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of L1-regularized log-linear models.
In Proceed-ings of the 24th International Conference on MachineLearning (ICML 2007), pages 33?40.Eiji Aramaki, Takeshi Imai, Kengo Miyo, and KazuhikoOhe.
2008.
Orthographic disambiguation incorporat-ing transliterated probability.
In Proceedings of theThird International Joint Conference on Natural Lan-guage Processing (IJCNLP 2008), pages 48?55.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.Shane Bergsma and Grzegorz Kondrak.
2007.Alignment-based discriminative string similarity.
InProceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics (ACL 2007), pages656?663.Mikhail Bilenko and Raymond J. Mooney.
2003.
Adap-tive duplicate detection using learnable string simi-larity measures.
In Proceedings of the ninth ACMSIGKDD international conference on Knowledge dis-covery and data mining (KDD 2003), pages 39?48.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.
InProceedings of the 38th Annual Meeting on the As-sociation for Computational Linguistics (ACL 2000),pages 286?293.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a case study455in part-of-speech tagging.
Computational Linguistics,21(4):543?565.Andrew Carlson and Ian Fette.
2007.
Memory-basedcontext-sensitive spelling correction at web scale.
InProceedings of the Sixth International Conference onMachine Learning and Applications (ICMLA 2007),pages 166?171.Qing Chen, Mu Li, and Ming Zhou.
2007.
Improv-ing query spelling correction using web search results.In Proceedings of the Joint Conference on EmpiricalMethods in Natural Language Processing and Compu-tational Natural Language Learning (EMNLP-CoNLL2007), pages 181?189.Hercules Dalianis and Bart Jongejan.
2006.
Hand-crafted versus machine-learned inflectional rules: Theeuroling-siteseeker stemmer and cst?s lemmatiser.
InIn Proceedings of the 6th International Conferenceon Language Resources and Evaluation (LREC 2006),pages 663?666.Okan Kolak and Philip Resnik.
2002.
OCR error correc-tion using a noisy channel model.
In Proceedings ofthe second international conference on Human Lan-guage Technology Research (HLT 2002), pages 257?262.Grzegorz Kondrak.
2005.
Cognates and word alignmentin bitexts.
In Proceedings of the Tenth Machine Trans-lation Summit (MT Summit X), pages 305?312.Lillian Lee.
1999.
Measures of distributional similarity.In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics (ACL 1999),pages 25?32.Vladimir I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707?710.Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou.
2006.Exploring distributional similarity based models forquery spelling correction.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th Annual Meeting of the Association forComputational Linguistics (Coling-ACL 2006), pages1025?1032.Andrew McCallum, Kedar Bellare, and Fernando Pereira.2005.
A conditional random field for discriminatively-trained finite-state string edit distance.
In Proceedingsof the 21st Conference on Uncertainty in Artificial In-telligence (UAI 2005), pages 388?395.I.
Dan Melamed.
1999.
Bitext maps and alignmentvia pattern recognition.
Computational Linguistics,25(1):107?130.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Gonzalo Navarro.
2001.
A guided tour to approximatestring matching.
ACM Computing Surveys (CSUR),33(1):31?88.Andrew Y. Ng.
2004.
Feature selection, L1 vs. L2 regu-larization, and rotational invariance.
In Proceedings ofthe twenty-first international conference on Machinelearning (ICML 2004), pages 78?85.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Claude E. Shannon.
1948.
A mathematical theoryof communication.
Bell System Technical Journal,27(3):379?423.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), 58(1):267?288.Yoshimasa Tsuruoka, John McNaught, and Sophia Ana-niadou.
2008.
Normalizing biomedical terms by min-imizing ambiguity and variability.
BMC Bioinformat-ics, Suppl 3(9):S2.W.
John Wilbur, Won Kim, and Natalie Xie.
2006.Spelling correction in the PubMed search engine.
In-formation Retrieval, 9(5):543?564.456
