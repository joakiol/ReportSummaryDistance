Proceedings of the ACL-HLT 2011 System Demonstrations, pages 133?138,Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational LinguisticsIMASS: An Intelligent Microblog Analysis and Summarization SystemJui-Yu Weng Cheng-Lun Yang Bo-Nian Chen Yen-Kai Wang Shou-De LinDepartment of Computer Science and Information EngineeringNational Taiwan University{r98922060,r99944042,f92025,b97081,sdlin}@csie.ntu.edu.twAbstractThis paper presents a system to summarizea Microblog post and its responses with thegoal to provide readers a more constructiveand concise set of information for efficientdigestion.
We introduce a novel two-phasesummarization scheme.
In the first phase,the post plus its responses are classified in-to four categories based on the intention,interrogation, sharing, discussion and chat.For each type of post, in the second phase,we exploit different strategies, includingopinion analysis, response pair identifica-tion, and response relevancy detection, tosummarize and highlight critical informa-tion to display.
This system provides an al-ternative thinking about machine-summarization: by utilizing AI approaches,computers are capable of constructing dee-per and more user-friendly abstraction.1 IntroductionAs Microblog services such as Twitter have be-come increasingly popular, it is critical to re-consider the applicability of the existing NLPtechnologies on this new media sources.
Takesummarization for example, a Microblog userusually has to browse through tens or even hun-dreds of posts together with their responses daily,therefore it can be beneficial if there is an intelli-gent tool assisting summarizing those information.Automatic text summarization (ATS) has beeninvestigated for over fifty years, but the majority ofthe existing techniques might not be appropriatefor Microblog write-ups.
For instance, a popularkind of approaches for summarization tries to iden-tify a subset of information, usually in sentenceform, from longer pieces of writings as summary(Das and Martins, 2007).
Such extraction-basedmethods can hardly be applied to Microblog textsbecause many posts/responses contain only onesentence.Below we first describe some special characte-ristics that deviates the Microblog summarizationtask from general text summarization.a.
The number of sentences is limited, and sen-tences are usually too short and casual to con-tain sufficient structural information or cuephrases.
Unlike normal blogs, there is a strictlimitation on the number of characters for eachpost (e.g.
140 characters for Twitter and Plurkmaximum).
Microblog messages cannot betreated as complete documents so that we can-not take advantage of the structural information.Furthermore, users tend to regard Microblog asa chatting board.
They write casually withslangs, jargons, and incorrect grammar.b.
Microblog posts can serve several differentpurposes.
At least three different types of postsare observed in Microblogs, expressing feeling,sharing information, and asking questions.Structured language is not the only means toachieve those goals.
For example, peoplesometimes use attachment, as links or files, forsharing, and utilize emoticons and pre-definedqualifiers to express their feelings.
The diver-sity of content differ Microblogs from generalnews articles.
Consequently, using one mold tofit all types of Microblog posts is not sufficient.Different summarization schemes for postswith different purposes are preferred.c.
Posts and responses in Microblogs are moresimilar to a multi-persons dialogue corpus.
Oneof the main purposes of a Microblog is to serveas the fast but not instant communicationchannel among multiple users.
Due to the free-chatting, multi-user characteristics, the topic ofa post/response thread can drift quickly.
Some-times, the topic of discussion at the end of thethread is totally unrelated to that of the post.133This paper introduces a framework that summariz-es a post with its responses.
Motivated by the ab-ovementioned characteristics of Microblogs, weplan to use a two-phase summarization scheme todevelop different summarization strategies for dif-ferent type of posts (see Figure 1).
In the firstphase, a post will be automatically classified intoseveral categories including interrogation, discus-sion, sharing and chat based on the intention of theusers.
In the second phase, the system chooses dif-ferent summarization components for differenttypes of posts.The novelties of this system are listed below.1.
Strategically, we propose an underlying 2-phaseframework for summarizing Microblog posts.The system can be accessed online athttp://mslab.csie.ntu.edu.tw/~fishyz/plurk/.2.
Tactically, we argue that it is possible to inte-grate post-intention classification, opinion anal-ysis, response relevancy and response-pairmining to create an intelligent summarizationframework for Microblog posts and responses.We also found that the content features are notas useful as the temporal or positional featuresfor text mining in Microblog.3.
Our work provides an alternative thinking aboutATS.
It is possible to go beyond the literalmeaning of summarization to exploit advancedtext mining methods to improve the quality andusability of a summarization system.2 Summarization Framework and Expe-rimentsBelow we discuss our two-phase summarizationframework and the experiment results on each in-dividual component.
Note that our experimentswere tested on the Plurk dataset, which is one ofthe most popular micro-blogging platforms in Asia.Our observation is that Microblog posts canhave different purposes.
We divide them into fourcategories, Interrogation, Sharing, Discussion, andChat.The Interrogation posts are questions asked inpublic with the hope to obtain some useful answersfrom friends or other users.
However, it is verycommon that some repliers do not provide mea-ningful answers.
The responses might serve thepurpose for clarification or, even worse, have noth-ing to do with the question.
Hence we believe themost appropriate summarization process for thiskind of posts is to find out which replies really re-spond to the question.
We created a response re-levance detection component to serve as itssummarization mechanism.The Sharing posts are very frequently observedin Microblog as Microbloggers like to share inter-esting websites, pictures, and videos with theirfriends.
Other people usually write down theircomments or feelings on the shared subjects in theresponses.
To summarize such posts, we obtain thestatistics on how many people have positive, neu-tral, and negative attitude toward the shared sub-jects.
We introduce the opinion analysiscomponent that provides the analysis on whetherthe information shared is recommended by the res-pondents.We also observe that some posts contain charac-teristics of both Interrogation and Sharing.
Theusers may share a hyperlink and ask for others?opinions at the same time.
We create a categorynamed Discussion for these posts, and apply bothresponse ranking and opinion analysis engines onthis type of posts.Finally, there are posts which simply act as thesolicitation for further chat.
For example, one userwrites ?so sad??
and another replies ?what hap-pened?.
We name this type of posts/responses asChat.
This kind of posts can sometimes involvemultiple persons and the topic may gradually driftto a different one.
We believe the plausible sum-marization strategy is to group different messagesbased on their topics.
Therefore for Chat posts, wedesigned a response pair identification system toaccomplish such goal.
We group the related res-ponses together for display, and the number ofgroups represents the number of different topics inthis thread.Figure 1 shows the flow of our summarizationFigure 1.
System architecture134framework.
When an input post with responsescomes in, the system first determines its intention,based on which the system adopts proper strategiesfor summarization.
Below we discuss the technicalparts of each sub-system with experiment results.2.1 Post Intention ClassificationThis stage aims to classify each post into four cat-egories, Interrogation, Sharing, Discussion, andChat.
One tricky issue is that the Discussion labelis essentially a combination of interrogation andsharing labels.
Therefore, simply treating it as anindependent label and use a typical multi-labellearning method can hurt the performance.
We ob-tain 76.7% (10-fold cross validation) in accuracyby training a four-class classifier using the 6-gramcharacter language model.
To improve the perfor-mance, we design a decision-tree based frameworkthat utilizes both manually-designed rules and dis-criminant classification engine (see Figure 2).
Thesystem first checks whether the posts containsURLs or pointers to files, then uses a binary clas-sifier to determine whether the post is interrogative.For the experiment, we manually annotate 6000posts consisting of 1840 interrogation, 2002 shar-ing, 1905 chat, and 254 discussion posts.
We traina 6-gram language model as the binary interroga-tion classifier.
Then we integrate the classifier intoour system and test on 6000 posts to obtain a test-ing accuracy of 82.8%, which is significantly bet-ter than 76.7% with multi-class classification.2.2 Opinion AnalysisOpinion analysis is used to evaluate public prefe-rence on the shared subject.
The system classifiesresponses into 3 categories, positive, negative, andneutral.Here we design a two-level classificationframework using Na?ve-Bayes classifiers whichtakes advantage of the learned 6-gram languagemodel probabilities as features.
First of all, wetrain a binary classifier to determine if a post or areply is opinionative.
This step is called the subjec-tivity test.
If the answer is yes, we then use anotherbinary classifier to decide if the opinion is positiveor negative.
The second step is called the polaritytest.For subjectivity test, we manually annotate 3244posts, in which half is subjective and half is objec-tive.
The 10-fold cross validation shows averageaccuracy of 70.5%.For polarity test, we exploit the built-in emoti-cons in Plurk to automatically extract posts withpositive and negative opinions.
We collect 10,000positive and 10,000 negative posts as training datato train a language model of Na?ve Bayes classifier,and evaluate on manually annotated data of 3121posts, with 1624 positive and 1497 negative to ob-tain accuracy of 0.722.2.3 Response Pair IdentificationConversation in micro-blogs tends to diverge intomultiple topics as the number of responses grows.Sometimes such divergence may result in res-ponses that are irrelevant to the original post, thuscreating problems for summarization.
Furthermore,because the messages are usually short, it is diffi-cult to identify the main topics of these dialogue-like responses using only keywords in the contentfor summarization.
Alternatively, we introduce asubcomponent to identify Response Pairs in micro-blogs.
A Response Pair is a pair of responses thatthe latter specifically responds to the former.
Basedon those pairs we can then form clusters of mes-sages to indicate different group of topics and mes-Figure 2.
The post classification procedureFeature Description WeightBackward Refe-rencingLatter response contentcontains former respond-er?s display name0.055Forward Refe-rencing of usernameFormer response containslatter response?s author?suser name0.018Response positiondifferenceNumber of responses inbetween responses0.13Content similarity Contents?
cosine similari-ty using n-gram models.0.025Response timedifferenceTime difference betweenresponses in seconds0.012Table 1.
Feature set with their description and weights135sages.Looking at the content of micro-blogs, we ob-serve that related responses are usually adjacent toeach other as users tend to closely follow whethertheir messages are responded and reply to the res-ponses from others quickly.
Therefore besides con-tent features, we decide to add the temporal andordering features (See Table 1) to train a classifierthat takes a pair of messages as inputs and returnwhether they are related.
By identifying the re-sponse pairs, our summarization system is able togroup the responses into different topic clustersand display the clusters separately.
We believesuch functionality can assist users to digest longMicroblog discussions.For experiment, the model is trained usingLIBSVM (Chang and Lin, 2001) (RBF kernel)with 6000 response pairs, half of the training setpositive and half negative.
The positive data can beobtained automatically based on Plurk?s built inannotation feature.
Responses with @user_namestring in the content are matched with earlier res-ponses by the author, user_name.
Based on thelearned weights of the features, we observe thatcontent feature is not very useful in determiningthe response pairs.
In a Microblog dialogue, res-pondents usually do not repeat the question norduplicate the keywords.
We also have noticed thatthere is high correlation between the responses re-latedness and the number of other responses be-tween them.
For example, users are less likely torespond to a response if there have been many rep-lies about this response already.
Statistical analy-sis on positive training data shows that the averagenumber of responses between related responses is2.3.We train the classifier using 6000 automatically-extracted pairs of both positive and negative in-stances.
We manually annotated 1600 pairs of datafor testing.
The experiment result reaches 80.52%accuracy in identifying response pairs.
The base-line model which uses only content similarity fea-ture reaches only 45% in accuracy.2.4 Response Relevance DetectionFor interrogative posts, we think the best summaryis to find out the relevent responses as potentialanswers.We introduce a response relevancy detectioncomponent for the problem.
Similar to previouscomponents, we exploit a supervised learning ap-proach and the features?
weights, learned byLIBSVM with RBF kernel, are shown in Table 2.Temporal and Positional FeaturesA common assertion is that the earlier responseshave higher probability to be the answers of thequestion.
Based on the learned weights, it is notsurprising that most important feature is the posi-tion of the response in the response hierarchy.Another interesting finding by our system is thatmeaningful replies do not come right away.
Res-ponses posted within ten seconds are usually forchatting/clarification or ads from robots.Content FeaturesWe use the length of the message, the cosine simi-larity of the post and the responses, and the occur-rence of the interrogative words in responsesentences as content features.Because the interrogation posts in Plurk are rela-tively few, we manually find a total of 382 positiveand 403 negative pairs for training and use 10-foldcross validation for evaluation.We implement the component using LIBSVM(RBF Kernel) classifier.
The baseline is to alwaysselect the first response as the only relevant answer.The results show that the accuracy of baselinereaches 67.4%, far beyond that of our system73.5%.3 System DemonstrationIn this section, we show some snapshots of oursummarization system with real examples usingPlurk dataset.
Our demo system is designed as aFeature WeightResponse position 0.170Response time difference 0.008Response length 0.003Occurrence of interrogativewords0.023Content similarity 0.023Table 2.
Feature set and their weightsFigure 3.
The IMASS interface136search engine (see Figure 3).
Given a query term,our system first returns several posts containing thequery string under the search bar.
When one of theposts is selected, it will generate a summary ac-cording to the detected intention and show it in apop-up frame.
We have recorded a video demon-strating our system.
The video can be viewed athttp://imss-acl11-demo.co.cc/.For interrogative posts, we perform the responserelevancy detection.
The summary contains thequestion and relevant answers.
Figure 4 is an ex-ample of summary of an interrogative post.
We cansee that responses other than the first and the lastresponses are filtered because they are less relevantto the question.For sharing posts, the summary consists of twoparts.
A pie chart that states the percentage of eachopinion group is displayed.
Then the system picksthree responses from the majority group or one re-sponse from each group if there is no significantdifference.
Figure 5 is an example that mostfriends of the user dfrag give positive feedback tothe shared video link.For discussion posts, we combine the responserelevancy detection subsystem and the opinionanalysis sub-system for summarization.
The formerfirst eliminates the responses that are not likely tobe the answer of the post.
The latter then generatesa summary for the post and relevant responses.
Theresult is similar to sharing posts.For chat posts, we apply the response pair iden-tification component to generate the summary.
Inthe example, Figure 6, the original Plurk post isabout one topic while the responses diverge to oneor more unrelated topics.
Our system clearly sepa-rates the responses into multiple groups.
This re-presentation helps the users to quickly catch upwith the discussion flow.
The users no longer haveto read interleaving responses from different topicsand guess which topic group a response is referringto.Figure 4.
An example of interrogative post.Figure 6.
An Example of chat postFigure 5.
An example of sharing post.1374 Related WorkWe have not seen many researches focusing on theissues of Microblog summarization.
We found on-ly one work that discusses about the issues ofsummarization for Microblogs (Sharifi et al, 2010).Their goal, however, is very different from ours asthey try to summarize multiple posts and do notconsider the responses.
They propose the PhraseReinforcement Algorithm to find the most com-monly used phrase that encompasses the topicphrase, and use these phrases to compose thesummary.
They are essentially trying to solve amulti-document summarization problem while ourproblem is more similar to short dialog summariza-tion because the dialogue nature of Microblogs isone of the most challenging part that we tried toovercome.In dialogue summarization, many researchershave pointed out the importance of detecting re-sponse pairs in a conversation.
Zechner (2001) per-forms an in depth analysis and evaluation in thearea of open domain spoken dialogue summariza-tion.
He uses decision tree classifier with lexicalfeatures like POS tags to identify questions andapplies heuristic rules like maximum distance be-tween speakers to extract answers.
Shrestha andMcKeown (2004) propose a supervised learningmethod to detect question-answer pairs in Emailconversations.
Zhou and Hovy (2005) concen-trates on summarizing dialogue-style technical in-ternet relay chats using supervised learningmethods.
Zhou further clusters chat logs into sev-eral topics and then extract some essential responsepairs to form summaries.
Liu et al (2006) proposeto identify question paragraph via analyzing eachparticipant?s status, and then use cosine measure toselect answer paragraphs for online news dataset.The major differences between our componentsand the systems proposed by others lie in the selec-tion of features.
Due to the intrinsic difference be-tween the writing styles of Microblog and otheronline sources, our experiments show that the con-tent feature is not as useful as the position andtemporal features.5 ConclusionIn terms of length and writing styles, Microblogspossess very different characteristics than otheronline information sources such as web blogs andnews articles.
It is therefore not surprising that dif-ferent strategies are needed to process Microblogmessages.
Our system uses an effective strategy tosummarize the post/response by first determine theintention and then perform different analysis de-pending on the post types.
Conceptually, this workintends to convey an alternative thinking aboutmachine-summarization.
By utilizing text miningand analysis techniques, computers are capable ofproviding more intelligent summarization than in-formation condensation.AcknowledgementsThis work was supported by National ScienceCouncil, National Taiwan University and IntelCorporation under Grants NSC99-2911-I-002-001,99R70600, and 10R80800.ReferencesChih-Chung Chang and Chih-Jen Lin.
2001.
LIBSVM :a library for support vector machines.
Softwareavailable at http://www.csie.ntu.edu.tw/~cjlin/libsvmDipanjan Das and Andr?
F.T.
Martins.
2007.
A Surveyon Automatic Text Summarization.
Literature Surveyfor the Language and Statistics II Course.
CMU.Chuanhan Liu, Yongcheng Wang, and Fei Zheng.
2006.Automatic Text Summarization for Dialogue Style.In Proceedings of the IEEE International Conferenceon Information Acquisition.
274-278Beaux Sharifi, Mark A. Hutton, and Jugal Kalita.
2010.Summarizing Microblogs Automatically.
In Proceed-ings of the Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL-HLT).
685-688Lokesh Shrestha and Kathleen McKeown.
2004.
Detec-tion of Question-Answer Pairs in Email Conversa-tions.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COLING2010).Klaus Zechner.
2001.
Automatic Generation of ConciseSummaries of Spoken Dialogues in UnrestrictedDomains.
In Proceedings of the 24th ACM-SIGIRInternational Conference on Research and Develop-ment in Information Retrieval.
199-207.Liang Zhou and Eduard Hovy.
2005.
Digesting virtualgeek culture: The summarization of technical internetrelay chats, in Proceedings of  the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL 2005).
298-305.138
