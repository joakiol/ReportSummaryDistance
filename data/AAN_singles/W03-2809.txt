Living up to standardsMargaret KingTIM/ISSCOETIUniversity of GenevaMargaret.King@issco.unige.chAbstractThis paper attacks one part of thequestion "Are evaluation methods,metrics and resources reusable" byarguing that a set of ISO standardsdeveloped for the evaluation of softwarein general are as applicable to naturallanguage processing software as to anyother.
Main features of the ISO proposalsare presented, and a number ofapplications where they have beenapplied are mentioned, although notdiscussed in any detail.AcknowledgementsThe work recorded here is far from being all myown.
I would like first to record my thanks toNigel Bevan, technical editor of the ISOstandards discussed for much interesting andenlightening discussion.
Then many thanks mustgo to all my colleagues in the EAGLES and ISLEprojects, especially Sandra Manzi and AndreiPopescu-Belis.
Finally, I must thank all thosewhose work on applying the standards reportedhere provoked reflection and helped to convinceme of the value of the approach: Marc Blasband,Maria Canelli, Dominique Estival, DanieleGrasso, V?ronique Sauron, Marianne Starlanderand Nancy Underwood.1 IntroductionThis paper is constructed around a syllogism:1.
ISO standards 9126 and 14598 areapplicable to the evaluation of any typeof software2.
Natural language processing software isa type of software3.
ISO standards 9126 and 14598 areapplicable to the evaluation of naturallanguage processing software.In support of the major premise, I shall set outsome of the major features of the ISO standardsin question.
The minor premise needs no support:indeed, it is almost a tautology.
The truth of theconclusion will logically depend therefore onwhether I have managed to convince the readerof the truth of the major premise.
There will belittle explicit argument in this direction: simplysetting out key features of the approach shouldsuffice.
I will try, however, to reinforce theconclusion by briefly reviewing a number ofnatural language processing applications wherethe ISO standards have been followed withencouraging results.
My hope, of course, is toencourage readers to apply the standardsthemselves.2 ISO standards work on softwareevaluationISO has been publishing standards on softwareevaluation since 1991.
The bibliography gives adetailed picture of what standards have alreadybeen published and of what standards are inpreparation.
ISO/IEC 9126 was the first standardto appear.
It has subsequently been modified, andin its new versions the original content of 1991has been refined, modified and distributed over aseries of separate but inter-related standards.The keystone of ISO work is that the basis ofan evaluation is an explicit and detailed statementof what is required of the object to be evaluated.This statement is formulated very early in theprocess of defining an evaluation and is called a?quality model?.
The process of evaluationinvolves defining how measurements can beapplied to the object to be evaluated in order todiscover how closely it meets the requirementsset out in the quality model.
?The object to be evaluated?
is a clumsyphrase.
It has been used because, in the ISOpicture, evaluation may take place at any point inthe lifecycle of a software product, and may haveas its object not only the final product butintermediate products, including specificationsand code which has not yet been executed.
Itfollows from this that a quality model may applyto a set of specifications just as much as to apiece of finished software.
Indeed, one mightenvisage using quality models as a way ofguiding the whole process of producing asoftware product, from initial research andprototyping through to delivering and fieldtesting the final product.
That this is in line withbest practice in software engineering constitutes,to my mind, an argument in favour of the ISOproposals.As well as a set of standards relating to thedefinition of quality models (the 9126 series) ISOalso offers a set of standards relating to theprocess of evaluation (the 14598 series).
Onedocument sets out a standard for the evaluationprocess seen at its most generic level, furtherproposals relate definition of the process to theparticular viewpoints of software developers, ofacquirers of software and of evaluators typicallyworking as third party evaluators.
Otherdocuments in the 14598 series provide supportingmaterial for those involved in evaluation,offering standards for planning and managementof evaluations and for documentation ofevaluation modules.
Of the 9126 series, only thefirst document which directly deals with qualitymodels has as yet been published.
Documents inpreparation deal with standards for the metricswhich form a critical accompaniment to anyquality model.
It would be unrealistic in thespace of a single paper to discuss even thedocuments already published in any detail.
Inwhat follows, we concentrate on outlining thefoundations of the ISO proposals, the qualitymodel and the process of evaluation.3 Quality models (ISO 9126)A quality model consists of a set of qualitycharacteristics, each of which is decomposed intoa set of quality sub-characteristics.
Metricsmeasure how an object to be evaluated performswith respect to the quality characteristics andsub-characteristics.
The quality characteristicsand sub-characteristics making up the qualitymodel of ISO 9126-1/01 are shown in figure 1,on the next page.
All that figure 1 shows arenames: ISO 9126-1/01 gives both definitions anddiscussion.The quality characteristics are intended to beapplicable to any piece of software product orintermediate product.
They are thus necessarilydefined at a rather high level of generality, andneed to be made more specific before they areapplicable to any particular piece of software.They are also defined through natural languagedefinitions, and are thus not formal in themathematical or logical sense.
This being so,they are open to interpretation.
Defining aspecific evaluation implies deciding on anappropriate interpretation for that evaluation.ISO 9126/01, whilst not barring thepossibility that a quality model other than thatcontained in the standard might be used, requiresthat if another model is used, it should be clearlydescribed.
?Software quality shall be evaluated using adefined quality model.
A quality model shall beused when setting quality goals for softwareproducts and intermediate products.
This part ofISO/IEC 9126 provides a recommended qualitymodel which can be used as a checklist of issuesrelating to quality (although other ways ofcategorising quality may be more appropriate inparticular circumstances).
When a quality modelother than that in this part of ISO/IEC 9126 isused it shall be clearly described.?
(ISO 9126/01,1.5, Quality relationships).Work within the EAGLES project ondefining a general framework for evaluationdesign extended this model by allowing thequality sub-characteristics in their turn to bedecomposed; the process of decomposition beingrepeated if necessary.suitabilityaccuracyinteroperabilitysecurityfunctionalityFigure 1The structure thus obtained is hierarchical, and,theoretically of unlimited depth.
ISO 9126-1/01does not rigidly specify the relationship betweenquality characteristics and metrics.
The EAGLESextension requires that each terminal node of thestructure has at least one metric associated withit.
The structure then becomes a hierarchy ofattribute value pairs, where each node is labelledwith the name of an attribute.
The values of theattributes at the terminal nodes are directlyobtained by the application of metrics.
The valueof a higher level node is obtained by combiningthe values of attributes nodes immediatelydominated by the higher level node: valuespercolate upwards.
Exactly how the combinationof values is done is determined by a combiningfunction which reflects the relative importance ofthe attributes in a particular evaluation.
Thisformalization provides an operational semanticsfor any particular instantiation of the qualitymodel.
Once the evaluation designer has decidedwhat attributes to include in his quality modelsoftwareproductqualityreliabilityusabilitymaturityfault tolerancerecoverabilityunderstandabilitylearnabilityoperabilityattractivenesstime behaviourresource utilisationanalysabilitychangeabilitystabilitytestabilityportabilityefficiencymaintainabilityadaptabilityinstallabilityco-existencereplaceabilityand how to organise them, and once he hasdefined and assigned metrics to the terminalnodes, what functionality, for example, meanswithin that quality model is defined by thedecomposition of the functionality node and bythe associated metrics.Metrics will be discussed only briefly here.The ISO standard distinguishes betweeninternal metrics, external metrics and quality inuse metrics.
The difference between them isdetermined by what kind of an evaluationobject they are applied to.Internal metrics apply to static properties ofsoftware, that is software consideredindependently of its execution.
Examplesmight be the number of lines of code or theprogramming language used.
As can be seenfrom the inclusion of the programminglanguage in this list, metrics are not necessarilyquantitative in their nature, although theyshould, of course, be as objective as possible.
(This is one of the points we shall not go intofurther here.
)External metrics apply to software when it isbeing executed, to the behaviour of the systemas seen from outside.
Thus they may measurethe accuracy of the results, the response timeof the software, the learnability of the userinterface and a host of other attributes that goto make up the quality of the software as apiece of software.Quality in use metrics apply when thesoftware is being used to accomplish aparticular task in a particular environment.They are more concerned with the effects ofusing the software than with the softwareitself.
Quality in use metrics are therefore verydependent on a particular environment and aparticular task.
Quality in use is itself a super-ordinate aspect of quality, for these samereasons.
It is clearly influenced by the qualitycharacteristics which make up the qualitymodel, but is determined by the interaction ofdifferent quality characteristics in a particulartask environment.The ISO standards published so far say littleabout what makes a metric a good metric.Some work elsewhere (Popescu-Belis, 1999,Hovy et al 2003) has made some suggestions.First, metrics should be coherent, in thesense that they should respect the followingcriteria:?
A metric should reach its highest valuefor perfect quality (with respect to theattribute being measured), and, reciprocally,only reach its highest level when quality isperfect.?
A metric should reach its lowest level onlyfor the worst possible quality (again, withrespect to the attribute being tested)?
A metric should be monotonic: that is, if thequality of software A is higher than that ofsoftware B, then the score of A should behigher than the score of B.We might compare two metrics (or more strictlytwo rating functions: see the section on processbelow) by saying that a metric m1 is more severethan a metric m2 if it yields lower scores than m2 forevery possible quality level.
Conversely, one metricmay be more lenient than another.To these rather formal considerations, we mightadd:?
A metric must be clear and intuitive?
It must correlate well with humanjudgements under all conditions?
It must measure what it is supposed tomeasure?
It must be reliable, exhibiting as littlevariance as possible across evaluators or forequivalent inputs?
It must be cheap to prepare and to apply?
It should be automated if possible4 Evaluation process (ISO 14598)A first section of ISO 14598-1/99 is concerned withan overview of how all the different 9126 and14596 documents concerned with softwareevaluation fit together.
This overview can besummarized quite briefly.
It is fundamental to thepreparation of any evaluation that a quality modelreflecting the user?s requirements of the object to beevaluated be constructed.
The 9126 series ofdocuments is intended to support construction ofthe quality model.The 14598 series is concerned with theprocess of evaluation, seen from differentviewpoints.
Separate documents in the seriestackle evaluation from the point of view ofdevelopers, acquirers and (third party)evaluators.
All of these make use of the 9126series, and are further supported by the secondhalf of 14598-1, which sets out a genericpicture of the process of evaluation, and bytwo further documents, the first concernedwith planning and management of a softwareevaluation process, the second with guidancefor documenting evaluation modules.Although these other documents in theseries are clearly important, we limit ourselveshere to summarizing the process of evaluation,as set out in ISO 14598-1.The evaluation process is conceived asbeing generic: it applies to componentevaluation as well as to system evaluation, andmay be applied at any appropriate phase of theproduct life cycle.The evaluation process is broken down intofour main stages, each of which is consideredseparately below:Stage I: Establish evaluation requirements.This step is broken down into a further threesteps:a) Establish the purpose of theevaluationThe commentary on this point reveals just howwide the scope of the standard is intended tobe.
The purpose of evaluating the quality of anintermediate product may be to:?
Decide on the acceptance of anintermediate product from a sub-contractor?
Decide on the completion of a processand when to send products to the nextprocess?
Predict or estimate end product quality?
Collect information on intermediateproducts in order to control and managethe process(The reader will remember that intermediateproduct means, for example, specifications or codebefore it is executed).The purpose of evaluating an end product may beto:?
Decide on the acceptance of the product?
Decide when to release the product?
Compare the product with competitiveproducts?
Select a product from among alternativeproducts?
Assess both positive and negative effects of aproduct when it is used?
Decide when to enhance or replace theproduct.It follows from this very broad range of possibilitiesthat the standard is meant to apply not only to anykind of intermediate or final software product, butto any evaluation scenario, including comparativeevaluation.b) Identify types of products to be evaluatedTypes of products here does not mean applicationsoftware, but rather is concerned with the stagereached in the product?s life cycle, whichdetermines whether and what intermediate productor final product is to be evaluated.c) Specify quality modelThe quality model is, of course, to be defined usingISO 9126-1/01 as a guide.
However, a note quotedagain below adds:?The actual characteristics and sub-characteristicswhich are relevant in any particular situation willdepend on the purpose of the evaluation and shouldbe identified by a quality requirements study.
TheISO/IEC 9126-1 characteristics and sub-characteristics provide a useful checklist of issuesrelated to quality, but other ways of categorisingquality may be more appropriate in particularcircumstances.?
(ISO 14598-1/99)An important word here is ?checklist?
: the basicpurpose of the ISO quality model is to serve as aguide and as a reminder for what should beincluded in evaluating software.
Arguing aboutthe exact interpretation of the qualitycharacteristics is pointless.
Their interpretationis given by the model in which they areincorporated.Stage II:Specify the evaluationThis too breaks down into three steps:a) Select metricsb) Establish rating levels for metricsc)   Establish criteria for assessmentQuality characteristics and sub-characteristicscannot be directly measured.
Metrics musttherefore be defined which correlate to thequality characteristic.
Different metrics may beused in different environments and at differentstages of a product?s development.
Metricshave already been discussed to some extent inthe section on quality models above.A metric typically involves producing ascore on some scale, reflecting the particularsystem?s performance with respect to thequality characteristic in question.
This score,uninterpreted, says nothing about whether thesystem performs satisfactorily.
To illustratethis idea, consider the Geneva educationsystem, where marks in examinations rangefrom 1 to 6.
How do you know, without beingtold, that 6 is the best mark and 1 the worst?
Infact, most people guess that it is so: they maythen have a difficult time in Zurich where 1 isthe highest mark.
Establishing rating levels formetrics involves determining thecorrespondence between the uninterpretedscore and the degree of satisfaction of therequirements.
Since quality refers to givenneeds, there can be no general rules for when ascore is satisfactory.
This must be determinedfor each specific evaluation.Each measure contributes to the overalljudgement of the product, but not necessarilyin a uniform way.
It may be, for example, thatone requirement is critical, whilst another isdesirable, but not strictly necessary.
In thiscase, if a system performs badly with respectto the critical characteristic, it will be assessednegatively no matter what happens to all theother characteristics.
If it performs badly withrespect to the desirable but not necessarycharacteristic, it is its performance with respect toall the other characteristics which will determinewhether the system is acceptable or not.This consideration feeds directly into the thirdstep, establishing criteria for assessment, whichinvolves defining a procedure for summarizing theresults of the evaluation of the differentcharacteristics, using for example decision tables orweighting functions of different kinds.Stage III: Design the evaluationDesigning the evaluation involves producing anevaluation plan, which describes the evaluationmethods and the schedule of the evaluator action.The other documents in the 14598 series expand onthis point, and the plan should be consistent with ameasurement plan, as described and discussed inthe document on planning and management.
(ISO14598-2/00)Stage IV: Execute the evaluationThis final stage again breaks down into three stages:a) Measurementb) Ratingc)   AssessmentThese steps are intuitively straightforward in thelight of the discussion above.
Measurement gives ascore on a scale appropriate to the metric beingused.
Rating determines the correlation between theraw score and the rating levels, in other words, tellsus whether the score can be considered to besatisfactory.
Assessment is a summary of the set ofrated levels and can be seen as a way of puttingtogether the individual ratings to give an overallpicture which also reflects the relative importanceof different characteristics in the light of theparticular quality requirements.
Final decisions aretaken on the basis of the assessment.5 ISO, EAGLES and natural languageapplications in practice.It would be impossible of course to claimknowledge of all applications of the ISO standards,even within the limited area of work on naturallanguage.
In this concluding section only thoseapplications that came to the author?scognisance through her involvement with workin the EAGLES, ISLE and Parmenides projectsare mentioned.The ISO model of 9126/91 as extended andformalized by the first EAGLES project hasbeen tested by application to a number ofdifferent language engineering applications.Within the TEMAA project it was applied tothe evaluation of spelling checkers, and initialwork was done on quality models for grammarcheckers and translation memory systems.
Aspart of the EAGLES project itself, a number ofprojects in the general field of informationretrieval were asked to apply the framework,and produced, in those cases where the projectincluded a substantial evaluation component,encouraging results.
The second EAGLESproject was, for the evaluation group,essentially a consolidation and disseminationproject, where an attempt was made toencourage use of earlier results.
During thistime, the model was also applied in the contextof the ARISE project, which developed aprototype system whereby information onrailway timetables could be obtained throughspoken dialogue.
Similarly, an Australianmanufacturer of speech software used theframework to evaluate a spoken languagedialogue system.
Case studies undertaken inthe context of post-graduate work have appliedthe ISO/EAGLES methodology to theevaluation of dictation systems, grammarcheckers and terminology extraction tools.
Onepart of the ISLE project, now coming to anend, has been applying the methodology to theconstruction of a large scale quality model ofmachine translation systems.
Many of theresults of this work can be consulted bylooking at the EAGLES and ISLE web sites.Recently, work has begun on theParmenides project.
This project is concernedwith ontology based semantic mining ofinformation from web based documents, with aspecial interest in keeping track of informationwhich changes over time.
Evaluation plays animportant role in the project.
Three separateuser groups are supplying the basis for casestudies.
At the time of writing, userrequirements are being defined, which will betranslated into quality requirements for the softwareto be developed within the project and which willserve as the basis for the quality models to be usedin on-going and final evaluation.6 Conclusion.The workshop for which this paper has been writtenaddresses the question of whether there is anythingthat can be shared between evaluations.
The answerwhich I hope to have made convincing is that onething which can be shared is a way of thinkingabout how evaluations should be designed andcarried out.
Adhering to an acknowledged standardin the construction of quality models and indeveloping the process of a specific evaluation canonly make it easier to share more detailed aspects ofevaluation and provides a common framework fordiscussion of such issues as metrics and theirvalidity.References.Blasband, M. 1999.
Practice of Validation: the ARISEApplication of the EAGLES Framework.
EELS(European Evaluation of Language Systems)Conference, Hoevelaken, The Netherlands.EAGLES Evaluation Working Group.
1996.
EAGLESEvaluation of Natural Lnaguage Processing Systems.Final Report, Center for Sprogteknologi, Copenhagen,Denmark.Hovy, E, King, M and Popescu-Belis, A.
2002.Computer-aided Specification of Quality Models forMT Evaluation.
Third International Conference onLanguage Resources and Evaluation (LREC).Hovy, E, King, M and Popescu-Belis, A.
2003.Principles of Context Based Machine TranslationEvaluation.
ISLE report.ISO/IEC 9126-1:2001 Software engineering ?
productquality ?
Part 1: Quality Model.
Geneva, InternationalOrganization for Standardization and InternationalElectrotechnical Commission.ISO/IEC DTR 9126-2 (in preparation): Softwareengineering ?
product quality ?
Part 2: Externalmetrics.
.
Geneva, International Organization forStandardization and International ElectrotechnicalCommissionISO/IEC CD TR 9126-3 (in preparation): Softwareengineering ?
product quality ?
Part 3: Internalmetrics.
.
Geneva, International Organization forStandardization and InternationalElectrotechnical CommissionISO/IEC CD 9126-4 (in preparation): Softwareengineering ?
product quality ?
Part 4: Qualityin use metrics.
.
Geneva, InternationalOrganization for Standardization andInternational Electrotechnical CommissionISO/IEC CD 9126-30 (in preparation): Softwareengineering ?
Software product qualityrequirements and evaluation ?
Part 30: Qualitymetrics ?
Metrics reference model and guide.
.Geneva, International Organization forStandardization and InternationalElectrotechnical CommissionISO/IEC 14598-1:1999 Information technology ?Software product evaluation ?
Part 1: GeneralOverview.
Geneva, International Organizationfor Standardization and InternationalElectrotechnical CommissionISO/IEC 14598-2:2000?
Software engineering -product evaluation ?
Part 2: Planning andManagement.
Geneva, InternationalOrganization for Standardization andInternational Electrotechnical CommissionISO/IEC 14598-3:2000?
Software engineering -product evaluation ?
Part 3: Process fordevelopers.
.
Geneva, International Organizationfor Standardization and InternationalElectrotechnical CommissionISO/IEC 14598-5:1998 Information technology ?Software product evaluation ?
Part 5: Processfor evaluators Geneva, InternationalOrganization for Standardization andInternational Electrotechnical CommissionISO/IEC 14598-4:1999?
Software engineering -product evaluation ?
Part 4: Process foracquirers     Geneva, International Organizationfor Standardization and InternationalElectrotechnical CommissionISO/IEC 14598-6:2001?
Software engineering -product evaluation ?
Part 6: Documentation ofevaluation modules Geneva, InternationalOrganization for Standardization andInternational Electrotechnical CommissionKing, M. 1996.
Evaluating Natural LanguageProcessing Systems.
Communications of theAssociation for Computing Machinery (CACM),Vol.
39, Number 1.Popescu-Belis, A.
1999.
Evaluation of naturalanguage processing systems: a model forcoherence verification of quality measures.
M.Blasband and P. Paroubek, eds, A Blueprint for aGeneral Infrastructure for Natural LanguageProcessing Systems Evaluation Using Semi-AutomaticQuantitative Approach Black Box Approach in aMultilingual Environment.
ELSE project.
(Evaluationin Speech and Language Engineering).Sparck-Jones, K. and Galliers J.R. 1996.
EvaluatingNatural Language Processing Systems:An Analysisand Review.
Lecture Notes in Artificial Intelligence1083.
Springer-Verlag.
