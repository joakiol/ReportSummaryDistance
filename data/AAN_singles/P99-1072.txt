Improving Summaries by Revising ThemI nder jeet  Man i  and Barbara  Gates  and Er i c  B loedornThe MITRE Corporat ion11493 Sunset Hills Rd.Reston, VA 22090, USA{imani,blgates,bloedorn}@mitre.orgAbst rac tThis paper describes a program which revises adraft text by aggregating together descriptionsof discourse ntities, in addition to deleting ex-traneous information.
In contrast to knowledge-rich sentence aggregation approaches exploredin the past, this approach exploits statisticalparsing and robust coreference detection.
Inan evaluation involving revision of topic-relatedsummaries using informativeness measures fromthe T IPSTER SUMMAC evaluation, the resultsshow gains in informativeness without compro-mising readability.1 In t roduct ionWriting improves with revision.
Authors are fa-miliar with the process of condensing a long pa-per into a shorter one: this is an iterative pro-cess, with the results improved over successivedrafts.
Professional abstractors carry out sub-stantial revision and editing of abstracts (Crem-rains 1996).
We therefore xpect revision to beuseful in automatic text summarization.
Priorresearch exploring the use of revision in sum-marization, e.g., (Gabriel 1988), (Robin 1994),(McKeown et al 1995) has focused mainly onstructured ata as the input.
Here, we exam-ine the use of revision in summarization of textinput.First, we review some summarization termi-nology.
In revising draft summaries, these con-densation operations, as well as stylistic reword-ing of sentences, play an important role.
Sum-maries can be used to indicate what topics areaddressed in the source text, and thus can beused to alert the user as to the source con-tent (the indicative function).
Summaries canalso be used to cover the concepts in the sourcetext to the extent possible given the compres-sion requirements for the summary (the in for-mative function).
Summaries can be tailored toa reader's interests and expertise, yielding topic-related summaries, or they can be aimed at aparticular- usually broad - readership commu-nity, as in the cash of (so-called) generic sum-maries.
Revision here applies to generic andtopic-related informative summaries, intendedfor publishing and dissemination.Summarization can be viewed as a text-to-text reduction operation involving three maincondensation operations: selection of salientportions of the text, aggregation of informationfrom different portions of the text, and abstrac-tion of specific information with more generalinformation (Mani and Maybury 1999).
Ourapproach to revision is to construct an initialdraft summary of a source text and then to addto the draft additional background information.Rather than concatenate material in the draft(as surface-oriented, sentence xtraction sum-marizers do), information in the draft is com-bined and excised based on revision rules in-volving aggregation (Dalianis and Hovy 1996)and elimination operations.
Elimination canincrease the amount of compression (summarylength/source l ngth) available, while aggrega-tion can potentially gather and draw in relevantbackground information, in the form of descrip-tions of discourse ntities from different parts ofthe source.
We therefore hypothesize that theseoperations can result in packing in more infor-mation per unit compression than possible byconcatenation.
Rather than opportunisticallyadding as much background information thatcan fit in the available compression, as in (Robin1994), our approach adds background informa-tion from the source text to the draft based onan information weighting function.Our revision approach assumes input sen-tences are represented as syntactic trees whose558nodes are annotated with coreference informa-tion.
In order to provide open-domain cover-age the approach does not assume a meaning-level representation f each sentence, and so, un-like many generation systems, the system doesnot represent and reason about what is beingsaid 1.
Meaning-dependent revision operationsare restricted to situations where it is clear fromcoreference that the same entity is being talkedabout.There are several criteria our revision modelneeds to satisfy.
The final draft needs to beinformative, coherent, and grammatically well-formed.
Informativeness i  explored in Sec-tion 4.2.
We can also strive to guarantee, basedon our revision rule set, that each revision willbe syntactically well-formed.
Regarding coher-ence, revision alters rhetorical structure in away which can produce disfiuencies.
As rhetori-cal structure is hard to extract from the source 2,our program instead uses coreference to guidethe revision, and attempts to patch the coher-ence by adjusting references in revised drafts.2 The  Rev is ion  P rogramThe summary revision program takes as inputa source document, a draft summary specifi-cation, and a target compression rate.
Usingrevision rules, it generates a revised summarydraft whose compression rate is no more thanabove the target compression rate.
The initialdraft summary (and background) are specifiedin terms of a task-dependent weighting functionwhich indicates the relative importance of eachof the source document sentences.
The programrepeatedly selects the highest weighted sentencefrom the source and adds it to the initial draftuntil the given compression percentage of thesource has been extracted, rounded to the near-est sentence.
Next, for each rule in the sequenceof revision rules, the program repeatedly appliesthe rule until it can no longer be applied.
Eachrule application results in a revised draft.
Theprogram selects entences for rule application bygiving preference to higher weighted sentences.1Note that professional bstractors do not attempt ofully "understand" the text - often extremely technicalmaterial, but use surface-level features as above as wellas the overall discourse structure of the text (Cremmins1996).2However, recent progress on this problem (Marcu1997) is encouraging.A unary rule applies to a single sentence.
A bi-nary rule applies to a pair of sentences, at leastone of which must be in the draft, and where thefirst sentence precedes the second in the input.Control over sentence complexity is imposed byfailing rule application when the draft sentenceis too long, the parse tree is too deep 3, or if morethan two relative clauses would be stacked to-gether.
The program terminates when there areno more rules to apply or when the revised draftexceeds the required compression rate by morethan 5.The syntactic structure of each source sen-tence is extracted using Apple Pie 7.2 (Sekine1998), a statistical parser trained on Penn Tree-bank data.
It was evaluated by (Sekine 1998)as having 79% F-score accuracy (parseval) onshort sentences (less than 40 words) from theTreebank.
An informal assessment we made ofthe accuracy of the parser (based on intuitivejudgments) on our own data sets of news ar-ticles suggests about 66% of the parses wereacceptable, with almost half of the remain-ing parsing errors being due to part-of-speechtagging errors, many of which could be fixedby preprocessing the text.
To establish coref-erence between proper names, named entitiesare extracted from the document, along withcoreference r lations using SRA's NameTag 2.0(Krupka 1995), a MUC-6 fielded system.
In ad-dition, we implemented our own coreference ex-tension: A singular definite NP (e.g., beginningwith "the", and not marked as a proper name)is marked by our program as coreferential (i.e.,in the same coreference equivalence class) withthe last singular definite or singular indefiniteatomic NP with the same head, provided theyare within a distance 7 of each other.
On a cor-pus of 90 documents, drawn from the TIPSTERevaluation, described in Section 4.1 below, thiscoreference extension scored 94% precision (470valid coreference classes/501 total coreferenceclasses) on definite NP coreference.
Also, "he"(likewise "she") is marked, subject to 7, ascoreferential with the last person name men-tioned, with gender agreement enforced whenthe person's first name's gender is known (fromNameTag's list of common first names) 4.
Most3Lengths or depths greater than two standard devia-tions beyond the mean are treated as too long or deep.4 However, this very naive method was excluded from559rule-name: rel-clause-intro-which- 1patterns:?X1 ; ~ first sentence pattern?Y1 ?Y2 ?Y3 # second sentence patterntests:label-NP ?X1 ; not entity-class ?X1 person ;label-S ?Y1 ;root ?Y1 ;label-NP ?Y2 ;label-VP ?Y3 ;adjacent-sibling ?Y2 ?Y3 ;parent-child ?Y1 ?Y2 ;parent-child ?Y1 ?Y3 ;coref ?X1 ?Y2actions:subs ?X1 (NP ?X1 (, -COMMA-)(SBAR (WHNP (WP which))(S ?Y3)) (,-COMMA-));elim-root-of ?Y1 # removes econd sentenceFigure 2: Relative Clause Introduction Ruleshowing Aggregation and Elimination opera-tions.of the errors were caused by different sequencesof words between the determiner and the nounphrase head word (e.g., "the factory" -- "thecramped five-story pre-1915 factory" is OK, but"the virus program"-  "the graduate computerscience program" isn't).3 Rev is ion  Ru lesThe revision rules carry out three types of op-erations.
Elimination operations eliminate con-stituents from a sentence.
These include elim-ination of parentheticals, and sentence-initialPPs and adverbial phrases satisfying lexicaltests (such as "In particular,", "Accordingly,""In conclusion," etc.)
5.Aggregation operations combine constituentsfrom two sentences, at least one of which mustbe a sentence in the draft, into a new con-stituent which is inserted into the draft sen-tence.
The basis for combining sentences i thatof referential identity: if there is an NP in sen-tence i which is coreferential with an NP insentence j ,  then sentences i and j are candi-dates for aggregation.
The most common formof aggregation is expressed as tree-adjunction(Joshi 1998) (Oras 1999).
Figures 1 and 2show a relative clause introduction rule whichturns a VP of a (non-embedded) sentence whoseour analysis because of a system bug.5Such lexical tests help avoid misrepresenting themeaning of the sentence.subject is coreferential with an NP of an ear-lier (draft) sentence into a relative clause mod-ifier of the draft sentence NP.
Other appositivephrase insertion rules include copying and in-serting nonrestrictive relative clause modifiers(e.g., "Smith, who...,"), appositive modifiers ofproper names (e.g., "Peter G. Neumann, a com-puter security expert familiar with the case,..."),and proper name appositive modifiers of definiteNPs (e.g., "The network, named ARPANET, isoperated by ..").Smoothing operations apply to a single sen-tence, performing transformations so as to ar-rive at more compact, stylistically preferred sen-tences.
There are two types of smoothing.Reduction operations simplify coordinated con-stituents.
Ellipsis rules include subject ellipsis,which lowers the coordination from a pair ofclauses with coreferential subjects to their VPs(e.g., "The rogue computer program destroyedfiles over a five month period and the programinfected close to 100 computers at NASA fa-cilities" ==~ "The rogue computer program de-stroyed files over a five month period and in-fected close to 100 computers at NASA facil-ities").
It usually applies to the result of anaggregation rule which conjoins clauses whosesubjects are coreferential.
Relative clause re-duction includes rules which apply to clauseswhose VPs begin with "be" (e.g., "which is"is deleted) or "have" (e.g., "which have" : ,~"with"), as well as for other verbs, a rule delet-ing the relative pronoun and replacing the verbwith its present participle (i.e., "which V" ,~"V+ing").
Coordination rules include relativeclause coordination.
Reference Adjustment op-erations fix up the results of other revision oper-ations in order to improve discourse-level coher-ence, and as a result, they are run last 6.
Theyinclude substitution of a proper name with aname alias if the name is mentioned earlier, ex-pansion of a pronoun with a coreferential propername in a parenthetical ("pronoun expansion"),and ("indefinitization") replacement of a def-inite NP with a coreferential indefinite if thedefinite occurs without a prior indefinite.SSuch operations have been investigated earlier by(Robin 1994).560Draft sentence Other sentenceSN'P vP$11,IPl 'VPI (---~m~NP2 vPNP SBAR \SJ ~ vP1RexultsentenceFigure 1: Relative Clause Introduction showing tree NP2 being adjoined into tree S4 Eva luat ionEvaluation of text summarization and othersuch NLP technologies where there may bemany acceptable outputs, is a difficult task.
Re-cently, the U.S. government conducted a large-scale evaluation of summarization systems aspart of its TIPSTER text processing program(Mani et al 1999), which included both anextrinsic (relevance assessment) evaluation, aswell as an intrinsic (coverage of key ideas)evaluation.
The test set used in the latter(Q&:A) evaluation along with several automat-ically scored measures of informativeness hasbeen reused in evaluating the informativenessof our revision component.4.1 Background:  T IPSTER Q&AEvaluationIn this Q&A evaluation, the summarization sys-tem, given a document and a topic, needed toproduce an informative, topic-related summarythat contained the correct answers found in thatdocument o a set of topic-related questions.These questions covered "obligatory" informa-tion that has to be provided in any documentjudged relevant o the topic.
The topics chosen(3 in all) were drawn from the TREC (Harmanand Voorhees 1996) data sets.
For each topic,30 relevant TREC documents were chosen asthe source texts for topic-related summariza-tion.
The principal tasks of each Q&A evaluatorwere to prepare the questions and answer keysand to score the system summaries.
To con-struct the answer key, each evaluator markedoff any passages in the text that provided an an-swer to a question (example shown in Table 1).Two kinds of scoring were carried out.
Inthe first, a manual method, the answer to eachquestion was judged Correct, Partially Correct,or Missing based on guidelines involving a hu-man comparison of the summary of a docu-ment against he set of tagged passages for thatquestion in the answer key for that document.The second method of scoring was an automaticmethod.
This program 7 took as input a key fileand a summary to be scored, and returns aninformativeness score on four different metrics.The key file includes tags identifying passagesin the file which answer certain questions.
Thescoring uses the overlap measures shown in Ta-ble 2 s. The automatically computed V4 thruV7 informativeness scores were strongly corre-lated with the human-evaluated scores (Pearsonr > .97, ~ < 0.0001).
Given this correlation, wedecided to use these informativeness measures.4.2 Revision Evaluation:InformativenessTo evaluate the revised summaries, we first con-verted each summary into a weighting functionwhich scored each full-text sentence in the sum-mary's source in terms of its similarity to themost similar summary sentence.
The weightof a source document sentence s given a sum-7The program was reimplemented by us for use in therevision evaluation.S Passage matching here involves a sequential matchwith stop words and punctuation removed.561Tit le : Computer SecurityDescr ipt ion : Identify instances of illegal entry into sensitivecomputer networks by nonauthorized personnel.Narrat ive : Illegal entry into sensitive computer networksis a serious and potentially menacing problem.
Both 'hackers' andforeign agents have been known to acquire unauthorized entry intovarious networks.
Items relative this subject would include but notbe limited to instances of illegally entering networks containinginformation of a sensitive nature to specific countries, such asdefense or technology information, international banking, etc.
Itemsof a personal nature (e.g.
credit card fraud, changing of collegetest scores) should not be considered relevant.Questions1)Who is the known or suspected hacker accessing a sensitive computer or computer network?2) How is the hacking accomplished or putatively achieved?3) Who is the apparent target of the hacker?4) What did the hacker accomplish once the violation occurred?What was the purpose in performing the violation?5) What is the time period over which the breakins were occurring?As a federal grand jury decides whether he should be prosecuted, <Ol>a graduatestudent</Ql> linked to a ''virus'' that disrupted computers nationwide <Q5>lastmonth</q5>has been teaching his lawyer about the technical subject and turning downoffers for his life story ..... No charges have been filed against <ql>Norris</Ql>,who reportedly told friends that he designed the virus that temporarily clogged about<Q3>6,000 university and military computers</q3> <Q2>linked to the Pentagon's hrpanetnetwork</Q2> ......Table 1: Q&A Topic 258, topic-related questions, and part  of a relevant source document  showinganswer key annotat ions.Overlap MetricV4V5Definit ionfull credit if the text spans for all tagged key passagesare found in their entirety in the summaryfull credit if the text spans for all tagged key passagesare found in their entirety in the summary;haft credit if the text spans for all tagged key passagesare found in some combination of full or truncated form in the summaryfull credit if the text spans for all tagged key passagesare found in some combination of full or truncated form in the summarypercentage of credit assigned that is commensurate with the extent o whichthe text spans for tagged key passages are present in the summaryTable 2: Informativeness measures for Automat ic  Scoring of each question that  has an answeraccording to the key.PartyCGI/CMUCornell/SabIRGE 15.43ISI 19.57NMSU 16.54SRA 15.59UPenn 16.29Mean 16.48FOG KincaidBefore After Before16.49 15.50 13.2215.51 15.08 12.1515.14 12.1317.94 16.1815.52 13.3215.29 12.2616.21 12.9315.82 13.15After12.2311.7111.8714.5112.3011.9912.8312.51Table 3: Readabil i ty of Summaries Before (Original Summary)  and After Revision (A+E) .
Overall,both FOG and Kincaid scores show a slight but statistically significant drop on revision (~ <: 0.05).562100%90%80%70%60%50%40%30%20%10%0%iX A ~+]E X h. &+Z Z ,~ j~+~r Z ~.
.K+][~Lose?
M alntainIWiuFigure 3: Gains in Compression-Normalized Informativeness of revised summaries compared toinitial drafts.
E -- elimination, A - aggregation.
A, E, and A+E are shown in the order V4, V5,V6, and V7.<s l> Researchers today tried to trace a "virus" that infected computer systems nationwide,<Q4> slowing machines in universities, a NASA and nuclear weapons lab and other federalresearch centers linked by a Defense Department computer network.
</q4> <s3>Authorities aid the virus, which <FROM S16> <Q3> the virus infected only unclassifiedcomputers </Q3> and <FROM $15> <Q3> the virus affected the unclassified,non-secured computer  systems </q3> (and which <FROM S19> <Q4> the virus was %nainly justslowing down systems ) and slowing data ", </Q4> apparently <q4> destroyed no data but temporarilyhalted some research.
</Q4> <s14>.
The computer problem also was discovered lateWednesday at the <q3> Lawrence Livermore National Laboratory in Livermore, Calif. </Q3><s15> <s20> "the developer was clearly a very high order hacker,", <FROM $25> <QI> agraduate student </QI> <Q2> who made making a programming error in designing thevirus,causing the program to replicate faster than expected </q2> or computer buff, saidJohn McAfee, chairman of the Computer Virus Industry Association in Santa Clara, Calif..<s24> The Times reported today that the anonymous caller an anonymous caller tothe paper said his associate was responsible for the attack and had meant it tobe harmless.Figure 4: A revised summary specified in terms of an original draft (plain text) with added (bold-face) and deleted (italics) spans.
Sentence <s> and Answer Key <Q> tags are overlaid.mary is the match score of s's best-matchingsummary sentence, where the match score isthe percentage of content word occurrences ins that are also found in the summary sentence.Thus, we constructed an idealized model ofeach summary as a sentence extraction function.Since some of the participants truncated andoccasionally mangled the source text (in addi-tion, Penn carried out pronoun expansion), wewanted to avoid having to parse and apply revi-sion rules to such relatively ill-formed material.This idealization is highly appropriate, for eachof the summarizers considered 9 did carry outsentence xtraction; in addition, it helps levelthe playing field, avoiding penalization of indi-vidual summarizers simply because we didn'tcater to the particular form of their summary.Each summary was revised by calling the re-vision program with the full-text source, theoriginal compression rate of the summary, and9TextWise, which extracted named entities ratherthan passages, was excluded.563the summary weighting function (i.e., with theweight for each source sentence).
The 630 re-vised summaries (3 topics x 30 documents pertopic ?
7 participant summaries per document)were then scored against he answer keys usingthe overlap measures above.
The documentsconsisted of AP, Wall Street Journal, and Fi-nancial Times news articles from the TREC(Harman and Voorhees 1996) collection.The rules used in the system are very gen-eral, and were not modified for the evaluationexcept for turning off most of the reference ad-justment rules, as we wished to evaluate thatcomponent separately.
Since the answer keystypically do not contain names of commenta-tors, we wanted to focus the algorithm awayfrom such names (otherwise, it would aggregateinformation around those commentators).
Asa result, special rules were written in the revi-sion rule language to detect commentator namesin reported speech ("X said that ..", "X said...", ", said X..", ", said X..", etc.
), and thesenames were added to a stoplist for use in enti-tyhood and coreference tests during regular re-vision rule application.Figure 3 shows percentage of losses, main-tains, and wins in informativeness against theinitial draft (i.e., the result of applying the com-pression to the sentence weighting function).Informativeness u ing V7 is measured by V71?normalized for compression as:slnV7 = V7 * (1 - ~-~) (1)where sl  is summary length and sO is the sourcelength.
This initial draft is in itself not as in-formative as the original summary: in all casesexcept for Penn on 257, the initial draft eithermaintains or loses informativeness compared tothe original summary.As Figure 3 reveals (e.g., for nVT), revisingthe initial draft using elimination rules only (E)results in summaries which are less informativethan the initial draft 65% of the time, suggest-ing that these rules are removing informativematerial.
Revising the initial draft using aggre-gation rules alone (A), by contrast, results inmore informative summaries 47% of the time,and equally informative summaries another 13%1?V7 computes for each question the percentage ofits answer passages completely covered by the summary.This normalization is extended similarly for V4 thru V6.of the time.
This is due to aggregation folding inadditional informative material into the initialdraft when it can.
Inspection of the output sum-maries, an example of which is shown in Fig-ure 4, confirms the folding in behavior of aggre-gation.
Finally, revising the initial draft usingboth aggregation and elimination rules (ATE)does no more than maintain the informative-ness of the initial draft, suggesting A and E arecanceling each other out.
The same trend is ob-serving for nV4 thru nV6, confirming that therelative gain in informativeness due to aggrega-tion is robust across a variety of (closely related)measures.
Of course, if the revised summarieswere instead radically different in wording fromthe original drafts, such informativeness mea-sures would, perhaps, fall short.It is also worth noting the impact of aggrega-tion is modulated by the current control strat-egy; we don't know what the upper bound ison how well revision could do given other con-trol regimes.
Overall, then, while the resultsare hardly dramatic, they are certainly encour-aging zl.4.3 Revis ion Evaluation: Readabi l i tyInspection of the results of revision indicatesthat the syntactic well-formedness revision cri-terion is satisfied to a very great extent.
Im-proper extraction from coordinated NPs is anissue (see Figure 4), but we expect additionalrevision rules to handle such cases.
Coher-ence disfiuencies do occur; for example, since wedon't resolve possessive pronouns or plural def-inites, we can get infelicitous revisions like "Acomputer virus, which entered , the i r  comput-ers through ARPANET, infected systems fromMIT."
Other limitations in definite NP coref-erence can and do result in infelicitous refer-ence adjustments.
For one thing, we don't linkdefinites to proper name antecedents, result-ing in inappropriate indefinitization (e.g., "BillGates .
.
.
*A  computer  tycoon") .
In addition,the "same head word" test doesn't of course ad-dress inferential relationships between the defi-nite NP and its antecedent (even when the an-tecedent is explicitly mentioned), again result-ing in inappropriate indefinitization (e.g., "Theprogram .
.
.
.a  developer ~', and "The developer11 Similar results hold while using a variety of othercompression normalization metrics.564.
.
.
An anonymous caller said .a very high orderhacker was a graduate student").To measure fluency without conducting anelaborate experiment involving human judg-mentsl we fell back on some extremely coarsemeasurea based on word and sentence lengthcomputed by the (gnu) unix program style(Cherry 1981).
The FOG index sums the av-erage sentence length with the percentage ofwords over 3 syllables, with a "grade" level over12 indicating difficulty for the average reader.The Kincaid index, intended for technical text,computes a weighted sum of sentence l ngth andword length.
As can be seen from Table 3, thereis a slight but significant lowering of scores onboth metrics, revealing that according to thesemetrics revision is not resulting in more com-plex text.
This suggests that elimination ratherthan aggregation is mainly responsible for this.5 Conc lus ionThis paper demonstrates that recent advancesin information extraction and robust parsingcan be exploited effectively in an open-domainmodel of revision inspired by work in naturallanguage generation.
In the future, instead ofrelying on adjustment rules for coherence, itmay be useful to incorporate a level of text plan-ning.
We also hope to enrich the backgroundinformation by merging information from mul-tiple text and structured ata sources.Re ferencesCherry, L.L., and Vesterman, W. Writing Tools:The STYLE and DICTION programs, Com-puter Science Technical Report 91, Bell Lab-oratories, Murray Hill, N.J. (1981).Cremmins, E. T. 1996.
The Art of Abstracting.Information Resources Press.Dalianis, H., and Hov, E. 1996.
Aggregation inNatural Language Generation.
In Zock, M.,and Adorni, G., eds., Trends in Natural Lan-guage Generation: an Artificial IntelligencePerspective, pp.88-105.
Lecture Notes in Ar-tificial Intelligence, Number 1036, SpringerVerlag, Berlin.Dras, M. 1999.
Tree Adjoining Grammar andthe Reluctant Paraphrasing of Text, Ph.D.Thesis, Macquarie University, Australia.Gabriel, R. 1988.
Deliberate Writing.
In Mc-Donald, D.D., and Bolc, L., eds., Natu-ral Language Generation Systems, Springer-Verlag, NY.Harman, D.K.
and E.M. Voorhees.
1996.
Thefifth text retrieval conference (trec-5).
Na-tional Institute of Standards and TechnologyNIST SP 500-238.Joshi, A. K. and Schabes, Y.
1996.
"Tree-Adjoining Grammars".
In Rosenberg, G., andSalomaa, A., eds., Handbook of Formal Lan-guages, Vol.
3, 69-123.
Springer-Verlag, NY.Krupka, G. 1995.
"SRA: Description of the SRASystem as Used for MUC-6", Proceedings ofthe Sixth Message Understanding Conference(MUC-6), Columbia, Maryland, November1995.Marcu, D. 1997.
From discourse structures totext summaries, in Mani, L and Maybury,M., eds., Proceedings of the ACL/EACL '97Workshop on Intelligent Scalable Text Sum-marization.Mani, I. and M. Maybury, eds.
1999.
Ad-vances in Automatic Text Summarization.MIT Press.Mani, I., Firmin, T., House, D., Klein, G.,Hirschman, L., and Sundheim, B.
1999.
"The TIPSTER SUMMAC Text Summariza-tion Evaluation", Proceedings of EACL'99,Bergen, Norway, June 8-12, 1999.McKeown, K., J. Robin, and K. Kukich.
1995.Generating Concise Natural Language Sum-maries.
Information Processing and Manage-ment, 31, 5, 703- 733.Robin, J.
1994.
Revision-based generation ofnatural language summaries providing his-torical background: corpus-based analysis,design and implementation.
Ph.D. Thesis,Columbia University.Sekine, S. 1998.
Corpus-based Parsing and Sub-language Studies.
Ph.D. Dissertation, NewYork University, 1998.565
