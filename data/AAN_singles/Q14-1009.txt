Parallel Algorithms for Unsupervised TaggingSujith RaviGoogleMountain View, CA 94043sravi@google.comSergei VassilivitskiiGoogleMountain View, CA 94043sergeiv@google.comVibhor Rastogi?TwitterSan Francisco, CAvibhor.rastogi@gmail.comAbstractWe propose a new method for unsupervisedtagging that finds minimal models which arethen further improved by Expectation Max-imization training.
In contrast to previousapproaches that rely on manually specifiedand multi-step heuristics for model minimiza-tion, our approach is a simple greedy approx-imation algorithm DMLC (DISTRIBUTED-MINIMUM-LABEL-COVER) that solves thisobjective in a single step.We extend the method and show how to ef-ficiently parallelize the algorithm on modernparallel computing platforms while preservingapproximation guarantees.
The new methodeasily scales to large data and grammar sizes,overcoming the memory bottleneck in previ-ous approaches.
We demonstrate the powerof the new algorithm by evaluating on varioussequence labeling tasks: Part-of-Speech tag-ging for multiple languages (including low-resource languages), with complete and in-complete dictionaries, and supertagging, acomplex sequence labeling task, where thegrammar size alone can grow to millions ofentries.
Our results show that for all of thesesettings, our method achieves state-of-the-artscalable performance that yields high qualitytagging outputs.1 IntroductionSupervised sequence labeling with large labeledtraining datasets is considered a solved problem.
For?The research described herein was conducted while theauthor was working at Google.instance, state of the art systems obtain tagging ac-curacies over 97% for part-of-speech (POS) taggingon the English Penn Treebank.
However, learningaccurate taggers without labeled data remains a chal-lenge.
The accuracies quickly drop when faced withdata from a different domain, language, or whenthere is very little labeled information available fortraining (Banko and Moore, 2004).Recently, there has been an increasing amountof research tackling this problem using unsuper-vised methods.
A popular approach is to learn fromPOS-tag dictionaries (Merialdo, 1994), where weare given a raw word sequence and a dictionary oflegal tags for each word type.
Learning from POS-tag dictionaries is still challenging.
Complete word-tag dictionaries may not always be available for useand in every setting.
When they are available, thedictionaries are often noisy, resulting in high tag-ging ambiguity.
Furthermore, when applying tag-gers in new domains or different datasets, we mayencounter new words that are missing from the dic-tionary.
There have been some efforts to learn POStaggers from incomplete dictionaries by extendingthe dictionary to include these words using someheuristics (Toutanova and Johnson, 2008) or usingother methods such as type-supervision (Garretteand Baldridge, 2012).In this work, we tackle the problem of unsuper-vised sequence labeling using tag dictionaries.
Thefirst reported work on this problem was on POS tag-ging from Merialdo (1994).
The approach involvedtraining a standard Hidden Markov Model (HMM)using the Expectation Maximization (EM) algo-rithm (Dempster et al., 1977), though EM does not105Transactions of the Association for Computational Linguistics, 2 (2014) 105?118.
Action Editor: Sharon Goldwater.Submitted 11/2013; Revised 2/2014; Published 4/2014.
c?2014 Association for Computational Linguistics.perform well on this task (Johnson, 2007).
More re-cent methods have yielded better performance thanEM (see (Ravi and Knight, 2009) for an overview).One interesting line of research introduced byRavi and Knight (2009) explores the idea of per-forming model minimization followed by EM train-ing to learn taggers.
Their idea is closely relatedto the classic Minimum Description Length princi-ple for model selection (Barron et al., 1998).
They(1) formulate an objective function to find the small-est model that explains the text (model minimizationstep), and then, (2) fit the minimized model to thedata (EM step).
For POS tagging, this method (Raviand Knight, 2009) yields the best performance todate; 91.6% tagging accuracy on a standard testdataset from the English Penn Treebank.
The orig-inal work from (Ravi and Knight, 2009) uses an in-teger linear programming (ILP) formulation to findminimal models, an approach which does not scaleto large datasets.
Ravi et al.
(2010b) introduced atwo-step greedy approximation to the original ob-jective function (called the MIN-GREEDY algo-rithm) that runs much faster while maintaining thehigh tagging performance.
Garrette and Baldridge(2012) showed how to use several heuristics to fur-ther improve this algorithm (for instance, betterchoice of tag bigrams when breaking ties) and stackother techniques on top, such as careful initializationof HMM emission models which results in furtherperformance gains.
Their method also works un-der incomplete dictionary scenarios and can be ap-plied to certain low-resource scenarios (Garrette andBaldridge, 2013) by combining model minimizationwith supervised training.In this work, we propose a new scalable algorithmfor performing model minimization for this task.
Bymaking an assumption on the structure of the solu-tion, we prove that a variant of the greedy set coveralgorithm always finds an approximately optimal la-bel set.
This is in contrast to previous methods thatemploy heuristic approaches with no guarantee onthe quality of the solution.
In addition, we do nothave to rely on ad hoc tie-breaking procedures orcareful initializations for unknown words.
Finally,not only is the proposed method approximately op-timal, it is also easy to distribute, allowing it to eas-ily scale to very large datasets.
We show empiricallythat our method, combined with an EM training stepoutperforms existing state of the art systems.1.1 Our Contributions?
We present a new method, DISTRIBUTEDMINIMUM LABEL COVER, DMLC, for modelminimization that uses a fast, greedy algorithmwith formal approximation guarantees to thequality of the solution.?
We show how to efficiently parallelize the al-gorithm while preserving approximation guar-antees.
In contrast, existing minimization ap-proaches cannot match the new distributed al-gorithm when scaling from thousands to mil-lions or even billions of tokens.?
We show that our method easily scales to bothlarge data and grammar sizes, and does not re-quire the corpus or label set to fit into memory.This allows us to tackle complex tagging tasks,where the tagset consists of several thousandlabels, which results in more than one millionentries in the grammar.?
We demonstrate the power of the newmethod by evaluating under several differ-ent scenarios?POS tagging for multiple lan-guages (including low-resource languages),with complete and incomplete dictionaries, aswell as a complex sequence labeling task of su-pertagging.
Our results show that for all thesesettings, our method achieves state-of-the-artperformance yielding high quality taggings.2 Related WorkRecently, there has been an increasing amount ofresearch tackling this problem from multiple di-rections.
Some efforts have focused on inducingPOS tag clusters without any tags (Christodoulopou-los et al., 2010; Reichart et al., 2010; Moon etal., 2010), but evaluating such systems proves dif-ficult since it is not straightforward to map the clus-ter labels onto gold standard tags.
A more pop-ular approach is to learn from POS-tag dictionar-ies (Merialdo, 1994; Ravi and Knight, 2009), incom-plete dictionaries (Hasan and Ng, 2009; Garrette andBaldridge, 2012) and human-constructed dictionar-ies (Goldberg et al., 2008).106Another direction that has been explored in thepast includes bootstrapping taggers for a new lan-guage based on information acquired from other lan-guages (Das and Petrov, 2011) or limited annota-tion resources (Garrette and Baldridge, 2013).
Ad-ditional work focused on building supervised tag-gers for noisy domains such as Twitter (Gimpel etal., 2011).
While most of the relevant work in thisarea centers on POS tagging, there has been somework done for building taggers for more complexsequence labeling tasks such as supertagging (Raviet al., 2010a).Other related work include alternative methodsfor learning sparse models via priors in Bayesian in-ference (Goldwater and Griffiths, 2007) and poste-rior regularization (Ganchev et al., 2010).
But thesemethods only encourage sparsity and do not explic-itly seek to minimize the model size, which is the ob-jective function used in this work.
Moreover, taggerslearned using model minimization have been shownto produce state-of-the-art results for the problemsdiscussed here.3 ModelFollowing Ravi and Knight (2009), we formulate theproblem as that of label selection on the sentencegraph.
Formally, we are given a set of sequences,S = {S1, S2, .
.
.
, Sn} where each Si is a sequenceof words, Si = wi1, wi2, .
.
.
, wi,|Si|.
With eachword wij we associate a set of possible tags Tij .
Wewill denote by m the total number of (possibly du-plicate) words (tokens) in the corpus.Additionally, we define two special words w0 andw?
with special tags start and end, and considerthe modified sequences S?i = w0, Si, w?.
To sim-plify notation, we will refer to w?
= w|Si|+1.
Thesequence label problem asks us to select a valid tagtij ?
Tij for each word wij in the input to minimizea specific objective function.We will refer to a tag pair (ti,j?1, tij) as a label.Our aim is to minimize the number of distinct labelsused to cover the full input.
Formally, given a se-quence S?i and a tag tij for each word wij in S?i, letthe induced set of labels for sequence S?i beLi =|S?i|?j=1{(ti,j?1, tij)}.The total number of distinct labels used over all se-quences is then?
=??
?i Li| =??
?i|Si|+1?j=1{(ti,j?1, tij)}|.Note that the order of the tokens in the label makesa difference as {(NN, VP)} and {(VP, NN)} are twodistinct labels.Now we can define the problem formally, follow-ing (Ravi and Knight, 2009).Problem 1 (Minimum Label Cover).
Given a set Sof sequences of words, where each word wij has aset of valid tags Tij , the problem is to find a valid tagassignment tij ?
Tij for each word that minimizesthe number of distinct labels or tag pairs over allsequences, ?
= ??
?i?|Si|+1j=1 {(ti,j?1, tij)}| .The problem is closely related to the classical SetCover problem and is also NP-complete.
To reduceSet Cover to the label selection problem, map eachelement i of the Set Cover instance to a single wordsentence Si = wi1, and let the valid tags Ti1 con-tain the names of the sets that contain element i.Consider a solution to the label selection problem;every sentence Si is covered by two labels (w0, ki)and (ki, w?
), for some ki ?
Ti1, which correspondsto an element i being covered by set ki in the SetCover instance.
Thus any valid solution to the labelselection problem leads to a feasible solution to theSet Cover problem ({k1, k2, .
.
.})
of exactly half thesize.Finally, we will use {{.
.
.}}
notation to denote amultiset of elements, i.e.
a set where an element mayappear multiple times.4 AlgorithmIn this Section, we describe the DISTRIBUTED-MINIMUM-LABEL-COVER, DMLC, algorithm forapproximately solving the minimum label coverproblem.
We describe the algorithm in a central-ized setting, and defer the distributed implementa-tion to Section 5.
Before describing the algorithm,we briefly explain the relationship of the minimumlabel cover problem to set cover.4.1 Modification of Set CoverAs we pointed out earlier, the minimum label coverproblem is at least as hard as the Set Cover prob-1071: Input: A set of sequences S with eachwords wij having possible tags Tij .2: Output: A tag assignment tij ?
Tij foreach word wij approximately minimizinglabels.3: LetM be the multi set of all possible labelsgenerated by choosing each possible tag t ?Tij .M =?i?????|Si|+1?j=1?t?
?Ti,j?1t?Tij{{(t?, t)}}?????
(1)4: Let L = ?
be the set of selected labels.5: repeat6: Select the most frequent label not yet se-lected: (t?, t) = arg max(s?,s)/?L |M ?
(s?, s)|.7: For each bigram (wi,j?1, wij) where t?
?Ti,j?1 and t ?
Tij tentatively assign t?
towi,j?1 and t to wij .
Add (t?, t) to L.8: If a word gets two assignments, selectone at random with equal probability.9: If a bigram (wi,j?1, wij) is consistentwith assignments in (t?, t), fix the tenta-tive assignments, and set Ti,j?1 = {t?
}and Tij = {t}.
RecomputeM, the multi-set of possible labels, with the updatedTi,j?1 and Tij .10: until there are no unassigned wordsAlgorithm 1: MLC Algorithm1: Input: A set of sequences S with each words wijhaving possible tags Tij .2: Output: A tag assignment tij ?
Tij for each wordwij approximately minimizing labels.3: (Graph Creation) Initialize each vertex vij with theset of possible tags Tij and its neighbors vi,j+1 andvi,j?1.4: repeat5: (Message Passing) Each vertex vij sends its pos-sibly tags Tij to its forward neighbor vij+1.6: (Counter Update) Each vertex receives thethe tags Ti,j?1 and adds all possible labels{(s, s?
)|s ?
Ti,j?1, s?
?
Tij} to a global counter(M).7: (MaxLabel Selection) Each vertex queries theglobal counter M to find the maximum label(t, t?
).8: (Tentative Assignment) Each vertex vij selects atag tentatively as follows: If one of the tags t, t?is in the feasible set Tij , it tentatively selects thetag.9: (Random Assignment) If both are feasible it se-lects one at random.
The vertex communicatesits assignment to its neighbors.10: (Confirmed Assignment) Each vertex receivesthe tentative assignment from its neighbors.
Iftogether with its neighbors it can match the se-lected label, the assignment is finalized.
If theassigned tag is T , then the vertex vij sets thevalid tag set Tij to {t}.11: until no unassigned vertices exist.Algorithm 2: DMLC Implementationlem.
An additional challenge comes from the factthat labels are tags for a pair of words, and henceare related.
For example, if we label a word pair(wi,j?1, wij) as (NN, VP), then the label for the nextword pair (wij , wi,j+1) has to be of the form (VP, *),i.e., it has to start with VP.Previous work (Ravi et al., 2010a; Ravi et al.,2010b) recognized this challenge and employed twophase heuristic approaches.
Eschewing heuristics,we will show that with one natural assumption, evenwith this extra set of constraints, the standard greedyalgorithm for this problem results in a solution witha provable approximation ratio of O(logm).
Inpractice, however, the algorithm performs far betterthan the worst case ratio, and similar to the workof (Gomes et al., 2006), we find that the greedyapproach selects a cover approximately 11% worsethan the optimum solution.4.2 MLC AlgorithmWe present in Algorithm 1 our MINIMUM LABELCOVER algorithm to approximately solve the mini-mum label cover problem.
The algorithm is simple,efficient, and easy to distribute.The algorithm chooses labels one at a time, select-ing a label that covers as many words as possible in108every iteration.
For this, it generates and maintainsa multi-set of all possible labels M (Step 3).
Themulti-set contains an occurrence of each valid label,for example, if wi,j?1 has two possible valid tagsNN and VP, and wij has one possible valid tag VP,then M will contain two labels, namely (NN, VP)and (VP, VP).
Since M is a multi-set it will containduplicates, e.g.
the label (NN, VP) will appear foreach adjacent pair of words that have NN and VP asvalid tags, respectively.In each iteration, the algorithm picks a label withthe most number of occurrences inM and adds it tothe set of chosen labels (Step 6).
Intuitively, this isa greedy step to select a label that covers the mostnumber of word pairs.Once the algorithm picks a label (t?, t), it tries toassign as many words to tags t or t?
as possible (Step7).
A word can be assigned t?
if t?
is a valid tag forit, and t a valid tag for the next word in sequence.Similarly, a word can be assigned t, if t is a validtag for it, and t?
a valid tag for the previous word.Some words can get both assignments, in which casewe choose one tentatively at random (Step 8).
Ifa word?s tentative random tag, say t, is consistentwith the choices of its adjacent words (say t?
fromthe previous word), then the tentative choice is fixedas a permanent one.
Whenever a tag is selected, theset of valid tags Tij for the word is reduced to a sin-gleton {t}.
Once the set of valid tags Tij changes,the multi-setM of all possible labels also changes,as seen from Eq 1.
The multi-set is then recom-puted (Step 9) and the iterations repeated until allof words have been tagged.We can show that under a natural assumption thissimple algorithm is approximately optimal.Assumption 1 (c-feasibility).
Let c ?
1 be any num-ber, and k be the size of the optimal solution to theoriginal problem.
In each iteration, the MLC algo-rithm fixes the tags for some words.
We say that thealgorithm is c-feasible, if after each iteration thereexists some solution to the remaining problem, con-sistent with the chosen tags, with size at most ck .The assumption encodes the fact that a single badgreedy choice is not going to destroy the overallstructure of the solution, and a nearly optimal so-lution remains.
We note that this assumption of c-feasibility is not only sufficient, as we will formallyshow, but is also necessary.
Indeed, without any as-sumptions, once the algorithm fixes the tag for somewords, an optimal label may no longer be consis-tent with the chosen tags, and it is not hard to findcontrived examples where the size of the optimal so-lution doubles after each iteration of MLC.Since the underlying problem is NP-complete, itis computationally hard to give direct evidence ver-ifying the assumption on natural language inputs.However, on small examples we are able to showthat the greedy algorithm is within a small constantfactor of the optimum, specifically it is within 11%of the optimum model size for the POS taggingproblem using the standard 24k dataset (Ravi andKnight, 2009).
Combined with the fact that the finalmethod outperforms state of the art approaches, thisleads us to conclude that the structural assumption iswell justified.Lemma 1.
Under the assumption of c-feasibility,the MLC algorithm achieves a O(c logm) approx-imation to the minimum label cover problem, wherem =?i |Si| is the total number of tokens.Proof.
To prove the Lemma we will define an objec-tive function ?
?, counting the number of unlabeledword pairs, as a function of possible labels, andshow that ??
decreases by a factor of (1?O(1/ck)) atevery iteration.To define ?
?, we first define ?, the number of la-beled word pairs.
Consider a particular set of la-bels, L = {L1, L2, .
.
.
, Lk} where each label is apair (ti, tj).
Call {tij} a valid assignment of to-kens if for each wij , we have tij ?
Tij .
Then thescore of L under an assignment t, which we denoteby ?t, is the number of bigram labels that appear inL.
Formally, ?t(L) = | ?i,j {{(ti,j?1, tij) ?
L}}|.Finally, we define ?
(L) to be the best such assign-ment, ?
(L) = maxt ?t(L), and ??
(L) = m ?
?
(L)the number of uncovered labels.Consider the label selected by the algorithm in ev-ery step.
By the c-feasibility assumption, there ex-ists some solution having ck labels.
Thus, some la-bel from that solution covers at least a 1/ck fractionof the remaining words.
The selected label (t, t?
)maximizes the intersection with the remaining fea-sible labels.
The conflict resolution step ensures thatin expectation the realized benefit is at least a halfof the maximum, thereby reducing ??
by at least a109(1 ?
1/2ck) fraction.
Therefore, after O(kc logm)operations all of the labels are covered.4.3 Fitting the Model Using EMOnce the greedy algorithm terminates and returns aminimized grammar of tag bigrams, we follow theapproach of Ravi and Knight (2009) and fit the min-imized model to the data using the alternating EMstrategy.In this step, we run an alternating optimizationprocedure iteratively in phases.
In each phase,we initialize (and prune away) parameters withinthe two HMM components (transition or emissionmodel) using the output from the previous phase.We initialize this procedure by restricting the tran-sition parameters to only those tag bigrams selectedin the model minimization step.
We train in con-junction with the original emission model using EMalgorithm which prunes away some of the emissionparameters.
In the next phase, we alternate the ini-tialization by choosing the pruned emission modelalong with the original transition model (with fullset of tag bigrams) and retrain using EM.
The alter-nating EM iterations are terminated when the changein the size of the observed grammar (i.e., the numberof unique bigrams in the tagging output) is ?
5%.1We refer to our entire approach using greedy mini-mization followed by EM training as DMLC + EM.5 Distributed ImplementationThe DMLC algorithm is directly suited towardsparallelization across many machines.
We turn toPregel (Malewicz et al., 2010), and its open sourceversion Giraph (Apa, 2013).
In these systems thecomputation proceeds in rounds.
In every round, ev-ery machine does some local processing and thensends arbitrary messages to other machines.
Se-mantically, we think of the communication graph asfixed, and in each round each vertex performs somelocal computation and then sends messages to itsneighbors.
This mode of parallel programming di-rects the programmers to ?Think like a vertex.
?The specific systems like Pregel and Giraph buildinfrastructure that ensures that the overall system1For more details on the alternating EM strategy and howinitialization with minimized models improve EM performancein alternating iterations, refer to (Ravi and Knight, 2009).is fault tolerant, efficient, and fast.
In addition,they provide implementation of commonly used dis-tributed data structures, such as, for example globalcounters.
The programmer?s job is simply to specifythe code that each vertex will run at every round.We implemented the DMLC algorithm in Pregel.The implementation is straightforward and given inAlgorithm 2.
The multi-set M of Algorithm 1 isrepresented as a global counter in Algorithm 2.
Themessage passing (Step 3) and counter update (Step4) steps update this global counter and hence per-form the role of Step 3 of Algorithm 1.
Step 5 se-lects the label with largest count, which is equivalentto the greedy label picking step 6 of Algorithm 1.
Fi-nally steps 6, 7, and 8 update the tag assignment ofeach vertex performing the roles of steps 7, 8, and 9,respectively, of Algorithm 1.5.1 Speeding up the AlgorithmThe implementation described above directly copiesthe sequential algorithm.
Here we describe addi-tional steps we took to further improve the parallelrunning times.Singleton Sets: As the parallel algorithm pro-ceeds, the set of feasible sets associated with a nodeslowly decreases.
At some point there is only onetag that a node can take on, however this tag is rare,and so it takes a while for it to be selected using thegreedy strategy.
Nevertheless, if a node and one ofits neighbors have only a single tag left, then it issafe to assign the unique label 2.Modifying the Graph: As is often the case, thebottleneck in parallel computations is the commu-nication.
To reduce the amount of communicationwe reduce the graph on the fly, removing nodes andedges once they no longer play a role in the compu-tation.
This simple modification decreases the com-munication time in later rounds as the total size ofthe problem shrinks.6 Experiments and ResultsIn this Section, we describe the experimental setupfor various tasks, settings and compare empiricalperformance of our method against several existing2We must judiciously initialize the global counter to takecare of this assignment, but this is easily accomplished.110baselines.
The performance results for all systems(on all tasks) are measured in terms of tagging accu-racy, i.e.
% of tokens from the test corpus that werelabeled correctly by the system.6.1 Part-of-Speech Tagging Task6.1.1 Tagging Using a Complete DictionaryData: We use a standard test set (consisting of24,115 word tokens from the Penn Treebank) forthe POS tagging task.
The tagset consists of 45 dis-tinct tag labels and the dictionary contains 57,388word/tag pairs derived from the entire Penn Tree-bank.
Per-token ambiguity for the test data is about1.5 tags/token.
In addition to the standard 24kdataset, we also train and test on larger data sets?973k tokens from the Penn Treebank, 3M tokensfrom PTB+Europarl (Koehn, 2005) data.Methods: We evaluate and compare performancefor POS tagging using four different methods thatemploy the model minimization idea combined withEM training:?
EM: Training a bigram HMM model using EMalgorithm (Merialdo, 1994).?
ILP + EM: Minimizing grammar size usinginteger linear programming, followed by EMtraining (Ravi and Knight, 2009).?
MIN-GREEDY + EM: Minimizing grammarsize using the two-step greedy method (Ravi etal., 2010b).?
DMLC + EM: This work.Results: Table 1 shows the results for POS tag-ging on English Penn Treebank data.
On the smallertest datasets, all of the model minimization strate-gies (methods 2, 3, 4) tend to perform equally well,yielding state-of-the-art results and large improve-ment over standard EM.
When training (and testing)on larger corpora sizes, DMLC yields the best re-ported performance on this task to date.
A majoradvantage of the new method is that it can easilyscale to large corpora sizes and the distributed na-ture of the algorithm still permits fast, efficient op-timization of the global objective function.
So, un-like the earlier methods (such as MIN-GREEDY) itis fast enough to run on several millions of tokensto yield additional performance gains (shown in lastcolumn).Speedups: We also observe a significant speedupwhen using the parallelized version of the DMLCalgorithm.
Performing model minimization on the24k tokens dataset takes 55 seconds on a single ma-chine, whereas parallelization permits model mini-mization to be feasible even on large datasets.
Fig 1shows the running time for DMLC when run on acluster of 100 machines.
We vary the input datasize from 1M word tokens to about 8M word tokens,while holding the resources constant.
Both the algo-rithm and its distributed implementation in DMLCare linear time operations as evident by the plot.In fact, for comparison, we also plot a straight linepassing through the first two runtimes.
The straightline essentially plots runtimes corresponding to alinear speedup.
DMLC clearly achieves better run-times showing even better than linear speedup.
Thereason for this is that distributed version has a con-stant overhead for initialization, independent of thedata size.
While the running time for rest of the im-plementation is linear in data size.
Thus, as the datasize becomes larger, the constant overhead becomesless significant, and the distributed implementationappears to complete slightly faster as data size in-creases.Figure 1: Runtime vs. data size (measured in # of wordtokens) on 100 machines.
For comparison, we also plot astraight line passing through the first two runtimes.
Thestraight line essentially plots runtimes corresponding to alinear speedup.
DMLC clearly achieves better runtimesshowing a better than linear speedup.6.1.2 Tagging Using Incomplete DictionariesWe also evaluate our approach for POS taggingunder other resource-constrained scenarios.
Obtain-111Method Tagging accuracy (%)te=24k te=973ktr=24k tr=973k tr=3.7M1.
EM 81.7 82.32.
ILP + EM (Ravi and Knight, 2009) 91.63.
MIN-GREEDY + EM (Ravi et al., 2010b) 91.6 87.14.
DMLC + EM (this work) 91.4 87.5 87.8Table 1: Results for unsupervised part-of-speech tagging on English Penn Treebank dataset.
Tagging accuracies fordifferent methods are shown on multiple datasets.
te shows the size (number of tokens) in the test data, tr representsthe size of the raw text used to perform model minimization.ing a complete dictionary is often difficult, espe-cially for new domains.
To verify the utility of ourmethod when the input dictionary is incomplete, weevaluate against standard datasets used in previouswork (Garrette and Baldridge, 2012) and compareagainst the previous best reported performance forthe same task.
In all the experiments (describedhere and in subsequent sections), we use the fol-lowing terminology?raw data refers to unlabeledtext used by different methods (for model minimiza-tion or other unsupervised training procedures suchas EM), dictionary consists of word/tag entries thatare legal, and test refers to data over which taggingevaluation is performed.English Data: For English POS tagging with in-complete dictionary, we evaluate on the Penn Tree-bank (Marcus et al., 1993) data.
Following (Garretteand Baldridge, 2012), we extracted a word-tag dic-tionary from sections 00-15 (751,059 tokens) con-sisting of 39,087 word types, 45,331 word/tag en-tries, a per-type ambiguity of 1.16 yielding a per-token ambiguity of 2.21 on the raw corpus (treatingunknown words as having all 45 possible tags).
Asin their setup, we then use the first 47,996 tokensof section 16 as raw data and perform final evalua-tion on the sections 22-24.
We use the raw corpusalong with the unlabeled test data to perform modelminimization and EM training.
Unknown words areallowed to have all possible tags in both these pro-cedures.Italian Data: The minimization strategy pre-sented here is a general-purpose method that doesnot require any specific tuning and works for otherlanguages as well.
To demonstrate this, we also per-form evaluation on a different language (Italian) us-ing the TUT corpus (Bosco et al., 2000).
Follow-ing (Garrette and Baldridge, 2012), we use the samedata splits as their setting.
We take the first half ofeach of the five sections to build the word-tag dic-tionary, the next quarter as raw data and the lastquarter as test data.
The dictionary was constructedfrom 41,000 tokens comprised of 7,814 word types,8,370 word/tag pairs, per-type ambiguity of 1.07 anda per-token ambiguity of 1.41 on the raw data.
Theraw data consisted of 18,574 tokens and the test con-tained 18,763 tokens.
We use the unlabeled corpusfrom the raw and test data to perform model mini-mization followed by unsupervised EM training.Other Languages: In order to test the effective-ness of our method in other non-English settings, wealso report the performance of our method on sev-eral other Indo-European languages using treebankdata from CoNLL-X and CoNLL-2007 shared taskson dependency parsing (Buchholz and Marsi, 2006;Nivre et al., 2007).
The corpus statistics for the fivelanguages (Danish, Greek, Italian, Portuguese andSpanish) are listed below.
For each language, weconstruct a dictionary from the raw training data.The unlabeled corpus from the raw training and testdata is used to perform model minimization fol-lowed by unsupervised EM training.
As before, un-known words are allowed to have all possible tags.We report the final tagging performance on the testdata and compare it to baseline EM.Garrette and Baldridge (2012) treat unknownwords (words that appear in the raw text but aremissing from the dictionary) in a special manner anduse several heuristics to perform better initializationfor such words (for example, the probability that anunknown word is associated with a particular tag is112conditioned on the openness of the tag).
They alsouse an auto-supervision technique to smooth countslearnt from EM onto new words encountered dur-ing testing.
In contrast, we do not apply any suchtechnique for unknown words and allow them to bemapped uniformly to all possible tags in the dictio-nary.
For this particular set of experiments, the onlydifference from the Garrette and Baldridge (2012)setup is that we include unlabeled text from the testdata (but without any dictionary tag labels or specialheuristics) to our existing word tokens from raw textfor performing model minimization.
This is a stan-dard practice used in unsupervised training scenar-ios (for example, Bayesian inference methods) andin general for scalable techniques where the goal isto perform inference on the same data for which onewishes to produce some structured prediction.Language Train Dict Test(tokens) (entries) (tokens)DANISH 94386 18797 5852GREEK 65419 12894 4804ITALIAN 71199 14934 5096PORTUGUESE 206678 30053 5867SPANISH 89334 17176 5694Results: Table 2 (column 2) compares previouslyreported results against our approach for English.We observe that our method obtains a huge improve-ment over standard EM and gets comparable resultsto the previous best reported scores for the same taskfrom (Garrette and Baldridge, 2012).
It is encourag-ing to note that the new system achieves this per-formance without using any of the carefully-chosenheuristics employed by the previous method.
How-ever, we do note that some of these techniques canbe easily combined with our method to produce fur-ther improvements.Table 2 (column 3) also shows results on Ital-ian POS tagging.
We observe that our methodachieves significant improvements in tagging accu-racy over all the baseline systems including the pre-vious best system (+2.9%).
This demonstrates thatthe method generalizes well to other languages andproduces consistent tagging improvements over ex-isting methods for the same task.Results for POS tagging on CoNLL data in fivedifferent languages are displayed in Figure 2.
Notethat the proportion of raw data in test versus train5060708090DANISH GREEK ITALIAN PORTUGUESE SPANISH79.466.384.680.183.177.865.68278.581.3EM DMLC+EMFigure 2: Part-of-Speech tagging accuracy for differentlanguages on CoNLL data using incomplete dictionaries.
(from the standard CoNLL shared tasks) is muchsmaller compared to the earlier experimental set-tings.
In general, we observe that adding more rawdata for EM training improves the tagging quality(same trend observed earlier in Table 1: column 2versus column 3).
Despite this, DMLC + EM stillachieves significant improvements over the baselineEM system on multiple languages (as shown in Fig-ure 2).
An additional advantage of the new methodis that it can easily scale to larger corpora and it pro-duces a much more compact grammar that can beefficiently incorporated for EM training.6.1.3 Tagging for Low-Resource LanguagesLearning part-of-speech taggers for severely low-resource languages (e.g., Malagasy) is very chal-lenging.
In addition to scarce (token-supervised)labeled resources, the tag dictionaries avail-able for training taggers are tiny compared toother languages such as English.
Garrette andBaldridge (2013) combine various supervised andsemi-supervised learning algorithms into a commonPOS tagger training pipeline to address some ofthese challenges.
They also report tagging accuracyimprovements on low-resource languages when us-ing the combined system over any single algorithm.Their system has four main parts, in order: (1) Tagdictionary expansion using label propagation algo-rithm, (2) Weighted model minimization, (3) Ex-pectation maximization (EM) training of HMMs us-ing auto-supervision, (4) MaxEnt Markov Model(MEMM) training.
The entire procedure results ina trained tagger model that can then be applied totag any raw data.3 Step 2 in this procedure involves3For more details, refer (Garrette and Baldridge, 2013).113Method Tagging accuracy (%)English (PTB 00-15) Italian (TUT)1.
Random 63.53 62.812.
EM 69.20 60.703.
Type-supervision + HMM initialization (Garrette and Baldridge, 2012) 88.52 72.864.
DMLC + EM (this work) 88.11 75.79Table 2: Part-of-Speech tagging accuracy using PTB sections 00-15 and TUT to build the tag dictionary.
For compar-ison, we also include the results for the previously reported state-of-the-art system (method 3) for the same task.Method Tagging accuracy (%)Total Known UnknownLow-resource tagging using (Garrette and Baldridge, 2013) 80.7 (70.2) 87.6 (90.3) 66.1 (45.1)Low-resource tagging using DMLC + EM (this work) 81.1 (70.8) 87.9 (90.3) 66.7 (46.5)Table 3: Part-of-Speech tagging accuracy for a low-resource language (Malagasy) on All/Known/Unknown tokens inthe test data.
Tagging performance is shown for multiple experiments using different (incomplete) dictionary sizes:(a) small, (b) tiny (shown in parentheses).
The new method (row 2) significantly outperforms the existing method withp < 0.01 for small dictionary and p < 0.05 for tiny dictionary.a weighted version of model minimization whichuses the multi-step greedy approach from Ravi etal.
(2010b) enhanced with additional heuristics thatuses tag weights learnt via label propagation (in Step1) within the minimization process.We replace the model minimization procedure intheir Step 2 with our method (DMLC + EM) and di-rectly compare this new system with their approachin terms of tagging accuracy.
Note for all other stepsin the pipeline we follow the same procedure (andrun the same code) as Garrette and Baldridge (2013),including the same smoothing procedure for EM ini-tialization in Step 3.Data: We use the exact same setup as Garretteand Baldridge (2013) and run experiments on Mala-gasy, an Austronesian language spoken in Madagas-car.
We use the publicly available data4: 100k rawtokens for training, a word-tag dictionary acquiredwith 4 hours of human annotation effort (used fortype-supervision), and a held-out test dataset (5341tokens).
We provide the unlabeled corpus from theraw training data along with the word-tag dictionaryas input to model minimization and evaluate on thetest corpus.
We run multiple experiments for dif-ferent (incomplete) dictionary scenarios: (a) small =2773 word/tag pairs, (b) tiny = 329 word/tag pairs.Results: Table 3 shows results on Malagasydata comparing a system that employs (unweighted)4github.com/ dhgarrette/low-resource-pos-tagging-2013DMLC against the existing state-of-the-art systemthat incorporates a multi-step weighted model min-imization combined with additional heuristics.
Weobserve that switching to the new model minimiza-tion procedure alone yields significant improvementin tagging accuracy under both dictionary scenarios.It is encouraging that a better minimization proce-dure also leads to higher tagging quality on the un-known word tokens (column 4 in the table), evenwhen the input dictionary is tiny.6.2 SupertaggingCompared to POS tagging, a more challenging taskis learning supertaggers for lexicalized grammarformalisms such as Combinatory Categorial Gram-mar (CCG) (Steedman, 2000).
For example, CCG-bank (Hockenmaier and Steedman, 2007) contains1241 distinct supertags (lexical categories) and themost ambiguous word has 126 supertags.
This pro-vides a much more challenging starting point forthe semi-supervised methods typically applied tothe task.
Yet, this is an important task since cre-ating grammars and resources for CCG parsers fornew domains and languages is highly labor- andknowledge-intensive.As described earlier, our approach scales easily tolarge datasets as well as label sizes.
To evaluate it onthe supertagging task, we use the same dataset from(Ravi et al., 2010a) and compare against their base-line method that uses an modified (two-step) version114Method Supertagging accuracy (%)Ambiguous Total1.
EM 38.7 45.62.
ILP?
+ EM (Ravi et al., 2010a) 52.1 57.33.
DMLC + EM (this work) 55.9 59.3Table 4: Results for unsupervised supertagging with a dictionary.
Here, we report the total accuracy as well asaccuracy on just the ambiguous tokens (i.e., tokens which have more than one tagging possibility).
?The baselinemethod 2 requires several pre-processing steps in order to run feasibly for this task (described in Section 6.2).
Incontrast, the new approach (DMLC) runs fast and also permits efficient parallelization.of the ILP formulation for model minimization.Data: We use the CCGbank data for this ex-periment.
This data was created by semi- auto-matically converting the Penn Treebank to CCGderivations (Hockenmaier and Steedman, 2007).
Weuse the standard splits of the data used in semi-supervised tagging experiments (Banko and Moore,2004)?sections 0-18 for training (i.e., to constructthe word-tag dictionary), and sections 22-24 for test.Results: Table 4 compares the results for twobaseline systems?standard EM (method 1), and apreviously reported system using model minimiza-tion (method 2) for the same task.
We observethat DMLC produces better taggings than either ofthese and yields significant improvement in accu-racy (+2% overall, +3.8% on ambiguous tokens).Note that it is not feasible to run the ILP-basedbaseline (method 2 in the table) directly since it isvery slow in practice, so Ravi et al.
(2010a) usea set of pre-processing steps to prune the originalgrammar size (unique tag pairs) from >1M to sev-eral thousand entries followed by a modified two-step ILP minimization strategy.
This is required topermit their model minimization step to be run ina feasible manner.
On the other hand, the new ap-proach DMLC (method 3) scales better even whenthe data/label sizes are large, hence it can be run withthe full data using the original model minimizationformulation (rather than a two-step heuristic).Ravi et al.
(2010a) also report further improve-ments using an alternative approach involving anILP-based weighted minimization procedure.
InSection 7 we briefly discuss how the DMLC methodcan be extended to this setting and combined withother similar methods.7 Discussion and ConclusionWe present a fast, efficient model minimizationalgorithm for unsupervised tagging that improvesupon previous two-step heuristics.
We show that un-der a fairly natural assumption of c-feasibility thesolution obtained by our minimization algorithm isO(c logm)-approximate to the optimal.
Althoughin the case of two-step heuristics, the first step guar-antees an O(logm)-approximation, the second step,which is required to get a consistent solution, canintroduce many additional labels resulting in a so-lution arbitrarily away from the optimal.
Our onestep approach ensures consistency at each step of thealgorithm, while the c-feasibility assumption meansthat the solution does not diverge too much from theoptimal in each iteration.In addition to proving approximation guaranteesfor the new algorithm, we show that it is paralleliz-able, allowing us to easily scale to larger datasetsthan previously explored.
Our results show thatthe algorithm achieves state-of-the-art performance,outperforming existing methods on several differ-ent tasks (both POS tagging and supertagging) andworks well even with incomplete dictionaries andextremely low-resource languages like Malagasy.For future work, it would be interesting to apply aweighted version of the DMLC algorithm where la-bels (i.e., tag pairs) can have different weight distri-butions instead of uniform weights.
Our algorithmcan be extended to allow an input weight distribu-tion to be specified for minimization.
In order toinitialize the weights we could use existing strate-gies such as grammar-informed initialization (Raviet al., 2010a) or output distributions learnt via othermethods such as label propagation (Garrette andBaldridge, 2013).115References2013.
Apache giraph.
http://giraph.apache.org/.Michele Banko and Robert C. Moore.
2004.
Part-of-speech tagging in context.
In Proceedings of COLING,pages 556?561.Andrew R Barron, Jorma Rissanen, and Bin Yu.
1998.The Minimum Description Length Principle in Cod-ing and Modeling.
IEEE Transactions of InformationTheory, 44(6):2743?2760.Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,and Leonardo Lesmo.
2000.
Building a Treebank forItalian: a data-driven annotation schema.
In Proceed-ings of the Second International Conference on Lan-guage Resources and Evaluation LREC-2000, pages99?105.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of CoNLL, pages 149?164.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedPOS induction: How far have we come?
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 575?584.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1, pages 600?609.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of MachineLearning Research, 11:2001?2049.Dan Garrette and Jason Baldridge.
2012.
Type-supervised Hidden Markov Models for part-of-speechtagging with incomplete tag dictionaries.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 821?831.Dan Garrette and Jason Baldridge.
2013.
Learning apart-of-speech tagger from two hours of annotation.
InProceedings of the Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 138?147.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging forTwitter: annotation, features, and experiments.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies: short papers - Volume 2, pages 42?47.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.EM can find pretty good HMM POS-taggers (whengiven a good start).
In Proceedings of ACL, pages746?754.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In ACL.Fernando C. Gomes, Cludio N. Meneses, Panos M.Pardalos, and Gerardo Valdisio R. Viana.
2006.
Ex-perimental analysis of approximation algorithms forthe vertex cover and set covering problems.Kazi Saidul Hasan and Vincent Ng.
2009.
Weakly super-vised part-of-speech tagging for morphologically-rich,resource-scarce languages.
In Proceedings of the 12thConference on the European Chapter of the Associa-tion for Computational Linguistics, pages 363?371.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proceedings of the Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 296?305.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Machine Transla-tion Summit X, pages 79?86.Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik,James C. Dehnert, Ilan Horn, Naty Leiser, and Grze-gorz Czajkowski.
2010.
Pregel: a system for large-scale graph processing.
In Proceedings of the 2010ACM SIGMOD International Conference on Manage-ment of data, pages 135?146.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?171.Taesun Moon, Katrin Erk, and Jason Baldridge.
2010.Crouching Dirichlet, Hidden Markov Model: Unsu-pervised POS tagging with context local tag genera-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 196?206.116Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL, pages 915?932.Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proceed-ings of the Joint Conferenceof the 47th Annual Meet-ing of the Association for Computational Linguisticsand the 4th International Joint Conference on NaturalLanguage Processing of the Asian Federation of Natu-ral Language Processing (ACL-IJCNLP), pages 504?512.Sujith Ravi, Jason Baldridge, and Kevin Knight.
2010a.Minimized models and grammar-informed initializa-tion for supertagging with highly ambiguous lexicons.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages495?503.Sujith Ravi, Ashish Vaswani, Kevin Knight, and DavidChiang.
2010b.
Fast, greedy model minimization forunsupervised tagging.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(COLING), pages 940?948.Roi Reichart, Raanan Fattal, and Ari Rappoport.
2010.Improved unsupervised POS induction using intrinsicclustering quality and a Zipfian constraint.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning, pages 57?66.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA, USA.Kristina Toutanova and Mark Johnson.
2008.
ABayesian LDA-based model for semi-supervised part-of-speech tagging.
In Advances in Neural InformationProcessing Systems (NIPS), pages 1521?1528.117118
