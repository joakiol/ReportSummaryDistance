First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571?574,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsJU_CSE_NLP: Multi-grade Classification of Semantic SimilarityBetween Text PairsSnehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1            Alexander Gelbukh1Computer Science & Engineering Department                        Center for Computing ResearchJadavpur University, Kolkata, India                                 National Polytechnic Institute2Computer Science & Engineering Department                               Mexico City, MexicoJadavpur University, Kolkata, India                                 gelbukh@gelbukh.comIntern at Xerox Research Centre EuropeGrenoble, France{snehasis1981,parthapakray}@gmail.comsbandyopadhyay@cse.jdvu.ac.inAbstractThis article presents the experiments car-ried out at Jadavpur University as part ofthe participation in Semantic Textual Si-milarity (STS) of Task 6 @ SemanticEvaluation Exercises (SemEval-2012).Task-6 of SemEval- 2012 focused on se-mantic relations of text pair.
Task-6 pro-vides five different text pair files tocompare different semantic relations andjudge these relations through a similarityand confidence score.
Similarity score isone kind of multi way classification in theform of grade between 0 to 5.
We havesubmitted one run for the STS task.
Oursystem has two basic modules - one dealswith lexical relations and another dealswith dependency based syntactic relationsof the text pair.
Similarity score given to apair is the average of the scores of theabove-mentioned modules.
The scoresfrom each module are identified using rulebased techniques.
The Pearson Correlationof our system in the task is 0.3880.1 IntroductionTask-61 [1] of SemEval-2012 deals with seman-tic similarity of text pairs.
The task is to find thesimilarity between the sentences in the text pair(s1 and s2) and return a similarity score and anoptional confidence score.
There are five datasets1 http://www.cs.york.ac.uk/semeval-2012/task6/in the test data and with tab separated text pairs.The datasets are as follows:?
MSR-Paraphrase, Microsoft Research Pa-raphrase Corpus (750 pairs of sentences.)?
MSR-Video, Microsoft Research Video De-scription Corpus (750 pairs of sentences.)?
SMTeuroparl: WMT2008 development data-set (Europarl section) (459 pairs of sen-tences.)?
SMTnews: news conversation sentence pairsfrom WMT.
(399 pairs of sentences.)?
OnWN: pairs of sentences where the firstcomes from Ontonotes and the second from aWordNet definition.
(750 pairs of sentences.
)Similarity score ranges from 0 to 5 and confi-dence score from 0 to 100.
An s1-s2 pair gets asimilarity score of 5 if they are completelyequivalent.
Similarity score 4 is allocated formostly equivalent s1-s2 pair.
Similarly, score 3 isallocated for roughly equivalent pair.
Score 2, 1and 0 are allocated for non-equivalent detailssharing, non-equivalent topic sharing and totallydifferent pairs respectively.
Major challenge ofthis task is to find the similarity score based simi-larity for the text pair.
Generally text entailmenttasks refer whether sentence pairs are entailed ornot: binary classification (YES, NO) [2] or multi-classification (Forward, Backward, bidirectionalor no entailment) [3][4].
But multi grade classifi-cation of semantic similarity assigns a score tothe sentence pair.
Our system considers lexicaland dependency based syntactic measures forsemantic similarity.
Similarity scores are the ba-sic average of these module scores.
A subsequent571section describes the system architecture.
Section2 describes JU_NLP_CSE system for STS task.Section 3 describes evaluation and experimentalresults.
Conclusions are drawn in Section 4.2 System ArchitectureThe system of Semantic textual similarity taskhas two main modules: one is lexical module andanother one is dependency parsing based syntac-tic module.
Both these module have some pre-processing tasks such as stop word removal, co-reference resolution and dependency parsing etc.Figure 1 displays the architecture of the system.Figure 1: System Architecture2.1 Pre-processing ModuleThe system separates the s1-s2 sentence pairscontained in the different STS task datasets.These separated pairs are then passed through thefollowing sub modules:i.
Stop word Removal: Stop words are removedfrom s1 - s2 sentence pairs.ii.
Co-reference: Co-reference resolutions arecarried out on the datasets before passing throughthe TE module.
The objective is to increase thescore of the entailment percentage.
A word orphrase in the sentence is used to refer to an entityintroduced earlier or later in the discourse andboth having same things then they have the samereferent or co reference.
When the reader mustlook back to the previous context, reference iscalled "Anaphoric Reference".
When the readermust look forward, it is termed "Cataphoric Ref-erence".
To address this problem we used a toolcalled JavaRAP2 (A java based implementationof Anaphora Procedure (RAP) - an algorithm byLappin and Leass (1994)).iii.
Dependency Parsing: Separated s1 ?
s2 sen-tences are parsed using Stanford dependencyparser3 to produce the dependency relations inthe texts.
These dependency relations are usedfor WordNet based syntactic matching.2.2 Lexical Matching ModuleIn this module the TE system calculates differentmatching scores such as N ?
Gram match, TextSimilarity, Chunk match, Named Entity matchand POS match.i.
N-Gram Match module: The N-Gram matchbasically measures the percentage match of theunigram, bigram and trigram of hypothesispresent in the corresponding text.
These scoresare simply combined to get an overall N ?
Grammatching score for a particular pair.ii.
Chunk Match module: In this sub moduleour system evaluates the key NP-chunks of bothtext (s1) and hypothesis (s2) using NP Chunkerv1.13 (The University of Sheffield).
The hypo-thesis NP chunks are matched in the text NPchunks.
System calculates an overall value forthe chunk matching, i.e., number of text NPchunks that match the hypothesis NP chunks.
Ifthe chunks are not similar in their surface formthen our system goes for wordnet synonymsmatching for the words and if they match inwordnet synsets information, it will be encoun-tered as a similar chunk.
WordNet [5] is one ofmost important resource for lexical analysis.
TheWordNet 2.0 has been used for WordNet basedchunk matching.
The API for WordNet Search-ing (JAWS)4 is an API that provides Java appli-cations with the ability to retrieve data from theWordNet synsets.iii.
Text Similarity Module: System takes intoconsideration several text similarities calculated2 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html3 http://www.dcs.shef.ac.uk/~mark/phd/software/4 http://lyle.smu.edu/~tspell/jaws/index.html572over the s1-s2 pair.
These text similarity valuesare summed up to produce a total score for a par-ticular s1-s2 pair.
Major Text similarity measuresthat our system considers are:?
Cosine Similarity?
Lavenstine Distance?
Euclidean Distance?
MongeElkan Distance?
NeedlemanWunch Distance?
SmithWaterman Distance?
Block Distance?
Jaro Similarity?
MatchingCoefficient Distance?
Dice Similarity?
OverlapCoefficient?
QGrams Distanceiv.
Named Entity Matching: It is based on thedetection and matching of Named Entities in thes1-s2 pair.
Stanford Named Entity Recognizer5 isused to tag the named entities in both s1 and s2.System simply maps the number of hypothesis(s2) NEs present in the text (s1).
A score is allo-cated for the matching.NE_match = (Number of common NEs in Textand Hypothesis) / (Number of NE in Hypothesis).v.
Part ?of ?
Speech (POS) Matching: Thismodule basically deals with matching the com-mon POS tags between s1 and s2 sentences.Stanford POS tagger6 is used to tag the part ofspeech in both s1 and s2.
System matches theverb and noun POS words in the hypothesis thatmatch in the text.
A score is allocated based onthe number of POS matching.POS_match = (Number of common verb andnoun POS in Text and Hypothesis) / (Total num-ber of verb and noun POS in hypothesis).System calculates the sum of the entire sub mod-ule (modules described in section 2.2) scores andforms a single percentage score for the lexicalmatching.
This score is then compared with somepredetermined threshold value to assign a finallexical score for each pair.
If percentage value is5 http://nlp.stanford.edu/software/CRF-NER.shtml6 http://nlp.stanford.edu/software/tagger.shtmlabove 0.80 then lexical score 5 is allocated.
If thevalue is between 0.60 to 0.80 then lexical score 4is allocated.
Similarly, lexical score 3 is allocatedfor percentage score of 0.40 to 0.60 and so on.One lexical score is finally generated for eachtext pair.2.3.
Syntactic Matching Module:TE system considers the preprocessed dependen-cy parsed text pairs (s1 ?
s2) and goes for wordnet based matching technique.
After parsing thesentences, they have some attributes like subject,object, verb, auxiliaries and prepositions taggedby the dependency parser tag set.
System usesthese attributes for the matching procedure anddepending on the nature of matching a score isallocated to the s1-s2 pair.
Matching procedure isbasically done through comparison of the follow-ing features that are present in both the text andthe hypothesis.?
Subject ?
Subject comparison.?
Verb ?
Verb Comparison.?
Subject ?
Verbs Comparison.?
Object ?
Object Comparison.?
Cross Subject ?
Object Comparison.?
Object ?
Verbs Comparison.?
Prepositional phrase comparison.Each of these comparisons produces one match-ing score for the s1-s2 pair that are finally com-bined with previously generated lexical score togenerate the final similarity score by taking sim-ple average of lexical and syntactic matchingscores.
The basic heuristics are as follows:(i) If the feature of the text (s1) directly matchesthe same feature of the hypothesis (s2), matchingscore 5 is allocated for the text pair.
(ii) If the feature of either text (s1) or hypothesis(s2) matches with the wordnet synsets of the cor-responding text (s1) or hypothesis (s2), matchingscore 4 is allocated.
(iii) If wordnet synsets of the feature of the text(s1) match with one of the synsets of the featureof the hypothesis (s2), matching score 3 is givento the pair.
(iv) If wordnet synsets of the feature of eithertext (s1) or hypothesis (s2) match with the syn-sets of the corresponding text (s1) or hypothesis(s2) then matching score 2 is allocated for thepair.573(v) Similarly if in both the cases match occurs inthe second level of wordnet synsets, matchingscore 1is allocated.
(vi) Matching score 0 is allocated for the pairhaving no match in their features.After execution of the module, system generatessome scores.
Lexical module generates one lexi-cal score and wordnet based syntactic matchingmodule generates seven matching scores.
At thefinal stage of the system all these scores arecombined and the mean is evaluated on thiscombined score.
This mean gives the similarityscore for a particular s1-s2 pair of different data-sets of STS task.
Optional confidence score isalso allocated which is basically the similarityscore multiplied by 10, i.e., if the similarity scoreis 5.22, the confidence score will be 52.2.3.
Experiments on Dataset and ResultWe have submitted one run in SemEval-2012Task 6.
The results for Run on STS Test set areshown in Table 1.task6-JU_CSE_NLP-Semantic_Syntactic_ApproachCorrelationsALL    0.3880ALLnrm 0.6706Mean 0.4111MSRpar  0.3427MSRvid 0.3549SMT-eur 0.4271On-WN 0.5298SMT-news 0.4034Table 1: Results of Test SetALL: Pearson correlation with the gold standardfor the five datasets and the corresponding rank82.ALLnrm: Pearson correlation after the systemoutputs for each dataset are fitted to the goldstandard using least squares and the correspond-ing rank 86.Mean: Weighted mean across the 5 datasets,where the weight depends on the number of pairsin the dataset and the corresponding rank 76.The subsequent rows show the pearson correla-tion scores for each of the individual datasets.4.
ConclusionOur JU_CSE_NLP system for the STS taskmainly focus on lexical and syntactic approaches.There are some limitations in the lexical match-ing module that shows a correlation that is nothigher in the range.
In case of simple sentenceslexical matching is helpful for entailment but forcomplex and compound sentences the lexicalmatching module loses its accuracy.
Semanticgraph matching or conceptual graph implementa-tion can improve the system.
That is not consi-dered in our present work.
Machine learningtools can be used to learn the system based on thefeatures.
It can also improve the correlation.
Infuture work our system will include semanticgraph matching and a machine-learning module.AcknowledgmentsThe work was done under support of the DSTIndia-CONACYT Mexico project ?Answer Vali-dation through Textual Entailment?
funded byDST, Government of India.References[1] Eneko Agirre, Daniel Cer, Mona Diab and AitorGonzalez.
SemEval-2012 Task 6: A Pilot on Se-mantic Textual Similarity.
In Proceedings of the6th International Workshop on Semantic Evalua-tion (SemEval 2012), in conjunction with the FirstJoint Conference on Lexical and ComputationalSemantics (*SEM 2012).
(2012)[2] Dagan, I., Glickman, O., Magnini, B.: ThePASCAL Recognising Textual Entailment Chal-lenge.
Proceedings of the First PASCAL Recogniz-ing Textual Entailment Workshop.
(2005).
[3] H. Shima, H. Kanayama, C.-W. Lee, C.-J.
Lin,T.Mitamura, S. S. Y. Miyao, and K. Takeda.
Over-view of ntcir-9 rite: Recognizing inference in text.In NTCIR-9 Proceedings,2011.
[4] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-bukh, A.: A Textual Entailment System using Webbased Machine Translation System.
NTCIR-9, Na-tional Center of Sciences, Tokyo, Japan.
December6-9, 2011.
(2011).
[5] Fellbaum, C.: WordNet: An Electronic LexicalDatabase.
MIT Press (1998).574
