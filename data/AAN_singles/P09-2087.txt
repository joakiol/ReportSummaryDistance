Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 345?348,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPModeling Morphologically Rich Languages Using Split Words andUnstructured DependenciesDeniz YuretKoc?
University34450 Sariyer, Istanbul, Turkeydyuret@ku.edu.trErgun Bic?iciKoc?
University34450 Sariyer, Istanbul, Turkeyebicici@ku.edu.trAbstractWe experiment with splitting words intotheir stem and suffix components for mod-eling morphologically rich languages.
Weshow that using a morphological ana-lyzer and disambiguator results in a sig-nificant perplexity reduction in Turkish.We present flexible n-gram models, Flex-Grams, which assume that the n?1 tokensthat determine the probability of a giventoken can be chosen anywhere in the sen-tence rather than the preceding n?1 posi-tions.
Our final model achieves 27% per-plexity reduction compared to the standardn-gram model.1 IntroductionLanguage models, i.e.
models that assign prob-abilities to sequences of words, have been provenuseful in a variety of applications including speechrecognition and machine translation (Bahl et al,1983; Brown et al, 1990).
More recently, good re-sults on lexical substitution and word sense disam-biguation using language models have also beenreported (Hawker, 2007; Yuret, 2007).
Morpho-logically rich languages pose a challenge to stan-dard modeling techniques because of their rela-tively large out-of-vocabulary rates and the regu-larities they possess at the sub-word level.The standard n-gram language model ignoreslong-distance relationships between words anduses the independence assumption of a Markovchain of order n ?
1.
Morphemes play an im-portant role in the syntactic dependency structurein morphologically rich languages.
The depen-dencies are not only between stems but also be-tween stems and suffixes and if we use completewords as unit tokens, we will not be able to rep-resent these sub-word dependencies.
Our work-ing hypothesis is that the performance of a lan-guage model is correlated by how much the prob-abilistic dependencies mirror the syntactic depen-dencies.
We present flexible n-grams, FlexGrams,in which each token can be conditioned on tokensanywhere in the sentence, not just the precedingn?1 tokens.
We also experiment with words splitinto their stem and suffix forms, and define stem-suffix FlexGrams where one set of offsets is ap-plied to stems and another to suffixes.
We evaluatethe performance of these models on a morpholog-ically rich language, Turkish.2 The FlexGram ModelThe FlexGram model relaxes the contextual as-sumption of n-grams and assumes that the n ?
1tokens that determine the probability of a given to-ken can be chosen anywhere in the sentence ratherthan at the preceding n?
1 positions.
This allowsthe ability to model long-distance relationships be-tween tokens without a predefined left-to-right or-dering and opens the possibility of using differentdependency patterns for different token types.Formal definition An order-n FlexGram modelis specified by a tuple of dependency offsets[d1, d2, .
.
.
, dn?1] and decomposes the probabilityof a given sequence of tokens into a product ofconditional probabilities for every token:p(w1, .
.
.
, wk) =?wi?Sp(wi|wi+d1.
.
.
wi+dn?1)The offsets can be positive or negative and thesame set of offsets is applied to all tokens in thesequence.
In order to represent a properly nor-malized probability model over the set of all finitelength sequences, we check that the offsets of aFlexGram model does not result in a cycle.
Weshow that using differing dependency offsets forstems and suffixes can improve the perplexity.3453 DatasetWe used the Turkish newspaper corpus of Milliyetafter removing sentences with 100 or more tokens.The dataset contains about 600 thousand sentencesin the training set and 60 thousand sentences in thetest set (giving a total of about 10 million words).The versions of the corpus we use developed byusing different word-split strategies along with asample sentence are explained below:1.
The unsplit dataset contains the raw corpus:Kasparov b?ukemedi?gi eli ?opecek(Kasparov is going to kiss the hand he cannot bend)2.
The morfessor dataset was prepared using theMorfessor (Creutz et al, 2007) algorithm:Kasparov b?uke +medi?gi eli ?op +ecek3.
The auto-split dataset is obtained after usingour unsupervised morphological splitter:Kaspar +ov b?uk +emedi?gi eli ?op +ecek4.
The split dataset contains words that are splitinto their stem and suffix forms by using ahighly accurate supervised morphological an-alyzer (Yuret and T?ure, 2006):Kasparov b?uk +yAmA+dHk+sH el +sH ?op+yAcAk5.
The split+0 version is derived from the splitdataset by adding a zero-suffix to any stem thatis not followed by a suffix:Kasparov +0 b?uk +yAmA+dHk+sH el +sH?op +yAcAkSome statistics of the dataset are presented inTable 1.
The vocabulary is taken to be the to-kens that occur more than once in the training setand the OOV column shows the number of out-of-vocabulary tokens in the test set.
The uniqueand 1-count columns give the number of uniquetokens and the number of tokens that only occuronce in the training set.
Approximately 5% of thetokens in the unsplit test set are OOV tokens.
Incomparison, the ratio for a comparably sized En-glish dataset is around 1%.
Splitting the wordsinto stems and suffixes brings the OOV ratio closerto that of English.Model evaluation When comparing languagemodels that tokenize data differently:1.
We take into account the true cost of the OOVtokens using a separate character-based modelsimilar to Brown et al (1992).2.
When reporting averages (perplexity, bits-per-word) we use a common denominator: thenumber of unsplit words.Table 1: Dataset statistics (K for thousands, M for millions)Dataset Train Test OOV Unique 1-countunsplit 8.88M 0.91M 44.8K (4.94%) 430K 206Kmorfessor 9.45M 0.98M 10.3K (1.05%) 167K 34.4Kauto-split 14.3M 1.46M 13.0K (0.89%) 128K 44.8Ksplit 12.8M 1.31M 17.1K (1.31%) 152K 75.4Ksplit+0 17.8M 1.81M 17.1K (0.94%) 152K 75.4K4 ExperimentsIn this section we present a number of experimentsthat demonstrate that when modeling a morpho-logically rich language like Turkish, (i) splittingwords into their stem and suffix forms is beneficialwhen the split is performed using a morphologi-cal analyzer and (ii) allowing the model to choosestem and suffix dependencies separately and flex-ibly results in a perplexity reduction, however thereduction does not offset the cost of zero suffixes.We used the SRILM toolkit (Stolcke, 2002) tosimulate the behavior of FlexGram models by us-ing count files as input.
The interpolated Kneser-Ney smoothing was used in all our experiments.Table 2: Total log probability (M for millions of bits).Split Dataset Unsplit DatasetN Word logp OOV logp Word logp OOV logp1 14.2M 0.81M 11.7M 2.32M2 10.5M 0.64M 9.64M 1.85M3 9.79M 0.56M 9.46M 1.59M4 9.72M 0.53M 9.45M 1.38M5 9.71M 0.51M 9.45M 1.25M6 9.71M 0.50M 9.45M 1.19M4.1 Using a morphological tagger anddisambiguatorThe split version of the corpus contains wordsthat are split into their stem and suffix forms byusing a previously developed morphological an-alyzer (Oflazer, 1994) and morphological disam-biguator (Yuret and T?ure, 2006).
The analyzerproduces all possible parses of a Turkish word us-ing the two-level morphological paradigm and thedisambiguator chooses the best parse based on theanalysis of the context using decision lists.
The in-tegrated system was found to discover the correctmorphological analysis for 96% of the words ona hand annotated out-of-sample test set.
Table 2gives the total log-probability (using log2) for thesplit and unsplit datasets using n-gram modelsof different order.
We compute the perplexityof the two datasets using a common denomina-tor: 2?
log2(p)/Nwhere N=906,172 is taken to bethe number of unsplit tokens.
The best combina-tion (order-6 word model combined with an order-9 letter model) gives a perplexity of 2,465 forthe split dataset and 3,397 for the unsplit dataset,346which corresponds to a 27% improvement.4.2 Separation of stem and suffix modelsOnly 45% of the words in the split dataset havesuffixes.
Each sentence in the split+0 dataset hasa regular [stem suffix stem suffix ...] structure.
Ta-ble 3 gives the average cost of stems and suffixes inthe two datasets for a regular 6-gram word model(ignoring the common OOV words).
The log-probability spent on the zero suffixes in the split+0dataset has to be spent on trying to decide whetherto include a stem or suffix following a stem in thesplit dataset.
As a result the difference in total log-probability between the two datasets is small (only6% perplexity difference).
The set of OOV tokensis the same for both the split and split+0 datasets;therefore we ignore the cost of the OOV tokens asis the default SRILM behavior.Table 3: Total log probability for the 6-gram word modelson split and split+0 data.split dataset split+0 datasettoken number of total number of totaltype tokens ?
log2p tokens ?
log2pstem 0.91M 7.80M 0.91M 7.72Msuffix 0.41M 1.89M 0.41M 1.84M0-suffix ?
?
0.50M 0.21Mall 1.31M 9.69M 1.81M 9.78M4.3 Using the FlexGram modelWe perform a search over the space of dependencyoffsets using the split+0 dataset and considered n-gram orders 2 to 6 and picked the dependency off-sets within a window of 4n + 1 tokens centeredaround the target.
Table 4 gives the best mod-els discovered for stems and suffixes separatelyand compares them to the corresponding regularn-gram models on the split+0 dataset.
The num-bers in parentheses give perplexity and significantreductions can be observed for each n-gram order.Table 4: Regular ngram vs FlexGram models.N ngram-stem ngram-suffix2 -1 (1252) -1 (5.69)3 -2,-1 (418) -2,-1 (5.29)4 -3,-2,-1 (409) -3,-2,-1 (4.79)5 -4,-3,-2,-1 (365) -4,-3,-2,-1 (4.80)6 -5,-4,-3,-2,-1 (367) -5,-4,-3,-2,-1 (4.79)N flexgram-stem flexgram-suffix2 -2 (596) -1 (5.69)3 +1,-2 (289) +1,-1 (4.21)4 +2,+1,-1 (189) -2,+1,-1 (4.19)5 +4,+2,+1,-1 (176) -3,-2,+1,-1 (4.12)6 +4,+3,+2,+1,-1 (172) -4,-3,-2,+1,-1 (4.13)However, some of these models cannot be usedin combination because of cycles as we depict onthe left side of Figure 1 for order 3.
Table 5 givesthe best combined models without cycles.
Wewere able to exhaustively search all the patternsfor orders 2 to 4 and we used beam search for or-ders 5 and 6.
Each model is represented by itsoffset tuple and the resulting perplexity is givenin parentheses.
Compared to the regular n-grammodels from Table 4 we see significant perplexityreductions up to order 4.
The best order-3 stem-suffix FlexGram model can be seen on the rightside of Figure 1.Table 5: Best stem-suffix flexgram model combinations forthe split+0 dataset.N flexgram-stem flexgram-suffix perplexity reduction2 -2 (596) -1 (5.69) 52.3%3 -4,-2 (496) +1,-1 (4.21) 5.58%4 -4,-2,-1 (363) -3,-2,-1 (4.79) 11.3%5 -6,-4,-2,-1 (361) -3,-2,-1 (4.79) 1.29%6 -6,-4,-2,-1 (361) -3,-2,-1 (4.79) 1.52%5 Related workSeveral approaches attempt to relax the rigid or-dering enforced by the standard n-gram model.The skip-gram model (Siu and Ostendorf, Jan2000) allows the skipping of one word within agiven n-gram.
Variable context length languagemodeling (Kneser, 1996) achieves a 10% per-plexity reduction when compared to the trigramsby varying the order of the n-gram model basedon the context.
Dependency models (Rosenfeld,2000) use the parsed dependency structure of sen-tences to build the language model as in grammat-ical trigrams (Lafferty et al, 1992), structured lan-guage models (Chelba and Jelinek, 2000), and de-pendency language models (Chelba et al, 1997).The dependency model governs the whole sen-tence and each word in a sentence is likely to havea different dependency structure whereas in ourexperiments with FlexGrams we use two connec-tivity patterns: one for stems and one for suffixeswithout the need for parsing.6 ContributionsWe have analyzed the effect of word splitting andunstructured dependencies on modeling Turkish, amorphologically complex language.
Table 6 com-pares the models we have tested on our test corpus.We find that splitting words into their stem andsuffix components using a morphological analyzerand disambiguator results in significant perplexityreductions of up to 27%.
FlexGram models out-perform regular n-gram models (Tables 4 and 5)347Figure 1: Two FlexGram models where W represents a stem, s represents a suffix, and the arrows represent dependencies.The left model has stem offsets [+1,-2] and suffix offsets [+1,-1] and cannot be used as a directed graphical model becauseof the cycles.
The right model has stem offsets [-4,-2] and suffix offsets [+1,-1] and is the best order-3 FlexGram model forTurkish.Table 6: Perplexity for compared models.N unsplit split flexgram2 3929 4360 50433 3421 2610 30834 3397 2487 25575 3397 2468 25396 3397 2465 2539when using an alternating stem-suffix representa-tion of the sentences; however Table 6 shows thatthe cost of the alternating stem-suffix representa-tion (zero-suffixes) offsets this gain.ReferencesLalit R. Bahl, Frederick Jelinek, and Robert L.Mercer.
A maximum likelihood approach tocontinuous speech recognition.
IEEE Transac-tions on Pattern Analysis and Machine Intelli-gence, 5(2):179?190, 1983.Peter F. Brown, John Cocke, Stephen A.Della Pietra, Vincent J. Della Pietra, FrederickJelinek, John D. Lafferty, Robert L. Mercer, andPaul S. Roossin.
A statistical approach to ma-chine translation.
Computational Linguistics,16(2):79?85, 1990.Peter F. Brown et al An estimate of an upperbound for the entropy of english.
Computa-tional Linguistics, 18(1):31?40, 1992.Ciprian Chelba and Frederick Jelinek.
Recog-nition performance of a structured languagemodel.
CoRR, cs.CL/0001022, 2000.Ciprian Chelba, David Engle, Frederick Jelinek,Victor M. Jimenez, Sanjeev Khudanpur, LidiaMangu, Harry Printz, Eric Ristad, RonaldRosenfeld, Andreas Stolcke, and Dekai Wu.Structure and performance of a dependency lan-guage model.
In Proc.
Eurospeech ?97, pages2775?2778, Rhodes, Greece, September 1997.Mathias Creutz, Teemu Hirsim?aki, Mikko Ku-rimo, Antti Puurula, Janne Pylkk?onen, VesaSiivola, Matti Varjokallio, Ebru Arisoy, Mu-rat Saraclar, and Andreas Stolcke.
Morph-based speech recognition and modeling of out-of-vocabulary words across languages.
TSLP, 5(1), 2007.Tobias Hawker.
USYD: WSD and lexical substitu-tion using the Web1T corpus.
In SemEval-2007:4th International Workshop on Semantic Evalu-ations, 2007.R.
Kneser.
Statistical language modeling using avariable context length.
In Proc.
ICSLP ?96,volume 1, pages 494?497, Philadelphia, PA,October 1996.John Lafferty, Daniel Sleator, and Davy Tem-perley.
Grammatical trigrams: a probabilisticmodel of link grammar.
In AAAI Fall Sym-posium on Probabilistic Approaches to NLP,1992.Kemal Oflazer.
Two-level description of turkishmorphology.
Literary and Linguistic Comput-ing, 9(2):137?148, 1994.Ronald Rosenfeld.
Two decades of statistical lan-guage modeling: Where do we go from here.In Proceedings of the IEEE, volume 88, pages1270?1278, 2000.Manhung Siu and M. Ostendorf.
Variable n-gramsand extensions for conversational speech lan-guage modeling.
Speech and Audio Processing,IEEE Transactions on, 8(1):63?75, Jan 2000.ISSN 1063-6676. doi: 10.1109/89.817454.Andreas Stolcke.
Srilm ?
an extensible languagemodeling toolkit.
In Proc.
Int.
Conf.
SpokenLanguage Processing (ICSLP 2002), 2002.Deniz Yuret.
KU: Word sense disambiguation bysubstitution.
In SemEval-2007: 4th Interna-tional Workshop on Semantic Evaluations, June2007.Deniz Yuret and Ferhan T?ure.
Learning mor-phological disambiguation rules for turkish.
InHLT-NAACL 06, June 2006.348
