Proceedings of NAACL HLT 2007, pages 492?499,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsEfficient Phrase-table Representation for Machine Translation withApplications to Online MT and Speech TranslationRichard Zens and Hermann NeyHuman Language Technology and Pattern RecognitionLehrstuhl fu?r Informatik 6 ?
Computer Science DepartmentRWTH Aachen University, D-52056 Aachen, Germany{zens,ney}@cs.rwth-aachen.deAbstractIn phrase-based statistical machine transla-tion, the phrase-table requires a large amountof memory.
We will present an efficient repre-sentation with two key properties: on-demandloading and a prefix tree structure for thesource phrases.We will show that this representation scaleswell to large data tasks and that we are ableto store hundreds of millions of phrase pairsin the phrase-table.
For the large Chinese?English NIST task, the memory requirementsof the phrase-table are reduced to less than20MB using the new representation with noloss in translation quality and speed.
Addi-tionally, the new representation is not limitedto a specific test set, which is important foronline or real-time machine translation.One problem in speech translation is thematching of phrases in the input word graphand the phrase-table.
We will describe a novelalgorithm that effectively solves this com-binatorial problem exploiting the prefix treedata structure of the phrase-table.
This algo-rithm enables the use of significantly largerinput word graphs in a more efficient way re-sulting in improved translation quality.1 IntroductionIn phrase-based statistical machine translation, ahuge number of source and target phrase pairsis memorized in the so-called phrase-table.
Formedium sized tasks and phrase lengths, thesephrase-tables already require several GBs of mem-ory or even do not fit at all.
If the source text, whichis to be translated, is known in advance, a commontrick is to filter the phrase-table and keep a phrasepair only if the source phrase occurs in the text.
Thisfiltering is a time-consuming task, as we have togo over the whole phrase-table.
Furthermore, wehave to repeat this filtering step whenever we wantto translate a new source text.To address these problems, we will use an ef-ficient representation of the phrase-table with twokey properties: on-demand loading and a prefix treestructure for the source phrases.
The prefix treestructure exploits the redundancy among the sourcephrases.
Using on-demand loading, we will loadonly a small fraction of the overall phrase-table intomemory.
The majority will remain on disk.The on-demand loading is employed on a per sen-tence basis, i.e.
we load only the phrase pairs thatare required for one sentence into memory.
There-fore, the memory requirements are low, e.g.
less than20MB for the Chin.-Eng.
NIST task.
Another ad-vantage of the on-demand loading is that we are ableto translate new source sentences without filtering.A potential problem is that this on-demand load-ing might be too slow.
To overcome this, we use abinary format which is a memory map of the internalrepresentation used during decoding.
Additionally,we load coherent chunks of the tree structure insteadof individual phrases, i.e.
we have only few disk ac-cess operations.
In our experiments, the on-demandloading is not slower than the traditional approach.As pointed out in (Mathias and Byrne, 2006),one problem in speech translation is that we haveto match the phrases of our phrase-table against aword graph representing the alternative ASR tran-492scriptions.
We will present a phrase matching algo-rithm that effectively solves this combinatorial prob-lem exploiting the prefix tree data structure of thephrase-table.
This algorithm enables the use of sig-nificantly larger input word graphs in a more effi-cient way resulting in improved translation quality.The remaining part is structured as follows: wewill first discuss related work in Sec.
2.
Then, inSec.
3, we will describe the phrase-table represen-tation.
Afterwards, we will present applications inspeech translation and online MT in Sec.
4 and 5,respectively.
Experimental results will be presentedin Sec.
6 followed by the conclusions in Sec.
7.2 Related Work(Callison-Burch et al, 2005) and (Zhang and Vogel,2005) presented data structures for a compact rep-resentation of the word-aligned bilingual data, suchthat on-the-fly extraction of long phrases is possi-ble.
The motivation in (Callison-Burch et al, 2005)is that there are some long source phrases in thetest data that also occur in the training data.
How-ever, the more interesting question is if these longphrases really help to improve the translation qual-ity.
We have investigated this and our results are inline with (Koehn et al, 2003) showing that the trans-lation quality does not improve if we utilize phrasesbeyond a certain length.
Furthermore, the suffix ar-ray data structure of (Callison-Burch et al, 2005) re-quires a fair amount of memory, about 2GB in theirexample, whereas our implementation will use onlya tiny amount of memory, e.g.
less than 20MB forthe large Chinese-English NIST task.3 Efficient Phrase-table RepresentationIn this section, we will describe the proposed rep-resentation of the phrase-table.
A prefix tree, alsocalled trie, is an ordered tree data structure used tostore an associative array where the keys are symbolsequences.
In the case of phrase-based MT, the keysare source phrases, i.e.
sequences of source wordsand the associated values are the possible transla-tions of these source phrases.
In a prefix tree, alldescendants of any node have a common prefix,namely the source phrase associated with that node.The root node is associated with the empty phrase.The prefix tree data structure is quite common inautomatic speech translation.
There, the lexicon, i.e.the mapping of phoneme sequences to words, is usu-ally organized as a prefix tree (Ney et al, 1992).We convert the list of source phrases into a pre-fix tree and, thus, exploit that many of them sharethe same prefix.
This is illustrated in Fig.
1 (left).Within each node of the tree, we store a sorted ar-ray of possible successor words along with pointersto the corresponding successor nodes.
Additionally,we store a pointer to the possible translations.One property of the tree structure is that we canefficiently access the successor words of a given pre-fix.
This will be a key point to achieve an efficientphrase matching algorithm in Sec.
4.
When lookingfor a specific successor word, we perform a binarysearch in the sorted array.
Alternatively, we coulduse hashing to speed up this lookup.
We have chosenan array representation as this can be read very fastfrom disk.
Additionally, with the exception of theroot node, the branching factor of the tree is small,i.e.
the potential benefit from hashing is limited.
Atthe root node, however, the branching factor is closeto the vocabulary size of the source language, whichcan be large.
As we store the words internally as in-tegers and virtually all words occur as the first wordof some phrase, we can use the integers directly asthe position in the array of the root node.
Hence, thesearch for the successors at the root node is a simpletable lookup with direct access, i.e.
in O(1).If not filtered for a specific test set, the phrase-table becomes huge even for medium-sized tasks.Therefore, we store the tree structure on diskand load only the required parts into memory on-demand.
This is illustrated in Fig.
1 (right).
Here,we show the matching phrases for the source sen-tence ?c a a c?, where the matching phrases are set inbold and the phrases that are loaded into memory areset in italics.
The dashed part of the tree structure isnot loaded into memory.
Note that some nodes of thetree are loaded even if there is no matching phrase inthat node.
These are required to actually verify thatthere is no matching phrase.
An example is the ?bc?node in the lower right part of the figure.
This nodeis loaded to check if the phrase ?c a a?
occurs in thephrase-table.
The translations, however, are loadedonly for matching source phrases.In the following sections, we will describe appli-cations of this phrase-table representation for speechtranslation and online MT.4 Speech TranslationIn speech translation, the input to the MT system isnot a sentence, but a word graph representing alter-493a b a ca a b ba b b ca b c cb c ab a c a bb a a c a cb a babcababcacabcbca b a ca a b ba b b ca b c cb c ab a c a bb a a c a cb a babcababcacabcbcFigure 1: Illustration of the prefix tree.
Left: list of source phrases and the corresponding prefix tree.
Right:list of matching source phrases for sentence ?c a a c?
(bold phrases match, phrases in italics are loaded inmemory) and the corresponding partially loaded prefix tree (the dashed part is not in memory).jffiflfisGj,nffiflfisGj,1ffiflfisGj,Njffiflfi-       @@@@@@@RfGj,1fGj,nfGj,Nj............Figure 2: Illustration for graph G: node j with suc-cessor nodes sGj,1, ..., sGj,n..., sGj,Njand correspondingedge labels fGj,1, ..., fGj,n, ..., fGj,Nj.native ASR transcriptions.
As pointed out in (Math-ias and Byrne, 2006), one problem in speech trans-lation is that we have to match the phrases of ourphrase-table against the input word graph.
This re-sults in a combinatorial problem as the number ofphrases in a word graph increases exponentially withthe phrase length.4.1 Problem DefinitionIn this section, we will introduce the notation andstate the problem of matching source phrases ofan input graph G and the phrase-table, representedas prefix tree T .
The input graph G has nodes1, ..., j, ..., J .
The outgoing edges of a graph nodej are numbered with 1, ..., n, ..., Nj , i.e.
an edge inthe input graph is identified by a pair (j, n).
Thesource word labeling the nth outgoing edge of graphnode j is denoted as fGj,n and the successor node ofthis edge is denoted as sGj,n ?
{1, ..., J}.
This nota-tion is illustrated in Fig.
2.We use a similar notation for the prefix tree T withnodes 1, ..., k, ...,K. The outgoing edges of a treenode k are numbered with 1, ...,m, ...,Mk, i.e.
anedge in the prefix tree is identified by a pair (k,m).The source word labeling the mth outgoing edge oftree node k is denoted as fTk,m and the successornode of this edge is denoted as sTk,m ?
{1, ...,K}.Due to the tree structure, the successor nodes of atree node k are all distinct:sTk,m = sTk,m?
?
m = m?
(1)Let k0 denote the root node of the prefix tree andlet f?k denote the prefix that leads to tree node k.Furthermore, we define E(k) as the set of possibletranslations of the source phrase f?k.
These are theentries of the phrase-table, i.e.E(k) ={e????
p(e?|f?k) > 0}(2)We will need similar symbols for the input graph.Therefore, we define F (j?, j) as the set of sourcephrases of all paths from graph node j?
to node j, orformally:F (j?, j) ={f????
?
(ji, ni)Ii=1 : f?
= fGj1,n1 , ..., fGjI ,nI?
j1 = j?
?
?I?1i=1 sGji,ni = ji+1 ?
sjI ,nI = j}Here, the conditions ensure that the edge sequence(ji, ni)Ii=1 is a proper path from node j?
to node jin the input graph and that the corresponding sourcephrase is f?
= fGj1,n1 , ..., fGjI ,nI .
This definition canbe expressed in a recursive way; the idea is to extendthe phrases of the predecessor nodes by one word:F (j?, j) =?(j??,n):sGj??,n=j{f?fGj??,n???f?
?
F (j?, j??
)}(3)494Here, the set is expressed as a union over all in-bound edges (j?
?, n) of node j.
We concatenate eachsource phrase f?
that ends at the start node of suchan edge, i.e.
f?
?
F (j?, j??
), with the correspondingedge label fGj??,n.
Additionally, we define E(j?, j)as the set of possible translations of all paths fromgraph node j?
to graph node j, or formally:E(j?, j) ={e????
?f?
?
F (j?, j) : p(e?|f?)
> 0}(4)=?k:f?k?F (j?,j)E(k) (5)=?(j??,n):sGj?
?,n=j?k:f?k?F (j?,j??)m:fGj?
?,n=fTk,mE(sTk,m) (6)Here, the definition was first rewritten using Eq.
2and then using Eq.
3.
Again, the set is expressedrecursively as a union over the inbound edges.
Foreach inbound edge (j?
?, n), the inner union verifiesthat there exists a corresponding edge (k,m) in theprefix tree with the same label, i.e.
fGj?
?,n = fTk,m.Our goal is to find all non-empty sets of trans-lation options E(j?, j).
The naive approach wouldbe to enumerate all paths in the input graph fromnode j?
to node j, then lookup the correspondingsource phrase in the phrase-table and add the trans-lations, if there are any, to the set of translationoptions E(j?, j).
This solution has some obviousweaknesses: the number of paths between two nodesis typically huge and the majority of the correspond-ing source phrases do not occur in the phrase-table.We omitted the probabilities for notational conve-nience.
The extensions are straightforward.
Notethat we store only the target phrases e?
in the setof possible translations E(j?, j) and not the sourcephrases f?
.
This is based on the assumption that themodels which are conditioned on the source phrasef?
are independent of the context outside the phrasepair (f?
, e?).
This assumption holds for the standardphrase and word translation models.
Thus, we haveto keep only the target phrase with the highest prob-ability.
It might be violated by lexicalized distor-tion models (dependent on the configuration); in thatcase we have to store the source phrase along withthe target phrase and the probability, which is againstraightforward.4.2 AlgorithmThe algorithm for matching the source phrases of theinput graph G and the prefix tree T is presented inFigure 3: Algorithm phrase-match for match-ing source phrases of input graph G and prefix treeT .
Input: graph G, prefix tree T , translation optionsE(k) for all tree nodes k; output: translation optionsE(j?, j) for all graph nodes j?
and j.0 FOR j?
= 1 TO J DO1 stack.push(j?, k0)2 WHILE not stack.empty() DO3 (j, k) = stack.pop()4 E(j?, j) = E(j?, j) ?
E(k)5 FOR n = 1 TO Nj DO6 IF (fGj,n = )7 THEN stack.push(sGj,n, k)8 ELSE IF (?m : fGj,n = fTk,m)9 THEN stack.push(sGj,n, sTk,m)Fig.
3.
Starting from a graph node j?, we explore thepart of the graph which corresponds to known sourcephrase prefixes and generate the sets E(j?, j) incre-mentally based on Eq.
6.
The intermediate statesare represented as pairs (j, k) meaning that there ex-ists a path in the input graph from node j?
to node jwhich is labeled with the source phrase f?k, i.e.
thesource phrase that leads to node k in the prefix tree.These intermediate states are stored on a stack.
Afterthe initialization in line 1, the main loop starts.
Wetake one item from the stack and update the transla-tion options E(j?, j) in line 4.
Then, we loop overall outgoing edges of the current graph node j. Foreach edge, we first check if the edge is labeled withan  in line 6.
In this special case, we go to the suc-cessor node in the input graph sGj,n, but remain in thecurrent node k of the prefix tree.
In the regular case,i.e.
the graph edge label is a regular word, we checkin line 8 if the current prefix tree node k has an out-going edge labeled with that word.
If such an edgeis found, we put a new state on the stack with thetwo successor nodes in the input graph sGj,n and theprefix tree sTk,m, respectively.4.3 Computational ComplexityIn this section, we will analyze the computationalcomplexity of the algorithm.
The computationalcomplexity of lines 5-9 is in O(Nj logMk), i.e.
itdepends on the branching factors of the input graphand the prefix tree.
Both are typically small.
An ex-ception is the branching factor of the root node k0 ofthe prefix tree, which can be rather large, typically itis the vocabulary size of the source language.
But,as described in Sec.
3, we can access the successor495nodes of the root node of the prefix tree in O(1), i.e.in constant time.
So, if we are at the root node of theprefix tree, the computational complexity of lines 5-9 is inO(Nj).
Using hashing at the interior nodes ofthe prefix tree would result in a constant time lookupat these nodes as well.
Nevertheless, the sorted ar-ray implementation that we chose has the advantageof faster loading from disk which seems to be moreimportant in practice.An alternative interpretation of lines 5-9 is that wehave to compute the intersection of the two sets fGjand fTk , withfGj ={fGj,n??
n = 1, ..., Nj}(7)fTk ={fTk,m?
?m = 1, ...,Mk}.
(8)Assuming both sets are sorted, this could be done inlinear time, i.e.
in O(Nj + Mk).
In our case, onlythe edges in the prefix tree are sorted.
Obviously, wecould sort the edges in the input graph and then ap-ply the linear algorithm, resulting in an overall com-plexity of O(Nj logNj + Mk).
As the algorithmvisits nodes multiple times, we could do even betterby sorting all edges of the graph during the initial-ization.
Then, we could always apply the linear timemethod.
On the other hand, it is unclear if this paysoff in practice and an experimental comparison hasto be done which we will leave for future work.The overall complexity of the algorithm dependson how many phrases of the input graph occur in thephrase-table.
In the worst case, i.e.
if all phrases oc-cur in the phrase-table, the described algorithm isnot more efficient than the naive algorithm whichsimply enumerates all phrases.
Nevertheless, thisdoes not happen in practice and we observe an ex-ponential speed up compared to the naive algorithm,as will be shown in Sec.
6.3.5 Online Machine TranslationBeside speech translation, the presented phrase-table data structure has other interesting applica-tions.
One of them is online MT, i.e.
an MT sys-tem that is able to translate unseen sentences with-out significant delay.
These online MT systems aretypically required if there is some interaction withhuman users, e.g.
if the MT system acts as an in-terpreter in a conversation, or in real-time systems.This situation is different from the usual researchenvironment where typically a fair amount of timeis spent to prepare the MT system to translate a cer-tain set of source sentences.
In the research scenario,Table 1: NIST task: corpus statistics.Chinese EnglishTrain Sentence pairs 7MRunning words 199M 213MVocabulary size 222K 351KTest 2002 Sentences 878 3 512Running words 25K 105K2005 Sentences 1 082 4 328Running words 33K 148Kthis preparation usually pays off as the same set ofsentences is translated multiple times.
In contrast,an online MT system translates each sentence justonce.
One of the more time-consuming parts of thispreparation is the filtering of the phrase-table.
Us-ing the on-demand loading technique we describedin Sec.
3, we can avoid the filtering step and di-rectly translate the source sentence.
An additionaladvantage is that we load only small parts of the fullphrase-table into memory.
This reduces the mem-ory requirements significantly, e.g.
for the Chinese?English NIST task, the memory requirement of thephrase-table is reduced to less than 20MB using on-demand loading.
This makes the MT system usableon devices with limited hardware resources.6 Experimental Results6.1 Translation SystemFor the experiments, we use a state-of-the-artphrase-based statistical machine translation systemas described in (Zens and Ney, 2004).
We use alog-linear combination of several models: a four-gram language model, phrase-based and word-basedtranslation models, word, phrase and distortionpenalty and a lexicalized distortion model.
Themodel scaling factors are optimized using minimumerror rate training (Och, 2003).6.2 Empirical Analysis for a Large Data TaskIn this section, we present an empirical analysis ofthe described data structure for the large data trackof the Chinese-English NIST task.
The corpus statis-tics are shown in Tab.
1.The translation quality is measured using two ac-curacy measures: the BLEU and the NIST score.Additionally, we use the two error rates: the worderror rate (WER) and the position-independent worderror rate (PER).
These evaluation criteria are com-puted with respect to four reference translations.In Tab.
2, we present the translation quality as a496Table 2: NIST task: translation quality as a function of the maximum source phrase length.src NIST 2002 set (dev) NIST 2005 set (test)len WER[%] PER[%] BLEU[%] NIST WER[%] PER[%] BLEU[%] NIST1 71.9 46.8 27.07 8.37 78.0 49.0 23.11 7.622 62.4 41.2 34.36 9.39 68.5 42.2 30.32 8.743 62.0 41.1 34.89 9.33 67.7 42.1 30.90 8.744 61.7 41.1 35.05 9.27 67.6 41.9 30.99 8.755 61.8 41.2 34.95 9.25 67.6 41.9 30.93 8.72?
61.8 41.2 34.99 9.25 67.5 41.8 30.90 8.73Table 3: NIST task: phrase-table statistics.src number of distinct avg.
tgtlen src phrases src-tgt pairs candidates1 221 505 17 456 415 78.82 5 000 041 39 436 617 7.93 20 649 699 58 503 904 2.84 31 383 549 58 436 271 1.95 32 679 145 51 255 866 1.6total 89 933 939 225 089 073 2.5function of the maximum source phrase length.
Weobserve a large improvement when going beyondlength 1, but this flattens out very fast.
Using phrasesof lengths larger than 4 or 5 does not result in fur-ther improvement.
Note that the minor differencesin the evaluation results for length 4 and beyond aremerely statistical noise.
Even a length limit of 3, asproposed by (Koehn et al, 2003), would result inalmost optimal translation quality.
In the followingexperiments on this task, we will use a limit of 5 forthe source phrase length.In Tab.
3, we present statistics about the extractedphrase pairs for the Chinese?English NIST task asa function of the source phrase length, in this casefor length 1-5.
The phrases are not limited to a spe-cific test set.
We show the number of distinct sourcephrases, the number of distinct source-target phrasepairs and the average number of target phrases (ortranslation candidates) per source phrase.
In the ex-periments, we limit the number of translation can-didates per source phrase to 200.
We store a to-tal of almost 90 million distinct source phrases andmore than 225 million distinct source-target phrasepairs in the described data structure.
Obviously, itwould be infeasible to load this huge phrase-tablecompletely into memory.
Nevertheless, using on-demand loading, we are able to utilize all thesephrase pairs with minimal memory usage.In Fig.
4, we show the memory usage of the de-scribed phrase-table data structure per sentence for0 20 40 60 80 100percentage of test set68101214161820memory usage[MegaByte]Figure 4: NIST task: phrase-table memory usageper sentence (sorted).the NIST 2002 test set.
The sentences were sortedaccording to the memory usage.
The maximumamount of memory for the phrase-table is 19MB;for more than 95% of the sentences no more than15MB are required.
Storing all phrase pairs for thistest set in memory requires about 1.7GB of mem-ory, i.e.
using the described data structures, we notonly avoid the limitation to a specific test set, but wealso reduce the memory requirements by about twoorders of a magnitude.Another important aspect that should be consid-ered is translation speed.
In our experiments, thedescribed data structure is not slower than the tradi-tional approach.
We attribute this to the fact that weuse a binary format that is a memory map of the datastructure used internally and that we load the data inrather large, coherent chunks.
Additionally, there isvirtually no initialization time for the phrase-tablewhich decreases the overhead of parallelization andtherefore speeds up the development cycle.6.3 Speech TranslationThe experiments for speech translation were con-ducted on the European Parliament Plenary Sessions(EPPS) task.
This is a Spanish-English speech-to-speech translation task collected within the TC-Star497Table 4: EPPS task: corpus statistics.Train Spanish EnglishSentence pairs 1.2 MRunning words 31 M 30 MVocabulary size 140 K 94 KTest confusion networks Full PrunedSentences 1 071Avg.
length 23.6Avg.
/ max.
depth 2.7 / 136 1.3 / 11Avg.
number of paths 1075 264Kproject.
The training corpus statistics are presentedin Tab.
4.
The phrase-tables for this task were kindlyprovided by ITC-IRST.We evaluate the phrase-match algorithm inthe context of confusion network (CN) decoding(Bertoldi and Federico, 2005), which is one ap-proach to speech translation.
CNs (Mangu et al,2000) are interesting for MT because the reorderingcan be done similar to single best input.
For moredetails on CN decoding, please refer to (Bertoldi etal., 2007).
Note that the phrase-match algo-rithm is not limited to CNs, but can work on arbi-trary word graphs.Statistics of the CNs are also presented in Tab.
4.We distinguish between the full CNs and prunedCNs.
The pruning parameters were chosen such thatthe resulting CNs are similar in size to the largestones in (Bertoldi and Federico, 2005).
The averagedepth of the full CNs, i.e.
the average number of al-ternatives per position, is about 2.7 words whereasthe maximum is as high as 136 alternatives.In Fig.
5, we present the average number ofphrase-table look-ups for the full EPPS CNs as afunction of the source phrase length.
The curve ?CNtotal?
represents the total number of source phrasesin the CNs for a given length.
This is the numberof phrase-table look-ups using the naive algorithm.Note the exponential growth with increasing phraselength.
Therefore, the naive algorithm is only appli-cable for very short phrases and heavily pruned CNs,as e.g.
in (Bertoldi and Federico, 2005).The curve ?CN explored?
is the number of phrase-table look-ups using the phrase-match algo-rithm described in Fig.
3.
We do not observe theexponential explosion as for the naive algorithm.Thus, the presented algorithm effectively solves thecombinatorial problem of matching phrases of theinput CNs and the phrase-table.
For comparison,we plotted also the number of look-ups using thephrase-match algorithm in the case of single-0 2 4 6 8 10 12 14source phrase length0.1110100100010000100000100000010000000100000000phrase table look-ups CN totalCN exploredsingle-best exploredFigure 5: EPPS task: avg.
number of phrase-tablelook-ups per sentence as a function of the sourcephrase length.Table 5: EPPS task: translation quality and time fordifferent input conditions (CN=confusion network,time in seconds per sentence).Input type BLEU[%] Time [sec]Single best 37.6 2.7CN pruned 38.5 4.8full 38.9 9.2best input, labeled ?single-best explored?.
The maxi-mum phrase length for these experiments is seven.For CN input, this length can be exceeded as theCNs may contain -transitions.In Tab.
5, we present the translation results andthe translation times for different input conditions.We observe a significant improvement in translationquality as more ASR alternatives are taken into ac-count.
The best results are achieved for the fullCNs.
On the other hand, the decoding time in-creases only moderately.
Using the new algorithm,the ratio of the time for decoding the CNs and thetime for decoding the single best input is 3.4 for thefull CNs and 1.8 for the pruned CNs.
In previouswork (Bertoldi and Federico, 2005), the ratio for thepruned CNs was about 25 and the full CNs could notbe handled.To summarize, the presented algorithm has twomain advantages for speech translation: first, itenables us to utilize large CNs, which was pro-hibitively expensive beforehand and second, the ef-ficiency is improved significantly.Whereas the previous approaches required care-ful pruning of the CNs, we are able to utilize the un-pruned CNs.
Experiments on other tasks have shownthat even larger CNs are unproblematic.4987 ConclusionsWe proposed an efficient phrase-table data structurewhich has two key properties:1.
On-demand loading.We are able to store hundreds of millions ofphrase pairs and require only a very smallamount of memory during decoding, e.g.
lessthan 20MB for the Chinese-English NIST task.This enables us to run the MT system on deviceswith limited hardware resources or alternativelyto utilize the freed memory for other models.
Ad-ditionally, the usual phrase-table filtering is obso-lete, which is important for online MT systems.2.
Prefix tree data structure.Utilizing the prefix tree structure enables us to ef-ficiently match source phrases against the phrase-table.
This is especially important for speechtranslation where the input is a graph represent-ing a huge number of alternative sentences.
Us-ing the novel algorithm, we are able to handlelarge CNs, which was prohibitively expensivebeforehand.
This results in more efficient decod-ing and improved translation quality.We have shown that this data structure scales verywell to large data tasks like the Chinese-EnglishNIST task.
The implementation of the describeddata structure as well as the phrase-match al-gorithm for confusion networks is available as opensource software in the MOSES toolkit1.Not only standard phrase-based systems can ben-efit from this data structure.
It should be ratherstraightforward to apply this data structure as well asthe phrase-match algorithm to the hierarchicalapproach of (Chiang, 2005).
As the number of rulesin this approach is typically larger than the numberof phrases in a standard phrase-based system, thegains should be even larger.The language model is another model with highmemory requirements.
It would be interesting to in-vestigate if the described techniques and data struc-tures are applicable for reducing the memory re-quirements of language models.Some aspects of the phrase-match algorithmare similar to the composition of finite-state au-tomata.
An efficient implementation of on-demandloading (not only on-demand computation) for a1http://www.statmt.org/mosesfinite-state toolkit would make the whole range offinite-state operations applicable to large data tasks.AcknowledgmentsThis material is partly based upon work supported by theDARPA under Contract No.
HR0011-06-C-0023, and waspartly funded by the European Union under the integratedproject TC-STAR (IST-2002-FP6-506738, http://www.tc-star.org).
Additionally, we would like to thank all groupmembers of the JHU 2006 summer research workshop OpenSource Toolkit for Statistical Machine Translation.ReferencesN.
Bertoldi and M. Federico.
2005.
A new decoder for spo-ken language translation based on confusion networks.
InProc.
IEEE Automatic Speech Recognition and Understand-ing Workshop, pages 86?91, Mexico, November/December.N.
Bertoldi, R. Zens, and M. Federico.
2007.
Speech trans-lation by confusion networks decoding.
In Proc.
IEEEInt.
Conf.
on Acoustics, Speech, and Signal Processing(ICASSP), Honolulu, Hawaii, April.C.
Callison-Burch, C. Bannard, and J. Schroeder.
2005.
Scal-ing phrase-based statistical machine translation to larger cor-pora and longer phrases.
In Proc.
43rd Annual Meeting of theAssoc.
for Computational Linguistics (ACL), pages 255?262,Ann Arbor, MI, June.D.
Chiang.
2005.
A hierarchical phrase-based model for statis-tical machine translation.
In Proc.
43rd Annual Meeting ofthe Assoc.
for Computational Linguistics (ACL), pages 263?270, Ann Arbor, MI, June.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In Proc.
Human Language TechnologyConf.
/ North American Chapter of the Assoc.
for Compu-tational Linguistics Annual Meeting (HLT-NAACL), pages127?133, Edmonton, Canada, May/June.L.
Mangu, E. Brill, and A. Stolcke.
2000.
Finding consensusin speech recognition: Word error minimization and otherapplications of confusion networks.
Computer, Speech andLanguage, 14(4):373?400, October.L.
Mathias and W. Byrne.
2006.
Statistical phrase-basedspeech translation.
In Proc.
IEEE Int.
Conf.
on Acoustics,Speech, and Signal Processing (ICASSP), volume 1, pages561?564, Toulouse, France, May.H.
Ney, R. Haeb-Umbach, B. H. Tran, and M. Oerder.
1992.Improvements in beam search for 10000-word continuousspeech recognition.
In Proc.
IEEE Int.
Conf.
on Acoustics,Speech, and Signal Processing (ICASSP), volume 1, pages9?12, San Francisco, CA, March.F.
J. Och.
2003.
Minimum error rate training in statistical ma-chine translation.
In Proc.
41st Annual Meeting of the As-soc.
for Computational Linguistics (ACL), pages 160?167,Sapporo, Japan, July.R.
Zens and H. Ney.
2004.
Improvements in phrase-based sta-tistical machine translation.
In Proc.Human Language Tech-nology Conf.
/ North American Chapter of the Assoc.
forComputational Linguistics Annual Meeting (HLT-NAACL),pages 257?264, Boston, MA, May.Y.
Zhang and S. Vogel.
2005.
An efficient phrase-to-phrasealignment model for arbitrarily long phrases and large cor-pora.
In Proc.
10th Annual Conf.
of the European Assoc.
forMachine Translation (EAMT), pages 294?301, Budapest,Hungary, May.499
