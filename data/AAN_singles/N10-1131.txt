Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885?893,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAn extractive supervised two-stage method for sentence compressionDimitrios Galanis?
and Ion Androutsopoulos?+?Department of Informatics, Athens University of Economics and Business, Greece+Digital Curation Unit ?
IMIS, Research Centre ?Athena?, GreeceAbstractWe present a new method that compressessentences by removing words.
In a first stage,it generates candidate compressions by re-moving branches from the source sentence?sdependency tree using a Maximum Entropyclassifier.
In a second stage, it chooses thebest among the candidate compressions usinga Support Vector Machine Regression model.Experimental results show that our methodachieves state-of-the-art performance withoutrequiring any manually written rules.1 IntroductionSentence compression is the task of producing ashorter form of a single given sentence, so that thenew form is grammatical and retains the most im-portant information of the original one (Jing, 2000).Sentence compression is valuable in many applica-tions, for example when displaying texts on smallscreens (Corston-Oliver, 2001), in subtitle genera-tion (Vandeghinste and Pan, 2004), and in text sum-marization (Madnani et al, 2007).People use various methods to shorten sentences,including word or phrase removal, using shorterparaphrases, and common sense knowledge.
How-ever, reasonable machine-generated sentence com-pressions can often be obtained by only remov-ing words.
We use the term extractive to refer tomethods that compress sentences by only removingwords, as opposed to abstractive methods, wheremore elaborate transformations are also allowed.Most of the existing compression methods are ex-tractive (Jing, 2000; Knight and Marcu, 2002; Mc-Donald, 2006; Clarke and Lapata, 2008; Cohn andLapata, 2009).
Although abstractive methods havealso been proposed (Cohn and Lapata, 2008), andthey may shed more light on how people compresssentences, they do not always manage to outperformextractive methods (Nomoto, 2009).
Hence, from anengineering perspective, it is still important to inves-tigate how extractive methods can be improved.In this paper, we present a new extractive sentencecompression method that relies on supervised ma-chine learning.1 In a first stage, the method gener-ates candidate compressions by removing branchesfrom the source sentence?s dependency tree using aMaximum Entropy classifier (Berger et al, 2006).
Ina second stage, it chooses the best among the candi-date compressions using a Support Vector MachineRegression (SVR) model (Chang and Lin, 2001).
Weshow experimentally that our method compares fa-vorably to a state-of-the-art extractive compressionmethod (Cohn and Lapata, 2007; Cohn and Lapata,2009), without requiring any manually written rules,unlike other recent work (Clarke and Lapata, 2008;Nomoto, 2009).
In essence, our method is a two-tier over-generate and select (or rerank) approach tosentence compression; similar approaches have beenadopted in natural language generation and parsing(Paiva and Evans, 2005; Collins and Koo, 2005).2 Related workKnight and Marcu (2002) presented a noisy channelsentence compression method that uses a languagemodel P (y) and a channel model P (x|y), where x1An implementation of our method will be freely availablefrom http://nlp.cs.aueb.gr/software.html885is the source sentence and y the compressed one.P (x|y) is calculated as the product of the proba-bilities of the parse tree tranformations required toexpand y to x.
The best compression of x is theone that maximizes P (x|y) ?
P (y), and it is foundusing a noisy channel decoder.
In a second, alter-native method Knight and Marcu (2002) use a tree-to-tree transformation algorithm that tries to rewritedirectly x to the best y.
This second method usesC4.5 (Quinlan, 1993) to learn when to perform treerewriting actions (e.g., dropping subtrees, combin-ing subtrees) that transform larger trees to smallertrees.
Both methods were trained and tested ondata from the Ziff-Davis corpus (Knight and Marcu,2002), and they achieved very similar grammatical-ity and meaning preservation scores, with no statis-tically significant difference.
However, their com-pression rates (counted in words) were very dif-ferent: 70.37% for the noisy-channel method and57.19% for the C4.5-based one.McDonald (2006) ranks each candidate compres-sion using a function based on the dot product of avector of weights with a vector of features extractedfrom the candidate?s n-grams, POS tags, and depen-dency tree.
The weights were learnt from the Ziff-Davis corpus.
The best compression is found us-ing a Viterbi-like algorithm that looks for the bestsequence of source words that maximizes the scor-ing function.
The method outperformed Knight andMarcu?s (2002) tree-to-tree method in grammatical-ity and meaning preservation on data from the Ziff-Davis corpus, with a similar compression rate.Clarke and Lapata (2008) presented an unsuper-vised method that finds the best compression usingInteger Linear Programming (ILP).
The ILP obejc-tive function takes into account a language modelthat indicates which n-grams are more likely to bedeleted, and a significance model that shows whichwords of the input sentence are important.
Man-ually defined constraints (in effect, rules) that op-erate on dependency trees indicate which syntacticconstituents can be deleted.
This method outper-formed McDonald?s (2006) in grammaticality andmeaning preservation on test sentences from Edin-burgh?s ?written?
and ?spoken?
corpora.2 However,the compression rates of the two systems were dif-2See http://homepages.inf.ed.ac.uk/s0460084/data/.ferent (72.0% vs. 63.7% for McDonald?s method,both on the written corpus).We compare our method against Cohn and Lap-ata?s T3 system (Cohn and Lapata, 2007; Cohn andLapata, 2009), a state-of-the-art extractive sentencecompression system that learns parse tree transduc-tion operators from a parallel extractive corpus ofsource-compressed trees.
T3 uses a chart-based de-coding algorithm and a Structured Support VectorMachine (Tsochantaridis et al, 2005) to learn toselect the best compression among those licensedby the operators learnt.3 T3 outperformed McDon-ald?s (2006) system in grammaticality and meaningpreservation on Edinburgh?s ?written?
and ?spoken?corpora, achieving comparable compression rates(Cohn and Lapata, 2009).
Cohn and Lapata (2008)have also developed an abstractive version of T3,which was reported to outperform the original, ex-tractive T3 in meaning preservation; there was nostatistically significant difference in grammaticality.Finally, Nomoto (2009) presented a two-stage ex-tractive method.
In the first stage, candidate com-pressions are generated by chopping the source sen-tence?s dependency tree.
Many ungrammatical com-pressions are avoided using hand-crafted drop-me-not rules for dependency subtrees.
The candidatecompressions are then ranked using a function thattakes into account the inverse document frequen-cies of the words, and their depths in the sourcedependency tree.
Nomoto?s extractive method wasreported to outperform Cohn and Lapata?s abstrac-tive version of T3 on a corpus collected via RSSfeeds.
Our method is similar to Nomoto?s, in thatit uses two stages, one that chops the source depen-dency tree generating candidate compressions, andone that ranks the candidates.
However, we experi-mented with more elaborate ranking models, and ourmethod does not employ any manually crafted rules.3 Our methodAs already mentioned, our method first generatescandidate compressions, which are then ranked.
Thecandidate compressions generator operates by re-moving branches from the dependency tree of the3T3 appears to be the only previous sentence compres-sion method whose implementation is publicly available; seehttp://www.dcs.shef.ac.uk/?tcohn/t3/.886input sentence (figure 1); this stage is discussed insection 3.1 below.
We experimented with differentranking functions, discussed in section 3.2, whichuse features extracted from the source sentence sand the candidate compressions c1, .
.
.
, ck.3.1 Generating candidate compressionsOur method requires a parallel training corpus con-sisting of sentence-compression pairs ?s, g?.
Thecompressed sentences g must have been formed byonly deleting words from the corresponding sourcesentences s. The ?s, g?
training pairs are used to es-timate the propability that a dependency edge e of adependency tree Ts of an input sentence s is retainedor not in the dependency tree Tg of the compressedsentence g. More specifically, we want to estimatethe probabilities P (Xi|context(ei)) for every edgeei of Ts, where Xi is a variable that can take oneof the following three values: not del, for not delet-ing ei; del u for deleting ei along with its head; anddel l for deleting e along with its modifier.
The head(respectively, modifier) of ei is the node ei originatesfrom (points to) in the dependency tree.
context(ei)is a set of features that represents ei?s local contextin Ts, as well as the local context of the head andmodifier of ei in s.The propabilities above can be estimated usingthe Maximum Entropy (ME) framework (Berger etal., 2006), a method for learning the distributionP (X|V ) from training data, where X is discrete-valued variable and V = ?V1, .
.
.
, Vn?
is a real ordiscrete-valued vector.
Here, V = context(ei) andX = Xi.
We use the following features in V :?
The label of the dependency edge ei, as well asthe POS tag of the head and modifier of ei.?
The entire head-label-modifier triple of ei.
Thisfeature overlaps with the previous two features,but it is common in ME models to use featurecombinations as additional features, since theymay indicate a category more strongly than theindividual initial features.4?
The POS tag of the father of ei?s head, and thelabel of the dependency that links the father toei?s head.4http://nlp.stanford.edu/pubs/maxent-tutorial-slides.pdf.?
The POS tag of each one of the three previousand the three following words of ei?s head andmodifier in s (12 features).?
The POS tag bi-grams of the previous two andthe following two words of ei?s head and mod-ifier in s (4 features).?
Binary features that show which of the possiblelabels occur (or not) among the labels of theedges that have the same head as ei in Ts (onefeature for each possible dependency label).?
Two binary features that show if the subtreerooted at the modifier of ei or ei?s uptree (therest of the tree, when ei?s subtree is removed)contain an important word.
A word is consid-ered important if it appears in the document swas drawn from significantly more often thanin a background corpus.
In summarization,such words are called signature terms and arethought to be descriptive of the input; they canbe identified using the log-likelihood ratio ?
ofeach word (Lin and Hovy, 2000; Gupta et al,2007).For each dependency edge ei of a source trainingsentence s, we create a training vector V with theabove features.
If ei is retained in the dependencytree of the corresponding compressed sentence g inthe corpus, V is assigned the category not del.
Ifei is not retained, it is assigned the category del lor del u, depending on whether the head (as in theccomp of ?said?
in Figure 1) or the modifier (as inthe dobj of ?attend?)
of ei has also been removed.When the modifier of an edge is removed, the entiresubtree rooted at the modifier is removed, and simi-larly for the uptree, when the head is removed.
Wedo not create training vectors for the edges of theremoved subtree of a modifier or the edges of theremoved uptree of a head.Given an input sentence s and its dependency treeTs, the candidate compressions generator producesthe candidate compressed sentences c1, .
.
.
, cn bydeleting branches of Ts and putting the remainingwords of the dependency tree in the same order as ins.
The candidates c1, .
.
.
, cn correspond to possibleassignments of values to theXi variables (recall thatXi = not del|del l|del u) of the edges ei of Ts.887source: gold:saidccompnsubjK%%KKattendnsubjauxKK%%KKattendnusbjauxJJ%%JJ dobj*j*j***j*jprep+k+k+k+k+k+++k+k+k+k+k+khe MothernumnumJJ%%JJJ amodTTTT**TTTTwillMothernumnumJJ$$JJJ amodTTTT**TTTTwill hearingdet##GGonpobjF""Catherine 82 superiormeasureCatherine 82 superiormeasurethe Friday motherdetmotherdetthetheFigure 1: Dependency trees of a source sentence and its compression by a human (taken from Edinburgh?s ?written?corpus).
The source sentence is: ?Mother Catherine, 82, the mother superior, will attend the hearing on Friday, hesaid.?
The compressed one is: ?Mother Catherine, 82, the mother superior, will attend.?
Deleted edges and words areshown curled and underlined, respectively.Hence, there are at most 3m?1 candidate compres-sions, where m is the number of words in s. Thisis a large number of candidates, even for modestlylong input sentences.
In practice, the candidates arefewer, because del l removes an entire subtree anddel u an entire uptree, and we do not need to makedecisions Xi about the edges of the deleted subtreesand uptrees.
To reduce the number of candidatesfurther, we ignore possible assignments that containdecisions Xi = x to which the ME model assignsprobabilities below a threshold t; i.e., the ME modelis used to prune the space of possible assignments.When generating the possible assignments to theXi variables, we examine the edges ei of Ts in atop-down breadth-first manner.
In the source tree ofFigure 1, for example, we first consider the edgesof ?said?
; the left-to-right order is random, but letus assume that we consider first the ccomp edge.There are three possible actions: retain the edge(not del), remove it along with the head ?said?
(del u), or remove it along with the modifier ?at-tend?
and its subtree (del l).
If the ME model assignsa low probability to one of the three actions, that ac-tion is ignored.
For each one of the (remaining) ac-tions, we obtain a new form of Ts, and we continueto consider its (other) edges.
We process the edgesin a top-down fashion, because the ME model allowsdel l actions much more often than del u actions,and when del l actions are performed near the rootof Ts, they prune large parts of the space of possibleassignments to the Xi variables.
Some of the candi-date compressions that were generated for an inputsentence by setting t = 0.2 are shown in Table 1,along with the gold (human-authored) compression.3.2 Ranking candidate compressionsGiven that we now have a method that generatescandidate compressions c1, .
.
.
, ck for a sentence s,we need a function F (ci|s) that will rank the candi-date compressions.
Many of them are ungrammat-ical and/or do not convey the most important infor-mation of s. F (ci|s) should help us select a shortcandidate that is grammatical and retains the mostimportant information of s.3.2.1 Grammaticality and importance rateA simple way to rank the candidate compressionsis to assign to each one a score intended to measureits grammaticality and importance rate.
By gram-maticality, Gramm(ci), we mean how grammati-cally well-formed candidate ci is.
A common wayto obtain such a measure is to use an n-gram lan-888s: Then last week a second note, in the same handwriting, informed Mrs Allan that the search wason the wrong side of the bridge.g: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge.c1: Last week a second note informed Mrs Allan that the search was on the side.c2: Last week a second note informed Mrs Allan that the search was.c3: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge.c4: Last week in the same handwriting informed Mrs Allan the search was on the wrong side of the bridge.Table 1: A source sentence s, its gold (human authored) compression g, and candidate compressions c1, .
.
.
, c4.guage model trained on a large background corpus.However, language models tend to assign smallerprobabilities to longer sentences; therefore they fa-vor short sentences, but not necessarily the most ap-propriate compressions.
To overcome this problem,we follow Cordeiro et al (2009) and normalize thescore of a trigram language model as shown below,where w1, .
.
.
, wm are the words of candidate ci.Gramm(ci) = logPLM (ci)1/m =(1/m) ?
log(m?j=1P (wj |wj?1, wj?2)) (1)The importance rate ImpRate(ci|s), defined be-low, estimates how much information of the originalsentence s is retained in candidate ci.
tf(wi) is theterm frequency of wi in the document that contained?
(?
= ci, s), and idf(wi) is the inverse documentfrequency of wi in a background corpus.
We actu-ally compute idf(wi) only for nouns and verbs, andset idf(wi) = 0 for other words.ImpRate(ci|s) = Imp(ci)/Imp(s) (2)Imp(?)
=?wi?
?tf(wi) ?
idf(wi) (3)The ranking F (c|s) is then defined as a linearcombination of grammaticality and importance rate:F (ci|s) = ?
?Gramm(ci) + (1?
?)
??
ImpRate(ci|s)?
?
?
CR(ci|s) (4)A compression rate penalty factor CR(ci|s) =|c|/|s| is included, to bias our method towards gen-erating shorter or longer compressions; | ?
| denotesword length in words (punctuation is ignored).
Weexplain how the weigths ?, ?
are tuned in followingsections.
We call LM-IMP the configuration of ourmethod that uses the ranking function of equation 4.3.2.2 Support Vector RegressionA more sophisticated way to select the best com-pression is to train a Support Vector Machines Re-gression (SVR) model to assign scores to feature vec-tors, with each vector representing a candidate com-pression.
SVR models (Chang and Lin, 2001) aretrained using l training vectors (x1, y1), .
.
.
, (xl, yl),where xi ?
Rn and yi ?
R, and learn a functionf : Rn ?
R that generalizes the training data.
Inour case, xi is a feature vector representing a candi-date compression ci, and yi is a score indicating howgood a compression ci is.
We use 98 features:?
Gramm(ci) and ImpRate(ci|s), as above.?
2 features indicating the ratio of important andunimportant words of s, identified as in section3.1, that were deleted.?
2 features that indicate the average depth ofthe deleted and not deleted words in the depen-dency tree of s.?
92 features that indicate which POS tags appearin s and how many of them were deleted in ci.For every POS tag label, we use two features,one that shows how many POS tags of that la-bel are contained in s and one that shows howmany of these POS tags were deleted in ci.To assign a regression score yi to each trainingvector xi, we experimented with the following func-tions that measure how similar ci is to the gold com-pression g, and how grammatical ci is.?
Grammatical relations overlap: In this case, yiis theF1-score of the dependencies of ci againstthose of the gold compression g. This measurehas been shown to correlate well with humanjudgements (Clarke and Lapata, 2006).
As in889the ranking function of section 3.2.1, we add acompression rate penalty factor.yi = F1(d(ci)), d(g))?
?
?
CR(ci|s) (5)d(?)
denotes the set of dependencies.
We callSVR-F1 the configuration of our system thatuses equation 5 to rank the candidates.?
Tokens accuracy and grammaticality: Tokensaccuracy, TokAcc(ci|s, g), is the percentage oftokens of s that were correctly retained or re-moved in ci; a token was correctly retainedor removed, if it was also retained (or re-moved) in the gold compression g. To cal-culate TokAcc(ci|s, g), we need the word-to-word alignment of s to g, and s to ci.
Thesealignments were obtained as a by-product ofcomputing the corresponding (word) edit dis-tances.
We also want the regression model tofavor grammatical compressions.
Hence, weuse a linear combination of tokens accuracyand grammaticality of ci:yi = ?
?
TokAcc(ci|s, g) +(1?
?)
?Gramm(ci)?
?
?
CR(ci|s) (6)Again, we add a compression rate penalty, tobe able to generate shorter or longer compres-sions.
We call SVR-TOKACC-LM the config-uration of our system that uses equation 6.4 Baseline and T3As a baseline, we use a simple algorithm based onthe ME classifier of section 3.1.
The baseline pro-duces a single compression c for every source sen-tence s by considering sequentially the edges ei ofs?s dependency tree in a random order, and perform-ing at each ei the single action (not del, del u, ordel l) that the ME model considers more probable;the words of the chopped dependency tree are thenput in the same order as in s. We call this systemGreedy-Baseline.We compare our method against the extractiveversion of T3 (Cohn and Lapata, 2007; Cohn andLapata, 2009), a state-of-the-art sentence compres-sion system that applies sequences of transductionoperators to the syntax trees of the source sentences.The available tranduction operators are learnt fromthe syntax trees of a set of source-gold pairs.
Ev-ery operator transforms a subtree ?
to a subtree ?,rooted at symbols X and Y , respectively.To find the best sequence of transduction opera-tors that can be applied to a source syntax tree, achart-based dynamic programming decoder is used,which finds the best scoring sequence q?:q?
= arg maxqscore(q;w) (7)where score(q;w) is the dot product ??
(q), w?.?
(q) is a vector-valued feature function, and w is avector of weights learnt using a Structured SupportVector Machine (Tsochantaridis et al, 2005).?
(q) consists of: (i) the log-probability of the re-sulting candidate, as returned by a tri-gram languagemodel; and (ii) features that describe how the opera-tors of q are applied, for example the number of theterminals in each operator?s ?
and ?
subtrees, thePOS tags of the X and Y roots of ?
and ?
etc.5 ExperimentsWe used Stanford?s parser (de Marneffe et al, 2006)and ME classifier (Manning et al, 2003).5 Forthe (trigram) language model, we used SRILM withmodified Kneser-Ney smoothing (Stolcke, 2002).6The language model was trained on approximately4.5 million sentences of the TIPSTER corpus.
Toobtain idf(wi) values, we used approximately 19.5million verbs and nouns from the TIPSTER corpus.T3 requires the syntax trees of the source-goldpairs in Penn Treebank format, as well as a trigramlanguage model.
We obtained T3?s trees using Stan-ford?s parser, as in our system, unlike Cohn and La-pata (2009) that use Bikel?s (2002) parser.
The lan-guage models in T3 and our system are trained onthe same data and with the same options used byCohn and Lapata (2009).
T3 also needs a word-to-word alignment of the source-gold pairs, which wasobtained by computing the edit distance, as in Cohnand Lapata (2009) and SVR-TOKACC-LM.We used Edinburgh?s ?written?
sentence com-pression corpus (section 2), which consists ofsource-gold pairs (one gold compression per source5Both available from http://nlp.stanford.edu/.6See http://www.speech.sri.com/projects/srilm/.890sentence).
The gold compressions were created bydeleting words.
We split the corpus in 3 parts: 1024training, 324 development, and 291 testing pairs.5.1 Best configuration of our methodWe first evaluated experimentally the three configu-rations of our method (LM-IMP, SVR-F1, SVR-TOKACC-LM), using the F1-measure of the de-pendencies of the machine-generated compressionsagainst those of the gold compressions as an auto-matic evaluation measure.
This measure has beenshown to correlate well with human judgements(Clarke and Lapata, 2006).In all three configurations, we trained the MEmodel of section 3.1 on the dependency trees of thesource-gold pairs of the training part of the corpus.We then used the trained ME classifier to generatethe candidate compressions of each source sentenceof the training part.
We set t = 0.2, which led toat most 10,000 candidates for almost every sourcesentence.
We kept up to 1.000 candidates for eachsource sentence, and we selected randonly approx-imately 10% of them, obtaining 18,385 candidates,which were used to train the two SVR configurations;LM-IMP requires no training.To tune the ?
parameters of LM-IMP and SVR-TOKACC-LM in equations 4 and 6, we initially set?
= 0 and we experimented with different val-ues of ?.
For each one of the two configurationsand for every different ?
value, we computed theaverage compression rate of the machine-generatedcompressions on the development set.
In the restof the experiments, we set ?
to the value that gavean average compression rate approximatelly equal tothat of the gold compressions of the training part.We then experimented with different values of ?in all three configurations, in equations 4?6, to pro-duce smaller or longer compression rates.
The ?
pa-rameter provides a uniform mechanism to fine-tunethe compression rate in all three configurations, evenin SVR-F1 that has no ?.
The results on the de-velopment part are shown in Figure 2, along withthe baseline?s results.
The baseline has no param-eters to tune; hence, its results are shown as a sin-gle point.
Both SVR models outperform LM-IMP,which in turn outperforms the baseline.
Also, SVR-TOKACC-LM performs better or as well as SVR-F1 for all compression rates.
Note, also, that theperfomance of the two SVR configurations might beimproved further by using more training examples,whereas LM-IMP contains no learning component.Figure 2: Evaluation results on the development set.5.2 Our method against T3We then evaluated the best configuration of ourmethod (SVR-TOKACC-LM) against T3, both au-tomatically (F1-measure) and with human judges.We trained both systems on the training set of thecorpus.
In our system, we used the same ?
value thatwe had obtained from the experiments of the previ-ous section.
We then varied the values of our sys-tem?s ?
parameter to obtain approximately the samecompression rate as T3.For the evaluation with the human judges, we se-lected randomly 80 sentences from the test part.
Foreach source sentence s, we formed three pairs, con-taining s, the gold compression, the compressionof SVR-TOKACC-LM, and the compression of T3,repsectively, 240 pairs in total.
Four judges (grad-uate students) were used.
Each judge was given 60pairs in a random sequence; they did not know howthe compressed sentenes were obtained and no judgesaw more than one compression of the same sourcesentence.
The judges were told to rate (in a scalefrom 1 to 5) the compressed sentences in terms ofgrammaticality, meaning preservation, and overallquality.
Their average judgements are shown in Ta-ble 2, where the F1-scores are also included.
Cohnand Lapata (2009) have reported very similar scores891for T3 on a different split of the corpus (F1: 49.48%,CR: 61.09%).system G M Ov F1 (%) CR (%)T3 3.83 3.28 3.23 47.34 59.16SVR 4.20 3.43 3.57 52.09 59.85gold 4.73 4.27 4.43 100.00 78.80Table 2: Results on 80 test sentences.
G: grammaticality,M: meaning preservation, Ov: overall score, CR: com-pression rate, SVR: SVR-TOKACC-LM.Our system outperforms T3 in all evaluation mea-sures.
We used Analysis of Variance (ANOVA) fol-lowed by post-hoc Tukey tests to check whether thejudge ratings differ significantly (p < 0.1); all judgeratings of gold compressions are significantly differ-ent from T3?s and those of our system; also, our sys-tem differs significantly from T3 in grammaticality,but not in meaning preservation and overall score.We also performed Wilcoxon tests, which showedthat the difference in the F1 scores of the two sys-tems is statistically significant (p < 0.1) on the 80test sentences.
Table 3 shows the F1 scores and theaverage compression rates for all 291 test sentences.Both systems have comparable compression rates,but again our system outperforms T3 in F1, with astatistically significant difference (p < 0.001).system F1 CRSVR-TOKACC-LM 53.75 63.72T3 47.52 64.16Table 3: F1 scores on the entire test set.Finally, we computed the Pearson correlation r ofthe overall (Ov) scores that the judges assigned tothe machine-generated compressions with the corre-sponding F1 scores.
The two measures were foundto corellate reliably (r = 0.526).
Similar resultshave been reported (Clarke and Lapata, 2006) forEdinburgh?s ?spoken?
corpus (r = 0.532) and theZiff-Davis corpus (r = 0.575).6 Conclusions and future workWe presented a new two-stage extractive methodfor sentence compression.
The first stage gener-ates candidate compressions by removing or notedges from the source sentence?s dependency tree;an ME model is used to prune unlikely edge deletionor non-deletions.
The second stage ranks the can-didate compressions; we experimented with threeranking models, achieving the best results with anSVR model trained with an objective function thatcombines token accuracy and a language model.We showed experimentally, via automatic evalua-tion and with human judges, that our method com-pares favorably to a state-of-the-art extractive sys-tem.
Unlike other recent approaches, our systemuses no hand-crafted rules.
In future work, we planto support more complex tranformations, instead ofonly removing words and experiment with differentsizes of training data.The work reported in this paper was carried out inthe context of project INDIGO, where an autonomousrobotic guide for museum collections is being devel-oped.
The guide engages the museum?s visitors inspoken dialogues, and it describes the exhibits thatthe visitors select by generating textual descriptions,which are passed on to a speech synthesizer.
Thetexts are generated from logical facts stored in an on-tology (Galanis et al, 2009) and from canned texts;the latter are used when the corresponding informa-tion is difficult to encode in symbolic form (e.g., tostore short stories about the exhibits).
The descrip-tions of the exhibits are tailored depending on thetype of the visitor (e.g., child vs. adult), and an im-portant tailoring aspect is the generation of shorteror longer descriptions.
The parts of the descrip-tions that are generated from logical facts can beeasily made shorter or longer, by conveying feweror more facts.
The methods of this paper are usedto automatically shorten the parts of the descrip-tions that are canned texts, instead of requiring mul-tiple (shorter and longer) hand-written versions ofthe canned texts.AcknowledgementsThis work was carried out in INDIGO, an FP6 ISTproject funded by the European Union, with addi-tional funding provided by the Greek General Sec-retariat of Research and Technology.77Consult http://www.ics.forth.gr/indigo/.892ReferencesA.L.
Berger, S.A. Della Pietra, and V.J.
Della Pietra.2006.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71.D.
Bikel.
2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceedingsof the 2nd International Conference on Human Lan-guage Technology Research, pages 24?27.C.C Chang and C.J Lin.
2001.
LIBSVM: a library forSupport Vector Machines.
Technical report.
Softwareavailable at http://www.csie.ntu.edu.tw/?cjlin/libsvm.J.
Clarke and M. Lapata.
2006.
Models for sentencecompression: A comparison across domains, trainingrequirements and evaluation measures.
In Proceedingsof COLING, pages 377?384.J.
Clarke and M. Lapata.
2008.
Global inference forsentence compression: An integer linear programmingapproach.
Artificial Intelligence Research, 1(31):399?429.T.
Cohn and M. Lapata.
2007.
Large margin syn-chronous generation and its application to sentencecompression.
In Proceedings of EMNLP-CoNLL,pages 73?82.T.
Cohn and M. Lapata.
2008.
Sentence compressionbeyond word deletion.
In Proceedings of COLING,pages 137?144.T.
Cohn and M. Lapata.
2009.
Sentence compressionas tree to tree tranduction.
Artificial Intelligence Re-search, 34:637?674.M.
Collins and T. Koo.
2005.
Discriminative rerankingfor natural language parsing.
Computational Linguis-tics, 31(1):25?69.J.
Cordeiro, G. Dias, and P. Brazdil.
2009.
Unsupervisedinduction of sentence compression rules.
In Proceed-ings of the ACL Workshop on Language Generationand Summarisation, pages 391?399.S.
Corston-Oliver.
2001.
Text compaction for displayon very small screens.
In Proceedings of the NAACLWorkshop on Automatic Summarization, pages 89?98.M.C.
de Marneffe, B. MacCartney, and C. Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of LREC,pages 449?454.Dimitrios Galanis, George Karakatsiotis, GerasimosLampouras, and Ion Androutsopoulos.
2009.
Anopen-source natural language generator for OWL on-tologies and its use in protege and second life.
In Pro-ceedings of the Demonstrations Session at EACL 2009,pages 17?20, Athens, Greece, April.
Association forComputational Linguistics.S.
Gupta, A. Nenkova, and D. Jurafsky.
2007.
Measur-ing importance and query relevance in topic-focusedmulti-document summarization.
In Proceedings ofACL, pages 193?196.H.
Jing.
2000.
Sentence reduction for automatic textsummarization.
In Proceedings of ANLP, pages 310?315.K.
Knight and D. Marcu.
2002.
Summarization beyondsentence extraction: A probalistic approach to sen-tence compression.
Artificial Intelligence, 139(1):91?107.C.W.
Lin and E. Hovy.
2000.
The automated acqui-sition of topic signatures for text summarization.
InProceedings of ACL, pages 495?501.N.
Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.2007.
Multiple alternative sentence compressionsfor automatic text summarization.
In Proceedings ofDUC.C.
D. Manning, D. Klein, and C. Manning.
2003.
Op-timization, maxent models, and conditional estimationwithout magic.
In tutorial notes of HLT-NAACL 2003and ACL 2003.R.
McDonald.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proceedings ofEACL, pages 297?304.T.
Nomoto.
2009.
A comparison of model free versusmodel intensive approaches to sentence compression.In Proceedings of EMNLP, pages 391?399.D.
Paiva and R. Evans.
2005.
Empirically-based con-trol of natural language generation.
In Proceedings ofACL.J.
R. Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proceedings of the International Con-ference on Spoken Language Processing, pages 901?904.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2005.
Support vector machine learning for indepen-dent and structured output spaces.
Machine LearningResearch, 6:1453?1484.V.
Vandeghinste and Y. Pan.
2004.
Sentence compres-sion for automated subtitling: A hybrid approach.
InProceedings of the ACL Workshop ?Text Summariza-tion Branches Out?, pages 89?95.893
