Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136?144,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsCONE: Metrics for Automatic Evaluation of Named EntityCo-reference ResolutionBo Lin, Rushin Shah, Robert Frederking, Anatole GershmanLanguage Technologies Institute, School of Computer ScienceCarnegie Mellon University5000 Forbes Ave., PA 15213, USA{bolin,rnshah,ref,anatoleg}@cs.cmu.eduAbstractHuman annotation for Co-reference Resolu-tion (CRR) is labor intensive and costly, andonly a handful of annotated corpora are cur-rently available.
However, corpora withNamed Entity (NE) annotations are widelyavailable.
Also, unlike current CRR systems,state-of-the-art NER systems have very highaccuracy and can generate NE labels that arevery close to the gold standard for unlabeledcorpora.
We propose a new set of metrics col-lectively called CONE for Named Entity Co-reference Resolution (NE-CRR) that use asubset of gold standard annotations, with theadvantage that this subset can be easily ap-proximated using NE labels when gold stan-dard CRR annotations are absent.
We defineCONE B3 and CONE CEAF metrics based onthe traditional B3 and CEAF metrics and showthat CONE B3 and CONE CEAF scores of anyCRR system on any dataset are highly corre-lated with its B3 and CEAF scores respectively.We obtain correlation factors greater than 0.6for all CRR systems across all datasets, and abest-case correlation factor of 0.8.
We alsopresent a baseline method to estimate the goldstandard required by CONE metrics, and showthat CONE B3 and CONE CEAF scores usingthis estimated gold standard are also correlatedwith B3 and CEAF scores respectively.
Wethus demonstrate the suitability of CONEB3and CONE CEAF for automatic evaluationof NE-CRR.1 IntroductionCo-reference resolution (CRR) is the problem ofdetermining whether two entity mentions in atext refer to the same entity in real world or not.Noun Phrase CRR (NP-CRR) considers all nounphrases as entities, while Named Entity CRRrestricts itself to noun phrases that describe aNamed Entity.
In this paper, we consider the taskof Named Entity CRR (NE-CRR) only.
Most, ifnot all, recent efforts in the field of CRR haveconcentrated on machine-learning based ap-proaches.
Many of them formulate the problemas a pair-wise binary classification task, in whichpossible co-reference between every pair of men-tions is considered, and produce chains of co-referring mentions for each entity as their output.One of the most important problems in CRR isthe evaluation of CRR results.
Different evalua-tion metrics have been proposed for this task.
B-cubed (Bagga and Baldwin, 1998) and CEAF(Luo, 2005) are the two most popular metrics;they compute Precision, Recall and F1 measurebetween matched equivalent classes and useweighted sums of Precision, Recall and F1 toproduce a global score.
Like all metrics, B3 andCEAF require gold standard annotations; howev-er, gold standard CRR annotations are scarce,because producing such annotations involves asubstantial amount of human effort since it re-quires an in-depth knowledge of linguistics and ahigh level of understanding of the particular text.Consequently, very few corpora with gold stan-dard CRR annotations are available (NIST, 2003;MUC-6, 1995; Agirre, 2007).
By contrast, goldstandard Named Entity (NE) annotations are easyto produce; indeed, there are many NE annotatedcorpora of different sizes and genres.
Similarly,there are few CRR systems and even the bestscores obtained by them are only in the region ofF1 = 0.5 - 0.6.
There are only four such CRRsystems freely available, to the best of our know-ledge (Bengston and Roth, 2007; Versley et al,2008; Baldridge and Torton, 2004; Baldwin andCarpenter, 2003).
In comparison, there are nu-merous Named Entity recognition (NER) sys-tems, both general-purpose and specialized, andmany of them achieve scores better than F1 =0.95 (Ratinov and Roth, 2009; Finkel et al,1362005).
Although these facts can be partly attri-buted to the ?hardness?
of CRR compared toNER, they also reflect the substantial gap be-tween NER and CRR research.
In this paper, wepresent a set of metrics, collectively calledCONE, that leverage widely available NER sys-tems and resources and tools for the task of eva-luating co-reference resolution systems.
The ba-sic idea behind CONE is to predict a CRR sys-tem?s performance for the task of full NE-CRRon some dataset using its performance for thesubtask of named mentions extraction and group-ing (NMEG) on that dataset.
The advantage ofdoing so is that measuring NE-CRR performancerequires the co-reference information of all men-tions of a Named Entity, including named men-tions, nominal and pronominal references, whilemeasuring the NMEG performance only requiresco-reference information of named mentions of aNE, and this information is relatively easy to ob-tain automatically even in the absence of goldstandard annotations.
We compute correlationbetween CONE B3, B3, CONE CEAF and CEAFscores for various CRR systems on various gold-standard annotated datasets and show that theCONE B3 and B3 scores are highly correlated forall such combinations of CRR systems and data-sets, as are CONE CEAF and CEAF scores, witha best-case correlation of 0.8.
We produce esti-mated gold standard annotations for the Enronemail corpus, since no actual gold standard CRRannotations exist for it, and then use CONE B3and CONE CEAF with these estimated goldstandard annotations to compare the performanceof various NE-CRR systems on this corpus.
Nosuch comparison has been previously performedfor the Enron corpus.We adopt the same terminology as in (Luo,2005): a mention refers to each individual phraseand an entity refers to the equivalence class orco-reference chain with several mentions.
Thisallows us to note some differences between NE-CRR and NP-CRR.
NE-CRR involves indentify-ing named entities and extracting their co-referring mentions; equivalences classes withoutany NEs are not considered.
NE-CRR is thusclearly a subset of NP-CRR, where all co-referring mentions and equivalence classes areconsidered.
However, we focus on NE-CRR be-cause it is currently a more active research areathan NP-CRR and a better fit for target applica-tions such as text forensics and web mining, andalso because it is more amenable to the automaticevaluation approach that we propose.The research questions that motivate our workare:(1) Is it possible to use only NER resources toevaluate NE-CRR systems?
If so, how is thisproblem formulated?
(2) How does one perform evaluation in a waythat is accurate and automatic with least hu-man intervention?
(3) How does one perform evaluation on largeunlabeled datasets?We show that our CONE metrics achieve goodresults and represent a promising first step to-ward answering these questions.The rest of the paper is organized as follows.
Wepresent related work in the field of automaticevaluation methods for natural languageprocessing tasks in Section 2.
In Section 3, wegive an overview of the standard metrics current-ly used for evaluating co-reference resolution.We define our new metrics CONE B3 and CONECEAF in Section 4.
In section 5, we provide ex-perimental results that illustrate the performanceof CONE B3 and CONE CEAF compared to B3and CEAF respectively.
In Section 6, we give anexample of the application of CONE metrics byevaluating NE-CRR systems on an unlabeleddataset, and discuss possible drawbacks and ex-tensions of these metrics.
Finally, in section 7 wepresent our conclusions and ideas for futurework.2 Related WorkThere has been a substantial amount of researchdevoted to automatic evaluation for natural lan-guage processing, especially tasks involving lan-guage generation.
The BLEU score (Papineni etal., 2002) proposed for evaluating machine trans-lation results is the best known example of this.It uses n-gram statistics between machine gener-ated results and references.
It inspired theROUGE metric (Lin and Hovy, 2003) and othermethods (Louis and Nenkova, 2009) to performautomatic evaluation of text summarization.
Boththese metrics have show strong correlation be-tween automatic evaluation results and humanjudgments.
The two metrics successfully reducethe need for human judgment and help speed upresearch by allowing large-scale evaluation.Another example is the alignment entropy (Per-vouchine et al, 2009) for evaluating translitera-tion alignment.
It reduces the need for alignmentgold standard and highly correlates with transli-teration system performance.
Thus it is able to137serve as a good metric for transliteration align-ment.
We contrast our work with (Stoyanov et al,2009), who show that the co-reference resolutionproblem can be separated into different parts ac-cording to the type of the mention.
Some partsare relatively easy to solve.
The resolver per-forms equally well in each part across datasets.They use the statistics of mentions in differentparts with test results on other datasets as a pre-dictor for unseen datasets, and obtain promisingresults with good correlations.
We approach theproblem from a different perspective.
In ourwork, we show the correlation between thescores on traditional metrics and scores on ourCONE metrics, and show how to automaticallyestimate the gold standard required by CONEmetrics.
Thus our method is able to predict theco-reference resolution performance withoutgold standard at all.
We base our new metrics onthe standard B3 and CEAF metrics used for com-puting CRR scores.
(Vilian et al, 1995; Baggaand Baldwin, 1998; Luo, 2005).
B3 and CEAFare believed to be more discriminative and inter-pretable than earlier metrics and are widelyadopted especially for machine-learning basedapproaches.3 Standard Metrics: B3 and CEAFWe now provide an overview of the standard B3and CEAF metrics used to evaluate CRR sys-tems.
Both metrics assume that a CRR systemproduces a set of equivalence classes {O} andassigns each mention to only one class.
Let Oi bethe class to which the ith mention was assignedby the system.
We also assume that we have a setof correct equivalence classes {G} (the goldstandard).
Let Gi be the gold standard class towhich the ith mention should belong.
Let Ni de-note the number of mentions in Oi which are alsoin Gi ?
the correct mentions.
B3 computes thepresence rate of correct mentions in the sameequivalent classes.
The individual precision andrecall score is defined as follows:|| iii ONP ?|| iii GNR ?Here |Oi| and |Gi| are the cardinalities of sets Oiand Gi.The final precision and recall scores are:??
?niii PwP1??
?niii RwR1Here, in the simplest case the weight wi is set to1/n, equal for all mentions.CEAF (Luo, 2005) produces the optimalmatching between output classes and true classesfirst, with the constraint that one true class, Gi,can be mapped to at most one output class, sayOf(i) and vice versa.
This can be solved by theKM algorithm (Kuhn, 1955; Munkres, 1957) formaximum matching in a bipartite graph.
CEAFthen computes the precision and recall score asfollows:???iiiifiOMP)(,??
?iiiifiGMR)(,jiji GOM ?
?,We use the terms Mi,j from CEAF to re-write B3,its formulas then reduce to:????
i j ijiii OMOP2,1????
i j ijiii GMGR2,1We can see that B3 simply iterates through allpairs of matchings instead of considering the oneto one mappings as CEAF does.
Thus, B3 com-putes the weighted sum of the F-measures foreach individual mention which helps alleviate thebias in the pure link-based F-measure, whileCEAF computes the same as B3 but enforces atmost one matched equivalence class for everyclass in the system output and gold standard out-put.4 CONE B3 and CONE CEAF Metrics:We now formally define the new CONE B3 andCONE CEAF metrics that we propose forautomatic evaluation of NE-CRR systems.Let G denote the set of gold standardannotations and O denote the output of an NE-CRR system.
Let Gi denote the equivalent classof entity i in the gold standard and Oj denote theequivalence class for entity j in the system output.Also let Gij denote the jth mention in theequivalence class of entity i in the gold standardand Oij denote the jth mention in the systemoutput.As described earlier, the standard B3 and CEAFmetrics evaluate scores using G and O and canbe thought of as functions of the form B3(G, O).and CEAF(G, O) respectively.
Let us useScore(G, O) to collectively refer to both these138functions.
An equivalence class Gi in G maycontain three types of mentions: named mentionsgNMij, nominal mentions gNOij, and pronominalmentions gPRij.
Similarly, we can define oNMij,oNOij and oPRij for a class Oi in O.
Now for eachgold standard equivalence class Gi and systemoutput equivalence class Oi, we define thefollowing sets GNMi  and  ONMi:iijNMijNMiNM GggGi ???
},{,iijNMijNMiNM OooOi ???
},{,In other words, GNMi and ONMi are the subsets ofGi and Oi containing all named mentions and nomentions of any other type.Let GNM denote the set of all such equivalanceclasses GNMi and ONM denote the set of allequivalence classes ONMi.
It is clear that GNM andONM are pruned versions of the gold standardannotations and system output respectively.We now define CONE B3 and CONE CEAF asfollows:CONE B3 = B3(GNM, ONM)CONE CEAF = CEAF(GNM, ONM)Following our previous notation, we denoteCONE B3 and CONE CEAF collectively asScore(GNM, ONM).
We observe that Score(GNM,ONM) measures a NE-CRR system?sperformance for the NE-CRR subtask of namedmentions extraction and grouping (NMEG).
Wefind that Score(GNM, ONM) is highly correlatedwith Score(G, O) for all the freely available NE-CRR systems over various datasets.
Thisprovides the neccessary  justification for the useof Score(GNM, ONM).We use SYNERGY (Shah et al, 2010), anensemble NER system that combines the UIUCNER (Ritanov and Roth, 2009) and StanfordNER (Finkel et al, 2005) systems, to produceGNM and ONM from G and O by  selecting namedmentions.
However, any other good NER systemwould serve the same purpose.We see that while standard evaluation metricsrequire the use of G, i.e.
the full set of NE-CRRgold standard annotations including named,nominal and pronimal mentions, CONE metricsrequire only GNM, i.e.
gold standard annotationsconsisting of named mentions only.
The keyadvantage of using CONE metrics is that GNMcan be automatically approximated using anNER system with a good degree of accuracy.This is because state-of-the-art NER systemsachieve near-optimal performance, exceeding F1= 0.95 in many cases, and after obtaining theiroutput, the task of estimating GNM reduces tosimply clustering it to seperate mentions ofdiffrerent real-world entities.
This clustering canbe thought of as a form of named entity matching,which is not a very hard problem.
There existsystems that perform such matching in asophisticated manner with a high degree ofaccuracy.
We use simple heuristics such as exactmatching, word matches, matches between in-itials, etc.
to design such a matching systemourselves and use it to obtain estimates of GNM,say GNM-approx.
We then calculate CONE B3 andCONE CEAF scores using GNM-approx instead ofGNM; in other words, we perform fully automaticevaluation of NE-CRR systems by usingScore(GNM-approx, ONM) instead of Score(GNM,ONM).
In order to show the validity of thisevaluation, we calculate the correlation betweenthe Score(GNM-approx, ONM) and Score(G, O) fordifferent NE-CRR systems across differentdatasets and find that they are indeed correlated.CONE thus makes automatic evaluation of NE-CRR systems possible.
By leveraging the widelyavailable named entity resources, it reduces theneed for gold standard annotations in theevaluation process.4.1 AnalysisThere are two major kinds of errors that affectthe performance of NE-CRR systems for the fullNE-CRR task:?
Missing Named Entity (MNE): If a namedmention is missing from the system output,it is very likely that its nearby nominal andanaphoric mentions will be lost, too?
Incorrectly grouped Named Entity (IGNE):Even if the named mention is correctly iden-tified with its nearby nominal and anaphoricmentions to form a chain, it is still possibleto misclassify the named mentions and itsco-reference chainConsider the following example of these twotypes of errors.
Here, the alphabets represent thenamed mentions and numbers represent othertype of mentions:Gold standard, G: (A, B, C, 1, 2, 3, 4)Output from System 1, O1: (A, B, 1, 2, 3)Output from System 2, O2: (A, C, 1, 2, 4), (B, 3)O1 shows an example of an MNE error, whileO2 shows an example of an IGNE error.Both these types of errors are in fact rooted innamed mention extraction and grouping(NMEG).
Therefore, we hypothesize that theymust be preserved in a NE-CRR system?s output139for the subtask of named mentions extraction andgrouping (NMEG) and will be reflected in theCONE B3 and CONE CEAF metrics that eva-luate scores for this subtask.
Consider the follow-ing extension of the previous example:GNM: (A, B, C)O1NM: (A, B)O2NM: (A, C), (B)We observe that the MNE error in O1 is pre-served in O1NM, and the IGNE error in O2 is pre-served in O2NM.
Empirically we sample severaloutput files in our experiments and observe thesame phenomena.
Therefore, we argue that it ispossible to capture the two major kinds of errorsdescribed by considering only GNM and ONM in-stead of G and O.We now provide a more detailed theoreticalanalysis of the CONE metrics.
For a given NE-CRR system and dataset, consider the systemoutput O and gold standard annotation G. Let Pand R indicate precision and recall scores ob-tained by evaluating O against G, using CEAF.
Ifwe replace both G and O with their subsets GNMand ONM respectively, such that GNM and ONMcontain only named mentions, we can modify theequations for precision and recall for CEAF toderive the following equations for precision PNMand recall RNM for CONE CEAF:?
?iiNMOOSum NM }{?
?iiNMNM GGSum }{?
?iNMifiNMNMOSumMP }{)(,?
?iNMifiNMNMGSumMR }{)(,The corresponding equations for CONE B3 Pre-cision are:????iNMiNMjjiNMNMOSumOMP}{2,???
?iNMiNMjjiNMRSumRMR}{'2,In order to support the hypothesis that CONEmetrics evaluated using (GNM, ONM) represent aneffective substitute for standard metrics that use(G, O), we compute entity level correlation be-tween the corresponding CONE and standardmetrics.
For example, in the case of CEAF /CONE CEAF Precision, we calculate correlationbetween the following quantities:???
}{)(,NMifiNMNMSSumMP?and???
}{)(,SSumMP ifi?We perform this experiment with the LBJ andBART CRR systems on the ACE Phase 2 corpus.We illustrate the correlation results in Figure 1.Figure 1.
Correlation between NMP?
andP?
-Entity Level CEAF PrecisionFrom Figure 1, we can see that the twomeasures are highly correlated.
In fact, we findthat the Pearson?s correlation coefficient (Soperet al, 1917; Cohen, 1988) is 0.73.
The pointslining up on the x-axis and y=1.0 represent verysmall equivalence classes and are a form of noise;their removal doesn?t affect this coefficient.
Toshow that this strong correlation is not astatistical anomaly, we also compute entity-levelcorrelation using (Gi - GNMi, Oj - ONMj) and (Gi,Oj) instead of (GNMi, ONMj) and (Gi, Oj) and findthat the coefficient drops to 0.03, which isobviously not correlated at all.We now know NMP?
andP?
are highly correlated.Assume the correlation is linear, with thefollowing equation:??
??
iNMi PPwhere ?
and ?
are the linear regressionparameters.Thus?
?
????
nPnPPP NMiiNMi i?????
?
?Here, n is the number of equivalence classes.We conclude that the overall CEAF Precisionand CONE CEAF Precision should be highly140correlated too.
We repeat this experiment withCEAF / CONE CEAF Recall, B3 / CONE B3Precision and B3 / CONE B3 Recall and obtainsimilar results, allowing us to conclude that thesesets of measures should also be highly correlated.We note here some generally acceptedterminology regarding correlation: If twoquantities have a Pearson?s correlationcoefficient greater than 0.7, they are considered"strongly correlated", if their correlation isbetween 0.5 and 0.7, they are considered "highlycorrelated", if it is between 0.3 and 0.5, they areconsidered "correlated", and otherwise they areconsidered "not correlated".It is important to note that like all automaticevaluation metrics, CONE B3 and CONE CEAFtoo can be easily ?cheated?, e.g.
a NE-CRR sys-tem that performs NER and named entity match-ing well but does not even detect and classifyanaphora or nominal mentions would nonethe-less score highly on these metrics.
A possiblesolution to this problem would be to create goldstandard annotations for a small subset of thedata, call these annotations G?, and report twoscores: B3 / CEAF (G?
), and CONE B3 / CONECEAF (GNM-approx).
Discrepancies between thesetwo scores would enable the detection of such?cheating?.
A related point is that designers ofNE-CRR systems should not optimize for CONEmetrics alone, since by using GNM-approx (or GNMwhere gold standard annotations are available),these metrics are obviously biased towardsnamed mentions.
This issue can also be ad-dressed by having gold standard annotations G?for a small subset.
One could then train a systemby optimizing both B3 / CEAF (G?)
and CONEB3 / CONE CEAF (GNM-approx).
This can bethought of as a form of semi-supervised learning,and may be useful in areas such as domain adap-tation, where we could use some annotated test-set in a standard domain, e.g.
newswire as thesmaller set and an unlabeled large testset fromsome other domain, such as e-mail or biomedicaldocuments.
An interesting future direction is tomonitor the effectiveness of our metrics overtime.
As co-reference resolution systems evolvein strength, our metrics might be less effective,however this could be a good indicator to discri-minate on different subtasks the improvementsgained by the co-reference resolution systems.5 Experimental ResultsWe present experimental results in support of thevalidity and effectiveness of CONE metrics.
Asmentioned earlier, we used the following fourpublicly available CRR systems: UIUC?s LBJsystem (L), BART from JHU Summer Workshop(B), LingPipe from Alias-i (LP), and OpenNLP(OP) (Bengston and Roth, 2007; Versley et al,2008; Baldridge and Torton, 2004; Baldwin andCarpenter, 2003).
All these CRR systems per-form Noun Phrase co-reference resolution (NP-CRR), not NE-CRR.
So, we must first eliminateall equivalences classes that do not contain anynamed mentions.
We do so using the SYNERGYNER system to separate named mentions fromunnamed ones.
Note that this must not be con-fused with the use of SYNERGY to produce GNMand ONM from G and O respectively.
For that task,all equivalence classes in G and O already con-tain at least one named mention and we removeall unnamed mentions from each class.
Thisprocess effectively converts the NP-CRR resultsof these systems into NE-CRR ones.
We use theACE Phase 2 NWIRE and ACE 2005 Englishdatasets.
We avoid using the ACE 2004 andMUC6 datasets because the UIUC LBJ systemwas trained on ACE 2004 (Bengston and Roth,2008), while BART and LingPipe were trainedon MUC6.
There are 29 files in the test set ofACE Phrase 2 and 81 files in ACE 2005, sum-ming up to 120 files with around 50,000 tokenswith 5000 valid co-reference mentions.
Tables 1and 2 show the Pearson?s correlation coefficientsbetween CONE metric scores of the typeScore(GNM, ONM) and standard metric scores ofthe type Score(G, O) for combinations of variousCRR systems and datasets.B3/CONE B3  CEAF/CONE CEAFP R F1 P R F1L 0.82 0.71 0.7 0.81 0.71 0.77B 0.85 0.5 0.66 0.71 0.61 0.68LP 0.84 0.66 0.67 0.74 0.71 0.73OP 0.31 0.57 0.61 0.79 0.72 0.79Table 1.
GNM: Correlation on ACE Phase 2B3/CONE B3  CEAF/CONE CEAFP R F1 P R F1L 0.6 0.62 0.62 0.75 0.61 0.68B 0.74 0.82 0.84 0.72 0.68 0.67LP 0.91 0.65 0.73 0.44 0.57 0.53OP 0.48 0.77 0.8 0.54 0.67 0.65Table 2.
GNM: Correlation on ACE 2005We observe from Tables 1 and 2 that CONE B3and CONE CEAF scores are highly correlated141with B3 and CEAF scores respectively, and thisholds true for Precision, Recall and F1 scores, forall combinations of CRR systems and datasets.This justifies our assumption that a system?s per-formance for the subtask of NMEG is a goodpredictor of its performance for the full task ofNE-CRR.
These correlation coefficients aregraphically illustrated in Figures 2 and 3.We now use our baseline named entity matchingmethod to automatically generate estimated goldstandard annotations GNM-approx and recalculateCONE CEAF and CONE B3 scores using GNM-approx instead of GNM.
Tables 3 and 4 show thecorrelation coefficients between the new CONEscores and the standard metric scores.B3/CONE B3  CEAF/CONE CEAFP R F1 P R F1L 0.31 0.23 0.22 0.33 0.55 0.56B 0.71 0.44 0.43 0.61 0.63 0.71LP 0.57 0.43 0.49 0.36 0.25 0.31OP 0.1 0.6 0.64 0.35 0.53 0.53Table 3.
GNM-approx: Correlation on ACE Phase 2B3/CONE B3  CEAF/CONE CEAFP R F1 P R F1L 0.33 0.32 0.42 0.22 0.34 0.36B 0.25 0.66 0.65 0.2 0.45 0.37LP 0.19 0.33 0.34 0.77 0.68 0.72OP 0.26 0.66 0.67 0.28 0.42 0.38Table 4.
GNM-approx: Correlation on ACE Phase 2We observe from Tables 3 and 4 that these corre-lation factors are encouraging, but not as good asthose in Tables 1 and 2.
All the correspondingCONE B3 and CONE CEAF scores are corre-lated, but very few are highly correlated.
Weshould note however that our baseline system tocreate GNM-approx uses relatively simple clusteringmethods and heuristics.
It is easy to observe thata sophisticated named entity matching systemwould produce a GNM-approx that better approx-imates GNM than our baseline method, and CONEB3 and CONE CEAF scores calculated using thisGNM-approx would be more correlated with stan-dard B3 and CEAF scores.We note from the above results that correlationsscores are very similar across different systemsand datasets.
In order to formalize this assertion,we calculate correlation scores in a system-independent and data-independent manner.
Wecombine all the data points across all four differ-ent systems and plot them in Figure 2 and 3 forACE Phase 2 NWIRE corpus and in Figure 4 and5 for ACE 2005 corpus respectively.
We illu-strate only F1 scores; the results for precisionand recall are similar.Figure 2.
Correlation between B3 F1 and CONEB3 F1 for all systems on ACE 2Figure 3.
Correlation between CEAF F1 andCONE CEAF F1 for all systems on ACE 2Figure 2 reflects a Pearson?s correlation coeffi-cient of 0.70, suggesting that all the B3 F1 andCONE B3 F1 scores for different systems arehighly correlated and that CONE B3 F1 does notbias towards any particular system.
Figure 3 re-flects a Pearson?s correlation coefficient of 0.83,providing similar evidence for the system-independence of correlation between CEAF F1and CONE CEAF F1 scores.
Figures 4 and 5corresponding to ACE 2005 reflect similar corre-lation coefficients of 0.89 and 0.82, and thussupport the idea that the correlations between B3F1 and CONE B3 F1, as well as between CEAFF1and CONE CEAF F1, are dataset-independentin addition to being system-independent.142Figure 4.
Correlation between B3 F1 and CONEB3 F1 for all systems on ACE 2005Figure 5.
Correlation between CEAF F1 andCONE CEAF F1 for all systems on ACE 20056 Application and DiscussionTo illustrate the applicability of CONE metrics,we consider the Enron e-mail corpus.
It is of adifferent genre than the newswire corpora thatCRR systems are usually trained on, and no CRRgold standard annotations exist for it.
Conse-quently, no CRR systems have been evaluated onit so far.
We used CONE B3 and CONE CEAF toevaluate and compare the NE-CRR performanceof various CRR systems on a subset of the Enrone-mail corpus (Klimt and Yang, 2004) that wascleaned and stripped of spam messages.
We re-port the results in Table 5.CONE B3  CONE CEAFP R F1 P R F1L 0.43 0.21 0.23 0.31 0.17 0.21B 0.26 0.18 0.2 0.26 0.16 0.2LP 0.61 0.51 0.53 0.58 0.53 0.54OP 0.19 0.03 0.05 0.11 0.02 0.04Table 5.
GNM-approx Scores on Enron corpusWe find that LingPipe is the best of all the sys-tems we considered, and LBJ is slightly ahead ofBART in all measures.
We suspect that sinceLingPipe is a commercial system, it may haveextra training resources in the form of non-traditional corpora.
Nevertheless, we believe ourmethod is robust and scalable for large corporawithout NE-CRR gold standard annotations.7 Conclusion and Future WorkWe propose the CONE B3 and CONE CEAF me-trics for automatic evaluation of Named EntityCo-reference Resolution (NE-CRR).
These me-trics measures a NE-CRR system?s performanceon the subtask of named mentions extraction andgrouping (NMEG) and use it to estimate the sys-tem?s performance on the full task of NE-CRR.We show that CONE B3 and CONE CEAFscores of various systems across different data-sets are strongly correlated with their standard B3and CEAF scores respectively.
The advantage ofCONE metrics compared to standard ones is thatinstead of the full gold standard data G, they onlyrequire a subset GNM of named mentions whicheven if not available can be closely approximatedby using a state-of-the-art NER system and clus-tering its results.
Although we use a simple base-line algorithm for producing the approximategold standard GNM-approx, CONE B3 and CONECEAF scores of various systems obtained usingthis GNM-approx still prove to be correlated withtheir standard B3 and CEAF scores obtained us-ing the full gold standard G. CONE metrics thusreduce the need of expensive labeled corpora.We use CONE B3 and CONE CEAF to evaluatethe NE-CRR performance of various CRR sys-tems on a subset of the Enron email corpus, forwhich no gold standard annotations exist and nosuch evaluations have been performed so far.
Inthe future, we intend to use more sophisticatednamed entity matching schemes to produce betterapproximate gold standards GNM-approx.
We alsointend to use the CONE metrics to evaluate NE-CRR systems on new datasets in domains such aschat, email, biomedical literature, etc.
where veryfew corpora with gold standard annotations exist.AcknowledgmentsWe would like to thank Prof. Ani Nenkova fromthe University of Pennsylvania for her talk aboutautomatic evaluation for text summarization atthe spring 2010 CMU LTI Colloquium and ano-nymous reviewers for insightful comments.143ReferencesE.
Agirre, L. M?rquez and R. Wicentowski, Eds.2007.
Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval).A.
Bagga and B. Baldwin.
1998.
Algorithms for Scor-ing Coreference Chains.
Proceedings of LRECWorkshop on Linguistic Coreference.J.
Baldridge and T. Morton.
2004.
OpenNLP.http://opennlp.sourceforge.net/.B.
Baldwin and B. Carpenter.
2003.
LingPipe.
Alias-i.E.
Bengtson and D. Roth.
2008.
Understanding theValue of Features for Coreference Resolution.
Pro-ceedings of EMNLP.J.
Cohen.
1988.
Statistical power analysis for the be-havioral sciences.
(2nd ed.)A.K.
Elmagarmid, P.G.
Ipeirotis and V.S.
Verykios.2007.
Duplicate Record Detection: A Survey.
IEEETransactions on Knowledge and Data Engineering,v.19 n.1, 2007.J.R.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating Non-local Information into Informa-tion Extraction Systems by Gibbs Sampling.
Pro-ceedings of ACL.B.
Klimt and Y. Yang.
2004.
The Enron corpus: Anew dataset for email classification research.
Pro-ceedings of ECML.H.W.
Kuhn.
1955.
The Hungarian method for theassignment problem.
Naval Research LogisticsQuarterly, 2(83).C.
Lin and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.Proceedings of HLT-NAACL.C.
Lin and F.J. Och.
2004.
Automatic evaluation ofmachine translation quality using longest commonsubsequence and skip-bigram statistics.
Proceed-ings of ACL.A.
Louis and A. Nenkova.
2009.
Automatically Eva-luating Content Selection in Summarization with-out Human Models.
Proceedings of EMNLP, pages306?314, Singapore, 6-7 August 2009.X.
Luo.
2005.
On coreference resolution performancemetrics.
Proceedings of EMNLP.MUC-6.
1995.
Proceedings of the Sixth Understand-ing Conference (MUC-6).J.
Munkres.
1957.
Algorithms for the assignment andtransportation problems.
Journal of SIAM, 5:32-38.NIST.
2003.
The ACE evaluation plan.www.nist.gov/speech/tests/ace/index.htm.K Papineni, S Roukos, T Ward and W.J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
Proceedings of ACL.V.
Pervouchine, H. Li and B. Lin.
2009.
Translitera-tion alignment.
Proceedings of ACL.L.
Ratinov and D. Roth.
2009.
Design Challenges andMisconceptions in Named Entity Recognition.Proceedings of CoNLL.R.
Shah, B. Lin, A. Gershman and R. Frederking.2010.
SYNERGY: a named entity recognition sys-tem for resource-scarce languages such as Swahiliusing online machine translation.
Proceedings ofLREC Workshop on African Language Technology.H.E.
Soper, A.W.
Young, B.M.
Cave, A. Lee and K.Pearson.
1917.
On the distribution of the correla-tion coefficient in small samples.
Appendix II tothe papers of "Student" and R. A. Fisher.
A co-operative study.
Biometrika, 11, 328-413.V.
Stoyanov, N. Gilbert, C. Cardie and E. Riloff.2009.
Conundrums in Noun Phrase CoreferenceResolution: Making Sense of the State-of-the-Art.Proceedings of ACL.Y.
Versley, S.P.
Ponzetto, M. Poesio, V. Eidelman, A.Jern, J. Smith, X. Yang and A. Moschitti.
2008.BART: A Modular Toolkit for Coreference Reso-lution.
Proceedings of EMNLP.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly and L.Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
Proceedings of MUC 6.144
